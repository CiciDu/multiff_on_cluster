{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## thoughts: could you use neural data to decode target position?\n",
    "\n",
    "what about 2nd target's position?\n",
    "(can either use 1st target's decoder, or train and separate decoder for 2nd target)\n",
    "\n",
    "also...try GPFA at some point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed the directory to 'Multifirefly-Project'.\n",
      "Added /Users/dusiyi/Documents/Multifirefly-Project/multiff_analysis/multiff_code/methods to the path.\n",
      "done\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "import os, sys\n",
    "for p in [Path.cwd()] + list(Path.cwd().parents):\n",
    "    if p.name == 'Multifirefly-Project':\n",
    "        os.chdir(p)\n",
    "        sys.path.insert(0, str(p / 'multiff_analysis/multiff_code/methods'))\n",
    "        break\n",
    "\n",
    "\n",
    "import sys\n",
    "from data_wrangling import specific_utils, process_monkey_information, specific_utils, process_monkey_information, time_calib_utils, retrieve_raw_data, general_utils, time_calib_class, time_calib_utils\n",
    "from pattern_discovery import pattern_by_trials, pattern_by_trials, cluster_analysis, organize_patterns_and_features, monkey_landing_in_ff\n",
    "from visualization.matplotlib_tools import plot_behaviors_utils\n",
    "from planning_analysis.show_planning import nxt_ff_utils\n",
    "from neural_data_analysis.neural_analysis_tools.get_neural_data import neural_data_processing\n",
    "from neural_data_analysis.neural_analysis_tools.visualize_neural_data import plot_neural_data, plot_modeling_result\n",
    "from neural_data_analysis.neural_analysis_tools.model_neural_data import transform_vars, neural_data_modeling, drop_high_corr_vars, drop_high_vif_vars\n",
    "from neural_data_analysis.neural_analysis_tools.cca_methods import cca_class, cca_utils, cca_cv_utils\n",
    "from neural_data_analysis.neural_analysis_tools.cca_methods.cca_plotting import cca_plotting, cca_plot_lag_vs_no_lag, cca_plot_cv\n",
    "from neural_data_analysis.topic_based_neural_analysis.neural_vs_behavioral import prep_monkey_data, prep_target_data, neural_vs_behavioral_class\n",
    "from neural_data_analysis.topic_based_neural_analysis.planning_and_neural import planning_and_neural_class, pn_utils\n",
    "\n",
    "\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "import seaborn as sns\n",
    "from matplotlib import rc\n",
    "from os.path import exists\n",
    "from numpy import random\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from importlib import reload\n",
    "import neo\n",
    "import gc\n",
    "import rcca\n",
    "from statsmodels.multivariate.cancorr import CanCorr\n",
    "\n",
    "import sys\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import torch\n",
    "from numpy import pi\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import scipy.linalg as linalg\n",
    "import scipy.interpolate as interpolate\n",
    "from scipy.signal import fftconvolve\n",
    "\n",
    "\n",
    "from scipy.io import loadmat\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sys\n",
    "\n",
    "import scipy.sparse as sparse\n",
    "import matplotlib.pylab as plt\n",
    "import subprocess\n",
    "\n",
    "plt.rcParams[\"animation.html\"] = \"html5\"\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "rc('animation', html='jshtml')\n",
    "matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n",
    "matplotlib.rcParams['animation.embed_limit'] = 2**128\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "np.set_printoptions(suppress=True)\n",
    "print(\"done\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Schro/data_0328\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Schro/data_0404\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Schro/data_0410\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved monkey_information\n",
      "The number of points that were removed due to delta_position exceeding the ceiling is 0\n",
      "Warning: ff_closest_stop_time_sorted has 35 points where monkey is outside of 605 points that are outside of the reward boundary, which is 5.79% of the points. They are replaced with the original ff_caught_T in ff_caught_T_new.\n",
      "Warning: ff_caught_T_new is not monotonically increasing. Will make it monotonically increasing.\n",
      "Note: ff_caught_T_sorted is replaced with ff_caught_T_new\n",
      "Retrieved ff_dataframe from all_monkey_data/processed_data/monkey_Schro/data_0410/ff_dataframe.h5\n",
      "When take out monkey subset for GUAT, 271 clusters out of 374 are too close to the target or the last target. Those clusters are filtered out.\n",
      "The number of new trials that are used to separate stop clusters is 605\n",
      "Retrieved all_trial_patterns\n",
      "Retrieved pattern_frequencies\n",
      "Retrieved all_trial_features\n",
      "Retrieved feature_statistics\n",
      "Retrieved scatter_around_target_df\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dusiyi/Documents/Multifirefly-Project/multiff_analysis/multiff_code/methods/neural_data_analysis/topic_based_neural_analysis/neural_vs_behavioral/prep_monkey_data.py:52: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  monkey_info_in_bins = monkey_info_in_bins.bfill(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved target_df\n",
      "Retrieved target_cluster_df\n",
      "Window width changed from 0.25 to 0.26 to make it odd\n",
      "Loaded binned_spikes_df from all_monkey_data/processed_neural_data/monkey_Schro/data_0410/binned_spikes_df_0p02.csv\n"
     ]
    }
   ],
   "source": [
    "data_item = neural_vs_behavioral_class.NeuralVsBehavioralClass(raw_data_folder_path=raw_data_folder_path)\n",
    "data_item.streamline_preparing_neural_and_behavioral_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3319058519.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mstop!\u001b[39m\n        ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "stop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['bin', 'num_alive_ff', 'num_visible_ff', 'min_ff_distance',\n",
       "       'min_abs_ff_angle', 'min_abs_ff_angle_boundary',\n",
       "       'min_visible_ff_distance', 'min_abs_visible_ff_angle',\n",
       "       'min_abs_visible_ff_angle_boundary', 'catching_ff', 'any_ff_visible',\n",
       "       'LDy', 'LDz', 'RDy', 'RDz', 'gaze_mky_view_x', 'gaze_mky_view_y',\n",
       "       'gaze_world_x', 'gaze_world_y', 'speed', 'monkey_angle',\n",
       "       'ang_speed', 'ang_accel', 'accel', 'num_distinct_stops',\n",
       "       'num_caught_ff', 'stop_rate', 'stop_success_rate',\n",
       "       'avg_target_distance', 'avg_target_angle',\n",
       "       'avg_target_angle_to_boundary', 'avg_target_last_seen_time',\n",
       "       'avg_target_cluster_last_seen_time',\n",
       "       'avg_target_last_seen_distance_frozen',\n",
       "       'avg_target_last_seen_angle_frozen',\n",
       "       'avg_target_last_seen_angle_to_boundary_frozen',\n",
       "       'avg_target_cluster_last_seen_distance_frozen',\n",
       "       'avg_target_cluster_last_seen_angle_frozen',\n",
       "       'avg_target_cluster_last_seen_angle_to_boundary_frozen',\n",
       "       'min_target_has_disappeared_for_last_time_dummy',\n",
       "       'min_target_cluster_has_disappeared_for_last_time_dummy',\n",
       "       'max_target_visible_dummy', 'max_target_cluster_visible_dummy',\n",
       "       'try_a_few_times_indice_dummy', 'give_up_after_trying_indice_dummy',\n",
       "       'ignore_sudden_flash_indice_dummy', 'two_in_a_row',\n",
       "       'visible_before_last_one', 'disappear_latest', 'ignore_sudden_flash',\n",
       "       'try_a_few_times', 'give_up_after_trying', 'cluster_around_target',\n",
       "       'waste_cluster_around_target'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_item.y_var.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_item.monkey_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_signal_output, marker_list, smr_sampling_rate = retrieve_raw_data.extract_smr_data(raw_data_folder_path)\n",
    "signal_df = None\n",
    "# Considering the first smr file, using channel_signal_output[0]\n",
    "channel_signal_smr = channel_signal_output[0]\n",
    "juice_timestamp = marker_list[0]['values'][marker_list[0]['labels'] == 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marker_list[0]['values'][marker_list[0]['labels'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "juice_timestamp - data_item.ff_caught_T_sorted[:len(juice_timestamp)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## heatmap of correlations between features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_item.make_or_retrieve_y_var_vif_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_columns = data_item.y_var_vif_df.loc[data_item.y_var_vif_df['vif'] > 5, \"var\"].values\n",
    "specific_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_coeff = data_item.y_var[specific_columns].corr()\n",
    "plt.figure(figsize = (15, 15))\n",
    "sns.heatmap(corr_coeff, cmap='coolwarm', annot=True, linewidths=1, vmin=-1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## see autocorrelation for each variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see autocorrelation for each variable\n",
    "for column in data_item.final_behavioral_data.columns:\n",
    "    print(column)\n",
    "    for lag in range(1, 8):\n",
    "        print('lag' , lag, ', autocorr:', data_item.final_behavioral_data[column].autocorr(lag))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduce multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### y_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually dropped some more columns\n",
    "data_item._reduce_y_var()\n",
    "# check vif_df_again\n",
    "data_item.make_or_retrieve_y_var_reduced_vif_df()\n",
    "data_item.y_var_reduced_vif_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### y_var_lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_item.make_or_retrieve_y_var_lags_reduced()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression\n",
    "\n",
    "Regressing the behavioral variables individually (as y_var) against all neural activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## put results in df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_item.make_or_retrieve_y_var_lr_df(exists_ok=True)\n",
    "data_item.y_var_lr_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  plot all neural clusters vs one behavioral var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conduct linear regression on X and y\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 10)\n",
    "bins_to_plot = range(data_item.final_behavioral_data.bin.max())\n",
    "## we didn't use the line below because we want to plot in the order of decreasing r squared\n",
    "# for i, column in enumerate(data_item.final_behavioral_data.columns): \n",
    "for i, column in enumerate(data_item.y_var_lr_df['var'].values):\n",
    "    plot_neural_data.plot_regression(data_item.final_behavioral_data, column, data_item.binned_spikes_matrix, min_r_squared_to_plot=0.2)\n",
    "\n",
    "    if i == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  plot one neural cluster vs one behavioral var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot one neural cluster against one behavioral variable\n",
    "cluster_num, behavioral_column = 6, 'speed'\n",
    "bins_to_plot = range(1000, 1200)\n",
    "x_values, y_values = data_item.binned_spikes_matrix[bins_to_plot, cluster_num], data_item.final_behavioral_data[behavioral_column][bins_to_plot]\n",
    "reg = LinearRegression().fit(x_values.reshape(-1, 1), y_values)\n",
    "\n",
    "plt.scatter(x_values, y_values, color='blue', s=1)\n",
    "plt.plot(x_values, reg.predict(x_values.reshape(-1, 1)), color='red', linewidth=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCA\n",
    "\n",
    "https://medium.com/@pozdrawiamzuzanna/canonical-correlation-analysis-simple-explanation-and-python-example-a5b8e97648d2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No lagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cca_no_lag = cca_class.CCAclass(X1=data_item.x_var, X2=data_item.y_var_reduced, lagging_included=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cca_no_lag.conduct_cca()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cca_lags = cca_class.CCAclass(X1=data_item.x_var, X2=data_item.y_var_lags_reduced, lagging_included=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cca_lags.conduct_cca()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compare lag vs no lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "canon_df = pd.DataFrame(cca_no_lag.canon_corr, columns = ['no_lag'])\n",
    "canon_df[f'lag_{data_item.max_lag_number}'] = cca_lags.canon_corr\n",
    "canon_df['component'] = [f'CC {i+1}' for i in range(cca_lags.n_components)]\n",
    "# convert canon_df to long format\n",
    "canon_df_long = pd.melt(canon_df, id_vars=['component'], var_name='lag', value_name='canon_coeff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a sns bar plot on canon_df_long\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x='component', y='canon_coeff', data=canon_df_long, hue='lag')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cca_inst (choose one between lags and no lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose lags\n",
    "cca_inst = cca_lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose no lag\n",
    "cca_inst = cca_no_lag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loadings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cca_inst.plot_ranked_loadings(X1_or_X2='X1', squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cca_inst.plot_ranked_loadings(X1_or_X2='X2', squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## squared loadings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cca_inst.plot_ranked_loadings(X1_or_X2='X1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cca_inst.plot_ranked_loadings(X1_or_X2='X2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## abs weights ranked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cca_inst.X1_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cca_inst.plot_ranked_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cca_inst.plot_ranked_weights(X1_or_X2='X2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot real weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cca_inst.plot_ranked_weights(abs_value=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cca_inst.plot_ranked_weights(X1_or_X2='X2', abs_value=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## distribution of each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cca_inst.X2_sc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_sc_df = pd.DataFrame(cca_inst.X2_sc, columns = cca_inst.X2.columns)\n",
    "X2_sc_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in X2_sc_df.columns[:4]:\n",
    "    plt.figure(figsize=(8, 2))\n",
    "    sns.boxplot(X2_sc_df[column], orient='h')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## heatmap of weights\n",
    "raw canonical coefficients are interpreted in a manner analogous to interpreting regression coefficients. For example: a one unit increase in reading leads to a .0446 decrease in the first canonical variate of set 2 when all of the other variables are held constant (in some other data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_df = cca_inst.X2_weight_df.copy()\n",
    "weight_df = weight_df.set_index('feature').drop(columns='feature_category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(15, 25))\n",
    "sns.heatmap(weight_df.iloc[:20, :10], cmap='coolwarm', annot=True, linewidths=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1, test1, train2, test2 = train_test_split(cca_inst.X1_sc, cca_inst.X2_sc, test_size=0.3, random_state=42)\n",
    "# use training and testing set\n",
    "nComponents = 10\n",
    "cca2 = rcca.CCA(kernelcca = False, reg = 0., numCC = nComponents)\n",
    "cca2.train([train1, train2])\n",
    "testcorrs = cca2.validate([test1, test2])\n",
    "testcorrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compute explained variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cca2.compute_ev([test1, test2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test for p values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_cca = CanCorr(cca_inst.X1_sc, cca_inst.X2_sc)\n",
    "print(stats_cca.corr_test().summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PGAM (not used anymore because of importing issue after I made a new venv & higher version of python)\n",
    "\n",
    "Code is from https://github.com/BalzaniEdoardo/PGAM\n",
    "\n",
    "I might need to run this on google colab ...? Or, look into clusters? or maybe mac pro is enough?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgam_path = '/Users/dusiyi/Documents/Multifirefly-Project/multiff_analysis/external/pgam/src/'\n",
    "import sys\n",
    "if not pgam_path in sys.path: \n",
    "    sys.path.append(pgam_path)\n",
    "    \n",
    "import numpy as np\n",
    "import sys\n",
    "from PGAM.GAM_library import *\n",
    "import PGAM.gam_data_handlers as gdh\n",
    "import matplotlib.pylab as plt\n",
    "import pandas as pd\n",
    "from post_processing import postprocess_results\n",
    "from scipy.io import savemat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## individual steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### categorize variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgam_inst = pgam_class.PGAMclass(data_item.x_var, data_item.y_var, data_item.bin_width, data_item.processed_neural_data_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgam_inst.prepare_for_pgam(num_total_trials=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### temporal kernel\n",
    "\n",
    "modified from PGAM_Tutorial.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgam_inst._add_temporal_features_to_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(gdh.smooths_handler.add_smooth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spatial variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgam_inst._add_spatial_features_to_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgam_inst.run_pgam(neural_cluster_number=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgam_inst.post_processing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgam_inst.save_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iterate through all neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgam_inst = pgam_class.PGAMclass(data_item.x_var, data_item.y_var, data_item.bin_width, data_item.processed_neural_data_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(data_item.x_var.shape[1]):\n",
    "    print(f'neural_cluster_number: {i} out of {data_item.x_var.shape[1]}')\n",
    "    pgam_inst.streamline_pgam(neural_cluster_number=i, num_total_trials=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use reduced_var_list from PGAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use reduced_var_list from PGAM\n",
    "# reduced_var_list = reduced.var_list\n",
    "# temporal_vars = [col for col in temporal_vars if col in reduced_var_list]\n",
    "# spatial_vars = [col for col in spatial_vars if col in reduced_var_list]\n",
    "# data_item.y_var = data_item.y_var.loc[:, reduced_var_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try original code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "## inputs parameters  \n",
    "num_events = 6000\n",
    "time_points = 3 * 10 ** 5  # 30 mins at 0.006 ms resolution\n",
    "rate = 5. * 0.006  # Hz rate of the final kernel\n",
    "variance = 5.  # spatial input and nuisance variance\n",
    "int_knots_num = 20  # num of internal knots for the spline basis\n",
    "order = 4  # spline order\n",
    "\n",
    "## assume 200 trials\n",
    "trial_ids = np.repeat(np.arange(200),time_points//200)\n",
    "\n",
    "## create temporal input\n",
    "idx = np.random.choice(np.arange(time_points), num_events, replace=False)\n",
    "events = np.zeros(time_points)\n",
    "events[idx] = 1\n",
    "\n",
    "rv = sts.multivariate_normal(mean=[0, 0], cov= variance * np.eye(2))\n",
    "samp = rv.rvs(time_points)\n",
    "spatial_var = samp[:, 0]\n",
    "nuisance_var = samp[:, 1]\n",
    "\n",
    "# truncate X to avoid jumps in the resp function\n",
    "sele_idx = np.abs(spatial_var) < 5\n",
    "spatial_var = spatial_var[sele_idx]\n",
    "nuisance_var = nuisance_var[sele_idx]\n",
    "while spatial_var.shape[0] < time_points:\n",
    "    tmpX = rv.rvs(10 ** 4)\n",
    "    sele_idx = np.abs(tmpX[:, 0]) < 5\n",
    "    tmpX = tmpX[sele_idx, :]\n",
    "\n",
    "    spatial_var = np.hstack((spatial_var, tmpX[:, 0]))\n",
    "    nuisance_var = np.hstack((nuisance_var, tmpX[:, 1]))\n",
    "spatial_var = spatial_var[:time_points]\n",
    "nuisance_var = nuisance_var[:time_points]\n",
    "\n",
    "# create a resp function\n",
    "knots = np.hstack(([-5]*3, np.linspace(-5,5,8),[5]*3))\n",
    "beta = np.arange(10)\n",
    "beta = beta / np.linalg.norm(beta)\n",
    "beta = np.hstack((beta[5:], beta[:5][::-1]))\n",
    "resp_func = lambda x : np.dot(gdh.splineDesign(knots, x, order, der=0),beta)\n",
    "\n",
    "filter_used_conv = sts.gamma.pdf(np.linspace(0,20,100),a=2) - sts.gamma.pdf(np.linspace(0,20,100),a=5)\n",
    "filter_used_conv = np.hstack((np.zeros(101),filter_used_conv))*2\n",
    "# mean of the spike counts depending on spatial_var and events\n",
    "log_mu0 = resp_func(spatial_var)\n",
    "for tr in np.unique(trial_ids):\n",
    "    log_mu0[trial_ids == tr] = log_mu0[trial_ids == tr] + np.convolve(events[trial_ids == tr], filter_used_conv, mode='same')\n",
    "\n",
    "# adjust mean rate\n",
    "const = np.log(np.mean(np.exp(log_mu0)) / rate)\n",
    "log_mu0 = log_mu0 - const\n",
    "\n",
    "# generate spikes\n",
    "spk_counts = np.random.poisson(np.exp(log_mu0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the firing rate and the spike counts generated\n",
    "plt.figure(figsize=(8,3))\n",
    "plt.subplot(121)\n",
    "plt.plot(np.arange(1000) * 0.006, np.exp(log_mu0)[:1000]/0.006)\n",
    "plt.title('firing rate [Hz]', fontsize=16)\n",
    "plt.xlabel('time[sec]', fontsize=12)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(np.arange(1000) * 0.006, spk_counts[:1000])\n",
    "plt.title('6ms binned spike counts', fontsize=16)\n",
    "plt.xlabel('time[sec]', fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Create the *smooths_handler* object and fit the model\n",
    "Below we create the smooths_handler object and run a fit. We include a \"nuisance\" spatial variable, that is not driving the neuron, the fit will learn to discard the variable;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Creating the class\n",
    "sm_handler = smooths_handler()\n",
    "# Creating the knots (notice the over-representation of edge knots)\n",
    "knots = np.hstack(([-5]*3, np.linspace(-5,5,15),[5]*3))\n",
    "# Using smooths_handler class to add variables \n",
    "sm_handler.add_smooth('spatial', [spatial_var], knots=[knots], ord=4, is_temporal_kernel=False,\n",
    "                     trial_idx=trial_ids, is_cyclic=[False],penalty_type='der', der=2)\n",
    "\n",
    "sm_handler.add_smooth('nuisance', [nuisance_var], knots=[knots], ord=4, is_temporal_kernel=False,\n",
    "                     trial_idx=trial_ids, is_cyclic=[False],penalty_type='der', der=2)\n",
    "\n",
    "sm_handler.add_smooth('temporal', [events], knots=None, ord=4, is_temporal_kernel=True,\n",
    "                     trial_idx=trial_ids, is_cyclic=[False],penalty_type='der', der=2,\n",
    "                     knots_num=10, kernel_length=500, kernel_direction=1)\n",
    "\n",
    "\n",
    "# split trial in train and eval\n",
    "train_trials = trial_ids % 10 != 0\n",
    "eval_trials = ~train_trials\n",
    "\n",
    "\n",
    "link = sm.genmod.families.links.log()\n",
    "poissFam = sm.genmod.families.family.Poisson(link=link)\n",
    "\n",
    "# create the pgam model\n",
    "pgam = general_additive_model(sm_handler,\n",
    "                              sm_handler.smooths_var, # list of covariate we want to include in the model\n",
    "                              spk_counts, # vector of spike counts\n",
    "                              poissFam # poisson family with exponential link from statsmodels.api\n",
    "                             )\n",
    "\n",
    "# with with all covariate, remove according to stat testing, and then refit\n",
    "full, reduced = pgam.fit_full_and_reduced(sm_handler.smooths_var, \n",
    "                                          th_pval=0.001,# pval for significance of covariate icluseioon\n",
    "                                          max_iter=10 ** 2, # max number of iteration\n",
    "                                          use_dgcv=True, # learn the smoothing penalties by dgcv\n",
    "                                          trial_num_vec=trial_ids,\n",
    "                                          filter_trials=train_trials)\n",
    "\n",
    "print('Minimal subset of variables driving the activity:')\n",
    "print(reduced.var_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Post processing<a name=\"post-proc\">\n",
    "After a fit, it is possible to post-process the model fit output to obtain an easy to parse result in the form of a numpy.structarray. \n",
    "\n",
    "Each row will represent results for a specific input variable, additional information about the neuron (e.g. channel ID, electrode ID, or anything else) can be provided in the form of a dictionary, each dictionary value will be stored in the structured array with type \"object\".\n",
    "\n",
    "The output structure can be saved either as a \".npy\" via *numpy.save(\\<filename\\>)* or as a .mat (for MATLAB) via *scipy.io.savemat(\\<filename*\\>)*.\n",
    "\n",
    "Below is an example of the post-processing applied to the fit just obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# string with the neuron identifier\n",
    "neuron_id = 'neuron_000_session_1_monkey_001'\n",
    "# dictionary containing some information about the neuron, keys must be strings and values can be anything\n",
    "# since are stored with type object.\n",
    "info_save = {'x':100,\n",
    "             'y':801.2,\n",
    "             'z':301,\n",
    "             'brain_region': 'V1',\n",
    "             'subject':'monkey_001'\n",
    "            }\n",
    "\n",
    "# assume that we used 90% of the trials for training, 10% for evaluation\n",
    "res = postprocess_results(neuron_id, spk_counts, full, reduced, train_trials,\n",
    "                        sm_handler, poissFam, trial_ids, var_zscore_par=None,info_save=info_save,bins=100)\n",
    "\n",
    "# each row of res contains the info about a variable\n",
    "# some info are shared for all the variables (p-rsquared for example is a goodness of fit measure for the model\n",
    "# it is shared, not a property of the variable), while other, like the parameters of the b-splines, \n",
    "# are variable specific\n",
    "print('\\n\\n')\n",
    "print('Result structarray types\\n========================\\n')\n",
    "for name in res.dtype.names:\n",
    "    print('%s: \\t %s'%(name, type(res[name][0])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot tuning functions\n",
    "plt.figure(figsize=(8,4))\n",
    "\n",
    "for k in range(3):\n",
    "    plt.subplot(2,3,k+1)\n",
    "    plt.title('log-space %s'%res['variable'][k])\n",
    "    x_kernel = res['x_kernel'][k].reshape(-1)\n",
    "    y_kernel = res['y_kernel'][k].reshape(-1)\n",
    "    ypCI_kernel = res['y_kernel_pCI'][k]\n",
    "    ymCI_kernel = res['y_kernel_mCI'][k]\n",
    "    \n",
    "    plt.plot(x_kernel, y_kernel, color='r')\n",
    "    plt.fill_between(x_kernel, ymCI_kernel, ypCI_kernel, color='r', alpha=0.3)\n",
    "    \n",
    "    \n",
    "    \n",
    "    x_firing = np.array(res['x_rate_Hz'][k]).reshape(-1)\n",
    "    y_firing_model = res['y_rate_Hz_model'][k].reshape(-1)\n",
    "    y_firing_raw = res['y_rate_Hz_raw'][k].reshape(-1)\n",
    "    \n",
    "    plt.subplot(2,3,k+4)\n",
    "    plt.title('rate-space %s'%res['variable'][k])\n",
    "    \n",
    "    plt.plot(x_firing, y_firing_raw, color='k',label='raw')\n",
    "    plt.plot(x_firing, y_firing_model, color='r',label='model')\n",
    "    \n",
    "    #plt.legend()\n",
    "    plt.tight_layout()\n",
    "plt.show()\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## both_ff_across_time_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The code below is not meant to be run, but serves as a record\n",
    "\n",
    "both_ff_across_time_df['time_rel_to_stop'] = both_ff_across_time_df['time'] - both_ff_across_time_df['stop_time']\n",
    "both_ff_across_time_df['time_when_cur_ff_first_seen_rel_to_stop'] = both_ff_across_time_df['time_cur_ff_first_seen'] - both_ff_across_time_df['time_rel_to_stop']\n",
    "both_ff_across_time_df['time_when_cur_ff_last_seen_rel_to_stop']\n",
    "both_ff_across_time_df['time_when_nxt_ff_first_seen_rel_to_stop']\n",
    "both_ff_across_time_df['time_when_nxt_ff_last_seen_rel_to_stop']\n",
    "\n",
    "both_ff_across_time_df['cur_ff_angle']\n",
    "both_ff_across_time_df['nxt_ff_angle']\n",
    "both_ff_across_time_df['ff_angle_when_cur_ff_first_seen']\n",
    "both_ff_across_time_df['ff_angle_when_cur_ff_last_seen']\n",
    "both_ff_across_time_df['ff_angle_when_nxt_ff_first_seen']\n",
    "both_ff_across_time_df['ff_angle_when_nxt_ff_last_seen']\n",
    "\n",
    "both_ff_across_time_df['cur_ff_distance']\n",
    "both_ff_across_time_df['nxt_ff_distance']\n",
    "both_ff_across_time_df['ff_distance_when_cur_ff_first_seen']\n",
    "both_ff_across_time_df['ff_distance_when_cur_ff_last_seen']\n",
    "both_ff_across_time_df['ff_distance_when_nxt_ff_first_seen']\n",
    "both_ff_across_time_df['ff_distance_when_nxt_ff_last_seen']\n",
    "\n",
    "both_ff_across_time_df['cur_arc_curv']\n",
    "both_ff_across_time_df['nxt_arc_curv']\n",
    "both_ff_across_time_df['arc_curv_when_cur_ff_first_seen']\n",
    "both_ff_across_time_df['arc_curv_when_cur_ff_last_seen']\n",
    "both_ff_across_time_df['arc_curv_when_nxt_ff_first_seen']\n",
    "both_ff_across_time_df['arc_curv_when_nxt_ff_last_seen']\n",
    "\n",
    "both_ff_across_time_df['cur_arc_dheading']\n",
    "both_ff_across_time_df['nxt_arc_dheading']\n",
    "both_ff_across_time_df['arc_dheading_when_cur_ff_first_seen']\n",
    "both_ff_across_time_df['arc_dheading_when_cur_ff_last_seen']\n",
    "both_ff_across_time_df['arc_dheading_when_nxt_ff_first_seen']\n",
    "both_ff_across_time_df['arc_dheading_when_nxt_ff_last_seen']\n",
    "\n",
    "both_ff_across_time_df['cur_opt_arc_curv']\n",
    "both_ff_across_time_df['nxt_opt_arc_curv']\n",
    "both_ff_across_time_df['opt_arc_curv_when_cur_ff_first_seen']\n",
    "both_ff_across_time_df['opt_arc_curv_when_cur_ff_last_seen']\n",
    "both_ff_across_time_df['opt_arc_curv_when_nxt_ff_first_seen']\n",
    "both_ff_across_time_df['opt_arc_curv_when_nxt_ff_last_seen']\n",
    "\n",
    "both_ff_across_time_df['cur_opt_arc_dheading']\n",
    "both_ff_across_time_df['nxt_opt_arc_dheading']\n",
    "both_ff_across_time_df['opt_arc_dheading_when_cur_ff_first_seen']\n",
    "both_ff_across_time_df['opt_arc_dheading_when_cur_ff_last_seen']\n",
    "both_ff_across_time_df['opt_arc_dheading_when_nxt_ff_first_seen']\n",
    "both_ff_across_time_df['opt_arc_dheading_when_nxt_ff_last_seen']\n",
    "\n",
    "both_ff_across_time_df['curv_of_traj']\n",
    "both_ff_across_time_df['traj_curv_when_cur_ff_first_seen']\n",
    "both_ff_across_time_df['traj_curv_when_cur_ff_last_seen']\n",
    "both_ff_across_time_df['traj_curv_when_nxt_ff_first_seen']\n",
    "both_ff_across_time_df['traj_curv_when_nxt_ff_last_seen']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monkey_information = both_ff_across_time_df.copy()\n",
    "monkey_information['whether_point_index_before_stop'] = False\n",
    "monkey_information.loc[dc.stops_near_ff_df['point_index_before_stop'], 'whether_point_index_before_stop'] = True\n",
    "monkey_information['whether_stop_point_index'] = False\n",
    "monkey_information.loc[dc.stops_near_ff_df['stop_point_index'], 'whether_stop_point_index'] = True\n",
    "monkey_information['whether_next_stop_point_index'] = False\n",
    "monkey_information.loc[dc.stops_near_ff_df['next_stop_point_index'], 'whether_next_stop_point_index'] = True\n",
    "monkey_information['whether_next_stop_point_index'] = False\n",
    "\n",
    "# monkey_information['during_instances'] = False # whether the point is during an instance used for analyzing planning\n",
    "# for index, row in dc.stops_near_ff_df.iterrows():\n",
    "#     monkey_information.loc[row['point_index_before_stop']:row['next_stop_point_index'], 'during_instances'] = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## single ff neural data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io # since this is an earlier version mat file, h5py can't be used here\n",
    "filepath = \"all_monkey_data/raw_monkey_data/single_ff_neural_data/m53s31.mat\"\n",
    "single_ff = scipy.io.loadmat(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_ff.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(single_ff['units'].flatten()[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit = 10\n",
    "trial = 200\n",
    "single_ff['units'].flatten()[unit][-2][:,trial]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a b-spline\n",
    "kernel_len = 15 #  about +- 325ms \n",
    "knots = np.hstack(([-1.001]*3, np.linspace(-1.001,1.001,5), [1.001]*3))\n",
    "tp = np.linspace(-1.,1.,kernel_len)\n",
    "bX = neural_data_processing.splineDesign(knots, tp, ord=4, der=0, outer_ok=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bX.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bX[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(bX.shape[1]):\n",
    "  plt.subplots(figsize=(7,7))\n",
    "  y = bX[:,i]\n",
    "  print(y)\n",
    "  plt.scatter(tp, y)\n",
    "  plt.show()\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## correlation between features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = range(10000, 10100)\n",
    "dv = (y_var_lags['speed_0'] - y_var_lags['speed_-1'])/data_item.bin_width\n",
    "plt.plot(range(num_points), (data_item.final_behavioral_data['accel']).iloc[points], label='ddv')\n",
    "plt.plot(range(num_points), dv.iloc[points], label='speed_0 - speed_-1')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = range(10000, 10100)\n",
    "num_points = len(points)\n",
    "plt.plot(range(num_points), y_var_lags['speed_0'].iloc[points], label='speed_0')\n",
    "plt.plot(range(num_points), y_var_lags['speed_-1'].iloc[points], label='speed_')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = range(10000, 10100)\n",
    "plt.plot(range(num_points), y_var_lags['accel_0'].iloc[points], label='accel_0')\n",
    "plt.plot(range(num_points), y_var_lags['accel_-1'].iloc[points], label='accel_-1')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_var_lags.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = range(10000, 10500)\n",
    "num_points = len(points)\n",
    "plt.plot(range(num_points), y_var_lags['stop_rate_0'].iloc[points], label='stop_rate_0')\n",
    "plt.plot(range(num_points), y_var_lags['num_distinct_stops_0'].iloc[points], label='num_distinct_stops_0')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_var_lags[['speed_0', 'speed_-1']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_var_lags[['accel_0', 'accel_-1']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_var_lags[['num_distinct_stops_0', 'stop_rate_0']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rename column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns in data_item.final_behavioral_data\n",
    "column_to_renames = {'catching_ff': 'whether catching ff at current time bin',\n",
    "                    'min_target_cluster_has_disappeared_for_last_time_dummy': 'whether target cluster has disappeared for last time',           \n",
    "                    'max_target_cluster_visible_dummy': 'whether target cluster is visible',\n",
    "                    'gaze_world_y': 'gaze y-coordinate',\n",
    "                    'speed': 'monkey linear speed',\n",
    "                    'ang_speed': 'monkey linear acceleration',\n",
    "                    'ang_accel': 'change in monkey linear acceleration',\n",
    "                    'accel': 'change in monkey angular acceleration',\n",
    "                    'avg_target_cluster_last_seen_distance': 'distance of target cluster last seen',\n",
    "                    'avg_target_cluster_last_seen_angle': 'angle of target cluster last seen'\n",
    "                    }\n",
    "\n",
    "y_var_renamed = data_item.y_var.rename(columns=column_to_renames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iterate in lap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This might be useful later...\n",
    "\n",
    "# # iterate across different lags\n",
    "# for n_lags in range(3, 4):\n",
    "    \n",
    "#     #print(\"n_lags\", n_lags, \"include_negative_lags:\", include_negative_lags)\n",
    "\n",
    "#     #lag_numbers = range(-n_lags,0+1)\n",
    "#     lag_numbers = range(-n_lags, n_lags+1)\n",
    "#     #lag_numbers = range(0, n_lags+1)\n",
    "#     print(\"lag_numbers:\", lag_numbers)\n",
    "\n",
    "#     x_var = neural_data_processing.add_lags_to_each_feature(data_item.binned_spikes_df.drop(columns=['bin'], errors='ignore'), lag_numbers = lag_numbers)\n",
    "#     y_var_lags = neural_data_processing.add_lags_to_each_feature(final_behavioral_data_renamed, lag_numbers= range(-3, 4))\n",
    "\n",
    "#     cca, X1_sc, X2_sc, X1_c, X2_c, canon_corr = neural_data_modeling.conduct_cca(x_var, y_var_lags, n_components=10, plot_correlations=True)\n",
    "\n",
    "# coef_df = pd.DataFrame(np.round(cca.coef_, 2), columns = [x_var.columns])\n",
    "# coef_df.index = y_var_lags.columns\n",
    "\n",
    "# lagging_included = True\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ff_venv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
