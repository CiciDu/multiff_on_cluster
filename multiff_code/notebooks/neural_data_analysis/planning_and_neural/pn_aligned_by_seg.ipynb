{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys, sys\n",
    "from pathlib import Path\n",
    "for p in [Path.cwd()] + list(Path.cwd().parents):\n",
    "    if p.name == 'Multifirefly-Project':\n",
    "        os.chdir(p)\n",
    "        sys.path.insert(0, str(p / 'multiff_analysis/multiff_code/methods'))\n",
    "        break\n",
    "    \n",
    "from data_wrangling import specific_utils, process_monkey_information, general_utils\n",
    "from pattern_discovery import pattern_by_trials, pattern_by_trials, cluster_analysis, organize_patterns_and_features\n",
    "from visualization.matplotlib_tools import plot_behaviors_utils\n",
    "from neural_data_analysis.neural_analysis_tools.get_neural_data import neural_data_processing\n",
    "from neural_data_analysis.neural_analysis_tools.visualize_neural_data import plot_neural_data, plot_modeling_result\n",
    "from neural_data_analysis.neural_analysis_tools.model_neural_data import transform_vars, neural_data_modeling, drop_high_corr_vars, drop_high_vif_vars\n",
    "from neural_data_analysis.topic_based_neural_analysis.neural_vs_behavioral import prep_monkey_data, prep_target_data, neural_vs_behavioral_class\n",
    "from neural_data_analysis.topic_based_neural_analysis.planning_and_neural import planning_and_neural_class, pn_utils, pn_helper_class, pn_aligned_by_seg\n",
    "from neural_data_analysis.neural_analysis_tools.cca_methods import cca_class\n",
    "from neural_data_analysis.neural_analysis_tools.cca_methods import cca_class, cca_utils, cca_cv_utils\n",
    "from neural_data_analysis.neural_analysis_tools.cca_methods.cca_plotting import cca_plotting, cca_plot_lag_vs_no_lag, cca_plot_cv\n",
    "from machine_learning.ml_methods import regression_utils, regz_regression_utils, ml_methods_class, classification_utils, ml_plotting_utils, ml_methods_utils\n",
    "from planning_analysis.show_planning import nxt_ff_utils, show_planning_utils\n",
    "from neural_data_analysis.neural_analysis_tools.align_trials import time_resolved_regression, time_resolved_gpfa_regression,plot_time_resolved_regression\n",
    "from neural_data_analysis.neural_analysis_tools.gpfa_methods import elephant_utils, fit_gpfa_utils, plot_gpfa_utils, plot_gpfa_utils2, gpfa_helper_class\n",
    "\n",
    "import sys\n",
    "import math\n",
    "import gc\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from importlib import reload\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import rc\n",
    "from scipy import linalg, interpolate\n",
    "from scipy.signal import fftconvolve\n",
    "from scipy.io import loadmat\n",
    "from scipy import sparse\n",
    "import torch\n",
    "from numpy import pi\n",
    "\n",
    "# Machine Learning imports\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.multivariate.cancorr import CanCorr\n",
    "\n",
    "# Neuroscience specific imports\n",
    "import neo\n",
    "import rcca\n",
    "import quantities as pq\n",
    "\n",
    "plt.rcParams[\"animation.html\"] = \"html5\"\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "rc('animation', html='jshtml')\n",
    "matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n",
    "matplotlib.rcParams['animation.embed_limit'] = 2**128\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "np.set_printoptions(suppress=True)\n",
    "print(\"done\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# retrieve data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Bruno/data_0402\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Schro/data_0416\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_y_var_lags = False\n",
    "planning_data_by_point_exists_ok = True\n",
    "y_data_exists_ok = True\n",
    "\n",
    "pn = pn_aligned_by_seg.PlanningAndNeuralSegmentAligned(raw_data_folder_path=raw_data_folder_path)\n",
    "pn.prep_data_to_analyze_planning(planning_data_by_point_exists_ok=planning_data_by_point_exists_ok)\n",
    "pn.planning_data_by_point, cols_to_drop = general_utils.drop_columns_with_many_nans(\n",
    "    pn.planning_data_by_point)\n",
    "pn.get_x_and_y_data_for_modeling(exists_ok=y_data_exists_ok, reduce_y_var_lags=reduce_y_var_lags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get planning_data by segment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get data and fit gpfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.prepare_seg_aligned_data(segment_duration=2, rebinned_max_x_lag_number=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.get_gpfa_traj(latent_dimensionality=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for regression later\n",
    "use_raw_spike_data_instead = False\n",
    "\n",
    "pn.get_concat_data_for_regression(use_raw_spike_data_instead=True,\n",
    "                                    use_lagged_raw_spike_data=True,\n",
    "                                    apply_pca_on_raw_spike_data=False,\n",
    "                                    num_pca_components=7)\n",
    "pn.print_data_dimensions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example trajectories\n",
    "for traj in pn.trajectories[:5]:\n",
    "    print(traj.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## point-wise segment regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.make_time_resolved_cv_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features_to_plot = [\n",
    "'time', 'time_rel_to_stop',\n",
    "'target_distance',\n",
    "'target_angle',\n",
    "'target_rel_x',\n",
    "'target_rel_y',\n",
    "'speed',\n",
    "'stop']\n",
    "\n",
    "pn.time_resolved_cv_scores.loc[pn.time_resolved_cv_scores['feature'] == 'monkey_speeddummy', 'feature'] = 'stop'\n",
    "pn.plot_time_resolved_regression(features_to_plot=features_to_plot, n_behaviors_per_plot=8)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.time_resolved_cv_scores.loc[pn.time_resolved_cv_scores['feature'] == 'monkey_speeddummy', 'feature'] = 'stop'\n",
    "for features in [['target_distance', 'target_rel_y'],\n",
    "                 ['target_rel_x', 'target_angle'],\n",
    "                 ['time', 'time_rel_to_stop'],\n",
    "                 ['speed', 'stop']]:\n",
    "    \n",
    "    pn.plot_time_resolved_regression(features_to_plot=features)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.plot_trial_counts_by_timepoint()  # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## concat data regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.separate_test_and_control_data()\n",
    "print(pn.concat_neural_trials.shape)\n",
    "print(pn.concat_behav_trials.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_utils.check_na_in_df(pn.concat_neural_trials)\n",
    "general_utils.check_na_in_df(pn.concat_behav_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Multivariate linear regression\n",
    "# pn.y_var_lr_df = neural_data_modeling.get_y_var_lr_df(\n",
    "#                 pn.concat_neural_trials.drop(columns=['new_segment', 'new_bin'], errors='ignore'), \n",
    "#                 pn.concat_behav_trials)\n",
    "\n",
    "# pn.y_var_lr_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## segment split regress CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(ml_methods_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_of_interest = ['whether_test', 'cur_ff_distance', 'cur_ff_angle', 'cur_ff_rel_x', 'cur_ff_rel_y', 'nxt_ff_distance', 'nxt_ff_rel_y', 'nxt_opt_arc_dheading', 'nxt_ff_rel_x', 'nxt_ff_angle', 'nxt_ff_angle_at_ref']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "for test_or_control in ['test', 'control', 'both']:\n",
    "    x_var, y_var = pn.get_concat_x_and_y_var_for_lr(test_or_control=test_or_control)\n",
    "    \n",
    "    results_summary = ml_methods_utils.run_segment_split_regression_cv(\n",
    "        x_var, \n",
    "        y_var, \n",
    "        columns_of_interest, \n",
    "        num_folds=5, \n",
    "    )\n",
    "    results_summary['test_or_control'] = test_or_control\n",
    "    all_results.append(results_summary)\n",
    "\n",
    "all_results = pd.concat(all_results)\n",
    "all_results.head()\n",
    "\n",
    "reg_results = all_results[all_results['Model'] == 'Linear Regression']\n",
    "class_results = all_results[all_results['Model'] == 'Logistic Regression']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Just 'both'\n",
    "\n",
    "all_results = []\n",
    "for test_or_control in ['both']:\n",
    "    x_var, y_var = pn.get_concat_x_and_y_var_for_lr(test_or_control=test_or_control)\n",
    "    \n",
    "    results_summary = ml_methods_utils.run_segment_split_regression_cv(\n",
    "        x_var, \n",
    "        y_var, \n",
    "        columns_of_interest, \n",
    "        num_folds=5, \n",
    "    )\n",
    "    results_summary['test_or_control'] = test_or_control\n",
    "    all_results.append(results_summary)\n",
    "\n",
    "all_results = pd.concat(all_results)\n",
    "all_results.head()\n",
    "\n",
    "reg_results = all_results[all_results['Model'] == 'Linear Regression']\n",
    "class_results = all_results[all_results['Model'] == 'Logistic Regression']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reg_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = reg_results.copy()\n",
    "\n",
    "# Filter only test_r2 rows\n",
    "df_test_r2 = df[df[\"Metric\"] == \"test_r2\"]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.bar(df_test_r2[\"Feature\"], df_test_r2[\"Mean\"], \n",
    "        yerr=df_test_r2[\"Std\"], capsize=4, color=\"skyblue\", edgecolor=\"k\")\n",
    "\n",
    "plt.axhline(0, color=\"red\", linestyle=\"--\", linewidth=1)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"Test R² (Mean ± Std)\")\n",
    "plt.title(\"Test R² Across Features\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression results\n",
    "for metric in ['test_pearson_r', 'test_r2']:\n",
    "    ml_methods_utils.make_barplot_to_compare_results(\n",
    "            reg_results, \n",
    "            metric=metric, \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification results\n",
    "for metric in ['test_accuracy']:\n",
    "    ml_methods_utils.make_barplot_to_compare_results(\n",
    "        class_results, \n",
    "        metric=metric, \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## segment split regress train-test\n",
    "Warning: results can be very unstable due to the stochasticity of train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_var = pn.concat_neural_trials\n",
    "y_var = pn.concat_behav_trials\n",
    "\n",
    "columns_of_interest = ['nxt_ff_rel_y', 'nxt_opt_arc_dheading', 'nxt_ff_rel_x', 'nxt_ff_angle', 'nxt_ff_angle_at_ref']\n",
    "ml_methods_utils.run_segment_split_regression(x_var, y_var, columns_of_interest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot latent dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gpfa_utils.plot_gpfa_traj_3d_timecolored_average(pn.trajectories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gpfa_utils.plot_gpfa_traj_3d_uniform_color(pn.trajectories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gpfa_utils.plot_gpfa_traj_3d_timecolored_average(pn.trajectories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep your coordinates, but auto-pick the best azimuth/elevation\n",
    "plot_gpfa_utils2.plot_gpfa_traj_3d_timecolored_average(pn.trajectories, auto_view=\"grid\", grid_step=5)\n",
    "\n",
    "# Rotate data to PCA (PC1/PC2/PC3) for maximal in-plane variance\n",
    "plot_gpfa_utils2.plot_gpfa_traj_3d_timecolored_average(pn.trajectories, auto_view=\"pca\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, enable interactive mode in your notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Import required modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "\n",
    "# Create the interactive plot\n",
    "fig, ax = plot_gpfa_utils.plot_gpfa_traj_3d(\n",
    "    trajectories=pn.trajectories,\n",
    "    figsize=(15, 5),\n",
    "    linewidth_single_trial=0.75,\n",
    "    alpha_single_trial=0.3,\n",
    "    linewidth_trial_average=2,\n",
    "    title='Latent dynamics extracted by GPFA',\n",
    "    view_azim=-5,\n",
    "    view_elev=60\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plot_gpfa_utils.plot_gpfa_traj_3d_plotly(trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find variance explained by each latent dimension\n",
    "traj_stack = np.stack(pn.trajectories, axis=0)  # shape: (n_trials, 3, T)\n",
    "var_by_dim = np.var(traj_stack, axis=(0, 2))    # variance across trials and time\n",
    "var_by_dim /= var_by_dim.sum()               # normalize to get explained variance ratio\n",
    "print(\"Variance explained by each latent dimension:\", var_by_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "ax.set_title('Latent dynamics extracted by GPFA')\n",
    "ax.set_xlabel('Time [s]')\n",
    "\n",
    "average_trajectory = np.mean(pn.trajectories, axis=0)\n",
    "time = np.arange(len(average_trajectory[0])) * pn.bin_width  # assuming all trajectories have the same length\n",
    "\n",
    "for i, x in enumerate(average_trajectory):\n",
    "    ax.plot(time, x, label=f'Dim {i+1}')\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## why poor performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neural_data_analysis.neural_analysis_tools.gpfa_methods.time_resolved_regression as time_resolved_regression\n",
    "\n",
    "# 1. Print number of trials per timepoint\n",
    "time_resolved_regression.print_trials_per_timepoint(pn.gpfa_neural_trials)\n",
    "\n",
    "# 2. Check for NaNs\n",
    "time_resolved_regression.check_for_nans_in_trials(pn.gpfa_neural_trials, name='latent')\n",
    "time_resolved_regression.check_for_nans_in_trials(pn.behav_trials, name='behavioral')\n",
    "\n",
    "# 3. Standardize trials\n",
    "latent_trials_std = time_resolved_regression.standardize_trials(pn.gpfa_neural_trials)\n",
    "behav_trials_std = time_resolved_regression.standardize_trials(pn.behav_trials)\n",
    "\n",
    "# 4. Plot latent and behavioral variables for a few trials\n",
    "time_resolved_regression.plot_latents_and_behav_trials(latent_trials_std, behav_trials_std, pn.bin_width, n_trials=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyperparams (still need to debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop! # this section is not finished yet\n",
    "\n",
    "# grid search\n",
    "\n",
    "import itertools\n",
    "from joblib import Parallel, delayed, cpu_count\n",
    "print(f\"Detected CPU cores: {cpu_count()}\")\n",
    "\n",
    "# # can add for smoothing:\n",
    "# # other forms of smoothing like (currently it's only uniform_filter1d)\n",
    "# from scipy.ndimage import gaussian_filter1d\n",
    "# # gpfa_neural_trials: list of trials, each trial shape (time_bins, n_neurons)\n",
    "# smoothed_trials = [\n",
    "#     gaussian_filter1d(trial, sigma=smooth_sigma, axis=0)\n",
    "#     for trial in gpfa_neural_trials\n",
    "# ]\n",
    "\n",
    "\n",
    "# Define your grid\n",
    "smoothing_windows = [1, 3]\n",
    "use_sqrt = [True, False]\n",
    "gpfa_dims = [3, 5]\n",
    "bin_widths = [0.02]\n",
    "ridge_alphas = [0.1, 1]\n",
    "regression_types = ['ridge']\n",
    "align_at_beginning_opts = [True]\n",
    "pca_components = [5, 10]\n",
    "\n",
    "param_grid_gpfa = list(itertools.product(\n",
    "    smoothing_windows, use_sqrt, gpfa_dims, bin_widths, ridge_alphas, regression_types, align_at_beginning_opts\n",
    "))\n",
    "\n",
    "# Baseline configs\n",
    "param_grid_raw = list(itertools.product(\n",
    "    smoothing_windows, use_sqrt, bin_widths, ridge_alphas, regression_types, align_at_beginning_opts\n",
    "))\n",
    "param_grid_pca = list(itertools.product(\n",
    "    smoothing_windows, use_sqrt, bin_widths, ridge_alphas, regression_types, align_at_beginning_opts, pca_components\n",
    "))\n",
    "\n",
    "# Run GPFA grid\n",
    "results_gpfa = Parallel(n_jobs=-1, verbose=10)(\n",
    "    delayed(gpfa_tuning.run_gpfa_experiment_time_resolved)(\n",
    "        dec, smoothing, sqrt, gpfa_dim, bin_width, ridge_alpha, regression_type, align_at_beginning, baseline=None\n",
    "    )\n",
    "    for (smoothing, sqrt, gpfa_dim, bin_width, ridge_alpha, regression_type, align_at_beginning) in param_grid_gpfa\n",
    ")\n",
    "\n",
    "# Run raw baseline grid\n",
    "results_raw = Parallel(n_jobs=-1, verbose=10)(\n",
    "    delayed(gpfa_tuning.run_gpfa_experiment_time_resolved)(\n",
    "        dec, smoothing, sqrt, None, bin_width, ridge_alpha, regression_type, align_at_beginning, baseline='raw'\n",
    "    )\n",
    "    for (smoothing, sqrt, bin_width, ridge_alpha, regression_type, align_at_beginning) in param_grid_raw\n",
    ")\n",
    "\n",
    "# Run PCA baseline grid\n",
    "results_pca = Parallel(n_jobs=-1, verbose=10)(\n",
    "    delayed(gpfa_tuning.run_gpfa_experiment_time_resolved)(\n",
    "        dec, smoothing, sqrt, None, bin_width, ridge_alpha, regression_type, align_at_beginning, baseline='pca', pca_components=pca_comp\n",
    "    )\n",
    "    for (smoothing, sqrt, bin_width, ridge_alpha, regression_type, align_at_beginning, pca_comp) in param_grid_pca\n",
    ")\n",
    "\n",
    "# Combine all results\n",
    "results_summary = results_gpfa + results_raw + results_pca\n",
    "df = pd.DataFrame(results_summary)\n",
    "print(df.sort_values('mean_r2', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "best = df.iloc[df['mean_r2'].idxmax()]\n",
    "plt.plot(best['times'], np.nanmean(np.array(best['r2_by_time']), axis=1))\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Mean R²')\n",
    "plt.title(f\"Best config: {best['model']} R² by time\")\n",
    "plt.show()\n",
    "\n",
    "# Compare models\n",
    "import seaborn as sns\n",
    "sns.catplot(data=df, x='model', y='mean_r2', kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML to decode single vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural_data = pn.x_var_lags\n",
    "# behavioral_data = pn.y_var_reduced\n",
    "\n",
    "neural_data = pn.concat_neural_trials\n",
    "behavioral_data = pn.concat_behav_trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_data_analysis.neural_analysis_tools.model_neural_data import transform_vars, ml_decoder_class, neural_data_modeling, drop_high_corr_vars, drop_high_vif_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General usage for any behavioral variable\n",
    "decoder = ml_decoder_class.MLBehavioralDecoder()\n",
    "models_to_use=['rf', 'nn', 'lr']\n",
    "successful_decodings = {}\n",
    "\n",
    "for var in ['nxt_ff_rel_y', 'nxt_ff_distance']:\n",
    "    result = decoder.decode_variable(neural_data, behavioral_data, var, models_to_use=models_to_use)\n",
    "    if result is not None:\n",
    "        successful_decodings[var] = result\n",
    "\n",
    "best_model, best_results = decoder.get_best_model('target_rel_y', 'test_r2')\n",
    "\n",
    "# Plot rf results for any variable\n",
    "decoder.plot_ml_results('target_rel_y', 'rf')\n",
    "\n",
    "successful_decodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compare different Models\n",
    "\n",
    "Let's compare the performance of different machine learning models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_data_analysis.topic_based_neural_analysis.target_decoder import behav_features_to_keep, target_decoder_class, prep_target_decoder, eval_target_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df = eval_target_decoder.compare_models(successful_decodings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot feature importance for RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature importance for Random Forest models\n",
    "for target_var in successful_decodings.keys():\n",
    "    if 'rf' in successful_decodings[target_var]:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"FEATURE IMPORTANCE: {target_var}\")\n",
    "        print('='*50)\n",
    "        \n",
    "        rf_model = successful_decodings[target_var]['rf']['model']\n",
    "        \n",
    "        if hasattr(rf_model, 'feature_importances_'):\n",
    "            # Get feature importance\n",
    "            importance_df = regression_utils._get_rf_feature_importances(rf_model, pn.neural_data.columns)\n",
    "            # Show top 10 most important features\n",
    "            print(f\"Top 10 most important neurons for {target_var}:\")\n",
    "            print(importance_df.head(10))\n",
    "            \n",
    "            # Plot feature importance\n",
    "            regression_utils.plot_feature_importance(importance_df, target_var)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Results (have yet to try)\n",
    "\n",
    "Finally, let's save our results for future analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import Dict, Any\n",
    "\n",
    "def create_experiment_info(decoder, monkey: str, session: str) -> Dict[str, Any]:\n",
    "    \"\"\"Create experiment information dictionary.\"\"\"\n",
    "    return {\n",
    "        'monkey': monkey,\n",
    "        'session': session,\n",
    "        'bin_width': decoder.bin_width,\n",
    "        'neural_data_shape': decoder.neural_data.shape,\n",
    "        'target_data_shape': decoder.target_data.shape\n",
    "    }\n",
    "\n",
    "def create_cca_results(decoder) -> Dict[str, Any]:\n",
    "    \"\"\"Create CCA results summary.\"\"\"\n",
    "    return {\n",
    "        'top_3_correlations': (\n",
    "            decoder.results['cca']['canonical_correlations'][:3].tolist() \n",
    "            if 'cca' in decoder.results else None\n",
    "        )\n",
    "    }\n",
    "\n",
    "def find_best_performances(successful_decodings: Dict) -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"Find best performing model for each target variable.\"\"\"\n",
    "    best_performances = {}\n",
    "    for target_var, models in successful_decodings.items():\n",
    "        best_model = None\n",
    "        best_score = -1\n",
    "        \n",
    "        for model_name, results in models.items():\n",
    "            score = results.get('test_r2', results.get('test_accuracy', results.get('cv_mean', 0)))\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_model = model_name\n",
    "        \n",
    "        best_performances[target_var] = {\n",
    "            'best_model': best_model,\n",
    "            'best_score': best_score\n",
    "        }\n",
    "    return best_performances\n",
    "\n",
    "def create_summary_report(decoder, successful_decodings: Dict, monkey: str, session: str) -> Dict[str, Any]:\n",
    "    \"\"\"Create complete summary report.\"\"\"\n",
    "    return {\n",
    "        'experiment_info': create_experiment_info(decoder, monkey, session),\n",
    "        'cca_results': create_cca_results(decoder),\n",
    "        'ml_results_summary': {\n",
    "            'successful_targets': list(successful_decodings.keys()),\n",
    "            'best_performances': find_best_performances(successful_decodings)\n",
    "        }\n",
    "    }\n",
    "\n",
    "def print_summary_report(summary_report: Dict[str, Any]):\n",
    "    \"\"\"Print formatted summary report.\"\"\"\n",
    "    print(\"\\nEXPERIMENT SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Neural data shape: {summary_report['experiment_info']['neural_data_shape']}\")\n",
    "    print(f\"Target data shape: {summary_report['experiment_info']['target_data_shape']}\")\n",
    "    \n",
    "    if summary_report['cca_results']['top_3_correlations']:\n",
    "        print(f\"Top 3 CCA correlations: {summary_report['cca_results']['top_3_correlations']}\")\n",
    "    \n",
    "    print(f\"Successfully decoded targets: {summary_report['ml_results_summary']['successful_targets']}\")\n",
    "    \n",
    "    print(\"\\nBest model performance for each target:\")\n",
    "    for target, perf in summary_report['ml_results_summary']['best_performances'].items():\n",
    "        print(f\"  {target}: {perf['best_model']} (score: {perf['best_score']:.4f})\")\n",
    "\n",
    "def save_experiment_results(decoder, successful_decodings: Dict, monkey: str, session: str, \n",
    "                          base_filename: str = None):\n",
    "    \"\"\"Save both detailed results and summary report.\"\"\"\n",
    "    if base_filename is None:\n",
    "        base_filename = f\"target_decoding_results_{monkey}_{session}\"\n",
    "    \n",
    "    pkl_filename = f\"{base_filename}.pkl\"\n",
    "    json_filename = f\"{base_filename}_summary.json\"\n",
    "    \n",
    "    # Save detailed results\n",
    "    print(\"Saving results...\")\n",
    "    decoder.save_results(pkl_filename)\n",
    "    \n",
    "    # Create and save summary report\n",
    "    summary_report = create_summary_report(decoder, successful_decodings, monkey, session)\n",
    "    print_summary_report(summary_report)\n",
    "    \n",
    "    with open(json_filename, 'w') as f:\n",
    "        json.dump(summary_report, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nResults saved to: {pkl_filename}\")\n",
    "    print(f\"Summary saved to: {json_filename}\")\n",
    "    \n",
    "    return pkl_filename, json_filename\n",
    "\n",
    "def load_experiment_results(base_filename: str = None, monkey: str = None, session: str = None):\n",
    "    \"\"\"Load both detailed results and summary report.\"\"\"\n",
    "    if base_filename is None:\n",
    "        if monkey and session:\n",
    "            base_filename = f\"target_decoding_results_{monkey}_{session}\"\n",
    "        else:\n",
    "            raise ValueError(\"Must provide either base_filename or both monkey and session\")\n",
    "    \n",
    "    pkl_filename = f\"{base_filename}.pkl\"\n",
    "    json_filename = f\"{base_filename}_summary.json\"\n",
    "    \n",
    "    try:\n",
    "        # Load detailed results\n",
    "        with open(pkl_filename, 'rb') as f:\n",
    "            decoder_results = pickle.load(f)\n",
    "        \n",
    "        # Load summary report\n",
    "        with open(json_filename, 'r') as f:\n",
    "            summary_report = json.load(f)\n",
    "        \n",
    "        print(f\"Loaded results from: {pkl_filename}\")\n",
    "        print(f\"Loaded summary from: {json_filename}\")\n",
    "        \n",
    "        return decoder_results, summary_report\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"File not found: {e}\")\n",
    "        return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading results: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# --- Usage Examples ---\n",
    "\n",
    "# Saving (replaces your original code):\n",
    "# save_experiment_results(decoder, successful_decodings, 'Bruno', 'data_0328')\n",
    "\n",
    "# Loading:\n",
    "# decoder_results, summary_report = load_experiment_results(monkey='Bruno', session='data_0328')\n",
    "# OR\n",
    "# decoder_results, summary_report = load_experiment_results(base_filename=\"target_decoding_results_bruno_0328\")\n",
    "\n",
    "# If you want to print the loaded summary:\n",
    "# if summary_report:\n",
    "#     print_summary_report(summary_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.x_var.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_utils.check_na_in_df(pn.planning_data_by_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.planning_data_by_point.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_utils.check_na_in_df(pn.planning_data_by_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_utils.check_na_in_df(pn.ctrl_inst.both_ff_across_time_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## analyze_trial_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_data_analysis.neural_analysis_tools.gpfa_methods import fix_gpfa_trial_length\n",
    "# First, analyze the trial lengths\n",
    "analysis = fix_gpfa_trial_length.analyze_trial_lengths(pn.spiketrains)\n",
    "print(\"Trial length analysis:\")\n",
    "for key, value in analysis.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ff_venv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
