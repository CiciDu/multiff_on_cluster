{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "import os, sys\n",
    "for p in [Path.cwd()] + list(Path.cwd().parents):\n",
    "    if p.name == 'Multifirefly-Project':\n",
    "        os.chdir(p)\n",
    "        sys.path.insert(0, str(p / 'multiff_analysis/multiff_code/methods'))\n",
    "        break\n",
    "    \n",
    "import sys\n",
    "from data_wrangling import specific_utils, process_monkey_information\n",
    "from pattern_discovery import pattern_by_trials, pattern_by_trials, cluster_analysis, organize_patterns_and_features\n",
    "from visualization.matplotlib_tools import plot_behaviors_utils\n",
    "from neural_data_analysis.neural_analysis_tools.get_neural_data import neural_data_processing\n",
    "from neural_data_analysis.neural_analysis_tools.visualize_neural_data import plot_neural_data, plot_modeling_result\n",
    "from neural_data_analysis.neural_analysis_tools.model_neural_data import transform_vars, neural_data_modeling, drop_high_corr_vars, drop_high_vif_vars\n",
    "from neural_data_analysis.topic_based_neural_analysis.neural_vs_behavioral import prep_monkey_data, prep_target_data, neural_vs_behavioral_class\n",
    "from neural_data_analysis.topic_based_neural_analysis.planning_and_neural import planning_and_neural_class, pn_utils\n",
    "\n",
    "\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "import seaborn as sns\n",
    "from matplotlib import rc\n",
    "from os.path import exists\n",
    "from numpy import random\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from importlib import reload\n",
    "import neo\n",
    "import gc\n",
    "\n",
    "import sys\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import torch\n",
    "from numpy import pi\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import scipy.linalg as linalg\n",
    "import scipy.interpolate as interpolate\n",
    "from scipy.signal import fftconvolve\n",
    "\n",
    "\n",
    "from scipy.io import loadmat\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sys\n",
    "\n",
    "import scipy.sparse as sparse\n",
    "import matplotlib.pylab as plt\n",
    "import subprocess\n",
    "\n",
    "plt.rcParams[\"animation.html\"] = \"html5\"\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "rc('animation', html='jshtml')\n",
    "matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n",
    "matplotlib.rcParams['animation.embed_limit'] = 2**128\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "np.set_printoptions(suppress=True)\n",
    "print(\"done\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Bruno/data_0330\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Schro/data_0416\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_item = neural_vs_behavioral_class.NeuralVsBehavioralClass(raw_data_folder_path=raw_data_folder_path)\n",
    "data_item.streamline_preparing_neural_and_behavioral_data()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore neural data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare x_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_var = data_item.binned_spikes_matrix.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_var_convolved = neural_data_processing.convolve_neural_data(x_var, kernel_len=7)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_item._get_y_var_lags()\n",
    "data_item.y_var_lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_data = np.concatenate([np.arange(20).reshape(-1,1), np.arange(1, 21).reshape(-1,1)], axis=1)\n",
    "synthetic_data_lags = neural_data_processing.add_lags_to_each_feature(synthetic_data, lag_numbers)\n",
    "synthetic_data_lags"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## firing rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get convolved windows\n",
    "all_windows = np.repeat(data_item.bin_width, len(x_var_convolved))\n",
    "convolved_windows = np.convolve(all_windows, data_item.convolve_pattern, 'same')\n",
    "\n",
    "# apply convolution to every column in binned_spikes_df\n",
    "firing_rate_df = data_item.binned_spikes_df.apply(lambda x: np.convolve(x, data_item.convolve_pattern, 'same')/convolved_windows, axis=0)\n",
    "firing_rate_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "CDYYJFqxzU_A"
   },
   "source": [
    "## Plot interspike intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_spikes = 1000\n",
    "plt.plot(range(num_spikes), np.diff(data_item.spikes_df['time'][:num_spikes+1].values))\n",
    "plt.title(\"Interspike intervals\")\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "bRYWkXndz0wy"
   },
   "source": [
    "## Differentiate spikes based on clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "aborted",
     "timestamp": 1685702046406,
     "user": {
      "displayName": "Cici Du",
      "userId": "17701548280142155870"
     },
     "user_tz": -480
    },
    "id": "DQw-RdKJ-3n2"
   },
   "outputs": [],
   "source": [
    "spikes_to_plot = range(10000, 10500)\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(data_item.spikes_df.time[spikes_to_plot], data_item.spikes_df.cluster[spikes_to_plot], s=2)\n",
    "# fix the yticks to be the cluster names\n",
    "plt.yticks(data_item.spikes_df.cluster[spikes_to_plot].unique(), data_item.spikes_df.cluster[spikes_to_plot].unique())\n",
    "plt.title(\"Spikes\")\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation between spike clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_threshold_to_mark = 0.5\n",
    "corr_coeff = pd.DataFrame(data_item.all_binned_spikes).corr()\n",
    "corr_coeff_matrix = corr_coeff.values\n",
    "\n",
    "# Take out the indices in the correlation matrix of the values that are above the threshold\n",
    "indices = np.where(np.abs(corr_coeff_matrix) > corr_threshold_to_mark)\n",
    "high_corr_df = pd.DataFrame({'cluster1': indices[0], 'cluster2': indices[1], 'corr_value': corr_coeff_matrix[indices]})\n",
    "# delete the rows where cluster1 == cluster2\n",
    "high_corr_df = high_corr_df[high_corr_df.cluster1 != high_corr_df.cluster2]\n",
    "high_corr_df.sort_values(by='corr_value', ascending=False, inplace=True)\n",
    "\n",
    "# also, delete the duplicates\n",
    "high_corr_df['both_clusters'] = high_corr_df.apply(lambda x: tuple(sorted([x['cluster1'], x['cluster2']])), axis=1)\n",
    "high_corr_df.drop_duplicates(subset='both_clusters', inplace=True)\n",
    "high_corr_df.drop(columns='both_clusters', inplace=True)\n",
    "high_corr_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a heatmap if it will not be too large\n",
    "if corr_coeff.shape[0] < 30:\n",
    "    plt.figure(figsize = (15, 15))\n",
    "    sns.heatmap(corr_coeff, cmap='coolwarm', annot=True, linewidths=1, vmin=-1)\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relating neural data to other variables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "83ZmaxlcdFKQ"
   },
   "source": [
    "### Catching a target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### individual instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_to_sample_from = data_item.ff_caught_T_new\n",
    "plot_neural_data.make_individual_spike_plots(time_to_sample_from, data_item.spikes_df, data_item.unique_clusters, \n",
    "                                                  max_plots=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### overlaid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_to_sample_from = data_item.ff_caught_T_new\n",
    "plot_neural_data.make_overlaid_spike_plot(time_to_sample_from, data_item.spikes_df, data_item.unique_clusters, \n",
    "                                               max_rows_to_plot=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "5X1qMHv38AQX"
   },
   "source": [
    "### Stop (whether or not resulting in a capture?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_to_sample_from = data_item.monkey_information[data_item.monkey_information['monkey_speeddummy'] == 0].time.values\n",
    "plot_neural_data.make_overlaid_spike_plot(time_to_sample_from, data_item.spikes_df, data_item.unique_clusters, \n",
    "                                               max_rows_to_plot=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_to_sample_from = data_item.monkey_information[data_item.monkey_information['speed'] > 100].time.values\n",
    "plot_neural_data.make_overlaid_spike_plot(time_to_sample_from, data_item.spikes_df, data_item.unique_clusters, \n",
    "                                               max_rows_to_plot=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visible_before_last_one instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_i = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_item.make_or_retrieve_target_clust_last_vis_df()\n",
    "max_plots = 2\n",
    "\n",
    "data_item.target_cluster_VBLO = pattern_by_trials.find_target_cluster_visible_before_last_one(data_item.target_clust_last_vis_df, data_item.ff_caught_T_new)\n",
    "plot_neural_data.make_individual_spike_plot_from_target_cluster_VBLO(data_item.target_cluster_VBLO, data_item.spikes_df, data_item.unique_clusters, starting_row=current_i, max_plots=max_plots)\n",
    "current_i += max_plots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### overlaid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_to_sample_from = data_item.target_cluster_VBLO['caught_time'].values\n",
    "plot_neural_data.make_overlaid_spike_plot(time_to_sample_from, data_item.spikes_df, data_item.unique_clusters, \n",
    "                                               max_rows_to_plot=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "8lytlMUj9kkB"
   },
   "source": [
    "### Just try it randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_to_sample_from = random.uniform(100, 1000, 100)\n",
    "plot_neural_data.make_overlaid_spike_plot(time_to_sample_from, data_item.spikes_df, data_item.unique_clusters, \n",
    "                                               max_rows_to_plot=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See other neural data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plx \n",
    "(it's better to use Matlab to get time offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exists('/Volumes/Elements/multiff/Bruno/U-probe/7a/Mar 30 2018/neural data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pandas as pd\n",
    "import neo\n",
    "\n",
    "# Define file paths and file name\n",
    "original_file_path = '/Volumes/Elements/multiff/Bruno/U-probe/7a/Mar 30 2018/neural data'\n",
    "new_file_path = '/Users/dusiyi/Downloads/neural_data_temp_folder/'\n",
    "file_name = 'm51s022_ead'\n",
    "fname = os.path.join(original_file_path, file_name + '.plx')\n",
    "\n",
    "# Function to read event timestamps from a .plx file\n",
    "def plx_event_ts_modified(filename, ch):\n",
    "    reader = neo.io.PlexonIO(filename)\n",
    "    block = reader.read_block()\n",
    "    event_channel = block.segments[0].eventarrays[ch-1]  # ch is 1-based, neo is 0-based\n",
    "    ts = event_channel.times.rescale('s').magnitude  # Convert to seconds\n",
    "    sv = event_channel.labels\n",
    "    freq = reader.header['signal_channels'][0][2]  # Assuming the frequency is stored here\n",
    "    return len(ts), ts, sv, freq\n",
    "\n",
    "# Read the event timestamps\n",
    "n, ts, sv, freq = plx_event_ts_modified(fname, 257)\n",
    "ts_s = ts / freq\n",
    "\n",
    "# Create a DataFrame for aligning data\n",
    "neural_event_time = pd.DataFrame({'label': sv, 'timestamp': ts, 'time': ts_s})\n",
    "\n",
    "# Write the DataFrame to a text file\n",
    "output_file = os.path.join(new_file_path, file_name + '.txt')\n",
    "neural_event_time.to_csv(output_file, index=False, sep='\\t')\n",
    "\n",
    "# Display the contents of the text file\n",
    "with open(output_file, 'r') as file:\n",
    "    print(file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns6_path = '/Users/dusiyi/Documents/Multifirefly-Project/all_monkey_data/neural_data/monkey_Schro/data_0416/Sorted/m53s453.nev'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = neo.io.BlackrockIO(filename=ns6_path)\n",
    "reader.parse_header()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block = reader.read_block()\n",
    "event_data = block.segments[0].events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for event in event_data:\n",
    "    if event.name =='digital_input_port':\n",
    "        event_df = pd.DataFrame({'time': event.times, 'label': event.labels})\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if label == 1 occurs within the first 10 rows, then save the first 10 rows of event df\n",
    "if event_df.loc[event_df['label']=='1'].index.min() < 10:\n",
    "    event_df.iloc[:10].to_csv('event_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get the precise time offset\n",
    "event_df.loc[event_df['label']=='1', 'time'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ccf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccf_path = '/Volumes/Elements/multiff/Schro/Utah Array/MultiFirefly/Apr 02 2018/neural data/m53s412.ccf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccf_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'path/to/your/file.ccf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(ccf_path, 'r') as file:\n",
    "        for line in file:\n",
    "            print(line.strip())  # Process each line as needed\n",
    "except FileNotFoundError:\n",
    "    print(f\"The file at {ccf_path} was not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### other files in sorted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_neural_data_path = os.path.join(raw_data_folder_path, 'neural_data/Sorted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_neural_data_path = '/Users/dusiyi/Documents/Multifirefly-Project/all_monkey_data/neural_data/monkey_Schro/data_0416/Sorted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_neural_data_path = '/Users/dusiyi/Documents/Multifirefly-Project/all_monkey_data/neural_data/monkey_Bruno/data_0330/Sorted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(sorted_neural_data_path, \"spike_times.npy\")\n",
    "template_feature_ind = np.load(filepath)\n",
    "print(template_feature_ind.shape)\n",
    "template_feature_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See other data\n",
    "filepath = os.path.join(sorted_neural_data_path, 'cluster_KSLabel.tsv')\n",
    "cluster_KSLabel=pd.read_csv(filepath,sep='\\t')\n",
    "cluster_KSLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(sorted_neural_data_path, 'cluster_ContamPct.tsv')\n",
    "cluster_ContamPct=pd.read_csv(filepath,sep='\\t')\n",
    "cluster_ContamPct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(sorted_neural_data_path, 'cluster_Amplitude.tsv')\n",
    "cluster_Amplitude=pd.read_csv(filepath,sep='\\t')\n",
    "cluster_Amplitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(sorted_neural_data_path, \"pc_features.npy\")\n",
    "pc_feature_ind = np.load(filepath)\n",
    "print(\"shape:\", pc_feature_ind.shape)\n",
    "pc_feature_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(sorted_neural_data_path, \"pc_feature_ind.npy\")\n",
    "pc_feature_ind = np.load(filepath)\n",
    "print(pc_feature_ind.shape)\n",
    "pc_feature_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_feature_ind.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(sorted_neural_data_path, \"channel_map.npy\")\n",
    "channel_map = np.load(filepath)\n",
    "print(channel_map.shape)\n",
    "channel_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_map+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(sorted_neural_data_path, \"channel_positions.npy\")\n",
    "channel_positions = np.load(filepath)\n",
    "print(channel_positions.shape)\n",
    "channel_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot\n",
    "plt.scatter(channel_positions[:, 0], channel_positions[:, 1], s=10)\n",
    "plt.xlabel('X Position (µm)')\n",
    "plt.ylabel('Y Position (µm)')\n",
    "plt.title('Electrode Layout')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(sorted_neural_data_path, \"amplitudes.npy\")\n",
    "amplitudes = np.load(filepath)\n",
    "print(amplitudes.shape)\n",
    "amplitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(sorted_neural_data_path, \"whitening_mat.npy\")\n",
    "whitening_mat = np.load(filepath)\n",
    "print(whitening_mat.shape)\n",
    "whitening_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(sorted_neural_data_path, \"whitening_mat_inv.npy\")\n",
    "whitening_mat_inv = np.load(filepath)\n",
    "print(whitening_mat_inv.shape)\n",
    "whitening_mat_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(sorted_neural_data_path, \"templates.npy\")\n",
    "templates = np.load(filepath)\n",
    "print(templates.shape)\n",
    "templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(templates.shape[1]):\n",
    "    print(k, templates[:,k,:].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(sorted_neural_data_path, \"templates_ind.npy\")\n",
    "templates_ind = np.load(filepath)\n",
    "print(templates_ind.shape)\n",
    "templates_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(sorted_neural_data_path, \"template_features.npy\")\n",
    "template_features = np.load(filepath)\n",
    "print(template_features.shape)\n",
    "template_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(sorted_neural_data_path, \"template_feature_ind.npy\")\n",
    "template_feature_ind = np.load(filepath)\n",
    "print(template_feature_ind.shape)\n",
    "template_feature_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_feature_ind.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(sorted_neural_data_path, \"similar_templates.npy\")\n",
    "similar_templates = np.load(filepath)\n",
    "print(similar_templates.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_templates.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(sorted_neural_data_path, \"spike_times.npy\")\n",
    "spike_times = np.load(filepath)\n",
    "print(spike_times.shape)\n",
    "spike_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(sorted_neural_data_path, \"spike_templates.npy\")\n",
    "spike_templates = np.load(filepath)\n",
    "print(spike_templates.shape)\n",
    "spike_templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(sorted_neural_data_path, \"spike_clusters.npy\")\n",
    "spike_clusters = np.load(filepath)\n",
    "print(spike_clusters.shape)\n",
    "spike_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.where((spike_templates == spike_clusters))[0])/len(spike_templates.reshape(-1))\n",
    "# So the two arrays are exactly the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spike_clusters.reshape(-1).min(), spike_clusters.reshape(-1).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "filepath = os.path.join(sorted_neural_data_path, \"QualityMetr.mat\")\n",
    "QualityMetr = {}\n",
    "f = h5py.File(filepath)\n",
    "for k, v in f.items():\n",
    "    QualityMetr[k] = np.array(v)\n",
    "QualityMetr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(sorted_neural_data_path, \"rez.mat\")\n",
    "arrays = {}\n",
    "f = h5py.File(filepath)\n",
    "for k, v in f.items():\n",
    "    arrays[k] = np.array(v)\n",
    "arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get mapping tables\n",
    "(The mapping tables relate the paths on the hard drive to the local paths.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bruno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monkey_name = 'monkey_Bruno'\n",
    "hdrive_dir = '/Volumes/Elements/multiff/Bruno/U-probe/7a'\n",
    "neural_data_folder_name = 'neural data'\n",
    "filter_neural_file_func = lambda x: [f for f in x if ('plx' in f) & ('ead' not in f)]\n",
    "bruno_mapping_table = neural_data_processing.get_mapping_table_between_hard_drive_and_local_folders(monkey_name, hdrive_dir, neural_data_folder_name, filter_neural_file_func)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code will work:\n",
    "os.listdir('/Volumes/Elements/multiff/Bruno/U-probe/7a')\n",
    "\n",
    "# # The code below will return \"Invalid argument,\" and I'm still not exactly sure hwy\n",
    "# os.listdir('/Volumes/Elements/multiff/Bruno/U-probe/7a/Mar 30 2018/neural data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monkey_name = 'monkey_Schro'\n",
    "hdrive_dir = '/Volumes/Elements/multiff/Schro/Utah Array/MultiFirefly'\n",
    "neural_data_folder_name = 'neural data/Sorted'\n",
    "filter_neural_file_func = lambda x: [f for f in x if ('nev' in f)]\n",
    "schro_mapping_table = neural_data_processing.get_mapping_table_between_hard_drive_and_local_folders(monkey_name, hdrive_dir, neural_data_folder_name, filter_neural_file_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add time_offset_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for monkey in ['schro', 'bruno']:\n",
    "    mapping_table = pd.read_csv(f'/Users/dusiyi/Documents/Multifirefly-Project/multiff_analysis/multiff_code/methods/eye_position_analysis/neural_data_analysis/get_neural_data/{monkey}_mapping_table.csv')\n",
    "    mapping_table['neural_event_time_path'] = mapping_table['local_path'].apply(lambda x: os.path.join(x.replace('neural_data', 'time_calibration'), 'neural_event_time.txt'))\n",
    "    mapping_table.to_csv('/Users/dusiyi/Documents/Multifirefly-Project/multiff_analysis/multiff_code/methods/eye_position_analysis/neural_data_analysis/get_neural_data/{monkey}_mapping_table.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### change column names in time_offset.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in mapping_table.iterrows():\n",
    "    neural_event_time = row['neural_event_time_path']\n",
    "    neural_event_time.replace('neural_data', 'processed_neural_data')\n",
    "    if exists(neural_event_time):\n",
    "        neural_event_time = pd.read_csv(neural_event_time)\n",
    "        neural_event_time.rename(columns={'sv': 'label',\n",
    "                                       'ts': 'timestamp',\n",
    "                                        'ts_s': 'time'}, inplace=True)\n",
    "        neural_event_time.to_csv(neural_event_time, index=False)\n",
    "        print(neural_event_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get neural_event_time_path.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bruno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main code is in '/Users/dusiyi/Documents/Multifirefly-Project/multiff_analysis/multiff_code/methods/eye_position_analysis/neural_data_analysis/MATLAB_processing/AlignNeuralDataXSessions.m'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in schro_mapping_table.iterrows():\n",
    "    ns6_path = row['hdrive_path']\n",
    "    neural_event_time_path = row['neural_event_time_path']\n",
    "    if not exists(neural_event_time_path):\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(neural_event_time_path), exist_ok=True)\n",
    "            print(ns6_path)\n",
    "            # print(neural_event_time_path)\n",
    "\n",
    "            reader = neo.io.BlackrockIO(filename=ns6_path)\n",
    "            reader.parse_header()\n",
    "            block = reader.read_block()\n",
    "            event_data = block.segments[0].events\n",
    "            for event in event_data:\n",
    "                if event.name =='digital_input_port':\n",
    "                    event_df = pd.DataFrame({'time': event.times, 'label': event.labels})\n",
    "                    break\n",
    "            event_df.to_csv(neural_event_time_path, index=False)\n",
    "        except Exception as e:\n",
    "            print(f'Error: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since problem are encountered for some sessions, one can extract time_offset.txt manually in matlab. See code in:\n",
    "/Users/dusiyi/Documents/Multifirefly-Project/multiff_analysis/multiff_code/methods/eye_position_analysis/neural_data_analysis/MATLAB_processing/ManuallyExtractTimeOffset.m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### from a specific session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns6_path = '/Users/dusiyi/Documents/MATLAB/m53s436.nev'\n",
    "neural_event_time_path = '/Users/dusiyi/Documents/Multifirefly-Project/all_monkey_data/time_calibration/monkey_Schro/data_0410'\n",
    "if not exists(neural_event_time_path):\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(neural_event_time_path), exist_ok=True)\n",
    "        print(ns6_path)\n",
    "        # print(neural_event_time_path)\n",
    "\n",
    "        reader = neo.io.BlackrockIO(filename=ns6_path)\n",
    "        reader.parse_header()\n",
    "        block = reader.read_block()\n",
    "        event_data = block.segments[0].events\n",
    "        for event in event_data:\n",
    "            if event.name =='digital_input_port':\n",
    "                event_df = pd.DataFrame({'time': event.times, 'label': event.labels})\n",
    "                break\n",
    "        event_df.to_csv(neural_event_time_path, index=False)\n",
    "    except Exception as e:\n",
    "        print(f'Error: {e}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "1psmc3cUQqVO0oqJ3z0nmI4LsHhCWLYZR",
     "timestamp": 1682806564169
    },
    {
     "file_id": "162-xByx3iTk35YJ06VcUmsXt23wEm77H",
     "timestamp": 1681009433942
    }
   ],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "ff_venv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
