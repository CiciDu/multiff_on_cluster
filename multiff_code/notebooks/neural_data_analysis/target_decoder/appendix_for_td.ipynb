{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sparsity of neural data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec.binned_spikes_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect neural data\n",
    "\n",
    "bins = dec.binned_spikes_df\n",
    "\n",
    "# Calculate percentage of non-zero rows for each column\n",
    "non_zero_percentages = (bins != 0).mean() * 100\n",
    "\n",
    "# Create a DataFrame with the results\n",
    "non_zero_df = pd.DataFrame({\n",
    "    'Column': non_zero_percentages.index,\n",
    "    'Percent_Non_Zero': non_zero_percentages.values\n",
    "})\n",
    "\n",
    "# Sort by percentage in descending order\n",
    "non_zero_df = non_zero_df.sort_values('Percent_Non_Zero', ascending=False)\n",
    "\n",
    "print(\"Percentage of non-zero values in each column:\")\n",
    "print(non_zero_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins.drop(columns='bin').mean(axis=1).describe()\n",
    "\n",
    "# plot the percentile of values of mean firing rates across neurons at each time bin\n",
    "mean_rates = bins.drop(columns='bin').mean(axis=1)\n",
    "\n",
    "# Calculate percentiles from 0 to 100\n",
    "percentiles = np.arange(0, 101, 1)\n",
    "percentile_values = np.percentile(mean_rates, percentiles)\n",
    "\n",
    "# Create plot\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(percentiles, percentile_values)\n",
    "plt.xlabel('Percentile')\n",
    "plt.ylabel('Mean Firing Rate')\n",
    "plt.title('Distribution of Mean Firing Rates Across Neurons')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### y var (behavioral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_var_vif = drop_high_vif_vars.get_vif_df(dec.y_var)\n",
    "print(y_var_vif.head(8))\n",
    "\n",
    "# calculate the correlation coefficient among the columns with VIF > 5\n",
    "# specific_columns = vif_df[vif_df['vif'] > 5].feature.values\n",
    "specific_columns = y_var_vif.feature.values[:10]\n",
    "corr_coeff = dec.y_var[specific_columns].corr()\n",
    "#plt.figure(figsize = (6, 6))\n",
    "plt.figure(figsize = (8, 6))\n",
    "sns.heatmap(corr_coeff, cmap='coolwarm', annot=True, linewidths=1, vmin=-1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try y_var_reduced\n",
    "\n",
    "y_var_vif = drop_high_vif_vars.get_vif_df(dec.y_var_reduced)\n",
    "print(y_var_vif.head(8))\n",
    "\n",
    "# calculate the correlation coefficient among the columns with VIF > 5\n",
    "# specific_columns = vif_df[vif_df['vif'] > 5].feature.values\n",
    "specific_columns = y_var_vif.feature.values[:10]\n",
    "corr_coeff = dec.y_var[specific_columns].corr()\n",
    "#plt.figure(figsize = (6, 6))\n",
    "plt.figure(figsize = (8, 6))\n",
    "sns.heatmap(corr_coeff, cmap='coolwarm', annot=True, linewidths=1, vmin=-1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check target_rel_x and y\n",
    "(The look correct after checking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pursuit_sub = dec.pursuit_data.loc[dec.pursuit_data['target_index']==65].copy()\n",
    "pursuit_sub['target_angle_deg'] = pursuit_sub['target_angle'] * 180/pi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pursuit_sub[['point_index', 'target_angle_deg', 'target_distance', 'target_rel_x', 'target_rel_y']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducing columns in lags: experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check for contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dec.y_var_lags_reduced.copy()\n",
    "# for target_feature in vif_df[vif_df['vif'] > 5].feature.values:\n",
    "for index, row in vif_df.iterrows():\n",
    "    target_feature = row.feature\n",
    "    print(f'\\n\\n {target_feature}: VIF = {row.vif}')\n",
    "    contributions = drop_high_vif_vars.check_vif_contribution(df, target_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check subset's vif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use y_var_lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [col for col in dec.y_var_lags.columns if ('LD' in col) or (\n",
    "                'RD' in col) or ('gaze_mky_view_angle' in col)]\n",
    "\n",
    "df_sub = dec.y_var_lags[columns].copy()\n",
    "print(df_sub.columns)\n",
    "sub_vif = drop_high_vif_vars.get_vif_df(df_sub)\n",
    "sub_vif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use y_var_lags_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns = [col for col in dec.y_var_lags_reduced.columns if ('LD' in col) or (\n",
    "#                 'RD' in col) or ('gaze_mky_view_angle' in col)]\n",
    "\n",
    "columns = [col for col in dec.y_var_lags_reduced.columns if ('x_r' in col) or ('y_r' in col)]\n",
    "\n",
    "df_sub = dec.y_var_lags_reduced[columns].copy()\n",
    "print(df_sub.columns)\n",
    "sub_vif = drop_high_vif_vars.get_vif_df(df_sub)\n",
    "sub_vif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## exp on subsets to reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec.get_x_and_y_data_for_modeling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_key_words, all_column_subsets = dec.get_subset_key_words_and_all_column_subsets_for_vif(\n",
    "            dec.y_var_lags_reduced)\n",
    "subset_key_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_rows, na_cols = general_utils.check_na_in_df(dec.y_var)\n",
    "duplicate_rows = general_utils.find_duplicate_rows(dec.y_var, column_subset=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test speed of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run profiler and save output to file\n",
    "cProfile.run('dec.streamline_making_behav_and_neural_data()', 'profile_output')\n",
    "\n",
    "# Load stats and sort by total time\n",
    "p = pstats.Stats('profile_output')\n",
    "p.strip_dirs().sort_stats('tottime').print_stats(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.strip_dirs().sort_stats('tottime').print_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See sizes of biggest variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### in dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pympler import asizeof\n",
    "\n",
    "sizes = []\n",
    "for attr in dir(dec):\n",
    "    if attr.startswith('__') and attr.endswith('__'):\n",
    "        continue  # skip dunder attributes\n",
    "    try:\n",
    "        val = getattr(dec, attr)\n",
    "        size = asizeof.asizeof(val)\n",
    "        sizes.append((attr, size))\n",
    "    except Exception:\n",
    "        pass  # ignore any errors\n",
    "\n",
    "# Sort and display largest attributes in MB\n",
    "for name, size in sorted(sizes, key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{name}: {size / (1024 * 1024):.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import types\n",
    "import warnings\n",
    "from pympler import asizeof\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "\n",
    "filtered = {\n",
    "    k: v for k, v in globals().items()\n",
    "    if not isinstance(v, types.ModuleType)\n",
    "}\n",
    "\n",
    "sizes = []\n",
    "for name, val in filtered.items():\n",
    "    try:\n",
    "        sizes.append((name, asizeof.asizeof(val)))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "for name, size in sorted(sizes, key=lambda x: x[1], reverse=True)[:20]:\n",
    "    print(f\"{name}: {size / (1024 * 1024):.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## why ratio of bin/target_index approaches constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_lengths = dec.pursuit_data[['target_index', 'bin']].groupby('target_index').count()\n",
    "trial_lengths.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = dec.y_var_reduced[['time', 'bin', 'target_index']]\n",
    "sub['factor'] = dec.y_var_reduced['bin']/dec.y_var_reduced['target_index']\n",
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.diff(dec.ff_caught_T_sorted), bins=30)\n",
    "plt.xlabel('Time difference')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of time differences between caught events')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec.ff_caught_T_sorted/np.arange(len(dec.ff_caught_T_sorted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compared with neural_data_modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec = neural_vs_behavioral_class.NeuralVsBehavioralClass(raw_data_folder_path=raw_data_folder_path)\n",
    "dec.streamline_preparing_neural_and_behavioral_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec.final_behavioral_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec.y_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec.y_var_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec.y_var.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec.y_var_reduced.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[col for col in dec.y_var.columns if col not in dec.y_var_reduced.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[col for col in dec.y_var_reduced.columns if col not in dec.y_var.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find out why there are rows of NA in dec.y_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_rows, na_cols = general_utils.check_na_in_df(dec.y_var)\n",
    "duplicate_rows = general_utils.find_duplicate_rows(dec.y_var, column_subset=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec.behav_data_by_bin.loc[118189:118195, ['bin', 'time', 'target_rel_x', 'target_rel_y','time_since_target_last_seen', 'target_last_seen_distance']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare old and new target df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df_ori = pd.read_csv('/Users/dusiyi/Documents/Multifirefly-Project/all_monkey_data/patterns_and_features/monkey_Schro/data_0416/target_df_ori.csv')\n",
    "df = target_df_ori[['target_index', 'point_index', 'time']].copy()\n",
    "for col in ['target_distance', 'time_since_target_last_seen']:\n",
    "    df[f'old_{col}'] = target_df_ori[col]   \n",
    "    df[f'new_{col}'] = dec.target_df[col]  \n",
    "\n",
    "df['old_target_last_seen_distance'] = target_df_ori['target_last_seen_distance_frozen']\n",
    "df['new_target_last_seen_distance'] = dec.target_df['target_last_seen_distance']\n",
    "\n",
    "df2 = df.loc[10068:]\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2[df2['point_index']>= 139910]\n",
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check gpfa's binned spikes vs my own binned spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec.prepare_spikes_for_gpfa(align_at_beginning=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### take out my own binned spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select data for a specific segment\n",
    "seg_index = 20\n",
    "p_sub = dec.pursuit_data[dec.pursuit_data['segment'] == seg]\n",
    "\n",
    "# Get corresponding binned spikes and reassign bin as index\n",
    "binned_spikes_sub = dec.binned_spikes_df[dec.binned_spikes_df['bin'].isin(p_sub['bin'])].copy()\n",
    "binned_spikes_sub['bin'] = binned_spikes_sub.index\n",
    "\n",
    "# Merge in time information from pursuit data\n",
    "binned_spikes_sub2 = binned_spikes_sub.merge(p_sub[['bin', 'time']], on='bin', how='left')\n",
    "\n",
    "# Identify the unit with the highest total spike count\n",
    "cluster_columns = [col for col in binned_spikes_sub2.columns if 'cluster_' in col]\n",
    "cluster_w_most_spikes = binned_spikes_sub2[cluster_columns].sum(axis=0).idxmax()\n",
    "cluster_index = cluster_w_most_spikes.split('_')[-1]\n",
    "\n",
    "# Extract bin, time, and spike count for the most active unit\n",
    "binned_spikes_sub3 = binned_spikes_sub2[['bin', 'time', cluster_w_most_spikes]].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get binned spikes (seqs) from gpfa_utils."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_idx_in_gpfa = np.where(np.sort(dec.spike_segs_df.cluster.unique()) == int(cluster_index))[0][0]\n",
    "seg = dec.spiketrain_corr_segs[seg_index]\n",
    "spiketrain = dec.spiketrains[seg_index][cluster_idx_in_gpfa]\n",
    "seqs = gpfa_util.get_seqs([spiketrain], dec.bin_width_w_unit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bin</th>\n",
       "      <th>time</th>\n",
       "      <th>unit_9</th>\n",
       "      <th>gpfa</th>\n",
       "      <th>same</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5702</td>\n",
       "      <td>114.05650</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5709</td>\n",
       "      <td>114.18956</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>1.41421</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5719</td>\n",
       "      <td>114.38853</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>1.41421</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     bin      time  unit_9    gpfa   same\n",
       "3   5702 114.05650 4.00000 2.00000  False\n",
       "10  5709 114.18956 2.00000 1.41421  False\n",
       "20  5719 114.38853 2.00000 1.41421  False"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trial_length = len(binned_spikes_sub3)\n",
    "if dec.align_at_beginning:\n",
    "    binned_spikes_sub3['gpfa'] = seqs[0][1][0][trial_length:]\n",
    "else:\n",
    "    binned_spikes_sub3['gpfa'] = seqs[0][1][0][-trial_length:] # when getting latent dimension for neural data, [-trial_length:] was also used\n",
    "binned_spikes_sub3['same'] = binned_spikes_sub3[cluster_w_most_spikes] == binned_spikes_sub3['gpfa']\n",
    "binned_spikes_sub3[binned_spikes_sub3['same']!=True]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ff_venv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
