{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if using google drive\n",
    "# %cd /content/drive/MyDrive/ff_repo/Multifirefly-Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os, sys\n",
    "for p in [Path.cwd()] + list(Path.cwd().parents):\n",
    "    if p.name == 'Multifirefly-Project':\n",
    "        os.chdir(p)\n",
    "        sys.path.insert(0, str(p / 'multiff_analysis/multiff_code/methods'))\n",
    "        break\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "from data_wrangling import specific_utils, combine_info_utils\n",
    "from pattern_discovery import pattern_by_trials, pattern_by_trials, cluster_analysis, organize_patterns_and_features, category_class\n",
    "from decision_making_analysis.cluster_replacement import cluster_replacement_utils\n",
    "from decision_making_analysis.decision_making import decision_making_class, decision_making_utils, intended_targets_classes\n",
    "from decision_making_analysis.GUAT import GUAT_collect_info_class, GUAT_combine_info_class\n",
    "from decision_making_analysis.compare_GUAT_and_TAFT import GUAT_vs_TAFT_class, GUAT_vs_TAFT_x_sessions_class, helper_GUAT_vs_TAFT_class\n",
    "from visualization.matplotlib_tools import plot_trials, plot_behaviors_utils\n",
    "from visualization.animation import animation_class\n",
    "from null_behaviors import show_null_trajectory, find_best_arc, curvature_utils, curv_of_traj_utils\n",
    "from machine_learning.ml_methods import regression_utils, classification_utils, prep_ml_data_utils, hyperparam_tuning_class\n",
    "from visualization.plotly_polar_tools import plotly_utils_polar, plotly_for_ff_polar, plotly_for_trajectory_polar\n",
    "from machine_learning.ml_methods import ml_methods_class\n",
    "from machine_learning.ml_methods.advanced_ml_methods import advanced_regression_utils, advanced_classification_utils, reg_feat_importance\n",
    "\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import exists\n",
    "import math\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "from scipy import stats\n",
    "from IPython.display import HTML\n",
    "from matplotlib import rc\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "import os, sys, sys\n",
    "from importlib import reload\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "\n",
    "plt.rcParams[\"animation.html\"] = \"html5\"\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "rc('animation', html='jshtml')\n",
    "matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n",
    "matplotlib.rcParams['animation.embed_limit'] = 2**128\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "np.set_printoptions(suppress=True)\n",
    "pd.options.display.max_rows = 50\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# Predict num stops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5820ef51",
   "metadata": {},
   "source": [
    "# Run overnight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b2338b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict num_stops\n",
    "combined_info_exists_ok = True\n",
    "for monkey_name in ['monkey_Bruno', 'monkey_Schro']:\n",
    "    gc_kwargs_temp = helper_GUAT_vs_TAFT_class.gc_kwargs.copy()\n",
    "    gc_kwargs_temp['num_old_ff_per_row'] = 2\n",
    "    gc_kwargs_temp['num_new_ff_per_row'] = 3\n",
    "\n",
    "    gas = GUAT_combine_info_class.GUATCombineInfoAcrossSessions(gc_kwargs_temp, monkey_name=monkey_name)\n",
    "    gas.retrieve_or_make_combined_info(gc_kwargs_temp, combined_info_exists_ok=combined_info_exists_ok, \n",
    "                                    traj_df_exist_in_GUAT_store_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## data from all sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_kwargs = helper_GUAT_vs_TAFT_class.gc_kwargs.copy()\n",
    "\n",
    "gc_kwargs_temp = {**gc_kwargs,\n",
    "                    'num_old_ff_per_row': 2,\n",
    "                    'num_new_ff_per_row': 3}\n",
    "\n",
    "gas = GUAT_combine_info_class.GUATCombineInfoAcrossSessions(gc_kwargs_temp, monkey_name='monkey_Bruno')\n",
    "gas.retrieve_or_make_combined_info(gc_kwargs_temp, combined_info_exists_ok=True, traj_df_exist_in_GUAT_store_ok=True)\n",
    "gas.unpack_and_reload_combined_info_back_to_self(gas.combined_info, gas.all_traj_feature_names)\n",
    "\n",
    "# shared part with 'data from one session'\n",
    "gas.process_current_and_alternative_ff_info()\n",
    "# more_ff_attributes = ['ff_distance', 'ff_angle', 'ff_angle_boundary', 'curv_diff']\n",
    "# ff_last_seen_attributes = ['last_seen_' + attribute for attribute in more_ff_attributes] + ['distance_from_monkey_now_to_monkey_when_ff_last_seen', 'angle_from_monkey_now_to_monkey_when_ff_last_seen']\n",
    "# ff_next_seen_attributes = ['next_seen_' + attribute for attribute in more_ff_attributes] + ['distance_from_monkey_now_to_monkey_when_ff_next_seen', 'angle_from_monkey_now_to_monkey_when_ff_next_seen']\n",
    "more_ff_attributes = ['ff_distance', 'ff_angle', 'ff_angle_boundary']\n",
    "ff_last_seen_attributes = ['last_seen_' + attribute for attribute in more_ff_attributes]\n",
    "ff_next_seen_attributes = ['next_seen_' + attribute for attribute in more_ff_attributes]\n",
    "gas.find_input_and_output(add_arc_info=True, add_current_curv_of_traj=True, \n",
    "                          ff_attributes=['ff_distance', 'ff_angle', 'ff_angle_boundary', 'time_since_last_vis', 'duration_of_last_vis_period']\n",
    "                          + ff_last_seen_attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## data from one session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_kwargs = helper_GUAT_vs_TAFT_class.gc_kwargs.copy()\n",
    "\n",
    "raw_data_folder_path = 'all_monkey_data/raw_monkey_data/monkey_Bruno/data_0330'\n",
    "gcc = GUAT_collect_info_class.GUATCollectInfoForSession(raw_data_folder_path=raw_data_folder_path, \n",
    "                                                        gc_kwargs=gc_kwargs, new_point_index_start=0)\n",
    "_ = gcc.streamline_process_to_collect_info_from_one_session(GUAT_w_ff_df_exists_ok=True,\n",
    "                                                            GUAT_info_exists_ok=False)\n",
    "\n",
    "gas = GUAT_combine_info_class.GUATCombineInfoAcrossSessions(gc_kwargs)\n",
    "gas.unpack_and_reload_combined_info_back_to_self(gcc.important_info, gcc.all_traj_feature_names)\n",
    "\n",
    "# shared part with 'data from all sessions'\n",
    "gas.process_current_and_alternative_ff_info()\n",
    "more_ff_attributes = ['ff_distance', 'ff_angle', 'curv_diff']\n",
    "ff_last_seen_attributes = ['last_seen_' + attribute for attribute in more_ff_attributes] + ['distance_from_monkey_now_to_monkey_when_ff_last_seen', 'angle_from_monkey_now_to_monkey_when_ff_last_seen']\n",
    "ff_next_seen_attributes = ['next_seen_' + attribute for attribute in more_ff_attributes] + ['distance_from_monkey_now_to_monkey_when_ff_next_seen', 'angle_from_monkey_now_to_monkey_when_ff_next_seen']\n",
    "gas.find_input_and_output(add_arc_info=True, add_current_curv_of_traj=True, \n",
    "                          ff_attributes=['ff_distance', 'ff_angle', 'time_since_last_vis', 'duration_of_last_vis_period']\n",
    "                          + ff_last_seen_attributes)\n",
    "# additional features we can try: 'time_till_next_visible'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### FIRST get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "gas.furnish_with_trajectory_data = True\n",
    "gas.prepare_data_for_machine_learning()\n",
    "y_var_df = pd.DataFrame(gas.num_stops, columns=['num_stops'])\n",
    "y_var_df['num_stops'] = y_var_df['num_stops'].astype(int)\n",
    "\n",
    "\n",
    "# and for classification\n",
    "y_var_df2 = y_var_df.copy()\n",
    "# y_var_df2.loc[y_var_df2['num_stops'] > 2, 'num_stops'] = 2\n",
    "y_var_df2.loc[y_var_df2['num_stops'] > 2, 'num_stops'] = 2\n",
    "y_var_df2['num_stops'] = y_var_df2['num_stops'].astype(int)\n",
    "y_var_df2['num_stops'] = y_var_df2['num_stops'] - 1  # to avoid problem during classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfd01cf",
   "metadata": {},
   "source": [
    "# Updated version (but unfinished)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8278dce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "num_stops\n",
       "1    74\n",
       "2    42\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mask = np.where(y_var_df['num_stops'] > 1, True, False)\n",
    "mask_X = gas.X_all_df.loc[mask, :].copy()\n",
    "mask_y = y_var_df.loc[mask, :].copy()\n",
    "mask_y.loc[mask_y['num_stops'] > 3, 'num_stops'] = 3\n",
    "mask_y['num_stops'] = mask_y['num_stops'] - 1\n",
    "mask_y['num_stops'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a104fbe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers dropped before train_test_split: 0 out of 116 samples.\n",
      "\n",
      "=== Model: gnb ===\n",
      "Test accuracy: 0.5417 | balanced_acc: 0.5769\n",
      "\n",
      "=== Model: logreg ===\n",
      "Test accuracy: 0.6667 | balanced_acc: 0.6573\n",
      "\n",
      "=== Model: ridge ===\n",
      "Test accuracy: 0.5000 | balanced_acc: 0.4965\n",
      "\n",
      "=== Model: sgd ===\n",
      "Test accuracy: 0.6250 | balanced_acc: 0.6189\n",
      "\n",
      "=== Model: dt ===\n",
      "Test accuracy: 0.4167 | balanced_acc: 0.4056\n",
      "\n",
      "=== Model: bagging ===\n",
      "Test accuracy: 0.5000 | balanced_acc: 0.4755\n",
      "\n",
      "=== Model: rf ===\n",
      "Test accuracy: 0.5833 | balanced_acc: 0.5524\n",
      "\n",
      "=== Model: extra_trees ===\n",
      "Test accuracy: 0.5417 | balanced_acc: 0.5070\n",
      "\n",
      "=== Model: boosting ===\n",
      "Test accuracy: 0.6250 | balanced_acc: 0.5979\n",
      "\n",
      "=== Model: grad_boosting ===\n",
      "Test accuracy: 0.5000 | balanced_acc: 0.4895\n",
      "\n",
      "=== Model: knn ===\n",
      "Test accuracy: 0.5417 | balanced_acc: 0.5070\n",
      "\n",
      "=== Model: mlp ===\n",
      "Test accuracy: 0.6250 | balanced_acc: 0.6049\n",
      "\n",
      "=== Model: xgb ===\n",
      "Test accuracy: 0.6250 | balanced_acc: 0.6049\n",
      "\n",
      "=== Model: lgbm ===\n",
      "[LightGBM] [Info] Number of positive: 31, number of negative: 61\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000178 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 941\n",
      "[LightGBM] [Info] Number of data points in the train set: 92, number of used features: 38\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.336957 -> initscore=-0.676887\n",
      "[LightGBM] [Info] Start training from score -0.676887\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Test accuracy: 0.5417 | balanced_acc: 0.5350\n",
      "\n",
      "=== Model: catboost ===\n",
      "Test accuracy: 0.5417 | balanced_acc: 0.5070\n",
      "\n",
      "=== Model: svm ===\n",
      "Test accuracy: 0.5417 | balanced_acc: 0.5000\n",
      "\n",
      "Model comparison:\n",
      "             model  accuracy  balanced_accuracy  precision_macro  recall_macro  \\\n",
      "0          logreg   0.66667            0.65734          0.66667       0.65734   \n",
      "1             sgd   0.62500            0.61888          0.62143       0.61888   \n",
      "2        boosting   0.62500            0.59790          0.67500       0.59790   \n",
      "3             mlp   0.62500            0.60490          0.63889       0.60490   \n",
      "4             xgb   0.62500            0.60490          0.63889       0.60490   \n",
      "5              rf   0.58333            0.55245          0.61905       0.55245   \n",
      "6             gnb   0.54167            0.57692          0.75000       0.57692   \n",
      "7     extra_trees   0.54167            0.50699          0.52273       0.50699   \n",
      "8             knn   0.54167            0.50699          0.52273       0.50699   \n",
      "9            lgbm   0.54167            0.53497          0.53571       0.53497   \n",
      "10       catboost   0.54167            0.50699          0.52273       0.50699   \n",
      "11            svm   0.54167            0.50000          0.27083       0.50000   \n",
      "12          ridge   0.50000            0.49650          0.49650       0.49650   \n",
      "13        bagging   0.50000            0.47552          0.46316       0.47552   \n",
      "14  grad_boosting   0.50000            0.48951          0.48889       0.48951   \n",
      "15             dt   0.41667            0.40559          0.40000       0.40559   \n",
      "\n",
      "    f1_macro      mcc  roc_auc  \n",
      "0    0.65714  0.32388  0.53846  \n",
      "1    0.61905  0.24030  0.54545  \n",
      "2    0.56364  0.26179  0.62238  \n",
      "3    0.59013  0.24140  0.60839  \n",
      "4    0.59013  0.24140  0.54545  \n",
      "5    0.49580  0.15803  0.59441  \n",
      "6    0.46667  0.27735  0.50350  \n",
      "7    0.41978  0.02521  0.59441  \n",
      "8    0.41978  0.02521  0.56993  \n",
      "9    0.53439  0.07068  0.50350  \n",
      "10   0.41978  0.02521  0.54545  \n",
      "11   0.35135  0.00000  0.27273  \n",
      "12   0.49650 -0.00699  0.52448  \n",
      "13   0.43750 -0.06006  0.51748  \n",
      "14   0.48571 -0.02159  0.58741  \n",
      "15   0.40000 -0.19433  0.40559  \n",
      "\n",
      "Best model by accuracy: logreg\n",
      "\n",
      "Chosen model accuracy: 0.6666666666666666\n",
      "\n",
      "Classification Report (encoded labels):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.67      0.77      0.71        13\n",
      "           2       0.67      0.55      0.60        11\n",
      "\n",
      "    accuracy                           0.67        24\n",
      "   macro avg       0.67      0.66      0.66        24\n",
      "weighted avg       0.67      0.67      0.66        24\n",
      "\n",
      "\n",
      "Confusion Matrix (original class names):\n",
      "           Predicted 1  Predicted 2\n",
      "Actual 1           10            3\n",
      "Actual 2            5            6\n"
     ]
    }
   ],
   "source": [
    "resume = False\n",
    "tune = False\n",
    "\n",
    "# ml_inst = ml_methods_class.MlMethods(x_var_df= gas.X_all_df,\n",
    "#                                      y_var_df=y_var_df2)\n",
    "\n",
    "ml_inst = ml_methods_class.MlMethods(x_var_df= mask_X,\n",
    "                                     y_var_df=mask_y)\n",
    "\n",
    "\n",
    "ml_inst.use_train_test_split(ml_inst.x_var_df, ml_inst.y_var_df)\n",
    "model, y_pred, model_comparison_df = advanced_classification_utils.use_advanced_model_for_classification(\n",
    "    ml_inst.X_train, ml_inst.y_train, ml_inst.X_test, ml_inst.y_test,\n",
    "    kfold_cv=5,\n",
    "    tune=tune,                # turn tuning on/off\n",
    "    n_iter=30,                # ~3k samples sweet spot\n",
    "    tune_scoring=\"balanced_accuracy\",\n",
    "    checkpoint_dir=f\"all_monkey_data/decision_making/{gas.monkey_name}/pred_num_stops/cls_runs\",   # folder to save progress\n",
    "    resume=resume,              # skip finished models on rerun\n",
    "    n_jobs=-1,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_inst = ml_methods_class.MlMethods(x_var_df= gas.X_all_df,\n",
    "                                     y_var_df=y_var_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_names=['linreg', 'grad_boosting', 'rf']\n",
    "model_names=None\n",
    "\n",
    "ml_inst.use_ml_model_for_regression(ml_inst.x_var_df, ml_inst.y_var_df, model_names=model_names)\n",
    "ml_inst.model_comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## advanced regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_inst.use_train_test_split(ml_inst.x_var_df, ml_inst.y_var_df)\n",
    "df, best = advanced_regression_utils.use_advanced_model_for_regression(\n",
    "    ml_inst.X_train, ml_inst.y_train, ml_inst.X_test, ml_inst.y_test,\n",
    "    use_cv=True, cv_splits=5, verbose=True,\n",
    "    save_dir=f\"all_monkey_data/decision_making/{gas.monkey_name}/pred_num_stops/cls_runs\",\n",
    "    resume=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### quicker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor, AdaBoostRegressor\n",
    "\n",
    "# Example: fit your models\n",
    "dt = DecisionTreeRegressor(random_state=42).fit(ml_inst.x_var_df, ml_inst.y_var_df)\n",
    "gb = GradientBoostingRegressor(random_state=42).fit(ml_inst.x_var_df, ml_inst.y_var_df)\n",
    "hgb = HistGradientBoostingRegressor(random_state=42).fit(ml_inst.x_var_df, ml_inst.y_var_df)\n",
    "ada = AdaBoostRegressor(random_state=42).fit(ml_inst.x_var_df, ml_inst.y_var_df)\n",
    "\n",
    "# Collect in a dict\n",
    "models = {\n",
    "    \"Decision Tree\": dt,\n",
    "    \"Gradient Boosting\": gb,\n",
    "    \"HistGradientBoosting\": hgb,\n",
    "    \"AdaBoost\": ada,\n",
    "}\n",
    "\n",
    "# Get feature importances for each model\n",
    "for name, model in models.items():\n",
    "    if hasattr(model, \"feature_importances_\"):\n",
    "        importances = model.feature_importances_\n",
    "        feature_importance = pd.DataFrame({\n",
    "            \"feature\": ml_inst.x_var_df.columns,\n",
    "            \"importance\": importances\n",
    "        }).sort_values(by=\"importance\", ascending=False)\n",
    "        \n",
    "        print(f\"\\n{name} Feature Importances:\")\n",
    "        print(feature_importance.head(10))  # top 10\n",
    "    else:\n",
    "        print(f\"\\n{name} does not expose feature_importances_.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### in the future I can figure out..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# models = {\n",
    "#     \"hist_gb\": fitted_hist_gradient_boosting_regressor_or_pipeline,\n",
    "#     \"dt\": fitted_decision_tree_regressor_or_pipeline,\n",
    "#     \"grad_boosting\": fitted_gradient_boosting_regressor_or_pipeline,\n",
    "#     \"boosting\": fitted_adaboost_or_xgboost_or_lightgbm_or_pipeline,\n",
    "# }\n",
    "\n",
    "# # For each model:\n",
    "# for name, mdl in models.items():\n",
    "#     try:\n",
    "#         df_imp = feature_importance_table(mdl, X_valid, y_valid)  # y_valid only needed if fallback\n",
    "#         print(f\"\\n{name} â€” top 10\")\n",
    "#         print(df_imp.head(10))\n",
    "#     except Exception as e:\n",
    "#         print(f\"{name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case you forgot to do this first:\n",
    "gas.furnish_with_trajectory_data = True\n",
    "gas.prepare_data_for_machine_learning() \n",
    "y_var_df = pd.DataFrame(gas.num_stops, columns=['num_stops'])\n",
    "y_var_df['num_stops'] = y_var_df['num_stops'].astype(int)\n",
    "\n",
    "\n",
    "# and for classification\n",
    "y_var_df2 = y_var_df.copy()\n",
    "y_var_df2.loc[y_var_df2['num_stops'] > 2, 'num_stops'] = 2\n",
    "y_var_df2['num_stops'] = y_var_df2['num_stops'].astype(int)\n",
    "y_var_df2['num_stops'] = y_var_df2['num_stops'] - 1  # to avoid problem during classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_inst = ml_methods_class.MlMethods(x_var_df= gas.X_all_df,\n",
    "                                     y_var_df=y_var_df2)\n",
    "ml_inst.use_train_test_split(ml_inst.x_var_df, ml_inst.y_var_df)\n",
    "ml_inst.use_ml_model_for_classification(ml_inst.x_var_df, ml_inst.y_var_df, model_names=model_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## advanced classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case you forgot to do this first:\n",
    "gas.furnish_with_trajectory_data = True\n",
    "gas.prepare_data_for_machine_learning() \n",
    "y_var_df = pd.DataFrame(gas.num_stops, columns=['num_stops'])\n",
    "y_var_df['num_stops'] = y_var_df['num_stops'].astype(int)\n",
    "\n",
    "\n",
    "# and for classification\n",
    "y_var_df2 = y_var_df.copy()\n",
    "y_var_df2.loc[y_var_df2['num_stops'] > 2, 'num_stops'] = 2\n",
    "y_var_df2['num_stops'] = y_var_df2['num_stops'].astype(int)\n",
    "y_var_df2['num_stops'] = y_var_df2['num_stops'] - 1  # to avoid problem during classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume = False\n",
    "tune = False\n",
    "\n",
    "ml_inst = ml_methods_class.MlMethods(x_var_df= gas.X_all_df,\n",
    "                                     y_var_df=y_var_df2)\n",
    "ml_inst.use_train_test_split(ml_inst.x_var_df, ml_inst.y_var_df)\n",
    "model, y_pred, model_comparison_df = advanced_classification_utils.use_advanced_model_for_classification(\n",
    "    ml_inst.X_train, ml_inst.y_train, ml_inst.X_test, ml_inst.y_test,\n",
    "    kfold_cv=5,\n",
    "    tune=tune,                # turn tuning on/off\n",
    "    n_iter=30,                # ~3k samples sweet spot\n",
    "    tune_scoring=\"balanced_accuracy\",\n",
    "    checkpoint_dir=\"pred_num_stops/cls_runs\",   # folder to save progress\n",
    "    resume=resume,              # skip finished models on rerun\n",
    "    n_jobs=-1,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "Best model by accuracy: catboost\n",
    "\n",
    "Chosen model accuracy: 0.7967741935483871\n",
    "\n",
    "Classification Report (encoded labels):\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.81      0.93      0.86       431\n",
    "           1       0.76      0.49      0.60       189\n",
    "\n",
    "    accuracy                           0.80       620\n",
    "   macro avg       0.78      0.71      0.73       620\n",
    "weighted avg       0.79      0.80      0.78       620\n",
    "\n",
    "\n",
    "Confusion Matrix (original class names):\n",
    "           Predicted 0  Predicted 1\n",
    "Actual 0          401           30\n",
    "Actual 1           96           93"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "# model's feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "statsmodels, logreg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "Can't use it here because we are not predicting 2 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statsmodels.api as sm\n",
    "\n",
    "# X2 = sm.add_constant(ml_inst.x_var_df)  # add intercept\n",
    "# logit_model = sm.Logit(ml_inst.y_var_df, X2)  # logistic regression\n",
    "# result = logit_model.fit()\n",
    "\n",
    "# print(result.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "## random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(ml_inst.x_var_df, ml_inst.y_var_df)\n",
    "\n",
    "importances = rf.feature_importances_\n",
    "feature_importance = pd.DataFrame({\n",
    "    \"feature\": ml_inst.x_var_df.columns,\n",
    "    \"importance\": importances\n",
    "}).sort_values(by=\"importance\", ascending=False)\n",
    "\n",
    "print(feature_importance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "## grad_boosting (so that we can see feature importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Define the model\n",
    "model = GradientBoostingClassifier(\n",
    "    learning_rate=0.05, max_depth=7, max_features='sqrt',\n",
    "    min_samples_leaf=2, min_samples_split=7,\n",
    "    n_estimators=500, subsample=0.5\n",
    ")\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "model.fit(ml_inst.x_var_df, ml_inst.y_var_df)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = model.feature_importances_\n",
    "\n",
    "# Create a DataFrame for feature importances\n",
    "feature_importances_df = pd.DataFrame({\n",
    "    'Feature': ml_inst.x_var_df.columns,\n",
    "    'Importance': feature_importances\n",
    "})\n",
    "\n",
    "# Sort the DataFrame by importance\n",
    "feature_importances_df = feature_importances_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(10, 18))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importances_df)\n",
    "plt.title('Feature Importances')\n",
    "plt.show()\n",
    "\n",
    "# Determine significant features (e.g., importance > 0.01)\n",
    "significant_features = feature_importances_df[feature_importances_df['Importance'] > 0.01]\n",
    "print(\"Significant features:\")\n",
    "print(significant_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "## sequential selection...? try it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# model = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "# model.fit(ml_inst.x_var_df, ml_inst.y_var_df)\n",
    "# important = model.coef_[0] != 0\n",
    "# X_new = ml_inst.x_var_df.loc[:, important]\n",
    "# X_new.columns\n",
    "\n",
    "# from sklearn.feature_selection import SequentialFeatureSelector\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# model = RandomForestClassifier()\n",
    "# sfs = SequentialFeatureSelector(model, n_features_to_select=10, direction=\"forward\")\n",
    "# X_new = sfs.fit_transform(ml_inst.x_var_df, ml_inst.y_var_df.values.ravel())\n",
    "\n",
    "\n",
    "# # Boolean mask of selected features\n",
    "# mask = sfs.get_support()\n",
    "\n",
    "# # Names of selected features\n",
    "# selected_features = ml_inst.x_var_df.columns[mask]\n",
    "\n",
    "# print(\"Selected features:\")\n",
    "# print(selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "# Old method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "gas.prepare_data_for_machine_learning()\n",
    "gas.split_data_to_train_and_test(scaling_data=True)\n",
    "\n",
    "#bagging = BaggingClassifier(n_estimators=200, max_features=0.9, bootstrap_features=True, bootstrap=True, random_state=42)\n",
    "#gas.use_machine_learning_model_for_classification(model=bagging) \n",
    "\n",
    "gas.use_machine_learning_model_for_classification() #MLPClassifier(hidden_layer_sizes=(100,100,100), max_iter=1000, activation='relu', solver='adam', random_state=1))\n",
    "\n",
    "gas.get_pred_results_df()  \n",
    "gas.add_additional_info_to_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the proportion of cases where the number of stops is 1\n",
    "len(gas.num_stops[gas.num_stops==1])/len(gas.num_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.histplot(gas.num_stops)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "### plot prediction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "gas.plot_prediction_results(max_plot_to_make=5, show_reward_boundary=True, use_more_ff_inputs=True, use_more_traj_points=True,\n",
    "                            predict_num_stops=True,\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "### show more plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_i = 0\n",
    "max_plot_to_make = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "selected_cases = np.arange(current_i, current_i+max_plot_to_make)\n",
    "gas.plot_prediction_results(selected_cases=selected_cases, \n",
    "                            max_plot_to_make=max_plot_to_make, \n",
    "                            show_reward_boundary=True, \n",
    "                            use_more_ff_inputs=True, \n",
    "                            use_more_traj_points=True,\n",
    "                            predict_num_stops=True)\n",
    "current_i += max_plot_to_make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "gas.use_machine_learning_model_for_classification(model = MLPClassifier(hidden_layer_sizes=(100, 100, 100), max_iter=1000))   \n",
    "gas.get_pred_results_df()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "### loop through num_ff (as hyperparam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop\n",
    "for num_old_ff_per_row in [1, 2, 3]:\n",
    "    for num_new_ff_per_row in [1, 2, 3]:\n",
    "        print('num_old_ff_per_row:', num_old_ff_per_row)\n",
    "        print('num_new_ff_per_row:', num_new_ff_per_row)\n",
    "\n",
    "        gc_kwargs_temp = {**gc_kwargs,\n",
    "                            'num_old_ff_per_row': num_old_ff_per_row,\n",
    "                            'num_new_ff_per_row': num_new_ff_per_row}\n",
    "\n",
    "        gas = GUAT_combine_info_class.GUATCombineInfoAcrossSessions(gc_kwargs_temp)\n",
    "        gas.retrieve_or_make_combined_info(gc_kwargs_temp, combined_info_exists_ok=True, traj_df_exist_in_GUAT_store_ok=True)\n",
    "        gas.unpack_and_reload_combined_info_back_to_self(gas.combined_info, gas.all_traj_feature_names)\n",
    "\n",
    "\n",
    "        gas.process_current_and_alternative_ff_info()\n",
    "        more_ff_attributes = ['ff_distance', 'ff_angle', 'curv_diff']\n",
    "        ff_last_seen_attributes = ['last_seen_' + attribute for attribute in more_ff_attributes] + ['distance_from_monkey_now_to_monkey_when_ff_last_seen', 'angle_from_monkey_now_to_monkey_when_ff_last_seen']\n",
    "        ff_next_seen_attributes = ['next_seen_' + attribute for attribute in more_ff_attributes] + ['distance_from_monkey_now_to_monkey_when_ff_next_seen', 'angle_from_monkey_now_to_monkey_when_ff_next_seen']\n",
    "\n",
    "\n",
    "        gas.find_input_and_output(add_arc_info=True, add_current_curv_of_traj=True, \n",
    "                                ff_attributes=['ff_distance', 'ff_angle', 'time_since_last_vis', 'time_till_next_visible', 'duration_of_last_vis_period']\n",
    "                                + ff_last_seen_attributes)\n",
    "\n",
    "        gas.prepare_data_for_machine_learning()\n",
    "        gas.split_data_to_train_and_test(scaling_data=True)\n",
    "        gas.use_machine_learning_model_for_classification(model=None) #MLPClassifier(hidden_layer_sizes=(100,100,100), max_iter=1000, activation='relu', solver='adam', random_state=1))\n",
    "\n",
    "        gas.get_pred_results_df()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "# Visualization (not yet fully organized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "## plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gcc.make_auto_annot()\n",
    "# gas.point_index_to_plot = gcc.auto_annot['starting_point_index'].iloc[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "gas.traj_points = gas.traj_points_df.values\n",
    "gas.polar_plots_kwargs['traj_points_to_plot'] = gas.traj_points[gas.indices_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [1, 3, 7, 8, 14]:\n",
    "    print(i)\n",
    "    current_plotly_polar_plot_kargs, all_ff_dict = plotly_utils_polar.prepare_to_make_one_plotly_polar_plot(i, gas.polar_plots_kwargs, gas.point_index_to_plot, gas.GUAT_joined_ff_info, gas.all_traj_feature_names, \n",
    "                                                                                                            more_ff_df=gas.more_ff_df, trajectory_features=gc_kwargs['trajectory_features'])\n",
    "\n",
    "    fig, customdata_columns = plotly_utils_polar.make_one_plotly_polar_plot(**current_plotly_polar_plot_kargs,\n",
    "                                                        symbol='group', color='time_since_last_vis', \n",
    "                                                        columns_for_annotation=['subgroup', 'curv_diff', 'time_till_next_visible', 'time_since_last_vis'],)\n",
    "                                                        #columns_for_annotation=['subgroup', 'ff_number', 'group', 'curv_diff'],)\n",
    "    print(all_ff_dict['more_monkey_info_for_ff_in_past_or_future'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "## dash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dash import Dash, html, dcc, Input, Output, callback\n",
    "from dash.exceptions import PreventUpdate\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "reload(plotly_utils_polar)\n",
    "def use_dash(i, gas):\n",
    "    current_plotly_polar_plot_kargs, all_ff_dict = plotly_utils_polar.prepare_to_make_one_plotly_polar_plot(i, gas.polar_plots_kwargs, gas.point_index_to_plot, gas.GUAT_joined_ff_info, gas.all_traj_feature_names, \n",
    "                                                                                            more_ff_df=gas.more_ff_df, trajectory_features=gas.trajectory_features,)\n",
    "    fig, customdata_columns = plotly_utils_polar.make_one_plotly_polar_plot(**current_plotly_polar_plot_kargs, columns_for_annotation=['subgroup', 'curv_diff', 'time_since_last_vis', 'time_till_next_visible'], \n",
    "                                                                            additional_customdata_columns=['ff_number'], color='group')\n",
    "    initial_hover_data = current_plotly_polar_plot_kargs['ff_df'][customdata_columns].iloc[0].values\n",
    "    ff_number_index = np.where(np.array(customdata_columns) == 'ff_number')[0][0]\n",
    "\n",
    "    external_stylesheets = ['https://codepen.io/chriddyp/pen/bWLwgP.css']\n",
    "    df = all_ff_dict['combined_ff_df'].copy()\n",
    "\n",
    "    app = Dash(__name__, external_stylesheets=external_stylesheets)\n",
    "\n",
    "    app.layout = html.Div([\n",
    "\n",
    "        html.Div([\n",
    "            dcc.Graph(\n",
    "                id='main_graph',\n",
    "                figure = fig,\n",
    "                hoverData={'points': [{'customdata': initial_hover_data}]} #['Original-Present', 0]}]}\n",
    "            )\n",
    "        ], style={'width': '50%', 'display': 'inline-block', 'padding': '0 0'}),\n",
    "        html.Div([\n",
    "            dcc.Graph(id='ff_history'),\n",
    "        ], style={'display': 'inline-block', 'width': '50%'}),\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "    @callback(\n",
    "        Output('ff_history', 'figure'),\n",
    "        Input('main_graph', 'hoverData'))\n",
    "    def update_ff_history(hoverData):\n",
    "        print('hoverData:', hoverData)\n",
    "        print('hoverData[points][0]:', hoverData['points'][0])\n",
    "        current_ff_number = hoverData['points'][0]['customdata'][ff_number_index]\n",
    "        # if current_ff_number is not int\n",
    "        if not isinstance(current_ff_number, int):\n",
    "            raise PreventUpdate(\"No update was made because current_ff_number is not an integer.\")\n",
    "        else:\n",
    "            dff = df[df['ff_number'] == current_ff_number]\n",
    "            current_group = dff['group'].iloc[0]\n",
    "            symbol_maps_for_groups = {'Original': 'circle-dot', 'Alternative': 'star-dot', 'More': 'diamond-dot'}\n",
    "\n",
    "            title = '<b>{}</b>'.format(current_ff_number)\n",
    "            current_plotly_polar_plot_kargs_temp = current_plotly_polar_plot_kargs.copy()\n",
    "            current_plotly_polar_plot_kargs_temp['ff_df'] = dff.copy()\n",
    "            fig, customdata_columns = plotly_utils_polar.make_one_plotly_polar_plot(**current_plotly_polar_plot_kargs_temp,\n",
    "                                                                columns_for_annotation=['subgroup', 'ff_distance', 'ff_angle', 'curv_diff', 'time_since_last_vis', 'time_till_next_visible'],\n",
    "                                                                add_colorbar=True, add_legend=True, add_real_and_predicted_labels=False, size=None,\n",
    "                                                                size_max=15, symbol='time_label', symbol_map={'Present':symbol_maps_for_groups[current_group]})\n",
    "            return fig\n",
    "\n",
    "    app.run(debug=True)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_dash(1, gas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_dash(3, gas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_dash(7, gas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "## Other important distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {},
   "source": [
    "### histplot of durations between stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the duration of the three stops\n",
    "diff_stop_time = GUAT_cluster_df['last_stop_time'] - GUAT_cluster_df['first_stop_time']\n",
    "sns.histplot(diff_stop_time, bins=20)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# make a line plot of 3 points on x-axis, with y-axis being time since first stop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {},
   "source": [
    "### histplot of time_till_next_visible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "## histplot of durations between stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(gcc.miss_abort_cur_ff_info['time_till_next_visible'], bins=20, stat='probability')\n",
    "plt.title('time till next visible')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75",
   "metadata": {},
   "source": [
    "### curvature upper lower bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see clean_curvature_info function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in ['curv_of_traj', 'curvature_lower_bound', 'curvature_upper_bound', 'opt_arc_curv']:\n",
    "    sns.boxplot(gas.curvature_df[column].values, orient='h')\n",
    "    plt.title(column)\n",
    "    plt.show()\n",
    "    \n",
    "   \n",
    "    sns.histplot(gas.curvature_df[column].values, bins=50)\n",
    "    plt.title(column)\n",
    "    plt.show()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ff_venv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
