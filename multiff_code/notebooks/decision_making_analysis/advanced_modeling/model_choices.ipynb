{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65873a0b",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53ee33c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set up logging configuration.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os, sys\n",
    "for p in [Path.cwd()] + list(Path.cwd().parents):\n",
    "    if p.name == 'Multifirefly-Project':\n",
    "        os.chdir(p)\n",
    "        sys.path.insert(0, str(p / 'multiff_analysis/multiff_code/methods'))\n",
    "        break\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "from data_wrangling import specific_utils, combine_info_utils\n",
    "from pattern_discovery import pattern_by_trials, pattern_by_trials, cluster_analysis, organize_patterns_and_features, category_class\n",
    "from decision_making_analysis.cluster_replacement import cluster_replacement_utils\n",
    "from decision_making_analysis.decision_making import decision_making_class, decision_making_utils, intended_targets_classes\n",
    "from decision_making_analysis.GUAT import GUAT_collect_info_class, GUAT_combine_info_class\n",
    "from decision_making_analysis.compare_GUAT_and_TAFT import GUAT_vs_TAFT_class, GUAT_vs_TAFT_x_sessions_class, helper_GUAT_vs_TAFT_class\n",
    "from visualization.matplotlib_tools import plot_trials, plot_behaviors_utils\n",
    "from visualization.animation import animation_class\n",
    "from null_behaviors import show_null_trajectory, find_best_arc, curvature_utils, curv_of_traj_utils\n",
    "from machine_learning.ml_methods import regression_utils, classification_utils, prep_ml_data_utils, hyperparam_tuning_class\n",
    "from visualization.plotly_polar_tools import plotly_utils_polar, plotly_for_ff_polar, plotly_for_trajectory_polar\n",
    "from machine_learning.ml_methods import ml_methods_class\n",
    "from visualization.dash_tools.dash_main_class_methods import dash_applied_to_GUAT_TAFT\n",
    "from decision_making_analysis.advanced_modeling import model_choices\n",
    "\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import exists\n",
    "import math\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "from scipy import stats\n",
    "from IPython.display import HTML\n",
    "from matplotlib import rc\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "import os, sys, sys\n",
    "from importlib import reload\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "\n",
    "plt.rcParams[\"animation.html\"] = \"html5\"\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "rc('animation', html='jshtml')\n",
    "matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n",
    "matplotlib.rcParams['animation.embed_limit'] = 2**128\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "np.set_printoptions(suppress=True)\n",
    "pd.options.display.max_rows = 50\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b4d9b4",
   "metadata": {},
   "source": [
    "# try funcs in class\n",
    "\n",
    "https://chatgpt.com/c/68a3381b-f374-8324-b3c5-bdf77f515b21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871523ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from decision_making_analysis.advanced_modeling.choice_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac550fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiff_code/methods/decision_making_analysis/advanced_modeling/model_choices.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440831b1",
   "metadata": {},
   "source": [
    "## 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d8fe11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate\n",
    "data = make_synthetic_dataset(num_decisions=800, E=4, D=6, K_cap=6)\n",
    "\n",
    "# Split\n",
    "idx = np.arange(len(data[\"ego\"]))\n",
    "np.random.shuffle(idx)\n",
    "ntr = int(0.8*len(idx))\n",
    "tr, va = idx[:ntr], idx[ntr:]\n",
    "\n",
    "# Success logs\n",
    "X = torch.tensor(data[\"succX\"], dtype=torch.float32)\n",
    "y = torch.tensor(data[\"succY\"], dtype=torch.float32)\n",
    "Xtr, ytr = X[tr], y[tr]; Xva, yva = X[va], y[va]\n",
    "\n",
    "# DataLoaders for choice\n",
    "tr_loader = DataLoader(DecisionDS(data, tr), batch_size=64, shuffle=True,  collate_fn=collate)\n",
    "va_loader = DataLoader(DecisionDS(data, va), batch_size=64, shuffle=False, collate_fn=collate)\n",
    "\n",
    "# Models\n",
    "E = tr_loader.dataset.D[\"E\"]\n",
    "Ddim = tr_loader.dataset.D[\"D\"]\n",
    "sp = SuccessPredictor(in_dim=Ddim, hidden=64, p_drop=0.1)\n",
    "cm = ContextAwareChoice(dim_ego=E, dim_retry=Ddim, dim_cand=Ddim, hidden=64)\n",
    "\n",
    "# --- Train SuccessPredictor ---\n",
    "opt1 = torch.optim.AdamW(sp.parameters(), lr=3e-3, weight_decay=1e-2)\n",
    "for ep in range(5):\n",
    "    sp.train(); opt1.zero_grad()\n",
    "    p = sp(Xtr); loss = F.binary_cross_entropy(p, ytr)\n",
    "    loss.backward(); opt1.step()\n",
    "    with torch.no_grad():\n",
    "        sp.eval()\n",
    "        tr_bce = F.binary_cross_entropy(sp(Xtr), ytr).item()\n",
    "        va_bce = F.binary_cross_entropy(sp(Xva), yva).item()\n",
    "        tr_acc = ((sp(Xtr)>0.5).float()==ytr).float().mean().item()\n",
    "        va_acc = ((sp(Xva)>0.5).float()==yva).float().mean().item()\n",
    "    print(f\"[Success] ep {ep+1} | train BCE {tr_bce:.3f} acc {tr_acc:.3f} | val BCE {va_bce:.3f} acc {va_acc:.3f}\")\n",
    "\n",
    "# --- Train Choice model ---\n",
    "opt2 = torch.optim.AdamW(cm.parameters(), lr=3e-3, weight_decay=1e-2)\n",
    "\n",
    "def run_epoch(loader, train=True):\n",
    "    tot, n, correct = 0.0, 0, 0\n",
    "    for b in loader:\n",
    "        B,K,Dd = b[\"cand\"].shape\n",
    "        with torch.no_grad():\n",
    "            pr = sp(b[\"retry\"])                                           # (B,)\n",
    "            pc = sp(b[\"cand\"].reshape(B*K, Dd)).reshape(B,K) if K>0 else torch.zeros(B,0)\n",
    "        scores, probs = cm(b[\"ego\"], b[\"retry\"], b[\"cand\"], b[\"tcr\"], b[\"tcc\"],\n",
    "                            b[\"ar\"], b[\"ac\"], pr, pc, b[\"mask\"])\n",
    "        loss = F.cross_entropy(scores, b[\"yidx\"])\n",
    "        if train:\n",
    "            opt2.zero_grad(); loss.backward(); opt2.step()\n",
    "        tot += loss.item()*B; n += B\n",
    "        pm = probs.clone()\n",
    "        if pm.shape[1]>1: pm[:,1:][ b[\"mask\"] ] = 0.0\n",
    "        correct += (pm.argmax(dim=1) == b[\"yidx\"]).sum().item()\n",
    "    return tot/max(1,n), correct/max(1,n)\n",
    "\n",
    "for ep in range(8):\n",
    "    tr_nll, tr_acc = run_epoch(tr_loader, True)\n",
    "    va_nll, va_acc = run_epoch(va_loader, False)\n",
    "    print(f\"[Choice ] ep {ep+1} | train NLL {tr_nll:.3f} acc {tr_acc:.3f} | val NLL {va_nll:.3f} acc {va_acc:.3f}\")\n",
    "\n",
    "# --- Show a few predicted probabilities (masking pads) ---\n",
    "b = next(iter(va_loader))\n",
    "B,K,Dd = b[\"cand\"].shape\n",
    "with torch.no_grad():\n",
    "    pr = sp(b[\"retry\"]); pc = sp(b[\"cand\"].reshape(B*K, Dd)).reshape(B,K) if K>0 else torch.zeros(B,0)\n",
    "    _, probs = cm(b[\"ego\"], b[\"retry\"], b[\"cand\"], b[\"tcr\"], b[\"tcc\"], b[\"ar\"], b[\"ac\"], pr, pc, b[\"mask\"])\n",
    "    if probs.shape[1]>1: probs[:,1:][ b[\"mask\"] ] = 0.0\n",
    "print(\"\\nSample predicted probs for first 3 items (col 0=retry, cols 1..K=candidates):\")\n",
    "print(probs[:3].numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33f63c3",
   "metadata": {},
   "source": [
    "## 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88faa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----- A) Pretrain success predictor on attempt logs -----\n",
    "# x_attempt: (N, D_succ)  features at attempt time (retry/candidate attempts pooled)\n",
    "# y_attempt: (N,)         0/1 labels for \"stopped inside boundary?\"\n",
    "pred = success_model(x_attempt)                      # (N,)\n",
    "loss_succ = F.binary_cross_entropy(pred, y_attempt.float())\n",
    "loss_succ.backward(); opt_succ.step(); opt_succ.zero_grad()\n",
    "\n",
    "# ----- B) Choice model training at decision points -----\n",
    "# Batch dict (from collate) with shapes noted above:\n",
    "batch = collate_retry_switch(list_of_examples)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "for k in batch: batch[k] = batch[k].to(device)\n",
    "\n",
    "# Split features for success vs choice if you keep separate featurizations;\n",
    "# here I reuse the same (retry_feat / cand_feat) for illustration:\n",
    "retry_feat_succ = batch[\"retry_feat\"]                 # (B, R_succ)\n",
    "cand_feat_succ  = batch[\"cand_feat\"]                  # (B, K, C_succ)\n",
    "\n",
    "# Compute success probabilities per option (vectorized over candidates):\n",
    "with torch.no_grad():  # keep SuccessPredictor frozen if you pretrain it\n",
    "    p_retry = success_model(retry_feat_succ)          # (B,)\n",
    "    B, K, Csucc = cand_feat_succ.shape\n",
    "    p_cand  = success_model(cand_feat_succ.reshape(B*K, Csucc)).reshape(B, K)  # (B, K)\n",
    "\n",
    "# Run the choice model to get scores/probs over {retry} ∪ {candidates}\n",
    "scores, probs = choice_model(\n",
    "    ego             = batch[\"ego\"],                   # (B, E)\n",
    "    retry_feat      = batch[\"retry_feat\"],            # (B, R)\n",
    "    cand_feat       = batch[\"cand_feat\"],             # (B, K, C)\n",
    "    time_cost_retry = batch[\"time_cost_retry\"],       # (B,)\n",
    "    time_cost_cand  = batch[\"time_cost_cand\"],        # (B, K)\n",
    "    attempts_retry  = batch[\"attempts_retry\"],        # (B,)\n",
    "    attempts_cand   = batch[\"attempts_cand\"],         # (B, K)\n",
    "    p_succ_retry    = p_retry,                        # (B,)\n",
    "    p_succ_cand     = p_cand,                         # (B, K)\n",
    "    mask            = batch[\"cand_mask\"]              # (B, K) True=PAD\n",
    ")\n",
    "\n",
    "# Cross-entropy over K+1 columns (0=retry, 1..K=candidates)\n",
    "loss_choice = F.cross_entropy(scores, batch[\"chosen_idx\"])  # scores are logits\n",
    "loss_choice.backward(); opt_choice.step(); opt_choice.zero_grad()\n",
    "\n",
    "# Optional: for metrics (argmax), zero out padded columns first\n",
    "probs_masked = probs.clone()\n",
    "probs_masked[:, 1:][ batch[\"cand_mask\"] ] = 0.0\n",
    "pred_idx = probs_masked.argmax(dim=1)  # (B,) predicted option index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c5cb42",
   "metadata": {},
   "source": [
    "# smoke test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c16db46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fee4d2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Success] epoch 1: loss=0.6924\n",
      "[Success] epoch 2: loss=0.6911\n",
      "[Choice] epoch 1: loss=1.1762\n",
      "[Choice] epoch 2: loss=1.1666\n",
      "Smoke test complete.\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "# 3A) Success head on random data\n",
    "Xs = torch.randn(512, 10)\n",
    "ys = torch.bernoulli(torch.full((512,), 0.5))\n",
    "ds = model_choices.SuccessDataset(Xs, ys)\n",
    "dl = DataLoader(ds, batch_size=64, shuffle=True)\n",
    "succ = model_choices.SuccessPredictor(in_dim=10)\n",
    "model_choices.train_success_head(succ, dl, epochs=2)\n",
    "\n",
    "# 3B) Choice model with masking\n",
    "items = []\n",
    "for _ in range(256):\n",
    "    K = torch.randint(low=2, high=6, size=(1,)).item()  # 2..5 options\n",
    "    option_feats = torch.randn(K, 8)\n",
    "    p_succ_opt = torch.sigmoid(torch.randn(K))\n",
    "    extra_costs = torch.randn(K, 2)\n",
    "    chosen = torch.randint(low=0, high=K, size=(1,)).item()\n",
    "    items.append({\n",
    "        \"option_features\": option_feats,\n",
    "        \"p_succ_opt\": p_succ_opt,\n",
    "        \"extra_costs\": extra_costs,\n",
    "        \"option_mask\": torch.ones(K, dtype=torch.bool),\n",
    "        \"chosen_index\": torch.tensor(chosen),\n",
    "    })\n",
    "dc = model_choices.ChoiceDataset(items)\n",
    "dlc = DataLoader(dc, batch_size=32, shuffle=True, collate_fn=model_choices.choice_collate)\n",
    "chooser = model_choices.ChoiceScorer(option_dim=8, extra_cost_dim=2, use_psucc=True)\n",
    "model_choices.train_choice_model(chooser, dlc, epochs=2)\n",
    "\n",
    "print(\"Smoke test complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bff880a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'decision_making_analysis.advanced_modeling.model_choices' from '/Users/dusiyi/Documents/Multifirefly-Project/multiff_analysis/multiff_code/methods/decision_making_analysis/advanced_modeling/model_choices.py'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_choices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13b5ac5",
   "metadata": {},
   "source": [
    "# toy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4ea3ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Success] epoch 1: loss=0.7340\n",
      "[Success] epoch 2: loss=0.7292\n",
      "[Success] epoch 3: loss=0.7245\n",
      "p_hat_succ: [0.46155235171318054, 0.4903356432914734, 0.45318105816841125, 0.45039984583854675]\n"
     ]
    }
   ],
   "source": [
    "# ---- 3A: success head toy data ----\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "X_stop = torch.tensor([\n",
    "    [ 0.2, -0.3, 0.1],   # stop features (D_s = 3)\n",
    "    [ 1.0,  0.4, 0.5],\n",
    "    [-0.7,  0.2, 0.9],\n",
    "    [ 0.3,  0.1, 0.2],\n",
    "], dtype=torch.float32)\n",
    "y_inside = torch.tensor([1., 1., 0., 1.], dtype=torch.float32)\n",
    "\n",
    "ds = SuccessDataset(X_stop, y_inside)\n",
    "dl = DataLoader(ds, batch_size=2, shuffle=True)\n",
    "\n",
    "model = SuccessPredictor(in_dim=3)\n",
    "train_success_head(model, dl, epochs=3, lr=1e-3)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(\"p_hat_succ:\", model(X_stop).tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20d9097e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.875237226486206\n",
      "train loss: 0.8737525939941406\n",
      "train loss: 0.8722797632217407\n"
     ]
    }
   ],
   "source": [
    "# ---- 3B: choice model toy batch (variable K) ----\n",
    "items = [\n",
    "    {   # episode 1: K=3 options (e.g., {retry, ff#7, ff#12})\n",
    "        \"option_features\": torch.tensor([[0.1, 0.2], [-0.3, 0.5], [0.4, -0.1]], dtype=torch.float32),  # [3, D_o=2]\n",
    "        \"p_succ_opt\":     torch.tensor([0.70, 0.25, 0.55], dtype=torch.float32),                       # [3]\n",
    "        \"extra_costs\":    torch.tensor([[0.9, 0.1], [0.2, 0.7], [0.6, 0.3]], dtype=torch.float32),     # [3, D_c=2]\n",
    "        \"chosen_index\":   torch.tensor(0),  # picked the first option\n",
    "    },\n",
    "    {   # episode 2: K=2 options\n",
    "        \"option_features\": torch.tensor([[0.0, -0.2], [0.8, 0.1]], dtype=torch.float32),               # [2, 2]\n",
    "        \"p_succ_opt\":     torch.tensor([0.40, 0.65], dtype=torch.float32),                              # [2]\n",
    "        \"extra_costs\":    torch.tensor([[0.3, 0.4], [0.1, 0.2]], dtype=torch.float32),                 # [2, 2]\n",
    "        \"chosen_index\":   torch.tensor(1),\n",
    "    },\n",
    "]\n",
    "\n",
    "loader = DataLoader(ChoiceDataset(items), batch_size=2, shuffle=True, collate_fn=choice_collate)\n",
    "chooser = ChoiceScorer(option_dim=2, extra_cost_dim=2, use_psucc=True)\n",
    "\n",
    "for _ in range(3):\n",
    "    for batch in loader:\n",
    "        logits = chooser(batch[\"option_features\"], batch[\"mask\"], batch[\"p_succ_opt\"], batch[\"extra_costs\"])\n",
    "        loss = masked_cross_entropy(logits, batch[\"targets\"], batch[\"mask\"])\n",
    "        chooser.zero_grad(); loss.backward()\n",
    "        for p in chooser.parameters(): \n",
    "            if p.grad is not None:\n",
    "                p.data -= 1e-2 * p.grad   # tiny manual SGD step\n",
    "    print(\"train loss:\", float(loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbd2f5d",
   "metadata": {},
   "source": [
    "# synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79303aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "[Success] epoch 1: loss=0.5714\n",
      "[Success] epoch 2: loss=0.4938\n",
      "[Success] epoch 3: loss=0.4550\n",
      "[Success] epoch 4: loss=0.4413\n",
      "[Success] epoch 5: loss=0.4312\n",
      "[Success/Val] acc=0.812  corr(p, -miss)=0.38  corr(p, bright)=-0.01  corr(p, align)=0.88\n",
      "[Choice] epoch 1: loss=1.0445\n",
      "[Choice] epoch 2: loss=0.8030\n",
      "[Choice] epoch 3: loss=0.7667\n",
      "[Choice] epoch 4: loss=0.7603\n",
      "[Choice] epoch 5: loss=0.7538\n",
      "[Choice/Val] top-1 accuracy=0.704\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "# 1) Train success head on synthetic stops\n",
    "Xtr, ytr, Xva, yva = generate_synthetic_success_data(N=5000)\n",
    "succ = SuccessPredictor(in_dim=6)\n",
    "dl_tr = DataLoader(SuccessDataset(Xtr, ytr), batch_size=128, shuffle=True)\n",
    "train_success_head(succ, dl_tr, epochs=5, lr=1e-3, device=device)\n",
    "evaluate_success_head(succ.to(device), Xva.to(device), yva.to(device))\n",
    "\n",
    "# 2) Build a synthetic choice dataset that uses p_succ from the success head\n",
    "items = build_choice_items(succ, N=3000)\n",
    "ntr = int(0.85 * len(items))\n",
    "dc_tr = ChoiceDataset(items[:ntr])\n",
    "dc_va = ChoiceDataset(items[ntr:])\n",
    "dl_tr_c = DataLoader(dc_tr, batch_size=64, shuffle=True, collate_fn=choice_collate)\n",
    "dl_va_c = DataLoader(dc_va, batch_size=128, shuffle=False, collate_fn=choice_collate)\n",
    "\n",
    "chooser = ChoiceScorer(option_dim=2, extra_cost_dim=2, use_psucc=True)\n",
    "train_choice_model(chooser, dl_tr_c, epochs=5, lr=1e-3, device=device)\n",
    "acc = eval_choice_top1(chooser.to(device), dl_va_c, device=device)\n",
    "print(f\"[Choice/Val] top-1 accuracy={acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af5d5be",
   "metadata": {},
   "source": [
    "# functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737ee0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Success] epoch 1: loss=0.6924\n",
      "[Success] epoch 2: loss=0.6911\n",
      "[Choice] epoch 1: loss=1.1762\n",
      "[Choice] epoch 2: loss=1.1666\n",
      "Smoke test complete.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Tiny PyTorch skeleton for MultiFF retry/switch modeling.\n",
    "\n",
    "Implements the two core training steps and leaves hooks for the optional\n",
    "variants. The code is intentionally lightweight: small MLPs, masking for\n",
    "variable option counts, and minimal training loops.\n",
    "\n",
    "You can paste this into a file and adapt the Dataset stubs to your\n",
    "preprocessing.\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Utilities\n",
    "# -------------------------------\n",
    "\n",
    "def mlp(in_dim: int, hidden: List[int], out_dim: int, dropout: float = 0.0) -> nn.Sequential:\n",
    "    layers: List[nn.Module] = []\n",
    "    last = in_dim\n",
    "    for h in hidden:\n",
    "        layers += [nn.Linear(last, h), nn.ReLU()]  # keep it simple\n",
    "        if dropout > 0:\n",
    "            layers += [nn.Dropout(dropout)]\n",
    "        last = h\n",
    "    layers += [nn.Linear(last, out_dim)]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "def masked_cross_entropy(logits: torch.Tensor, targets: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Cross-entropy for variable option sets.\n",
    "    logits: [B, Kmax]\n",
    "    targets: [B] (index of chosen option in 0..K-1)\n",
    "    mask: [B, Kmax] boolean (True for valid options)\n",
    "    \"\"\"\n",
    "    # Put -inf on invalid options so softmax ignores them\n",
    "    logits_masked = logits.masked_fill(~mask, -1e9)\n",
    "    return F.cross_entropy(logits_masked, targets)\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 3A) Capture-success predictor\n",
    "# -------------------------------\n",
    "\n",
    "class SuccessPredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    Predicts p_succ = P(next stop is inside boundary for that target).\n",
    "\n",
    "    Inputs: stop_features: [B, D_s]\n",
    "    Output: p_succ: [B]\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim: int, hidden: List[int] = [32]):\n",
    "        super().__init__()\n",
    "        self.net = mlp(in_dim, hidden, 1, dropout=0.0)\n",
    "\n",
    "    def forward(self, stop_features: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.sigmoid(self.net(stop_features)).squeeze(-1)\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 3B) Choice scorer over {retry} ∪ {other targets}\n",
    "# -------------------------------\n",
    "\n",
    "class ChoiceScorer(nn.Module):\n",
    "    \"\"\"\n",
    "    Scores each candidate option with a utility U_i(t) and produces logits.\n",
    "\n",
    "    Forward inputs:\n",
    "      - option_features: [B, Kmax, D_o]\n",
    "      - mask: [B, Kmax] boolean (True for valid candidates)\n",
    "      - p_succ_opt: Optional[Tensor] [B, Kmax] (per-option success proba)\n",
    "      - extra_costs: Optional[Tensor] [B, Kmax, D_c] (e.g., time-to-go, turn cost)\n",
    "\n",
    "    Returns:\n",
    "      - logits: [B, Kmax]\n",
    "    \"\"\"\n",
    "    def __init__(self, option_dim: int, extra_cost_dim: int = 0, use_psucc: bool = True,\n",
    "                 hidden: List[int] = [64, 32]):\n",
    "        super().__init__()\n",
    "        self.use_psucc = use_psucc\n",
    "        in_dim = option_dim + extra_cost_dim + (1 if use_psucc else 0)\n",
    "        self.net = mlp(in_dim, hidden, 1, dropout=0.0)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        option_features: torch.Tensor,\n",
    "        mask: torch.Tensor,\n",
    "        p_succ_opt: Optional[torch.Tensor] = None,\n",
    "        extra_costs: Optional[torch.Tensor] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        B, Kmax, D_o = option_features.shape\n",
    "        feats = [option_features]\n",
    "        if self.use_psucc:\n",
    "            if p_succ_opt is None:\n",
    "                raise ValueError(\"p_succ_opt must be provided when use_psucc=True\")\n",
    "            feats.append(p_succ_opt.unsqueeze(-1))  # [B, Kmax, 1]\n",
    "        if extra_costs is not None:\n",
    "            feats.append(extra_costs)\n",
    "        X = torch.cat(feats, dim=-1)  # [B, Kmax, Din]\n",
    "        logits = self.net(X).squeeze(-1)  # [B, Kmax]\n",
    "        # Mask invalid options with -inf to avoid accidental selection\n",
    "        logits = logits.masked_fill(~mask, -1e9)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Optional variants (4)\n",
    "# -------------------------------\n",
    "\n",
    "class RetrySwitchHead(nn.Module):\n",
    "    \"\"\"Binary logit: retry vs switch after a near-miss.\"\"\"\n",
    "    def __init__(self, in_dim: int, hidden: List[int] = [32]):\n",
    "        super().__init__()\n",
    "        self.net = mlp(in_dim, hidden, 1)\n",
    "\n",
    "    def forward(self, features: torch.Tensor) -> torch.Tensor:\n",
    "        # returns P(retry)\n",
    "        return torch.sigmoid(self.net(features)).squeeze(-1)\n",
    "\n",
    "\n",
    "class TwoStagePolicy(nn.Module):\n",
    "    \"\"\"\n",
    "    Stage 1: binary retry vs switch (on near-miss features).\n",
    "    Stage 2: if switch, choose among other targets using ChoiceScorer.\n",
    "    \"\"\"\n",
    "    def __init__(self, retry_in_dim: int, option_dim: int, extra_cost_dim: int = 0, use_psucc: bool = True):\n",
    "        super().__init__()\n",
    "        self.retry_head = RetrySwitchHead(retry_in_dim)\n",
    "        self.choice = ChoiceScorer(option_dim, extra_cost_dim, use_psucc)\n",
    "\n",
    "    def forward(self, retry_features: torch.Tensor,\n",
    "                option_features: torch.Tensor, mask: torch.Tensor,\n",
    "                p_succ_opt: Optional[torch.Tensor] = None,\n",
    "                extra_costs: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        p_retry = self.retry_head(retry_features)\n",
    "        logits_switch = self.choice(option_features, mask, p_succ_opt, extra_costs)\n",
    "        return p_retry, logits_switch\n",
    "\n",
    "\n",
    "class HazardHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Discrete-time hazard after a near-miss: h_t = P(switch at t | not switched yet).\n",
    "\n",
    "    Input per step features X_t -> h_t via sigmoid(MLP).\n",
    "    Loss implemented via discrete-time survival likelihood.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim: int, hidden: List[int] = [32]):\n",
    "        super().__init__()\n",
    "        self.net = mlp(in_dim, hidden, 1)\n",
    "\n",
    "    def forward(self, step_features: torch.Tensor, step_mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        step_features: [B, T, D]\n",
    "        step_mask: [B, T] (True for valid time steps)\n",
    "        Returns hazards h_t in [0,1]: [B, T]\n",
    "        \"\"\"\n",
    "        h = torch.sigmoid(self.net(step_features)).squeeze(-1)\n",
    "        return h * step_mask.float()\n",
    "\n",
    "    @staticmethod\n",
    "    def survival_nll(h: torch.Tensor, event_index: torch.Tensor, step_mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Negative log-likelihood for discrete-time hazards.\n",
    "        h: [B, T] hazards\n",
    "        event_index: [B] index of switch time; if censored (no switch in window), set to -1\n",
    "        step_mask: [B, T]\n",
    "        \"\"\"\n",
    "        # log S_{t} = sum_{k < t} log(1 - h_k);  log f(t) = log S_t + log h_t\n",
    "        eps = 1e-6\n",
    "        log1m_h = torch.log(torch.clamp(1 - h, min=eps)) * step_mask\n",
    "        cumsums = torch.cumsum(log1m_h, dim=1)  # [B, T]\n",
    "        B, T = h.shape\n",
    "        nll = []\n",
    "        for b in range(B):\n",
    "            t_star = event_index[b].item()\n",
    "            if t_star >= 0:  # observed switch\n",
    "                surv = cumsums[b, t_star - 1] if t_star > 0 else torch.tensor(0.0, device=h.device)\n",
    "                log_h = torch.log(torch.clamp(h[b, t_star], min=eps))\n",
    "                nll.append(-(surv + log_h))\n",
    "            else:  # censored at last valid step\n",
    "                last = int(step_mask[b].nonzero(as_tuple=False)[-1])\n",
    "                surv = cumsums[b, last]\n",
    "                nll.append(-surv)\n",
    "        return torch.stack(nll).mean()\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Belief / POMDP-ish helper (very simple)\n",
    "# -------------------------------\n",
    "\n",
    "@dataclass\n",
    "class BeliefState:\n",
    "    alpha: torch.Tensor  # evidence for success\n",
    "    beta: torch.Tensor   # evidence for failure\n",
    "\n",
    "    def p_succ(self) -> torch.Tensor:\n",
    "        return self.alpha / (self.alpha + self.beta + 1e-9)\n",
    "\n",
    "\n",
    "def update_belief(\n",
    "    belief: BeliefState,\n",
    "    flash_strength: torch.Tensor,\n",
    "    miss_distance: Optional[torch.Tensor] = None,\n",
    "    decay: float = 0.95,\n",
    ") -> BeliefState:\n",
    "    \"\"\"\n",
    "    Tiny heuristic updater: decay old evidence, add flash as positive evidence,\n",
    "    add miss_distance (scaled) as negative evidence.\n",
    "    \"\"\"\n",
    "    alpha = decay * belief.alpha + flash_strength\n",
    "    if miss_distance is not None:\n",
    "        beta = decay * belief.beta + miss_distance\n",
    "    else:\n",
    "        beta = decay * belief.beta\n",
    "    return BeliefState(alpha=alpha, beta=beta)\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Datasets (stubs you will replace)\n",
    "# -------------------------------\n",
    "\n",
    "class SuccessDataset(Dataset):\n",
    "    \"\"\"Each item: (stop_features [D_s], label {0,1})\"\"\"\n",
    "    def __init__(self, X: torch.Tensor, y: torch.Tensor):\n",
    "        assert X.ndim == 2 and y.ndim == 1\n",
    "        self.X, self.y = X.float(), y.long()\n",
    "\n",
    "    def __len__(self) -> int: return len(self.y)\n",
    "\n",
    "    def __getitem__(self, i: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        return self.X[i], self.y[i].float()\n",
    "\n",
    "\n",
    "class ChoiceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Each item is a dict with keys:\n",
    "      - option_features: [K, D_o]\n",
    "      - option_mask: [K] (bool)\n",
    "      - chosen_index: int in [0, K-1]\n",
    "      - p_succ_opt: Optional [K]\n",
    "      - extra_costs: Optional [K, D_c]\n",
    "    \"\"\"\n",
    "    def __init__(self, items: List[Dict[str, torch.Tensor]]):\n",
    "        self.items = items\n",
    "\n",
    "    def __len__(self): return len(self.items)\n",
    "\n",
    "    def __getitem__(self, i: int) -> Dict[str, torch.Tensor]:\n",
    "        return self.items[i]\n",
    "\n",
    "\n",
    "def choice_collate(batch: List[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:\n",
    "    Kmax = max(item[\"option_features\"].shape[0] for item in batch)\n",
    "    B = len(batch)\n",
    "    D_o = batch[0][\"option_features\"].shape[1]\n",
    "    option_features = torch.zeros(B, Kmax, D_o)\n",
    "    mask = torch.zeros(B, Kmax, dtype=torch.bool)\n",
    "    targets = torch.zeros(B, dtype=torch.long)\n",
    "    p_succ_opt = None\n",
    "    extra_costs = None\n",
    "\n",
    "    has_ps = all(\"p_succ_opt\" in item for item in batch)\n",
    "    has_ec = all(\"extra_costs\" in item for item in batch)\n",
    "\n",
    "    if has_ps:\n",
    "        p_succ_opt = torch.zeros(B, Kmax)\n",
    "    if has_ec:\n",
    "        D_c = batch[0][\"extra_costs\"].shape[1]\n",
    "        extra_costs = torch.zeros(B, Kmax, D_c)\n",
    "\n",
    "    for b, item in enumerate(batch):\n",
    "        K = item[\"option_features\"].shape[0]\n",
    "        option_features[b, :K] = item[\"option_features\"]\n",
    "        mask[b, :K] = True\n",
    "        targets[b] = int(item[\"chosen_index\"])  # ensure within 0..K-1\n",
    "        if has_ps:\n",
    "            p_succ_opt[b, :K] = item[\"p_succ_opt\"]\n",
    "        if has_ec:\n",
    "            extra_costs[b, :K] = item[\"extra_costs\"]\n",
    "\n",
    "    out = {\"option_features\": option_features, \"mask\": mask, \"targets\": targets}\n",
    "    if has_ps:\n",
    "        out[\"p_succ_opt\"] = p_succ_opt\n",
    "    if has_ec:\n",
    "        out[\"extra_costs\"] = extra_costs\n",
    "    return out\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Training loops (minimal)\n",
    "# -------------------------------\n",
    "\n",
    "def train_success_head(model: SuccessPredictor, loader: DataLoader, epochs: int = 10, lr: float = 1e-3,\n",
    "                       device: str = \"cpu\") -> None:\n",
    "    model.to(device)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    bce = nn.BCELoss()\n",
    "    model.train()\n",
    "    for ep in range(epochs):\n",
    "        total = 0.0\n",
    "        for X, y in loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            p = model(X)\n",
    "            loss = bce(p, y)\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "            total += loss.item() * X.size(0)\n",
    "        print(f\"[Success] epoch {ep+1}: loss={total/len(loader.dataset):.4f}\")\n",
    "\n",
    "\n",
    "def train_choice_model(model: ChoiceScorer, loader: DataLoader, epochs: int = 10, lr: float = 1e-3,\n",
    "                       device: str = \"cpu\") -> None:\n",
    "    model.to(device)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    for ep in range(epochs):\n",
    "        total = 0.0\n",
    "        for batch in loader:\n",
    "            opt.zero_grad()\n",
    "            option_features = batch[\"option_features\"].to(device)\n",
    "            mask = batch[\"mask\"].to(device)\n",
    "            targets = batch[\"targets\"].to(device)\n",
    "            p_succ_opt = batch.get(\"p_succ_opt\")\n",
    "            extra_costs = batch.get(\"extra_costs\")\n",
    "            if p_succ_opt is not None: p_succ_opt = p_succ_opt.to(device)\n",
    "            if extra_costs is not None: extra_costs = extra_costs.to(device)\n",
    "\n",
    "            logits = model(option_features, mask, p_succ_opt, extra_costs)\n",
    "            loss = masked_cross_entropy(logits, targets, mask)\n",
    "            loss.backward(); opt.step()\n",
    "            total += loss.item() * option_features.size(0)\n",
    "        print(f\"[Choice] epoch {ep+1}: loss={total/len(loader.dataset):.4f}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ff_venv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
