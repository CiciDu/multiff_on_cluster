{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65873a0b",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53ee33c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set up logging configuration.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os, sys\n",
    "for p in [Path.cwd()] + list(Path.cwd().parents):\n",
    "    if p.name == 'Multifirefly-Project':\n",
    "        os.chdir(p)\n",
    "        sys.path.insert(0, str(p / 'multiff_analysis/multiff_code/methods'))\n",
    "        break\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "from data_wrangling import specific_utils, combine_info_utils\n",
    "from pattern_discovery import pattern_by_trials, pattern_by_trials, cluster_analysis, organize_patterns_and_features, category_class\n",
    "from decision_making_analysis.cluster_replacement import cluster_replacement_utils\n",
    "from decision_making_analysis.decision_making import decision_making_class, decision_making_utils, intended_targets_classes\n",
    "from decision_making_analysis.GUAT import GUAT_collect_info_class, GUAT_combine_info_class\n",
    "from decision_making_analysis.compare_GUAT_and_TAFT import GUAT_vs_TAFT_class, GUAT_vs_TAFT_x_sessions_class, helper_GUAT_vs_TAFT_class\n",
    "from visualization.matplotlib_tools import plot_trials, plot_behaviors_utils\n",
    "from visualization.animation import animation_class\n",
    "from null_behaviors import show_null_trajectory, find_best_arc, curvature_utils, curv_of_traj_utils\n",
    "from machine_learning.ml_methods import regression_utils, classification_utils, prep_ml_data_utils, hyperparam_tuning_class\n",
    "from visualization.plotly_polar_tools import plotly_utils_polar, plotly_for_ff_polar, plotly_for_trajectory_polar\n",
    "from machine_learning.ml_methods import ml_methods_class\n",
    "from visualization.dash_tools.dash_main_class_methods import dash_applied_to_GUAT_TAFT\n",
    "from decision_making_analysis.advanced_modeling import model_choices\n",
    "\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import exists\n",
    "import math\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "from scipy import stats\n",
    "from IPython.display import HTML\n",
    "from matplotlib import rc\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "import os, sys, sys\n",
    "from importlib import reload\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "\n",
    "plt.rcParams[\"animation.html\"] = \"html5\"\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "rc('animation', html='jshtml')\n",
    "matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n",
    "matplotlib.rcParams['animation.embed_limit'] = 2**128\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "np.set_printoptions(suppress=True)\n",
    "pd.options.display.max_rows = 50\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b4d9b4",
   "metadata": {},
   "source": [
    "# try now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871523ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from decision_making_analysis.advanced_modeling.choice_model import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db83a7f9",
   "metadata": {},
   "source": [
    "# try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac550fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiff_code/methods/decision_making_analysis/advanced_modeling/model_choices.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c16db46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fee4d2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Success] epoch 1: loss=0.6924\n",
      "[Success] epoch 2: loss=0.6911\n",
      "[Choice] epoch 1: loss=1.1762\n",
      "[Choice] epoch 2: loss=1.1666\n",
      "Smoke test complete.\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "# 3A) Success head on random data\n",
    "Xs = torch.randn(512, 10)\n",
    "ys = torch.bernoulli(torch.full((512,), 0.5))\n",
    "ds = model_choices.SuccessDataset(Xs, ys)\n",
    "dl = DataLoader(ds, batch_size=64, shuffle=True)\n",
    "succ = model_choices.SuccessPredictor(in_dim=10)\n",
    "model_choices.train_success_head(succ, dl, epochs=2)\n",
    "\n",
    "# 3B) Choice model with masking\n",
    "items = []\n",
    "for _ in range(256):\n",
    "    K = torch.randint(low=2, high=6, size=(1,)).item()  # 2..5 options\n",
    "    option_feats = torch.randn(K, 8)\n",
    "    p_succ_opt = torch.sigmoid(torch.randn(K))\n",
    "    extra_costs = torch.randn(K, 2)\n",
    "    chosen = torch.randint(low=0, high=K, size=(1,)).item()\n",
    "    items.append({\n",
    "        \"option_features\": option_feats,\n",
    "        \"p_succ_opt\": p_succ_opt,\n",
    "        \"extra_costs\": extra_costs,\n",
    "        \"option_mask\": torch.ones(K, dtype=torch.bool),\n",
    "        \"chosen_index\": torch.tensor(chosen),\n",
    "    })\n",
    "dc = model_choices.ChoiceDataset(items)\n",
    "dlc = DataLoader(dc, batch_size=32, shuffle=True, collate_fn=model_choices.choice_collate)\n",
    "chooser = model_choices.ChoiceScorer(option_dim=8, extra_cost_dim=2, use_psucc=True)\n",
    "model_choices.train_choice_model(chooser, dlc, epochs=2)\n",
    "\n",
    "print(\"Smoke test complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bff880a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'decision_making_analysis.advanced_modeling.model_choices' from '/Users/dusiyi/Documents/Multifirefly-Project/multiff_analysis/multiff_code/methods/decision_making_analysis/advanced_modeling/model_choices.py'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_choices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13b5ac5",
   "metadata": {},
   "source": [
    "# try 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4ea3ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Success] epoch 1: loss=0.7340\n",
      "[Success] epoch 2: loss=0.7292\n",
      "[Success] epoch 3: loss=0.7245\n",
      "p_hat_succ: [0.46155235171318054, 0.4903356432914734, 0.45318105816841125, 0.45039984583854675]\n"
     ]
    }
   ],
   "source": [
    "# ---- 3A: success head toy data ----\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "X_stop = torch.tensor([\n",
    "    [ 0.2, -0.3, 0.1],   # stop features (D_s = 3)\n",
    "    [ 1.0,  0.4, 0.5],\n",
    "    [-0.7,  0.2, 0.9],\n",
    "    [ 0.3,  0.1, 0.2],\n",
    "], dtype=torch.float32)\n",
    "y_inside = torch.tensor([1., 1., 0., 1.], dtype=torch.float32)\n",
    "\n",
    "ds = SuccessDataset(X_stop, y_inside)\n",
    "dl = DataLoader(ds, batch_size=2, shuffle=True)\n",
    "\n",
    "model = SuccessPredictor(in_dim=3)\n",
    "train_success_head(model, dl, epochs=3, lr=1e-3)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(\"p_hat_succ:\", model(X_stop).tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20d9097e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.875237226486206\n",
      "train loss: 0.8737525939941406\n",
      "train loss: 0.8722797632217407\n"
     ]
    }
   ],
   "source": [
    "# ---- 3B: choice model toy batch (variable K) ----\n",
    "items = [\n",
    "    {   # episode 1: K=3 options (e.g., {retry, ff#7, ff#12})\n",
    "        \"option_features\": torch.tensor([[0.1, 0.2], [-0.3, 0.5], [0.4, -0.1]], dtype=torch.float32),  # [3, D_o=2]\n",
    "        \"p_succ_opt\":     torch.tensor([0.70, 0.25, 0.55], dtype=torch.float32),                       # [3]\n",
    "        \"extra_costs\":    torch.tensor([[0.9, 0.1], [0.2, 0.7], [0.6, 0.3]], dtype=torch.float32),     # [3, D_c=2]\n",
    "        \"chosen_index\":   torch.tensor(0),  # picked the first option\n",
    "    },\n",
    "    {   # episode 2: K=2 options\n",
    "        \"option_features\": torch.tensor([[0.0, -0.2], [0.8, 0.1]], dtype=torch.float32),               # [2, 2]\n",
    "        \"p_succ_opt\":     torch.tensor([0.40, 0.65], dtype=torch.float32),                              # [2]\n",
    "        \"extra_costs\":    torch.tensor([[0.3, 0.4], [0.1, 0.2]], dtype=torch.float32),                 # [2, 2]\n",
    "        \"chosen_index\":   torch.tensor(1),\n",
    "    },\n",
    "]\n",
    "\n",
    "loader = DataLoader(ChoiceDataset(items), batch_size=2, shuffle=True, collate_fn=choice_collate)\n",
    "chooser = ChoiceScorer(option_dim=2, extra_cost_dim=2, use_psucc=True)\n",
    "\n",
    "for _ in range(3):\n",
    "    for batch in loader:\n",
    "        logits = chooser(batch[\"option_features\"], batch[\"mask\"], batch[\"p_succ_opt\"], batch[\"extra_costs\"])\n",
    "        loss = masked_cross_entropy(logits, batch[\"targets\"], batch[\"mask\"])\n",
    "        chooser.zero_grad(); loss.backward()\n",
    "        for p in chooser.parameters(): \n",
    "            if p.grad is not None:\n",
    "                p.data -= 1e-2 * p.grad   # tiny manual SGD step\n",
    "    print(\"train loss:\", float(loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba165444",
   "metadata": {},
   "source": [
    "# try 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbd2f5d",
   "metadata": {},
   "source": [
    "## run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79303aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "[Success] epoch 1: loss=0.5714\n",
      "[Success] epoch 2: loss=0.4938\n",
      "[Success] epoch 3: loss=0.4550\n",
      "[Success] epoch 4: loss=0.4413\n",
      "[Success] epoch 5: loss=0.4312\n",
      "[Success/Val] acc=0.812  corr(p, -miss)=0.38  corr(p, bright)=-0.01  corr(p, align)=0.88\n",
      "[Choice] epoch 1: loss=1.0445\n",
      "[Choice] epoch 2: loss=0.8030\n",
      "[Choice] epoch 3: loss=0.7667\n",
      "[Choice] epoch 4: loss=0.7603\n",
      "[Choice] epoch 5: loss=0.7538\n",
      "[Choice/Val] top-1 accuracy=0.704\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "# 1) Train success head on synthetic stops\n",
    "Xtr, ytr, Xva, yva = generate_synthetic_success_data(N=5000)\n",
    "succ = SuccessPredictor(in_dim=6)\n",
    "dl_tr = DataLoader(SuccessDataset(Xtr, ytr), batch_size=128, shuffle=True)\n",
    "train_success_head(succ, dl_tr, epochs=5, lr=1e-3, device=device)\n",
    "evaluate_success_head(succ.to(device), Xva.to(device), yva.to(device))\n",
    "\n",
    "# 2) Build a synthetic choice dataset that uses p_succ from the success head\n",
    "items = build_choice_items(succ, N=3000)\n",
    "ntr = int(0.85 * len(items))\n",
    "dc_tr = ChoiceDataset(items[:ntr])\n",
    "dc_va = ChoiceDataset(items[ntr:])\n",
    "dl_tr_c = DataLoader(dc_tr, batch_size=64, shuffle=True, collate_fn=choice_collate)\n",
    "dl_va_c = DataLoader(dc_va, batch_size=128, shuffle=False, collate_fn=choice_collate)\n",
    "\n",
    "chooser = ChoiceScorer(option_dim=2, extra_cost_dim=2, use_psucc=True)\n",
    "train_choice_model(chooser, dl_tr_c, epochs=5, lr=1e-3, device=device)\n",
    "acc = eval_choice_top1(chooser.to(device), dl_va_c, device=device)\n",
    "print(f\"[Choice/Val] top-1 accuracy={acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6278825e",
   "metadata": {},
   "source": [
    "## functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a2c755",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Tiny PyTorch skeleton for MultiFF retry/switch modeling.\n",
    "\n",
    "Implements the two core training steps and leaves hooks for the optional\n",
    "variants. The code is intentionally lightweight: small MLPs, masking for\n",
    "variable option counts, and minimal training loops.\n",
    "\n",
    "You can paste this into a file and adapt the Dataset stubs to your\n",
    "preprocessing.\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Utilities\n",
    "# -------------------------------\n",
    "\n",
    "\n",
    "def masked_cross_entropy(logits: torch.Tensor, targets: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Cross-entropy for variable option sets.\n",
    "    logits: [B, Kmax]\n",
    "    targets: [B] (index of chosen option in 0..K-1)\n",
    "    mask: [B, Kmax] boolean (True for valid options)\n",
    "    \"\"\"\n",
    "    # Put -inf on invalid options so softmax ignores them\n",
    "    logits_masked = logits.masked_fill(~mask, -1e9)\n",
    "    return F.cross_entropy(logits_masked, targets)\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 3B) Choice scorer over {retry} ∪ {other targets}\n",
    "# -------------------------------\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Optional variants (4)\n",
    "# -------------------------------\n",
    "\n",
    "class RetrySwitchHead(nn.Module):\n",
    "    \"\"\"Binary logit: retry vs switch after a near-miss.\"\"\"\n",
    "    def __init__(self, in_dim: int, hidden: List[int] = [32]):\n",
    "        super().__init__()\n",
    "        self.net = mlp(in_dim, hidden, 1)\n",
    "\n",
    "    def forward(self, features: torch.Tensor) -> torch.Tensor:\n",
    "        # returns P(retry)\n",
    "        return torch.sigmoid(self.net(features)).squeeze(-1)\n",
    "\n",
    "\n",
    "class TwoStagePolicy(nn.Module):\n",
    "    \"\"\"\n",
    "    Stage 1: binary retry vs switch (on near-miss features).\n",
    "    Stage 2: if switch, choose among other targets using ChoiceScorer.\n",
    "    \"\"\"\n",
    "    def __init__(self, retry_in_dim: int, option_dim: int, extra_cost_dim: int = 0, use_psucc: bool = True):\n",
    "        super().__init__()\n",
    "        self.retry_head = RetrySwitchHead(retry_in_dim)\n",
    "        self.choice = ChoiceScorer(option_dim, extra_cost_dim, use_psucc)\n",
    "\n",
    "    def forward(self, retry_features: torch.Tensor,\n",
    "                option_features: torch.Tensor, mask: torch.Tensor,\n",
    "                p_succ_opt: Optional[torch.Tensor] = None,\n",
    "                extra_costs: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        p_retry = self.retry_head(retry_features)\n",
    "        logits_switch = self.choice(option_features, mask, p_succ_opt, extra_costs)\n",
    "        return p_retry, logits_switch\n",
    "\n",
    "\n",
    "class HazardHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Discrete-time hazard after a near-miss: h_t = P(switch at t | not switched yet).\n",
    "\n",
    "    Input per step features X_t -> h_t via sigmoid(MLP).\n",
    "    Loss implemented via discrete-time survival likelihood.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim: int, hidden: List[int] = [32]):\n",
    "        super().__init__()\n",
    "        self.net = mlp(in_dim, hidden, 1)\n",
    "\n",
    "    def forward(self, step_features: torch.Tensor, step_mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        step_features: [B, T, D]\n",
    "        step_mask: [B, T] (True for valid time steps)\n",
    "        Returns hazards h_t in [0,1]: [B, T]\n",
    "        \"\"\"\n",
    "        h = torch.sigmoid(self.net(step_features)).squeeze(-1)\n",
    "        return h * step_mask.float()\n",
    "\n",
    "    @staticmethod\n",
    "    def survival_nll(h: torch.Tensor, event_index: torch.Tensor, step_mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Negative log-likelihood for discrete-time hazards.\n",
    "        h: [B, T] hazards\n",
    "        event_index: [B] index of switch time; if censored (no switch in window), set to -1\n",
    "        step_mask: [B, T]\n",
    "        \"\"\"\n",
    "        # log S_{t} = sum_{k < t} log(1 - h_k);  log f(t) = log S_t + log h_t\n",
    "        eps = 1e-6\n",
    "        log1m_h = torch.log(torch.clamp(1 - h, min=eps)) * step_mask\n",
    "        cumsums = torch.cumsum(log1m_h, dim=1)  # [B, T]\n",
    "        B, T = h.shape\n",
    "        nll = []\n",
    "        for b in range(B):\n",
    "            t_star = event_index[b].item()\n",
    "            if t_star >= 0:  # observed switch\n",
    "                surv = cumsums[b, t_star - 1] if t_star > 0 else torch.tensor(0.0, device=h.device)\n",
    "                log_h = torch.log(torch.clamp(h[b, t_star], min=eps))\n",
    "                nll.append(-(surv + log_h))\n",
    "            else:  # censored at last valid step\n",
    "                last = int(step_mask[b].nonzero(as_tuple=False)[-1])\n",
    "                surv = cumsums[b, last]\n",
    "                nll.append(-surv)\n",
    "        return torch.stack(nll).mean()\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Belief / POMDP-ish helper (very simple)\n",
    "# -------------------------------\n",
    "\n",
    "@dataclass\n",
    "class BeliefState:\n",
    "    alpha: torch.Tensor  # evidence for success\n",
    "    beta: torch.Tensor   # evidence for failure\n",
    "\n",
    "    def p_succ(self) -> torch.Tensor:\n",
    "        return self.alpha / (self.alpha + self.beta + 1e-9)\n",
    "\n",
    "\n",
    "def update_belief(\n",
    "    belief: BeliefState,\n",
    "    flash_strength: torch.Tensor,\n",
    "    miss_distance: Optional[torch.Tensor] = None,\n",
    "    decay: float = 0.95,\n",
    ") -> BeliefState:\n",
    "    \"\"\"\n",
    "    Tiny heuristic updater: decay old evidence, add flash as positive evidence,\n",
    "    add miss_distance (scaled) as negative evidence.\n",
    "    \"\"\"\n",
    "    alpha = decay * belief.alpha + flash_strength\n",
    "    if miss_distance is not None:\n",
    "        beta = decay * belief.beta + miss_distance\n",
    "    else:\n",
    "        beta = decay * belief.beta\n",
    "    return BeliefState(alpha=alpha, beta=beta)\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Datasets (stubs you will replace)\n",
    "# -------------------------------\n",
    "\n",
    "class SuccessDataset(Dataset):\n",
    "    \"\"\"Each item: (stop_features [D_s], label {0,1})\"\"\"\n",
    "    def __init__(self, X: torch.Tensor, y: torch.Tensor):\n",
    "        assert X.ndim == 2 and y.ndim == 1\n",
    "        self.X, self.y = X.float(), y.long()\n",
    "\n",
    "    def __len__(self) -> int: return len(self.y)\n",
    "\n",
    "    def __getitem__(self, i: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        return self.X[i], self.y[i].float()\n",
    "\n",
    "\n",
    "class ChoiceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Each item is a dict with keys:\n",
    "      - option_features: [K, D_o]\n",
    "      - option_mask: [K] (bool)\n",
    "      - chosen_index: int in [0, K-1]\n",
    "      - p_succ_opt: Optional [K]\n",
    "      - extra_costs: Optional [K, D_c]\n",
    "    \"\"\"\n",
    "    def __init__(self, items: List[Dict[str, torch.Tensor]]):\n",
    "        self.items = items\n",
    "\n",
    "    def __len__(self): return len(self.items)\n",
    "\n",
    "    def __getitem__(self, i: int) -> Dict[str, torch.Tensor]:\n",
    "        return self.items[i]\n",
    "\n",
    "\n",
    "def choice_collate(batch: List[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:\n",
    "    Kmax = max(item[\"option_features\"].shape[0] for item in batch)\n",
    "    B = len(batch)\n",
    "    D_o = batch[0][\"option_features\"].shape[1]\n",
    "    option_features = torch.zeros(B, Kmax, D_o)\n",
    "    mask = torch.zeros(B, Kmax, dtype=torch.bool)\n",
    "    targets = torch.zeros(B, dtype=torch.long)\n",
    "    p_succ_opt = None\n",
    "    extra_costs = None\n",
    "\n",
    "    has_ps = all(\"p_succ_opt\" in item for item in batch)\n",
    "    has_ec = all(\"extra_costs\" in item for item in batch)\n",
    "\n",
    "    if has_ps:\n",
    "        p_succ_opt = torch.zeros(B, Kmax)\n",
    "    if has_ec:\n",
    "        D_c = batch[0][\"extra_costs\"].shape[1]\n",
    "        extra_costs = torch.zeros(B, Kmax, D_c)\n",
    "\n",
    "    for b, item in enumerate(batch):\n",
    "        K = item[\"option_features\"].shape[0]\n",
    "        option_features[b, :K] = item[\"option_features\"]\n",
    "        mask[b, :K] = True\n",
    "        targets[b] = int(item[\"chosen_index\"])  # ensure within 0..K-1\n",
    "        if has_ps:\n",
    "            p_succ_opt[b, :K] = item[\"p_succ_opt\"]\n",
    "        if has_ec:\n",
    "            extra_costs[b, :K] = item[\"extra_costs\"]\n",
    "\n",
    "    out = {\"option_features\": option_features, \"mask\": mask, \"targets\": targets}\n",
    "    if has_ps:\n",
    "        out[\"p_succ_opt\"] = p_succ_opt\n",
    "    if has_ec:\n",
    "        out[\"extra_costs\"] = extra_costs\n",
    "    return out\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Training loops (minimal)\n",
    "# -------------------------------\n",
    "\n",
    "def train_success_head(model: SuccessPredictor, loader: DataLoader, epochs: int = 10, lr: float = 1e-3,\n",
    "                       device: str = \"cpu\") -> None:\n",
    "    model.to(device)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    bce = nn.BCELoss()\n",
    "    model.train()\n",
    "    for ep in range(epochs):\n",
    "        total = 0.0\n",
    "        for X, y in loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            p = model(X)\n",
    "            loss = bce(p, y)\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "            total += loss.item() * X.size(0)\n",
    "        print(f\"[Success] epoch {ep+1}: loss={total/len(loader.dataset):.4f}\")\n",
    "\n",
    "\n",
    "def train_choice_model(model: ChoiceScorer, loader: DataLoader, epochs: int = 10, lr: float = 1e-3,\n",
    "                       device: str = \"cpu\") -> None:\n",
    "    model.to(device)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    for ep in range(epochs):\n",
    "        total = 0.0\n",
    "        for batch in loader:\n",
    "            opt.zero_grad()\n",
    "            option_features = batch[\"option_features\"].to(device)\n",
    "            mask = batch[\"mask\"].to(device)\n",
    "            targets = batch[\"targets\"].to(device)\n",
    "            p_succ_opt = batch.get(\"p_succ_opt\")\n",
    "            extra_costs = batch.get(\"extra_costs\")\n",
    "            if p_succ_opt is not None: p_succ_opt = p_succ_opt.to(device)\n",
    "            if extra_costs is not None: extra_costs = extra_costs.to(device)\n",
    "\n",
    "            logits = model(option_features, mask, p_succ_opt, extra_costs)\n",
    "            loss = masked_cross_entropy(logits, targets, mask)\n",
    "            loss.backward(); opt.step()\n",
    "            total += loss.item() * option_features.size(0)\n",
    "        print(f\"[Choice] epoch {ep+1}: loss={total/len(loader.dataset):.4f}\")\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Worked example with synthetic data (end-to-end)\n",
    "# -------------------------------\n",
    "\n",
    "def _pearsonr(x: torch.Tensor, y: torch.Tensor) -> float:\n",
    "    xm = (x - x.mean()) / (x.std() + 1e-8)\n",
    "    ym = (y - y.mean()) / (y.std() + 1e-8)\n",
    "    return float((xm * ym).mean().item())\n",
    "\n",
    "\n",
    "def generate_synthetic_success_data(N: int = 4000) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Return train/val splits for the success head.\n",
    "    Feature order (Ds=6): [miss_dist, flash_recency, flash_brightness, alignment, speed, curvature]\n",
    "    \"\"\"\n",
    "    torch.manual_seed(42)\n",
    "    miss = torch.rand(N) * 1.5                              # larger = worse\n",
    "    recency = torch.rand(N)                                  # more recent flash = better (lower recency)\n",
    "    bright = torch.rand(N)                                   # brighter = better\n",
    "    align = torch.rand(N) * 2 - 1                            # cosine alignment [-1,1]\n",
    "    speed = 0.5 + 0.5 * torch.rand(N)                        # 0.5..1.0\n",
    "    curvature = 0.3 * torch.rand(N)                          # small turns\n",
    "\n",
    "    X = torch.stack([miss, recency, bright, align, speed, curvature], dim=1)\n",
    "\n",
    "    # Ground-truth logit for synthetic labels\n",
    "    lin = (\n",
    "        -1.0\n",
    "        + (-2.5) * miss\n",
    "        + (-0.8) * recency\n",
    "        + (2.0) * bright\n",
    "        + (1.6) * align\n",
    "        + (0.3) * speed\n",
    "        + (-0.2) * curvature\n",
    "    )\n",
    "    p = torch.sigmoid(lin + 0.25 * torch.randn(N))\n",
    "    y = torch.bernoulli(p).float()\n",
    "\n",
    "    idx = torch.randperm(N)\n",
    "    ntr = int(0.8 * N)\n",
    "    tr, va = idx[:ntr], idx[ntr:]\n",
    "    return X[tr], y[tr], X[va], y[va]\n",
    "\n",
    "\n",
    "def evaluate_success_head(model: SuccessPredictor, Xva: torch.Tensor, yva: torch.Tensor) -> None:\n",
    "    with torch.no_grad():\n",
    "        p = model(Xva)\n",
    "        acc = ((p > 0.5) == (yva > 0.5)).float().mean().item()\n",
    "        rho_miss = _pearsonr(p, -Xva[:, 0])           # higher when miss smaller\n",
    "        rho_bri = _pearsonr(p, Xva[:, 2])             # higher when brighter\n",
    "        rho_align = _pearsonr(p, Xva[:, 3])           # higher when aligned\n",
    "    print(f\"[Success/Val] acc={acc:.3f}  corr(p, -miss)={rho_miss:.2f}  corr(p, bright)={rho_bri:.2f}  corr(p, align)={rho_align:.2f}\")\n",
    "\n",
    "\n",
    "def build_choice_items(success_model: SuccessPredictor, N: int = 2500) -> List[Dict[str, torch.Tensor]]:\n",
    "    \"\"\"Construct a synthetic choice dataset using the same latent factors.\n",
    "    We include p_succ from the trained success head.\n",
    "    option_features: [alignment, cluster_density]  (Do=2)\n",
    "    extra_costs:    [time_to_go, turn_cost]       (Dc=2)\n",
    "    \"\"\"\n",
    "    torch.manual_seed(123)\n",
    "    items: List[Dict[str, torch.Tensor]] = []\n",
    "    for _ in range(N):\n",
    "        K = int(torch.randint(low=2, high=6, size=(1,)).item())  # 2..5 options\n",
    "\n",
    "        # Latents per option\n",
    "        cluster = torch.rand(K)                           # more = denser cluster nearby\n",
    "        distance = 0.2 + 1.8 * torch.rand(K)             # time-to-go proxy\n",
    "        turn = torch.rand(K)                              # normalized turn cost 0..1\n",
    "        align = torch.clamp(1.0 - 2.0 * turn + 0.2 * torch.randn(K), -1.0, 1.0)  # correlated with turn\n",
    "        recency = torch.rand(K)\n",
    "        bright = torch.rand(K)\n",
    "        speed = 0.5 + 0.5 * torch.rand(K)\n",
    "        curvature = 0.3 * torch.rand(K)\n",
    "        miss = torch.clamp(0.2 * distance + 0.7 * (1 - bright) + 0.1 * torch.randn(K), 0.0, 1.5)\n",
    "\n",
    "        # True success logits (same as in success generator, without extra noise)\n",
    "        lin_true = (\n",
    "            -1.0 + (-2.5) * miss + (-0.8) * recency + 2.0 * bright + 1.6 * align + 0.3 * speed + (-0.2) * curvature\n",
    "        )\n",
    "        p_true = torch.sigmoid(lin_true)\n",
    "\n",
    "        # p_succ from the trained head\n",
    "        stop_feats = torch.stack([miss, recency, bright, align, speed, curvature], dim=1)\n",
    "        with torch.no_grad():\n",
    "            p_hat = success_model(stop_feats)\n",
    "\n",
    "        # Underlying utility that generates the observed choice\n",
    "        # U = w1*logit(p_succ_true) - w2*time - w3*turn + w4*cluster + w5*align + noise\n",
    "        logit_true = torch.logit(p_true.clamp(1e-4, 1 - 1e-4))\n",
    "        U = 1.6 * logit_true - 1.0 * distance - 0.8 * turn + 0.6 * cluster + 0.3 * align + 0.30 * torch.randn(K)\n",
    "        chosen = int(torch.argmax(U).item())\n",
    "\n",
    "        option_features = torch.stack([align, cluster], dim=1)  # [K,2]\n",
    "        extra_costs = torch.stack([distance, turn], dim=1)      # [K,2]\n",
    "\n",
    "        items.append({\n",
    "            \"option_features\": option_features,\n",
    "            \"extra_costs\": extra_costs,\n",
    "            \"p_succ_opt\": p_hat,\n",
    "            \"option_mask\": torch.ones(K, dtype=torch.bool),\n",
    "            \"chosen_index\": torch.tensor(chosen),\n",
    "        })\n",
    "    return items\n",
    "\n",
    "\n",
    "def eval_choice_top1(model: ChoiceScorer, loader: DataLoader, device: str = \"cpu\") -> float:\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            option_features = batch[\"option_features\"].to(device)\n",
    "            mask = batch[\"mask\"].to(device)\n",
    "            targets = batch[\"targets\"].to(device)\n",
    "            p_succ_opt = batch.get(\"p_succ_opt\")\n",
    "            extra_costs = batch.get(\"extra_costs\")\n",
    "            if p_succ_opt is not None: p_succ_opt = p_succ_opt.to(device)\n",
    "            if extra_costs is not None: extra_costs = extra_costs.to(device)\n",
    "\n",
    "            logits = model(option_features, mask, p_succ_opt, extra_costs)\n",
    "            pred = torch.argmax(logits, dim=1)\n",
    "            correct += (pred == targets).sum().item()\n",
    "            total += targets.numel()\n",
    "    return correct / max(1, total)\n",
    "\n",
    "\n",
    "\n",
    "# --- Example with evaluation & validation (drop-in runnable) ---\n",
    "# You can import this file as a module and call `run_toy_example_with_eval()` to see\n",
    "# a full train/val split, metrics (accuracy, log-loss, ROC-AUC for success head;\n",
    "# top-1/top-2 accuracy & NLL for the choice model), and a concrete episode that\n",
    "# matches the toy you asked about.\n",
    "\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "def _roc_auc_basic(y_true: np.ndarray, y_score: np.ndarray) -> float:\n",
    "    \"\"\"Compute ROC-AUC from scratch (no sklearn).\"\"\"\n",
    "    y_true = y_true.astype(np.float64)\n",
    "    y_score = y_score.astype(np.float64)\n",
    "    n_pos = int(y_true.sum())\n",
    "    n_neg = int(len(y_true) - n_pos)\n",
    "    if n_pos == 0 or n_neg == 0:\n",
    "        return float(\"nan\")\n",
    "    # Rank scores (average tie handling via argsort twice)\n",
    "    order = np.argsort(y_score)\n",
    "    ranks = np.empty_like(order)\n",
    "    ranks[order] = np.arange(1, len(y_score) + 1)  # 1-based ranks\n",
    "    sum_ranks_pos = ranks[y_true == 1].sum()\n",
    "    auc = (sum_ranks_pos - n_pos * (n_pos + 1) / 2) / (n_pos * n_neg)\n",
    "    return float(auc)\n",
    "\n",
    "\n",
    "def _brier(y_true: np.ndarray, p: np.ndarray) -> float:\n",
    "    return float(np.mean((p - y_true) ** 2))\n",
    "\n",
    "\n",
    "def _make_toy_success_data(n: int = 1200, d: int = 6, seed: int = 123) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    rng = np.random.default_rng(seed)\n",
    "    X = rng.normal(size=(n, d))\n",
    "    w = rng.normal(size=(d,))\n",
    "    b = 0.15\n",
    "    logits = X @ w + b\n",
    "    p = 1 / (1 + np.exp(-logits))\n",
    "    y = rng.binomial(1, p)\n",
    "    return X.astype(np.float32), y.astype(np.float32)\n",
    "\n",
    "\n",
    "def _toy_episode_you_requested() -> Dict[str, torch.Tensor]:\n",
    "    return {\n",
    "        # episode 1: K=3 options (e.g., {retry, ff#7, ff#12})\n",
    "        \"option_features\": torch.tensor([[0.1, 0.2], [-0.3, 0.5], [0.4, -0.1]], dtype=torch.float32),  # [3, D_o=2]\n",
    "        \"p_succ_opt\":     torch.tensor([0.70, 0.25, 0.55], dtype=torch.float32),                       # [3]\n",
    "        \"extra_costs\":    torch.tensor([[0.9, 0.1], [0.2, 0.7], [0.6, 0.3]], dtype=torch.float32),     # [3, D_c=2]\n",
    "        \"chosen_index\":   torch.tensor(0),  # picked the first option (retry)\n",
    "    }\n",
    "\n",
    "\n",
    "def _make_toy_choice_items(n_episodes: int = 600, Do: int = 2, Dc: int = 2, seed: int = 7) -> List[Dict[str, torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Generates variable-K episodes. Includes the user-specified first episode, followed by\n",
    "    synthetic episodes where the 'observed' choice is argmax of a latent utility.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    items: List[Dict[str, torch.Tensor]] = []\n",
    "    # include the requested episode first\n",
    "    items.append(_toy_episode_you_requested())\n",
    "    # latent weights for generator\n",
    "    w_feat = rng.normal(size=(Do,))\n",
    "    w_cost = -0.6 * np.ones(Dc)\n",
    "    alpha_ps = 1.0\n",
    "    for _ in range(n_episodes - 1):\n",
    "        K = rng.integers(2, 6)\n",
    "        feat = rng.normal(size=(K, Do))\n",
    "        ps_logits = 0.9 * feat[:, 0] + 0.7 * (feat[:, 1] if Do > 1 else 0) + rng.normal(scale=0.4, size=K)\n",
    "        p_succ = 1 / (1 + np.exp(-ps_logits))\n",
    "        costs = rng.normal(size=(K, Dc))\n",
    "        util = feat @ w_feat + alpha_ps * p_succ + costs @ w_cost + rng.normal(scale=0.25, size=K)\n",
    "        chosen = int(np.argmax(util))\n",
    "        items.append({\n",
    "            \"option_features\": torch.tensor(feat, dtype=torch.float32),\n",
    "            \"p_succ_opt\": torch.tensor(p_succ, dtype=torch.float32),\n",
    "            \"extra_costs\": torch.tensor(costs, dtype=torch.float32),\n",
    "            \"chosen_index\": torch.tensor(chosen),\n",
    "        })\n",
    "    return items\n",
    "\n",
    "\n",
    "def _split_idx(n: int, val_frac: float = 0.2, seed: int = 0) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    rng = np.random.default_rng(seed)\n",
    "    perm = rng.permutation(n)\n",
    "    cut = int((1 - val_frac) * n)\n",
    "    return perm[:cut], perm[cut:]\n",
    "\n",
    "\n",
    "def _eval_choice(model: ChoiceScorer, loader: DataLoader, device: str = \"cpu\") -> Dict[str, Any]:\n",
    "    model.eval()\n",
    "    n, correct1, correct2 = 0, 0, 0\n",
    "    total_nll = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            option_features = batch[\"option_features\"].to(device)\n",
    "            mask = batch[\"mask\"].to(device)\n",
    "            targets = batch[\"targets\"].to(device)\n",
    "            p_succ_opt = batch.get(\"p_succ_opt\")\n",
    "            extra_costs = batch.get(\"extra_costs\")\n",
    "            if p_succ_opt is not None: p_succ_opt = p_succ_opt.to(device)\n",
    "            if extra_costs is not None: extra_costs = extra_costs.to(device)\n",
    "            logits = model(option_features, mask, p_succ_opt, extra_costs)\n",
    "            # CE per-sample\n",
    "            nll = masked_cross_entropy(logits, targets, mask)\n",
    "            total_nll += float(nll) * targets.size(0)\n",
    "            # top-1\n",
    "            top1 = torch.argmax(logits, dim=1)\n",
    "            correct1 += int((top1 == targets).sum())\n",
    "            # top-2\n",
    "            top2_vals, top2_idx = torch.topk(logits, k=2, dim=1)\n",
    "            correct2 += int(((top2_idx[:, 0] == targets) | (top2_idx[:, 1] == targets)).sum())\n",
    "            n += targets.size(0)\n",
    "    return {\n",
    "        \"top1_acc\": correct1 / max(n, 1),\n",
    "        \"top2_acc\": correct2 / max(n, 1),\n",
    "        \"avg_nll\": total_nll / max(n, 1),\n",
    "    }\n",
    "\n",
    "\n",
    "def run_toy_example_with_eval(device: str = \"cpu\") -> None:\n",
    "    \"\"\"\n",
    "    End-to-end demo with train/val split and metrics for both heads.\n",
    "    Prints:\n",
    "      - Success head: accuracy, log-loss, ROC-AUC, Brier score on val set\n",
    "      - Choice model: top-1 / top-2 accuracy and average NLL on val set\n",
    "    \"\"\"\n",
    "    # --- 3A) Success head ---\n",
    "    X, y = _make_toy_success_data(n=2000, d=6, seed=11)\n",
    "    tr_idx, va_idx = _split_idx(len(y), val_frac=0.2, seed=11)\n",
    "    Xtr, ytr = X[tr_idx], y[tr_idx]\n",
    "    Xva, yva = X[va_idx], y[va_idx]\n",
    "\n",
    "    succ_model = SuccessPredictor(in_dim=X.shape[1])\n",
    "    tr_loader = DataLoader(SuccessDataset(Xtr, ytr), batch_size=64, shuffle=True)\n",
    "    train_success_head(succ_model, tr_loader, epochs=8, lr=2e-3, device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        p_va = succ_model(torch.tensor(Xva, dtype=torch.float32)).cpu().numpy()\n",
    "    acc = float(((p_va >= 0.5).astype(np.float32) == yva).mean())\n",
    "    eps = 1e-6\n",
    "    logloss = float(-(yva * np.log(np.clip(p_va, eps, 1 - eps)) + (1 - yva) * np.log(np.clip(1 - p_va, eps, 1 - eps))).mean())\n",
    "    auc = _roc_auc_basic(yva, p_va)\n",
    "    brier = _brier(yva, p_va)\n",
    "\n",
    "    print(\"=== Success head (val) ===\")\n",
    "    print(f\"Accuracy: {acc:.3f}  LogLoss: {logloss:.3f}  ROC-AUC: {auc:.3f}  Brier: {brier:.3f}\")\n",
    "\n",
    "    # --- 3B) Choice model ---\n",
    "    items = _make_toy_choice_items(n_episodes=1200, Do=2, Dc=2, seed=22)\n",
    "    tr_idx, va_idx = _split_idx(len(items), val_frac=0.2, seed=22)\n",
    "    train_items = [items[i] for i in tr_idx]\n",
    "    val_items   = [items[i] for i in va_idx]\n",
    "\n",
    "    train_loader = DataLoader(ChoiceDataset(train_items), batch_size=64, shuffle=True, collate_fn=choice_collate)\n",
    "    val_loader   = DataLoader(ChoiceDataset(val_items),   batch_size=64, shuffle=False, collate_fn=choice_collate)\n",
    "\n",
    "    chooser = ChoiceScorer(option_dim=2, extra_cost_dim=2, use_psucc=True)\n",
    "    train_choice_model(chooser, train_loader, epochs=8, lr=2e-3, device=device)\n",
    "\n",
    "    stats = _eval_choice(chooser, val_loader, device=device)\n",
    "    print(\"=== Choice model (val) ===\")\n",
    "    print(f\"Top-1 Acc: {stats['top1_acc']:.3f}  Top-2 Acc: {stats['top2_acc']:.3f}  Avg NLL: {stats['avg_nll']:.3f}\")\n",
    "\n",
    "    # Also print the first episode (your requested one) with the model's softmax\n",
    "    # scores so you can see how it ranks {retry, ff#7, ff#12}.\n",
    "    first = items[0]\n",
    "    with torch.no_grad():\n",
    "        logits = chooser(\n",
    "            first[\"option_features\"][None, ...],\n",
    "            torch.ones(1, first[\"option_features\"].shape[0], dtype=torch.bool),\n",
    "            first[\"p_succ_opt\"][None, ...],\n",
    "            first[\"extra_costs\"][None, ...],\n",
    "        )\n",
    "        probs = torch.softmax(logits, dim=1).cpu().numpy()[0]\n",
    "    print(\"Episode 1 — softmax over options [retry, ff#7, ff#12]:\", np.round(probs, 3).tolist(), \" chosen:\", int(first[\"chosen_index\"]))\n",
    "\n",
    "# End of example section\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af5d5be",
   "metadata": {},
   "source": [
    "# functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "737ee0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Success] epoch 1: loss=0.6924\n",
      "[Success] epoch 2: loss=0.6911\n",
      "[Choice] epoch 1: loss=1.1762\n",
      "[Choice] epoch 2: loss=1.1666\n",
      "Smoke test complete.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Tiny PyTorch skeleton for MultiFF retry/switch modeling.\n",
    "\n",
    "Implements the two core training steps and leaves hooks for the optional\n",
    "variants. The code is intentionally lightweight: small MLPs, masking for\n",
    "variable option counts, and minimal training loops.\n",
    "\n",
    "You can paste this into a file and adapt the Dataset stubs to your\n",
    "preprocessing.\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Utilities\n",
    "# -------------------------------\n",
    "\n",
    "def mlp(in_dim: int, hidden: List[int], out_dim: int, dropout: float = 0.0) -> nn.Sequential:\n",
    "    layers: List[nn.Module] = []\n",
    "    last = in_dim\n",
    "    for h in hidden:\n",
    "        layers += [nn.Linear(last, h), nn.ReLU()]  # keep it simple\n",
    "        if dropout > 0:\n",
    "            layers += [nn.Dropout(dropout)]\n",
    "        last = h\n",
    "    layers += [nn.Linear(last, out_dim)]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "def masked_cross_entropy(logits: torch.Tensor, targets: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Cross-entropy for variable option sets.\n",
    "    logits: [B, Kmax]\n",
    "    targets: [B] (index of chosen option in 0..K-1)\n",
    "    mask: [B, Kmax] boolean (True for valid options)\n",
    "    \"\"\"\n",
    "    # Put -inf on invalid options so softmax ignores them\n",
    "    logits_masked = logits.masked_fill(~mask, -1e9)\n",
    "    return F.cross_entropy(logits_masked, targets)\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 3A) Capture-success predictor\n",
    "# -------------------------------\n",
    "\n",
    "class SuccessPredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    Predicts p_succ = P(next stop is inside boundary for that target).\n",
    "\n",
    "    Inputs: stop_features: [B, D_s]\n",
    "    Output: p_succ: [B]\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim: int, hidden: List[int] = [32]):\n",
    "        super().__init__()\n",
    "        self.net = mlp(in_dim, hidden, 1, dropout=0.0)\n",
    "\n",
    "    def forward(self, stop_features: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.sigmoid(self.net(stop_features)).squeeze(-1)\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 3B) Choice scorer over {retry} ∪ {other targets}\n",
    "# -------------------------------\n",
    "\n",
    "class ChoiceScorer(nn.Module):\n",
    "    \"\"\"\n",
    "    Scores each candidate option with a utility U_i(t) and produces logits.\n",
    "\n",
    "    Forward inputs:\n",
    "      - option_features: [B, Kmax, D_o]\n",
    "      - mask: [B, Kmax] boolean (True for valid candidates)\n",
    "      - p_succ_opt: Optional[Tensor] [B, Kmax] (per-option success proba)\n",
    "      - extra_costs: Optional[Tensor] [B, Kmax, D_c] (e.g., time-to-go, turn cost)\n",
    "\n",
    "    Returns:\n",
    "      - logits: [B, Kmax]\n",
    "    \"\"\"\n",
    "    def __init__(self, option_dim: int, extra_cost_dim: int = 0, use_psucc: bool = True,\n",
    "                 hidden: List[int] = [64, 32]):\n",
    "        super().__init__()\n",
    "        self.use_psucc = use_psucc\n",
    "        in_dim = option_dim + extra_cost_dim + (1 if use_psucc else 0)\n",
    "        self.net = mlp(in_dim, hidden, 1, dropout=0.0)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        option_features: torch.Tensor,\n",
    "        mask: torch.Tensor,\n",
    "        p_succ_opt: Optional[torch.Tensor] = None,\n",
    "        extra_costs: Optional[torch.Tensor] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        B, Kmax, D_o = option_features.shape\n",
    "        feats = [option_features]\n",
    "        if self.use_psucc:\n",
    "            if p_succ_opt is None:\n",
    "                raise ValueError(\"p_succ_opt must be provided when use_psucc=True\")\n",
    "            feats.append(p_succ_opt.unsqueeze(-1))  # [B, Kmax, 1]\n",
    "        if extra_costs is not None:\n",
    "            feats.append(extra_costs)\n",
    "        X = torch.cat(feats, dim=-1)  # [B, Kmax, Din]\n",
    "        logits = self.net(X).squeeze(-1)  # [B, Kmax]\n",
    "        # Mask invalid options with -inf to avoid accidental selection\n",
    "        logits = logits.masked_fill(~mask, -1e9)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Optional variants (4)\n",
    "# -------------------------------\n",
    "\n",
    "class RetrySwitchHead(nn.Module):\n",
    "    \"\"\"Binary logit: retry vs switch after a near-miss.\"\"\"\n",
    "    def __init__(self, in_dim: int, hidden: List[int] = [32]):\n",
    "        super().__init__()\n",
    "        self.net = mlp(in_dim, hidden, 1)\n",
    "\n",
    "    def forward(self, features: torch.Tensor) -> torch.Tensor:\n",
    "        # returns P(retry)\n",
    "        return torch.sigmoid(self.net(features)).squeeze(-1)\n",
    "\n",
    "\n",
    "class TwoStagePolicy(nn.Module):\n",
    "    \"\"\"\n",
    "    Stage 1: binary retry vs switch (on near-miss features).\n",
    "    Stage 2: if switch, choose among other targets using ChoiceScorer.\n",
    "    \"\"\"\n",
    "    def __init__(self, retry_in_dim: int, option_dim: int, extra_cost_dim: int = 0, use_psucc: bool = True):\n",
    "        super().__init__()\n",
    "        self.retry_head = RetrySwitchHead(retry_in_dim)\n",
    "        self.choice = ChoiceScorer(option_dim, extra_cost_dim, use_psucc)\n",
    "\n",
    "    def forward(self, retry_features: torch.Tensor,\n",
    "                option_features: torch.Tensor, mask: torch.Tensor,\n",
    "                p_succ_opt: Optional[torch.Tensor] = None,\n",
    "                extra_costs: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        p_retry = self.retry_head(retry_features)\n",
    "        logits_switch = self.choice(option_features, mask, p_succ_opt, extra_costs)\n",
    "        return p_retry, logits_switch\n",
    "\n",
    "\n",
    "class HazardHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Discrete-time hazard after a near-miss: h_t = P(switch at t | not switched yet).\n",
    "\n",
    "    Input per step features X_t -> h_t via sigmoid(MLP).\n",
    "    Loss implemented via discrete-time survival likelihood.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim: int, hidden: List[int] = [32]):\n",
    "        super().__init__()\n",
    "        self.net = mlp(in_dim, hidden, 1)\n",
    "\n",
    "    def forward(self, step_features: torch.Tensor, step_mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        step_features: [B, T, D]\n",
    "        step_mask: [B, T] (True for valid time steps)\n",
    "        Returns hazards h_t in [0,1]: [B, T]\n",
    "        \"\"\"\n",
    "        h = torch.sigmoid(self.net(step_features)).squeeze(-1)\n",
    "        return h * step_mask.float()\n",
    "\n",
    "    @staticmethod\n",
    "    def survival_nll(h: torch.Tensor, event_index: torch.Tensor, step_mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Negative log-likelihood for discrete-time hazards.\n",
    "        h: [B, T] hazards\n",
    "        event_index: [B] index of switch time; if censored (no switch in window), set to -1\n",
    "        step_mask: [B, T]\n",
    "        \"\"\"\n",
    "        # log S_{t} = sum_{k < t} log(1 - h_k);  log f(t) = log S_t + log h_t\n",
    "        eps = 1e-6\n",
    "        log1m_h = torch.log(torch.clamp(1 - h, min=eps)) * step_mask\n",
    "        cumsums = torch.cumsum(log1m_h, dim=1)  # [B, T]\n",
    "        B, T = h.shape\n",
    "        nll = []\n",
    "        for b in range(B):\n",
    "            t_star = event_index[b].item()\n",
    "            if t_star >= 0:  # observed switch\n",
    "                surv = cumsums[b, t_star - 1] if t_star > 0 else torch.tensor(0.0, device=h.device)\n",
    "                log_h = torch.log(torch.clamp(h[b, t_star], min=eps))\n",
    "                nll.append(-(surv + log_h))\n",
    "            else:  # censored at last valid step\n",
    "                last = int(step_mask[b].nonzero(as_tuple=False)[-1])\n",
    "                surv = cumsums[b, last]\n",
    "                nll.append(-surv)\n",
    "        return torch.stack(nll).mean()\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Belief / POMDP-ish helper (very simple)\n",
    "# -------------------------------\n",
    "\n",
    "@dataclass\n",
    "class BeliefState:\n",
    "    alpha: torch.Tensor  # evidence for success\n",
    "    beta: torch.Tensor   # evidence for failure\n",
    "\n",
    "    def p_succ(self) -> torch.Tensor:\n",
    "        return self.alpha / (self.alpha + self.beta + 1e-9)\n",
    "\n",
    "\n",
    "def update_belief(\n",
    "    belief: BeliefState,\n",
    "    flash_strength: torch.Tensor,\n",
    "    miss_distance: Optional[torch.Tensor] = None,\n",
    "    decay: float = 0.95,\n",
    ") -> BeliefState:\n",
    "    \"\"\"\n",
    "    Tiny heuristic updater: decay old evidence, add flash as positive evidence,\n",
    "    add miss_distance (scaled) as negative evidence.\n",
    "    \"\"\"\n",
    "    alpha = decay * belief.alpha + flash_strength\n",
    "    if miss_distance is not None:\n",
    "        beta = decay * belief.beta + miss_distance\n",
    "    else:\n",
    "        beta = decay * belief.beta\n",
    "    return BeliefState(alpha=alpha, beta=beta)\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Datasets (stubs you will replace)\n",
    "# -------------------------------\n",
    "\n",
    "class SuccessDataset(Dataset):\n",
    "    \"\"\"Each item: (stop_features [D_s], label {0,1})\"\"\"\n",
    "    def __init__(self, X: torch.Tensor, y: torch.Tensor):\n",
    "        assert X.ndim == 2 and y.ndim == 1\n",
    "        self.X, self.y = X.float(), y.long()\n",
    "\n",
    "    def __len__(self) -> int: return len(self.y)\n",
    "\n",
    "    def __getitem__(self, i: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        return self.X[i], self.y[i].float()\n",
    "\n",
    "\n",
    "class ChoiceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Each item is a dict with keys:\n",
    "      - option_features: [K, D_o]\n",
    "      - option_mask: [K] (bool)\n",
    "      - chosen_index: int in [0, K-1]\n",
    "      - p_succ_opt: Optional [K]\n",
    "      - extra_costs: Optional [K, D_c]\n",
    "    \"\"\"\n",
    "    def __init__(self, items: List[Dict[str, torch.Tensor]]):\n",
    "        self.items = items\n",
    "\n",
    "    def __len__(self): return len(self.items)\n",
    "\n",
    "    def __getitem__(self, i: int) -> Dict[str, torch.Tensor]:\n",
    "        return self.items[i]\n",
    "\n",
    "\n",
    "def choice_collate(batch: List[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:\n",
    "    Kmax = max(item[\"option_features\"].shape[0] for item in batch)\n",
    "    B = len(batch)\n",
    "    D_o = batch[0][\"option_features\"].shape[1]\n",
    "    option_features = torch.zeros(B, Kmax, D_o)\n",
    "    mask = torch.zeros(B, Kmax, dtype=torch.bool)\n",
    "    targets = torch.zeros(B, dtype=torch.long)\n",
    "    p_succ_opt = None\n",
    "    extra_costs = None\n",
    "\n",
    "    has_ps = all(\"p_succ_opt\" in item for item in batch)\n",
    "    has_ec = all(\"extra_costs\" in item for item in batch)\n",
    "\n",
    "    if has_ps:\n",
    "        p_succ_opt = torch.zeros(B, Kmax)\n",
    "    if has_ec:\n",
    "        D_c = batch[0][\"extra_costs\"].shape[1]\n",
    "        extra_costs = torch.zeros(B, Kmax, D_c)\n",
    "\n",
    "    for b, item in enumerate(batch):\n",
    "        K = item[\"option_features\"].shape[0]\n",
    "        option_features[b, :K] = item[\"option_features\"]\n",
    "        mask[b, :K] = True\n",
    "        targets[b] = int(item[\"chosen_index\"])  # ensure within 0..K-1\n",
    "        if has_ps:\n",
    "            p_succ_opt[b, :K] = item[\"p_succ_opt\"]\n",
    "        if has_ec:\n",
    "            extra_costs[b, :K] = item[\"extra_costs\"]\n",
    "\n",
    "    out = {\"option_features\": option_features, \"mask\": mask, \"targets\": targets}\n",
    "    if has_ps:\n",
    "        out[\"p_succ_opt\"] = p_succ_opt\n",
    "    if has_ec:\n",
    "        out[\"extra_costs\"] = extra_costs\n",
    "    return out\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Training loops (minimal)\n",
    "# -------------------------------\n",
    "\n",
    "def train_success_head(model: SuccessPredictor, loader: DataLoader, epochs: int = 10, lr: float = 1e-3,\n",
    "                       device: str = \"cpu\") -> None:\n",
    "    model.to(device)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    bce = nn.BCELoss()\n",
    "    model.train()\n",
    "    for ep in range(epochs):\n",
    "        total = 0.0\n",
    "        for X, y in loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            p = model(X)\n",
    "            loss = bce(p, y)\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "            total += loss.item() * X.size(0)\n",
    "        print(f\"[Success] epoch {ep+1}: loss={total/len(loader.dataset):.4f}\")\n",
    "\n",
    "\n",
    "def train_choice_model(model: ChoiceScorer, loader: DataLoader, epochs: int = 10, lr: float = 1e-3,\n",
    "                       device: str = \"cpu\") -> None:\n",
    "    model.to(device)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    for ep in range(epochs):\n",
    "        total = 0.0\n",
    "        for batch in loader:\n",
    "            opt.zero_grad()\n",
    "            option_features = batch[\"option_features\"].to(device)\n",
    "            mask = batch[\"mask\"].to(device)\n",
    "            targets = batch[\"targets\"].to(device)\n",
    "            p_succ_opt = batch.get(\"p_succ_opt\")\n",
    "            extra_costs = batch.get(\"extra_costs\")\n",
    "            if p_succ_opt is not None: p_succ_opt = p_succ_opt.to(device)\n",
    "            if extra_costs is not None: extra_costs = extra_costs.to(device)\n",
    "\n",
    "            logits = model(option_features, mask, p_succ_opt, extra_costs)\n",
    "            loss = masked_cross_entropy(logits, targets, mask)\n",
    "            loss.backward(); opt.step()\n",
    "            total += loss.item() * option_features.size(0)\n",
    "        print(f\"[Choice] epoch {ep+1}: loss={total/len(loader.dataset):.4f}\")\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Tiny smoke test (random data)\n",
    "# -------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    # 3A) Success head on random data\n",
    "    Xs = torch.randn(512, 10)\n",
    "    ys = torch.bernoulli(torch.full((512,), 0.5))\n",
    "    ds = SuccessDataset(Xs, ys)\n",
    "    dl = DataLoader(ds, batch_size=64, shuffle=True)\n",
    "    succ = SuccessPredictor(in_dim=10)\n",
    "    train_success_head(succ, dl, epochs=2)\n",
    "\n",
    "    # 3B) Choice model with masking\n",
    "    items = []\n",
    "    for _ in range(256):\n",
    "        K = torch.randint(low=2, high=6, size=(1,)).item()  # 2..5 options\n",
    "        option_feats = torch.randn(K, 8)\n",
    "        p_succ_opt = torch.sigmoid(torch.randn(K))\n",
    "        extra_costs = torch.randn(K, 2)\n",
    "        chosen = torch.randint(low=0, high=K, size=(1,)).item()\n",
    "        items.append({\n",
    "            \"option_features\": option_feats,\n",
    "            \"p_succ_opt\": p_succ_opt,\n",
    "            \"extra_costs\": extra_costs,\n",
    "            \"option_mask\": torch.ones(K, dtype=torch.bool),\n",
    "            \"chosen_index\": torch.tensor(chosen),\n",
    "        })\n",
    "    dc = ChoiceDataset(items)\n",
    "    dlc = DataLoader(dc, batch_size=32, shuffle=True, collate_fn=choice_collate)\n",
    "    chooser = ChoiceScorer(option_dim=8, extra_cost_dim=2, use_psucc=True)\n",
    "    train_choice_model(chooser, dlc, epochs=2)\n",
    "\n",
    "    print(\"Smoke test complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ff_venv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
