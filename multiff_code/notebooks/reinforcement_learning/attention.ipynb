{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af1e58bd",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31f7119c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os, sys\n",
    "for p in [Path.cwd()] + list(Path.cwd().parents):\n",
    "    if p.name == 'Multifirefly-Project':\n",
    "        os.chdir(p)\n",
    "        sys.path.insert(0, str(p / 'multiff_analysis/multiff_code/methods'))\n",
    "        break\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "from data_wrangling import specific_utils, process_monkey_information, base_processing_class\n",
    "from pattern_discovery import pattern_by_trials, pattern_by_points, make_ff_dataframe, ff_dataframe_utils, pattern_by_trials, pattern_by_points, cluster_analysis, organize_patterns_and_features, category_class\n",
    "from decision_making_analysis.cluster_replacement import cluster_replacement_utils, plot_cluster_replacement\n",
    "from decision_making_analysis.decision_making import decision_making_utils, plot_decision_making, intended_targets_classes\n",
    "from decision_making_analysis.GUAT import GUAT_helper_class, GUAT_collect_info_class, GUAT_combine_info_class, add_features_GUAT_and_TAFT\n",
    "from decision_making_analysis import free_selection, replacement, trajectory_info\n",
    "from visualization.matplotlib_tools import plot_trials, plot_polar, additional_plots, plot_behaviors_utils, plot_statistics\n",
    "from visualization.animation import animation_func, animation_utils, animation_class\n",
    "from null_behaviors import sample_null_distributions, show_null_trajectory\n",
    "from machine_learning.ml_methods import regression_utils, classification_utils, prep_ml_data_utils, hyperparam_tuning_class\n",
    "from machine_learning.RL.env_related import env_for_lstm, env_utils, base_env, collect_agent_data, process_agent_data\n",
    "from machine_learning.RL.lstm import GRU_functions, LSTM_functions, LSTM_functions, lstm_for_multiff_class\n",
    "from machine_learning.RL.SB3 import interpret_neural_network, sb3_for_multiff_class, rl_for_multiff_utils, SB3_functions\n",
    "from eye_position_analysis import eye_positions\n",
    "from neural_data_analysis.neural_analysis_tools.model_neural_data import neural_data_modeling\n",
    "from decision_making_analysis.compare_GUAT_and_TAFT import find_GUAT_or_TAFT_trials\n",
    "from machine_learning.RL.SB3 import rl_for_multiff_utils, rl_for_multiff_class\n",
    "from machine_learning.RL.ff_attention import attn_sac_ff, attn_sac_rnn, env_attn_multiff\n",
    "\n",
    "import os, sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "from gymnasium import spaces, Env\n",
    "import torch\n",
    "import optuna\n",
    "from numpy import pi\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from torch.linalg import vector_norm\n",
    "from IPython.display import HTML\n",
    "from functools import partial\n",
    "from optuna.pruners import MedianPruner\n",
    "from optuna.samplers import TPESampler\n",
    "import gc\n",
    "from importlib import reload\n",
    "plt.rcParams[\"animation.html\"] = \"html5\"\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "device_idx = 0\n",
    "# device = torch.device(\"cuda:\" + str(device_idx) if torch.cuda.is_available() else \"cpu\")\n",
    "## if using Jupyter Notebook\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "model_folder_name = \"RL_models/LSTM_stored_models/all_agents/gen_0/LSTM_Aug_1_24\"\n",
    "os.makedirs(model_folder_name, exist_ok=True)\n",
    "PLAYER = \"agent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faac29f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86f4fb6",
   "metadata": {},
   "source": [
    "# streamline everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b6a7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env for curriculum training...\n",
    "\n",
    "env_kwargs = {\n",
    "    \"reward_per_ff\": 80,\n",
    "\n",
    "    \"dv_cost_factor\": 0,\n",
    "    \"dw_cost_factor\": 0,\n",
    "    \"w_cost_factor\": 0,\n",
    "    \"dt\": 0.25,\n",
    "    \"flash_on_interval\": 2.1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdbfd7c",
   "metadata": {},
   "source": [
    "### env1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af32875a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dt': 0.25,\n",
       " 'dv_cost_factor': 0,\n",
       " 'dw_cost_factor': 0,\n",
       " 'w_cost_factor': 0,\n",
       " 'print_ff_capture_incidents': True,\n",
       " 'print_episode_reward_rates': True,\n",
       " 'flash_on_interval': 0.3,\n",
       " 'max_in_memory_time': 0,\n",
       " 'num_obs_ff': 2,\n",
       " 'add_action_to_obs': False,\n",
       " 'angular_terminal_vel': 1,\n",
       " 'monitor_dir': 'RL_models/LSTM_stored_models/all_agents/gen_9_determ_eval/best_model_after_curriculum'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm.env_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43fe3f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'machine_learning.RL.SB3.rl_for_multiff_class' from '/Users/dusiyi/Documents/Multifirefly-Project/multiff_analysis/multiff_code/methods/machine_learning/RL_models/SB3/rl_for_multiff_class.py'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reload(env_for_lstm)\n",
    "reload(sb3_for_multiff_class)\n",
    "reload(rl_for_multiff_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6223efcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_file = os.path.join('RL_models/LSTM_stored_models/all_agents/gen_9_determ_eval/dv0_dw0_w0_mem0', 'env_params.txt')\n",
    "\n",
    "import pandas as pd\n",
    "params_file = pd.read_csv(params_file)\n",
    "params_file\n",
    "# lm = lstm_for_multiff_class.LSTMforMultifirefly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ad401d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_folder_name: RL_models/LSTM_stored_models/all_agents/gen_9_determ_eval/dv0_dw0_w0_mem0\n",
      "Soft Q Network (1,2):  QNetworkLSTM2(\n",
      "  (linear1): Linear(in_features=14, out_features=128, bias=True)\n",
      "  (lstm1): LSTM(128, 128, dropout=0.2)\n",
      "  (linear2): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "Policy Network:  SAC_PolicyNetworkLSTM(\n",
      "  (linear1): Linear(in_features=10, out_features=128, bias=True)\n",
      "  (linear2): Linear(in_features=12, out_features=128, bias=True)\n",
      "  (lstm1): LSTM(128, 128, dropout=0.2)\n",
      "  (linear3): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (mean_linear): Linear(in_features=128, out_features=2, bias=True)\n",
      "  (log_std_linear): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n",
      "Loaded existing agent\n",
      "There's a problem retrieving existing agent or replay_buffer found in RL_models/LSTM_stored_models/all_agents/gen_9_determ_eval/dv0_dw0_w0_mem0. Need to train a new agent. Error message [Errno 2] No such file or directory: 'RL_models/LSTM_stored_models/all_agents/gen_9_determ_eval/dv0_dw0_w0_mem0/lstm_q1'\n",
      "Failed to load existing agent. Need to train a new agent. Error:  There's a problem retrieving existing agent or replay_buffer found in RL_models/LSTM_stored_models/all_agents/gen_9_determ_eval/dv0_dw0_w0_mem0. Need to train a new agent.\n",
      "Soft Q Network (1,2):  QNetworkLSTM2(\n",
      "  (linear1): Linear(in_features=14, out_features=128, bias=True)\n",
      "  (lstm1): LSTM(128, 128, dropout=0.2)\n",
      "  (linear2): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "Policy Network:  SAC_PolicyNetworkLSTM(\n",
      "  (linear1): Linear(in_features=10, out_features=128, bias=True)\n",
      "  (linear2): Linear(in_features=12, out_features=128, bias=True)\n",
      "  (lstm1): LSTM(128, 128, dropout=0.2)\n",
      "  (linear3): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (mean_linear): Linear(in_features=128, out_features=2, bias=True)\n",
      "  (log_std_linear): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n",
      "There's a problem retrieving existing agent or replay_buffer found in RL_models/LSTM_stored_models/all_agents/gen_9_determ_eval/best_model_after_curriculum. Need to train a new agent. Error message [Errno 2] No such file or directory: 'RL_models/LSTM_stored_models/all_agents/gen_9_determ_eval/best_model_after_curriculum/lstm_q1'\n",
      "Need to train a new best_model_after_curriculum\n",
      "Starting curriculum training\n",
      "Soft Q Network (1,2):  QNetworkLSTM2(\n",
      "  (linear1): Linear(in_features=14, out_features=128, bias=True)\n",
      "  (lstm1): LSTM(128, 128, dropout=0.2)\n",
      "  (linear2): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "Policy Network:  SAC_PolicyNetworkLSTM(\n",
      "  (linear1): Linear(in_features=10, out_features=128, bias=True)\n",
      "  (linear2): Linear(in_features=12, out_features=128, bias=True)\n",
      "  (lstm1): LSTM(128, 128, dropout=0.2)\n",
      "  (linear3): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (mean_linear): Linear(in_features=128, out_features=2, bias=True)\n",
      "  (log_std_linear): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/multiff_clean/lib/python3.11/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\n",
      "/Users/dusiyi/Documents/Multifirefly-Project/multiff_analysis/multiff_code/methods/machine_learning/RL_models/lstm/LSTM_functions.py:551: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.soft_q_net1.load_state_dict(torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME before resetting: 0\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  1\n",
      "current dt:  0.25\n",
      "current full_memory:  0\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3.3\n",
      "current num_obs_ff:  2\n",
      "current max_in_memory_time:  0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "linear(): input and weight.T shapes cannot be multiplied (1x18 and 10x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Multifirefly-Project/multiff_analysis/multiff_code/methods/machine_learning/RL_models/lstm/lstm_for_multiff_class.py:226\u001b[39m, in \u001b[36mLSTMforMultifirefly.load_agent\u001b[39m\u001b[34m(self, load_replay_buffer, dir_name, model_name)\u001b[39m\n",
      "\u001b[32m    224\u001b[39m     dir_name = \u001b[38;5;28mself\u001b[39m.model_folder_name\n",
      "\u001b[32m--> \u001b[39m\u001b[32m226\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msac_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdir_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    228\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m load_replay_buffer:\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Multifirefly-Project/multiff_analysis/multiff_code/methods/machine_learning/RL_models/lstm/LSTM_functions.py:551\u001b[39m, in \u001b[36mSAC_Trainer.load_model\u001b[39m\u001b[34m(self, path, device)\u001b[39m\n",
      "\u001b[32m    550\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, path, device=\u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m):\n",
      "\u001b[32m--> \u001b[39m\u001b[32m551\u001b[39m     \u001b[38;5;28mself\u001b[39m.soft_q_net1.load_state_dict(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m    552\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/lstm_q1\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[32m    553\u001b[39m     \u001b[38;5;28mself\u001b[39m.soft_q_net2.load_state_dict(torch.load(\n",
      "\u001b[32m    554\u001b[39m         path + \u001b[33m'\u001b[39m\u001b[33m/lstm_q2\u001b[39m\u001b[33m'\u001b[39m, map_location=torch.device(device)))\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/multiff_clean/lib/python3.11/site-packages/torch/serialization.py:1319\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n",
      "\u001b[32m   1317\u001b[39m     pickle_load_args[\u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1319\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n",
      "\u001b[32m   1320\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n",
      "\u001b[32m   1321\u001b[39m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n",
      "\u001b[32m   1322\u001b[39m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n",
      "\u001b[32m   1323\u001b[39m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/multiff_clean/lib/python3.11/site-packages/torch/serialization.py:659\u001b[39m, in \u001b[36m_open_file_like\u001b[39m\u001b[34m(name_or_buffer, mode)\u001b[39m\n",
      "\u001b[32m    658\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n",
      "\u001b[32m--> \u001b[39m\u001b[32m659\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    660\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/multiff_clean/lib/python3.11/site-packages/torch/serialization.py:640\u001b[39m, in \u001b[36m_open_file.__init__\u001b[39m\u001b[34m(self, name, mode)\u001b[39m\n",
      "\u001b[32m    639\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n",
      "\u001b[32m--> \u001b[39m\u001b[32m640\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'RL_models/LSTM_stored_models/all_agents/gen_9_determ_eval/best_model_after_curriculum/lstm_q1'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Multifirefly-Project/multiff_analysis/multiff_code/methods/machine_learning/RL_models/SB3/rl_for_multiff_class.py:125\u001b[39m, in \u001b[36m_RLforMultifirefly.curriculum_training\u001b[39m\u001b[34m(self, best_model_after_curriculum_exists_ok, load_replay_buffer_of_best_model_after_curriculum)\u001b[39m\n",
      "\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m()\n",
      "\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_best_model_after_curriculum\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_replay_buffer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mload_replay_buffer_of_best_model_after_curriculum\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    127\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mLoaded best_model_after_curriculum\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Multifirefly-Project/multiff_analysis/multiff_code/methods/machine_learning/RL_models/lstm/lstm_for_multiff_class.py:247\u001b[39m, in \u001b[36mLSTMforMultifirefly.load_best_model_after_curriculum\u001b[39m\u001b[34m(self, load_replay_buffer)\u001b[39m\n",
      "\u001b[32m    246\u001b[39m \u001b[38;5;28mself\u001b[39m.make_agent()\n",
      "\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mload_replay_buffer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mload_replay_buffer\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    248\u001b[39m \u001b[43m                \u001b[49m\u001b[43mdir_name\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbest_model_after_curriculum_dir_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Multifirefly-Project/multiff_analysis/multiff_code/methods/machine_learning/RL_models/lstm/lstm_for_multiff_class.py:238\u001b[39m, in \u001b[36mLSTMforMultifirefly.load_agent\u001b[39m\u001b[34m(self, load_replay_buffer, dir_name, model_name)\u001b[39m\n",
      "\u001b[32m    236\u001b[39m \u001b[38;5;28mprint\u001b[39m(\n",
      "\u001b[32m    237\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThere\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms a problem retrieving existing agent or replay_buffer found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdir_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Need to train a new agent. Error message \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n",
      "\u001b[32m    239\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThere\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms a problem retrieving existing agent or replay_buffer found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdir_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Need to train a new agent.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\n",
      "\u001b[31mValueError\u001b[39m: There's a problem retrieving existing agent or replay_buffer found in RL_models/LSTM_stored_models/all_agents/gen_9_determ_eval/best_model_after_curriculum. Need to train a new agent.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 17\u001b[39m\n",
      "\u001b[32m      3\u001b[39m env_kwargs = {\u001b[33m'\u001b[39m\u001b[33mnum_obs_ff\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m2\u001b[39m,\n",
      "\u001b[32m      4\u001b[39m               \u001b[33m'\u001b[39m\u001b[33madd_action_to_obs\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "\u001b[32m      5\u001b[39m               \u001b[33m'\u001b[39m\u001b[33mangular_terminal_vel\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m1\u001b[39m,\n",
      "\u001b[32m   (...)\u001b[39m\u001b[32m     10\u001b[39m               \u001b[33m\"\u001b[39m\u001b[33mw_cost_factor\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m0\u001b[39m,\n",
      "\u001b[32m     11\u001b[39m             }   \n",
      "\u001b[32m     12\u001b[39m lm = lstm_for_multiff_class.LSTMforMultifirefly(overall_folder=\u001b[33m'\u001b[39m\u001b[33mRL_models/LSTM_stored_models/all_agents/gen_9_determ_eval/\u001b[39m\u001b[33m'\u001b[39m,\n",
      "\u001b[32m     13\u001b[39m                                                 **env_kwargs)\n",
      "\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[43mlm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstreamline_everything\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrentTrial_for_animation\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_trials_for_animation\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m40\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m     18\u001b[39m \u001b[43m                         \u001b[49m\u001b[43mbest_model_after_curriculum_exists_ok\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Multifirefly-Project/multiff_analysis/multiff_code/methods/machine_learning/RL_models/SB3/rl_for_multiff_class.py:400\u001b[39m, in \u001b[36m_RLforMultifirefly.streamline_everything\u001b[39m\u001b[34m(self, currentTrial_for_animation, num_trials_for_animation, duration, n_steps, use_curriculum_training, load_replay_buffer_of_best_model_after_curriculum, best_model_after_curriculum_exists_ok, model_exists_ok)\u001b[39m\n",
      "\u001b[32m    397\u001b[39m         to_train_agent = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m to_train_agent:\n",
      "\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_curriculum_training\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_curriculum_training\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_model_after_curriculum_exists_ok\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbest_model_after_curriculum_exists_ok\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    401\u001b[39m \u001b[43m                     \u001b[49m\u001b[43mload_replay_buffer_of_best_model_after_curriculum\u001b[49m\u001b[43m=\u001b[49m\u001b[43mload_replay_buffer_of_best_model_after_curriculum\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.successful_training:\n",
      "\u001b[32m    403\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mThe set of parameters has failed to produce a well-trained agent in the past. \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n",
      "\u001b[32m    404\u001b[39m \u001b[33m            Skip to the next set of parameters\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Multifirefly-Project/multiff_analysis/multiff_code/methods/machine_learning/RL_models/SB3/rl_for_multiff_class.py:432\u001b[39m, in \u001b[36m_RLforMultifirefly.train_agent\u001b[39m\u001b[34m(self, use_curriculum_training, best_model_after_curriculum_exists_ok, load_replay_buffer_of_best_model_after_curriculum, timesteps)\u001b[39m\n",
      "\u001b[32m    430\u001b[39m     \u001b[38;5;28mself\u001b[39m.regular_training(timesteps=timesteps)\n",
      "\u001b[32m    431\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m--> \u001b[39m\u001b[32m432\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcurriculum_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_model_after_curriculum_exists_ok\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbest_model_after_curriculum_exists_ok\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    433\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mload_replay_buffer_of_best_model_after_curriculum\u001b[49m\u001b[43m=\u001b[49m\u001b[43mload_replay_buffer_of_best_model_after_curriculum\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    434\u001b[39m \u001b[38;5;28mself\u001b[39m.training_time = time_package.time()-\u001b[38;5;28mself\u001b[39m.training_start_time\n",
      "\u001b[32m    435\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFinished training using\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m.training_time, \u001b[33m'\u001b[39m\u001b[33ms.\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Multifirefly-Project/multiff_analysis/multiff_code/methods/machine_learning/RL_models/SB3/rl_for_multiff_class.py:130\u001b[39m, in \u001b[36m_RLforMultifirefly.curriculum_training\u001b[39m\u001b[34m(self, best_model_after_curriculum_exists_ok, load_replay_buffer_of_best_model_after_curriculum)\u001b[39m\n",
      "\u001b[32m    128\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "\u001b[32m    129\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mNeed to train a new best_model_after_curriculum\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_progress_in_curriculum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    131\u001b[39m \u001b[38;5;28mself\u001b[39m._run_current_agent_after_curriculum_training()\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Multifirefly-Project/multiff_analysis/multiff_code/methods/machine_learning/RL_models/SB3/rl_for_multiff_class.py:143\u001b[39m, in \u001b[36m_RLforMultifirefly._progress_in_curriculum\u001b[39m\u001b[34m(self)\u001b[39m\n",
      "\u001b[32m    140\u001b[39m \u001b[38;5;28mself\u001b[39m._make_agent_for_curriculum_training()\n",
      "\u001b[32m    141\u001b[39m \u001b[38;5;28mself\u001b[39m.successful_training = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_use_while_loop_for_curriculum_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    144\u001b[39m \u001b[38;5;28mself\u001b[39m._further_process_best_model_after_curriculum_training()\n",
      "\u001b[32m    145\u001b[39m \u001b[38;5;28mself\u001b[39m.streamline_making_animation(currentTrial_for_animation=\u001b[38;5;28;01mNone\u001b[39;00m, num_trials_for_animation=\u001b[38;5;28;01mNone\u001b[39;00m, duration=[\u001b[32m10\u001b[39m, \u001b[32m40\u001b[39m], n_steps=\u001b[32m8000\u001b[39m,\n",
      "\u001b[32m    146\u001b[39m                                  video_dir=\u001b[38;5;28mself\u001b[39m.best_model_after_curriculum_dir_name)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Multifirefly-Project/multiff_analysis/multiff_code/methods/machine_learning/RL_models/lstm/lstm_for_multiff_class.py:136\u001b[39m, in \u001b[36m_use_while_loop_for_curriculum_training\u001b[39m\u001b[34m(self, eval_eps_freq)\u001b[39m\n",
      "\u001b[32m    133\u001b[39m num_eval_episodes = 1\n",
      "\u001b[32m    134\u001b[39m reward_threshold = rl_for_multiff_utils.calculate_reward_threshold_for_curriculum_training(\n",
      "\u001b[32m    135\u001b[39m     self.env, n_eval_episodes=num_eval_episodes, ff_caught_rate_threshold=0.1)\n",
      "\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m # reward_threshold = 1000\n",
      "\u001b[32m    137\u001b[39m self.regular_training(eval_eps_freq=eval_eps_freq, num_eval_episodes=num_eval_episodes,\n",
      "\u001b[32m    138\u001b[39m                       reward_threshold_to_stop_on=reward_threshold, dir_name=self.best_model_after_curriculum_dir_name,\n",
      "\u001b[32m    139\u001b[39m                       env_params_to_save=self.env_kwargs_for_curriculum_training)\n",
      "\u001b[32m    140\u001b[39m print('reward_threshold:', reward_threshold)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Multifirefly-Project/multiff_analysis/multiff_code/methods/machine_learning/RL_models/lstm/lstm_for_multiff_class.py:198\u001b[39m, in \u001b[36mregular_training\u001b[39m\u001b[34m(self, num_train_episodes, eval_eps_freq, num_eval_episodes, print_episode_reward, reward_threshold_to_stop_on, dir_name, env_params_to_save)\u001b[39m\n",
      "\u001b[32m      0\u001b[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Multifirefly-Project/multiff_analysis/multiff_code/methods/machine_learning/RL_models/lstm/LSTM_functions.py:588\u001b[39m, in \u001b[36mtrain_LSTM_agent\u001b[39m\u001b[34m(env, sac_model, num_train_episodes, eval_eps_freq, max_steps_per_eps, num_eval_episodes, best_avg_reward, best_avg_reward_record, print_episode_reward, reward_threshold_to_stop_on, dir_name)\u001b[39m\n",
      "\u001b[32m    586\u001b[39m list_of_epi_rewards = []\n",
      "\u001b[32m    587\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m eps \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_train_episodes):\n",
      "\u001b[32m--> \u001b[39m\u001b[32m588\u001b[39m     episode_reward = \u001b[43m_train_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msac_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps_per_eps\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    589\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[32m    590\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mALPHA (entropy-related): \u001b[39m\u001b[33m'\u001b[39m, sac_model.alpha)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Multifirefly-Project/multiff_analysis/multiff_code/methods/machine_learning/RL_models/lstm/LSTM_functions.py:701\u001b[39m, in \u001b[36m_train_episode\u001b[39m\u001b[34m(env, sac_model, max_steps_per_eps)\u001b[39m\n",
      "\u001b[32m    699\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_steps_per_eps):\n",
      "\u001b[32m    700\u001b[39m     hidden_in = hidden_out\n",
      "\u001b[32m--> \u001b[39m\u001b[32m701\u001b[39m     action, hidden_out = \u001b[43msac_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpolicy_net\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_action\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m    702\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    703\u001b[39m     next_state, reward, done, _, _ = env.step(action)\n",
      "\u001b[32m    705\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m step == \u001b[32m0\u001b[39m:\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Multifirefly-Project/multiff_analysis/multiff_code/methods/machine_learning/RL_models/lstm/LSTM_functions.py:389\u001b[39m, in \u001b[36mSAC_PolicyNetworkLSTM.get_action\u001b[39m\u001b[34m(self, state, last_action, hidden_in, deterministic, device)\u001b[39m\n",
      "\u001b[32m    385\u001b[39m state = torch.FloatTensor(state).unsqueeze(\u001b[32m0\u001b[39m).unsqueeze(\u001b[32m0\u001b[39m).to(\n",
      "\u001b[32m    386\u001b[39m     device)  \u001b[38;5;66;03m# increase 2 dims to match with training data\u001b[39;00m\n",
      "\u001b[32m    387\u001b[39m last_action = torch.FloatTensor(\n",
      "\u001b[32m    388\u001b[39m     last_action).unsqueeze(\u001b[32m0\u001b[39m).unsqueeze(\u001b[32m0\u001b[39m).to(device)\n",
      "\u001b[32m--> \u001b[39m\u001b[32m389\u001b[39m mean, log_std, hidden_out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_in\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    390\u001b[39m std = log_std.exp()\n",
      "\u001b[32m    392\u001b[39m normal = Normal(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Multifirefly-Project/multiff_analysis/multiff_code/methods/machine_learning/RL_models/lstm/LSTM_functions.py:346\u001b[39m, in \u001b[36mSAC_PolicyNetworkLSTM.forward\u001b[39m\u001b[34m(self, state, last_action, hidden_in)\u001b[39m\n",
      "\u001b[32m    344\u001b[39m last_action = last_action.permute(\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m)\n",
      "\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# branch 1\u001b[39;00m\n",
      "\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m fc_branch = F.relu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlinear1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[32m    347\u001b[39m \u001b[38;5;66;03m# branch 2\u001b[39;00m\n",
      "\u001b[32m    348\u001b[39m lstm_branch = torch.cat([state, last_action], -\u001b[32m1\u001b[39m)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/multiff_clean/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/multiff_clean/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n",
      "\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/multiff_clean/lib/python3.11/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n",
      "\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n",
      "\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[31mRuntimeError\u001b[39m: linear(): input and weight.T shapes cannot be multiplied (1x18 and 10x128)"
     ]
    }
   ],
   "source": [
    "# lm = lstm_for_multiff_class.LSTMforMultifirefly()\n",
    "\n",
    "env_kwargs = {'num_obs_ff': 2,\n",
    "              'add_action_to_obs': False,\n",
    "              'angular_terminal_vel': 1,\n",
    "              \"dt\": 0.25,\n",
    "              \"flash_on_interval\": 0.3,\n",
    "              \"dv_cost_factor\": 0,\n",
    "              \"dw_cost_factor\": 0,\n",
    "              \"w_cost_factor\": 0,\n",
    "            }   \n",
    "lm = lstm_for_multiff_class.LSTMforMultifirefly(overall_folder='RL_models/LSTM_stored_models/all_agents/gen_9_determ_eval/',\n",
    "                                                **env_kwargs)\n",
    "\n",
    "                                             \n",
    "\n",
    "lm.streamline_everything(currentTrial_for_animation=None, num_trials_for_animation=None, duration=[10, 40],\n",
    "                         best_model_after_curriculum_exists_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d6ea8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm.env.num_elem_per_ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7feb1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm.env.obs_space_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e216759d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dt': 0.25,\n",
       " 'dv_cost_factor': 0,\n",
       " 'dw_cost_factor': 0,\n",
       " 'w_cost_factor': 0,\n",
       " 'print_ff_capture_incidents': True,\n",
       " 'print_episode_reward_rates': True,\n",
       " 'flash_on_interval': 0.3,\n",
       " 'max_in_memory_time': 0,\n",
       " 'num_obs_ff': 2,\n",
       " 'add_action_to_obs': False,\n",
       " 'angular_terminal_vel': 1,\n",
       " 'monitor_dir': 'RL_models/LSTM_stored_models/all_agents/gen_9_determ_eval/best_model_after_curriculum'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm.env_kwargs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d982a4fd",
   "metadata": {},
   "source": [
    "# Feed-forward SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cd9bd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3994.21s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/dusiyi/Documents/Multifirefly-Project\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fd4e11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c00e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "/Users/dusiyi/Documents/Multifirefly-Project/multiff_analysis/multiff_code/test_env_debug.py\n",
    "from test_env_debug import test_env_debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "185f8e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiff_analysis.multiff_code import test_env_debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a030218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing debugged MultiFF environments\n",
      "==================================================\n",
      "✓ Successfully imported all environment classes\n",
      "\n",
      "Testing MultiFF (base)...\n",
      "✓ Created MultiFF (base) environment\n",
      "TIME before resetting: 0\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current full_memory:  30\n",
      "current dv_cost_factor:  10\n",
      "current dw_cost_factor:  10\n",
      "current w_cost_factor:  10\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  3\n",
      "current max_in_memory_time:  3\n",
      "✓ Reset successful, observation shape: (26,), dtype: float32\n",
      "✓ Step successful, reward: -5.0257\n",
      "✓ New observation shape: (26,), dtype: float32\n",
      "✓ MultiFF (base) test completed successfully\n",
      "\n",
      "Testing MultiFF_2...\n",
      "✓ Created MultiFF_2 environment\n",
      "TIME before resetting: 0\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current full_memory:  30\n",
      "current dv_cost_factor:  10\n",
      "current dw_cost_factor:  10\n",
      "current w_cost_factor:  10\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  3\n",
      "current max_in_memory_time:  3\n",
      "✓ Reset successful, observation shape: (14,), dtype: float32\n",
      "✓ Step successful, reward: -11.4106\n",
      "✓ New observation shape: (14,), dtype: float32\n",
      "✓ MultiFF_2 test completed successfully\n",
      "\n",
      "Testing BaseCollectInformation...\n",
      "✓ Created BaseCollectInformation environment\n",
      "TIME before resetting: 0\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current full_memory:  30\n",
      "current dv_cost_factor:  10\n",
      "current dw_cost_factor:  10\n",
      "current w_cost_factor:  10\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  3\n",
      "current max_in_memory_time:  3\n",
      "✓ Reset successful, observation shape: (26,), dtype: float32\n",
      "✓ Step successful, reward: -7.7901\n",
      "✓ New observation shape: (26,), dtype: float32\n",
      "✓ BaseCollectInformation test completed successfully\n",
      "\n",
      "Testing EnvForLSTM...\n",
      "✓ Created EnvForLSTM environment\n",
      "TIME before resetting: 0\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current full_memory:  0\n",
      "current dv_cost_factor:  10\n",
      "current dw_cost_factor:  10\n",
      "current w_cost_factor:  10\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  3\n",
      "current max_in_memory_time:  0\n",
      "✓ Reset successful, observation shape: (20,), dtype: float32\n",
      "✓ Step successful, reward: -1.0904\n",
      "✓ New observation shape: (20,), dtype: float32\n",
      "✓ EnvForLSTM test completed successfully\n",
      "\n",
      "Testing CollectInformationLSTM...\n",
      "✓ Created CollectInformationLSTM environment\n",
      "TIME before resetting: 0\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current full_memory:  0\n",
      "current dv_cost_factor:  10\n",
      "current dw_cost_factor:  10\n",
      "current w_cost_factor:  10\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  3\n",
      "current max_in_memory_time:  0\n",
      "✓ Reset successful, observation shape: (20,), dtype: float32\n",
      "✓ Step successful, reward: -2.8095\n",
      "✓ New observation shape: (20,), dtype: float32\n",
      "✓ CollectInformationLSTM test completed successfully\n",
      "\n",
      "Testing EnvForSB3...\n",
      "✓ Created EnvForSB3 environment\n",
      "TIME before resetting: 0\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current full_memory:  30\n",
      "current dv_cost_factor:  10\n",
      "current dw_cost_factor:  10\n",
      "current w_cost_factor:  10\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  3\n",
      "current max_in_memory_time:  3\n",
      "✓ Reset successful, observation shape: (26,), dtype: float32\n",
      "✓ Step successful, reward: -21.4836\n",
      "✓ New observation shape: (26,), dtype: float32\n",
      "✓ EnvForSB3 test completed successfully\n",
      "\n",
      "Testing EnvForSB3_2...\n",
      "✓ Created EnvForSB3_2 environment\n",
      "TIME before resetting: 0\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current full_memory:  30\n",
      "current dv_cost_factor:  10\n",
      "current dw_cost_factor:  10\n",
      "current w_cost_factor:  10\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  3\n",
      "current max_in_memory_time:  3\n",
      "✓ Reset successful, observation shape: (14,), dtype: float32\n",
      "✓ Step successful, reward: -3.3271\n",
      "✓ New observation shape: (14,), dtype: float32\n",
      "✓ EnvForSB3_2 test completed successfully\n",
      "\n",
      "Testing CollectInformation...\n",
      "✓ Created CollectInformation environment\n",
      "TIME before resetting: 0\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current full_memory:  30\n",
      "current dv_cost_factor:  10\n",
      "current dw_cost_factor:  10\n",
      "current w_cost_factor:  10\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  3\n",
      "current max_in_memory_time:  3\n",
      "✓ Reset successful, observation shape: (26,), dtype: float32\n",
      "✓ Step successful, reward: -0.3455\n",
      "✓ New observation shape: (26,), dtype: float32\n",
      "✓ CollectInformation test completed successfully\n",
      "\n",
      "==================================================\n",
      "TEST SUMMARY:\n",
      "==================================================\n",
      "MultiFF (base): PASS\n",
      "MultiFF_2: PASS\n",
      "BaseCollectInformation: PASS\n",
      "EnvForLSTM: PASS\n",
      "CollectInformationLSTM: PASS\n",
      "EnvForSB3: PASS\n",
      "EnvForSB3_2: PASS\n",
      "CollectInformation: PASS\n",
      "\n",
      "Overall: 8/8 environments passed\n",
      "🎉 All tests passed! All environments are working correctly.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_env_debug.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27d19b44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'machine_learning.RL.env_related.base_env' from '/Users/dusiyi/Documents/Multifirefly-Project/multiff_analysis/multiff_code/methods/machine_learning/RL_models/env_related/base_env.py'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(base_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75d6c126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME before resetting: 0\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current full_memory:  30\n",
      "current dv_cost_factor:  10\n",
      "current dw_cost_factor:  10\n",
      "current w_cost_factor:  10\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  -4767.613\n",
      "Cost breakdown:  {'dv_cost': np.float32(4319.2817), 'dw_cost': np.float32(271.71042), 'w_cost': np.float32(176.62337)}\n",
      "Ep    1 | steps=1025 | return=-4767.61 | buf=  1025\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current full_memory:  30\n",
      "current dv_cost_factor:  10\n",
      "current dw_cost_factor:  10\n",
      "current w_cost_factor:  10\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  1\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  -4457.3447\n",
      "Cost breakdown:  {'dv_cost': np.float32(4020.8406), 'dw_cost': np.float32(267.89056), 'w_cost': np.float32(168.61343)}\n",
      "Ep    2 | steps=1025 | return=-4457.34 | buf=  2050\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current full_memory:  30\n",
      "current dv_cost_factor:  10\n",
      "current dw_cost_factor:  10\n",
      "current w_cost_factor:  10\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  2\n",
      "[t=3000] alpha=0.759  Q=-13.88  losses: Q1=2.266 Q2=4.946 Pi=10.371\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  -2470.8625\n",
      "Cost breakdown:  {'dv_cost': np.float32(1996.8171), 'dw_cost': np.float32(291.5235), 'w_cost': np.float32(182.52104)}\n",
      "Ep    3 | steps=1025 | return=-2470.86 | buf=  3075\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current full_memory:  30\n",
      "current dv_cost_factor:  10\n",
      "current dw_cost_factor:  10\n",
      "current w_cost_factor:  10\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  3\n",
      "[t=4000] alpha=0.583  Q=-14.70  losses: Q1=0.693 Q2=0.756 Pi=12.025\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  -735.6872\n",
      "Cost breakdown:  {'dv_cost': np.float32(417.1395), 'dw_cost': np.float32(170.27016), 'w_cost': np.float32(148.2777)}\n",
      "Ep    4 | steps=1025 | return= -735.69 | buf=  4100\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current full_memory:  30\n",
      "current dv_cost_factor:  10\n",
      "current dw_cost_factor:  10\n",
      "current w_cost_factor:  10\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  4\n",
      "[t=5000] alpha=0.445  Q=-16.52  losses: Q1=0.606 Q2=0.637 Pi=13.947\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  -532.12994\n",
      "Cost breakdown:  {'dv_cost': np.float32(299.75), 'dw_cost': np.float32(118.326805), 'w_cost': np.float32(114.05319)}\n",
      "Ep    5 | steps=1025 | return= -532.13 | buf=  5125\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current full_memory:  30\n",
      "current dv_cost_factor:  10\n",
      "current dw_cost_factor:  10\n",
      "current w_cost_factor:  10\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  5\n",
      "[t=6000] alpha=0.341  Q=-17.58  losses: Q1=0.723 Q2=0.715 Pi=15.802\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  -418.9418\n",
      "Cost breakdown:  {'dv_cost': np.float32(240.82399), 'dw_cost': np.float32(92.47427), 'w_cost': np.float32(85.64396)}\n",
      "Ep    6 | steps=1025 | return= -418.94 | buf=  6150\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current full_memory:  30\n",
      "current dv_cost_factor:  10\n",
      "current dw_cost_factor:  10\n",
      "current w_cost_factor:  10\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  6\n",
      "[t=7000] alpha=0.262  Q=-18.62  losses: Q1=0.450 Q2=0.395 Pi=17.062\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  -347.02643\n",
      "Cost breakdown:  {'dv_cost': np.float32(195.82346), 'dw_cost': np.float32(75.90072), 'w_cost': np.float32(75.302574)}\n",
      "Ep    7 | steps=1025 | return= -347.03 | buf=  7175\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current full_memory:  30\n",
      "current dv_cost_factor:  10\n",
      "current dw_cost_factor:  10\n",
      "current w_cost_factor:  10\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  7\n",
      "[t=8000] alpha=0.202  Q=-19.74  losses: Q1=0.341 Q2=0.338 Pi=18.310\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  -310.24658\n",
      "Cost breakdown:  {'dv_cost': np.float32(186.56195), 'dw_cost': np.float32(60.560684), 'w_cost': np.float32(63.123936)}\n",
      "Ep    8 | steps=1025 | return= -310.25 | buf=  8200\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current full_memory:  30\n",
      "current dv_cost_factor:  10\n",
      "current dw_cost_factor:  10\n",
      "current w_cost_factor:  10\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  8\n",
      "[t=9000] alpha=0.157  Q=-20.57  losses: Q1=0.295 Q2=0.301 Pi=19.508\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  -253.93687\n",
      "Cost breakdown:  {'dv_cost': np.float32(157.09457), 'dw_cost': np.float32(48.894043), 'w_cost': np.float32(47.948463)}\n",
      "Ep    9 | steps=1025 | return= -253.94 | buf=  9225\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current full_memory:  30\n",
      "current dv_cost_factor:  10\n",
      "current dw_cost_factor:  10\n",
      "current w_cost_factor:  10\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  9\n",
      "[t=10000] alpha=0.124  Q=-21.55  losses: Q1=0.587 Q2=0.590 Pi=20.193\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  -223.62389\n",
      "Cost breakdown:  {'dv_cost': np.float32(139.97035), 'dw_cost': np.float32(41.93767), 'w_cost': np.float32(41.715866)}\n",
      "Ep   10 | steps=1025 | return= -223.62 | buf= 10250\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current full_memory:  30\n",
      "current dv_cost_factor:  10\n",
      "current dw_cost_factor:  10\n",
      "current w_cost_factor:  10\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  10\n",
      "[t=11000] alpha=0.098  Q=-22.11  losses: Q1=0.340 Q2=0.518 Pi=21.170\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  -189.75885\n",
      "Cost breakdown:  {'dv_cost': np.float32(116.41433), 'dw_cost': np.float32(37.834198), 'w_cost': np.float32(35.510273)}\n",
      "Ep   11 | steps=1025 | return= -189.76 | buf= 11275\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current full_memory:  30\n",
      "current dv_cost_factor:  10\n",
      "current dw_cost_factor:  10\n",
      "current w_cost_factor:  10\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  11\n",
      "[t=12000] alpha=0.080  Q=-22.38  losses: Q1=0.164 Q2=0.241 Pi=21.498\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  -175.89853\n",
      "Cost breakdown:  {'dv_cost': np.float32(117.93643), 'dw_cost': np.float32(29.655481), 'w_cost': np.float32(28.306587)}\n",
      "Ep   12 | steps=1025 | return= -175.90 | buf= 12300\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current full_memory:  30\n",
      "current dv_cost_factor:  10\n",
      "current dw_cost_factor:  10\n",
      "current w_cost_factor:  10\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  12\n",
      "[t=13000] alpha=0.066  Q=-22.17  losses: Q1=0.160 Q2=0.291 Pi=21.492\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  -165.73187\n",
      "Cost breakdown:  {'dv_cost': np.float32(112.86264), 'dw_cost': np.float32(26.750517), 'w_cost': np.float32(26.118635)}\n",
      "Ep   13 | steps=1025 | return= -165.73 | buf= 13325\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current full_memory:  30\n",
      "current dv_cost_factor:  10\n",
      "current dw_cost_factor:  10\n",
      "current w_cost_factor:  10\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  13\n",
      "[t=14000] alpha=0.056  Q=-22.57  losses: Q1=0.209 Q2=0.192 Pi=21.812\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  -142.58774\n",
      "Cost breakdown:  {'dv_cost': np.float32(95.21314), 'dw_cost': np.float32(23.41034), 'w_cost': np.float32(23.964321)}\n",
      "Ep   14 | steps=1025 | return= -142.59 | buf= 14350\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current full_memory:  30\n",
      "current dv_cost_factor:  10\n",
      "current dw_cost_factor:  10\n",
      "current w_cost_factor:  10\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  14\n",
      "[t=15000] alpha=0.051  Q=-22.18  losses: Q1=0.084 Q2=0.149 Pi=22.091\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  -142.71184\n",
      "Cost breakdown:  {'dv_cost': np.float32(97.70965), 'dw_cost': np.float32(22.458918), 'w_cost': np.float32(22.543434)}\n",
      "Ep   15 | steps=1025 | return= -142.71 | buf= 15375\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current full_memory:  30\n",
      "current dv_cost_factor:  10\n",
      "current dw_cost_factor:  10\n",
      "current w_cost_factor:  10\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  15\n",
      "[t=16000] alpha=0.049  Q=-22.79  losses: Q1=0.408 Q2=0.381 Pi=21.974\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  -135.48163\n",
      "Cost breakdown:  {'dv_cost': np.float32(93.241875), 'dw_cost': np.float32(20.384352), 'w_cost': np.float32(21.855326)}\n",
      "Ep   16 | steps=1025 | return= -135.48 | buf= 16400\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current full_memory:  30\n",
      "current dv_cost_factor:  10\n",
      "current dw_cost_factor:  10\n",
      "current w_cost_factor:  10\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  16\n",
      "[t=17000] alpha=0.048  Q=-22.76  losses: Q1=0.173 Q2=0.247 Pi=22.056\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  -131.26889\n",
      "Cost breakdown:  {'dv_cost': np.float32(90.32196), 'dw_cost': np.float32(20.441275), 'w_cost': np.float32(20.505486)}\n",
      "Ep   17 | steps=1025 | return= -131.27 | buf= 17425\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current full_memory:  30\n",
      "current dv_cost_factor:  10\n",
      "current dw_cost_factor:  10\n",
      "current w_cost_factor:  10\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  17\n",
      "[t=18000] alpha=0.049  Q=-22.71  losses: Q1=0.273 Q2=0.282 Pi=22.243\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  -143.5918\n",
      "Cost breakdown:  {'dv_cost': np.float32(100.32114), 'dw_cost': np.float32(20.786293), 'w_cost': np.float32(22.484402)}\n",
      "Ep   18 | steps=1025 | return= -143.59 | buf= 18450\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current full_memory:  30\n",
      "current dv_cost_factor:  10\n",
      "current dw_cost_factor:  10\n",
      "current w_cost_factor:  10\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  18\n",
      "[t=19000] alpha=0.049  Q=-22.76  losses: Q1=0.128 Q2=0.137 Pi=22.539\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  -137.67737\n",
      "Cost breakdown:  {'dv_cost': np.float32(96.40077), 'dw_cost': np.float32(20.246948), 'w_cost': np.float32(21.029783)}\n",
      "Ep   19 | steps=1025 | return= -137.68 | buf= 19475\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current full_memory:  30\n",
      "current dv_cost_factor:  10\n",
      "current dw_cost_factor:  10\n",
      "current w_cost_factor:  10\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  19\n",
      "[t=20000] alpha=0.048  Q=-22.62  losses: Q1=0.198 Q2=0.283 Pi=22.261\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  -139.3873\n",
      "Cost breakdown:  {'dv_cost': np.float32(97.14686), 'dw_cost': np.float32(21.128458), 'w_cost': np.float32(21.111868)}\n",
      "Ep   20 | steps=1025 | return= -139.39 | buf= 20500\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current full_memory:  30\n",
      "current dv_cost_factor:  10\n",
      "current dw_cost_factor:  10\n",
      "current w_cost_factor:  10\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  20\n",
      "[t=21000] alpha=0.049  Q=-22.72  losses: Q1=0.204 Q2=0.209 Pi=22.579\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  -136.8726\n",
      "Cost breakdown:  {'dv_cost': np.float32(93.5826), 'dw_cost': np.float32(21.755148), 'w_cost': np.float32(21.53495)}\n",
      "Ep   21 | steps=1025 | return= -136.87 | buf= 21525\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current full_memory:  30\n",
      "current dv_cost_factor:  10\n",
      "current dw_cost_factor:  10\n",
      "current w_cost_factor:  10\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  21\n",
      "[t=22000] alpha=0.047  Q=-22.97  losses: Q1=0.161 Q2=0.159 Pi=22.540\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  -130.66904\n",
      "Cost breakdown:  {'dv_cost': np.float32(95.58183), 'dw_cost': np.float32(17.721888), 'w_cost': np.float32(17.36523)}\n",
      "Ep   22 | steps=1025 | return= -130.67 | buf= 22550\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current full_memory:  30\n",
      "current dv_cost_factor:  10\n",
      "current dw_cost_factor:  10\n",
      "current w_cost_factor:  10\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  22\n",
      "[t=23000] alpha=0.048  Q=-22.83  losses: Q1=0.725 Q2=0.643 Pi=22.774\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  -138.98596\n",
      "Cost breakdown:  {'dv_cost': np.float32(97.82965), 'dw_cost': np.float32(20.862143), 'w_cost': np.float32(20.29418)}\n",
      "Ep   23 | steps=1025 | return= -138.99 | buf= 23575\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current full_memory:  30\n",
      "current dv_cost_factor:  10\n",
      "current dw_cost_factor:  10\n",
      "current w_cost_factor:  10\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  23\n",
      "[t=24000] alpha=0.048  Q=-23.21  losses: Q1=0.160 Q2=0.187 Pi=22.584\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  -134.64384\n",
      "Cost breakdown:  {'dv_cost': np.float32(95.323326), 'dw_cost': np.float32(18.943748), 'w_cost': np.float32(20.376703)}\n",
      "Ep   24 | steps=1025 | return= -134.64 | buf= 24600\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current full_memory:  30\n",
      "current dv_cost_factor:  10\n",
      "current dw_cost_factor:  10\n",
      "current w_cost_factor:  10\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  24\n",
      "[t=25000] alpha=0.049  Q=-23.15  losses: Q1=0.221 Q2=0.173 Pi=22.577\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  -132.23247\n",
      "Cost breakdown:  {'dv_cost': np.float32(95.608406), 'dw_cost': np.float32(18.008804), 'w_cost': np.float32(18.615286)}\n",
      "Ep   25 | steps=1025 | return= -132.23 | buf= 25625\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current full_memory:  30\n",
      "current dv_cost_factor:  10\n",
      "current dw_cost_factor:  10\n",
      "current w_cost_factor:  10\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  25\n",
      "[t=26000] alpha=0.048  Q=-22.83  losses: Q1=0.132 Q2=0.145 Pi=22.754\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  -137.47243\n",
      "Cost breakdown:  {'dv_cost': np.float32(96.15648), 'dw_cost': np.float32(20.012962), 'w_cost': np.float32(21.302917)}\n",
      "Ep   26 | steps=1025 | return= -137.47 | buf= 26650\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current full_memory:  30\n",
      "current dv_cost_factor:  10\n",
      "current dw_cost_factor:  10\n",
      "current w_cost_factor:  10\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  26\n",
      "[t=27000] alpha=0.049  Q=-23.42  losses: Q1=0.149 Q2=0.156 Pi=22.703\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  -137.93364\n",
      "Cost breakdown:  {'dv_cost': np.float32(99.15291), 'dw_cost': np.float32(18.805357), 'w_cost': np.float32(19.97543)}\n",
      "Ep   27 | steps=1025 | return= -137.93 | buf= 27675\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current full_memory:  30\n",
      "current dv_cost_factor:  10\n",
      "current dw_cost_factor:  10\n",
      "current w_cost_factor:  10\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  27\n",
      "[t=28000] alpha=0.049  Q=-23.00  losses: Q1=0.553 Q2=0.487 Pi=22.543\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  -131.10432\n",
      "Cost breakdown:  {'dv_cost': np.float32(90.53062), 'dw_cost': np.float32(19.767115), 'w_cost': np.float32(20.806637)}\n",
      "Ep   28 | steps=1025 | return= -131.10 | buf= 28700\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current full_memory:  30\n",
      "current dv_cost_factor:  10\n",
      "current dw_cost_factor:  10\n",
      "current w_cost_factor:  10\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  28\n",
      "[t=29000] alpha=0.049  Q=-22.75  losses: Q1=0.139 Q2=0.216 Pi=22.632\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  -139.9469\n",
      "Cost breakdown:  {'dv_cost': np.float32(98.39286), 'dw_cost': np.float32(21.267424), 'w_cost': np.float32(20.28664)}\n",
      "Ep   29 | steps=1025 | return= -139.95 | buf= 29725\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current full_memory:  30\n",
      "current dv_cost_factor:  10\n",
      "current dw_cost_factor:  10\n",
      "current w_cost_factor:  10\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  29\n",
      "[t=30000] alpha=0.049  Q=-23.15  losses: Q1=0.198 Q2=0.180 Pi=22.784\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  -128.5863\n",
      "Cost breakdown:  {'dv_cost': np.float32(85.798035), 'dw_cost': np.float32(19.893198), 'w_cost': np.float32(22.895025)}\n",
      "Ep   30 | steps=1025 | return= -128.59 | buf= 30750\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current full_memory:  30\n",
      "current dv_cost_factor:  10\n",
      "current dw_cost_factor:  10\n",
      "current w_cost_factor:  10\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  30\n",
      "[t=31000] alpha=0.050  Q=-22.80  losses: Q1=0.152 Q2=0.091 Pi=22.689\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  -129.34447\n",
      "Cost breakdown:  {'dv_cost': np.float32(92.13178), 'dw_cost': np.float32(18.471636), 'w_cost': np.float32(18.740904)}\n",
      "Ep   31 | steps=1025 | return= -129.34 | buf= 31775\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current full_memory:  30\n",
      "current dv_cost_factor:  10\n",
      "current dw_cost_factor:  10\n",
      "current w_cost_factor:  10\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  31\n",
      "[t=32000] alpha=0.050  Q=-23.04  losses: Q1=0.195 Q2=0.203 Pi=22.792\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  -131.47249\n",
      "Cost breakdown:  {'dv_cost': np.float32(94.77363), 'dw_cost': np.float32(17.938896), 'w_cost': np.float32(18.75989)}\n",
      "Ep   32 | steps=1025 | return= -131.47 | buf= 32800\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current full_memory:  30\n",
      "current dv_cost_factor:  10\n",
      "current dw_cost_factor:  10\n",
      "current w_cost_factor:  10\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  32\n",
      "[t=33000] alpha=0.049  Q=-22.73  losses: Q1=0.067 Q2=0.061 Pi=22.704\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  -131.71707\n",
      "Cost breakdown:  {'dv_cost': np.float32(88.904396), 'dw_cost': np.float32(21.448486), 'w_cost': np.float32(21.36409)}\n",
      "Ep   33 | steps=1025 | return= -131.72 | buf= 33825\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current full_memory:  30\n",
      "current dv_cost_factor:  10\n",
      "current dw_cost_factor:  10\n",
      "current w_cost_factor:  10\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  33\n",
      "[t=34000] alpha=0.049  Q=-22.72  losses: Q1=0.070 Q2=0.091 Pi=22.646\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  -123.96092\n",
      "Cost breakdown:  {'dv_cost': np.float32(83.8028), 'dw_cost': np.float32(20.844448), 'w_cost': np.float32(19.3136)}\n",
      "Ep   34 | steps=1025 | return= -123.96 | buf= 34850\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current full_memory:  30\n",
      "current dv_cost_factor:  10\n",
      "current dw_cost_factor:  10\n",
      "current w_cost_factor:  10\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  34\n",
      "[t=35000] alpha=0.048  Q=-22.53  losses: Q1=0.128 Q2=0.208 Pi=22.456\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  -120.74619\n",
      "Cost breakdown:  {'dv_cost': np.float32(80.86805), 'dw_cost': np.float32(19.344904), 'w_cost': np.float32(20.533312)}\n",
      "Ep   35 | steps=1025 | return= -120.75 | buf= 35875\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current full_memory:  30\n",
      "current dv_cost_factor:  10\n",
      "current dw_cost_factor:  10\n",
      "current w_cost_factor:  10\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  35\n",
      "[t=36000] alpha=0.048  Q=-22.98  losses: Q1=0.168 Q2=0.197 Pi=22.609\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  -132.57709\n",
      "Cost breakdown:  {'dv_cost': np.float32(92.91197), 'dw_cost': np.float32(18.737535), 'w_cost': np.float32(20.92758)}\n",
      "Ep   36 | steps=1025 | return= -132.58 | buf= 36900\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current full_memory:  30\n",
      "current dv_cost_factor:  10\n",
      "current dw_cost_factor:  10\n",
      "current w_cost_factor:  10\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  36\n",
      "[t=37000] alpha=0.048  Q=-22.71  losses: Q1=0.263 Q2=0.350 Pi=22.633\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  -133.11272\n",
      "Cost breakdown:  {'dv_cost': np.float32(94.36491), 'dw_cost': np.float32(19.989502), 'w_cost': np.float32(18.75828)}\n",
      "Ep   37 | steps=1025 | return= -133.11 | buf= 37925\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current full_memory:  30\n",
      "current dv_cost_factor:  10\n",
      "current dw_cost_factor:  10\n",
      "current w_cost_factor:  10\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  37\n",
      "[t=38000] alpha=0.047  Q=-22.94  losses: Q1=4.002 Q2=4.001 Pi=22.661\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  -129.08601\n",
      "Cost breakdown:  {'dv_cost': np.float32(91.113754), 'dw_cost': np.float32(19.162182), 'w_cost': np.float32(18.810022)}\n",
      "Ep   38 | steps=1025 | return= -129.09 | buf= 38950\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current full_memory:  30\n",
      "current dv_cost_factor:  10\n",
      "current dw_cost_factor:  10\n",
      "current w_cost_factor:  10\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  38\n",
      "[t=39000] alpha=0.046  Q=-22.85  losses: Q1=0.067 Q2=0.079 Pi=22.538\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  -129.45015\n",
      "Cost breakdown:  {'dv_cost': np.float32(89.75212), 'dw_cost': np.float32(20.178139), 'w_cost': np.float32(19.519964)}\n",
      "Ep   39 | steps=1025 | return= -129.45 | buf= 39975\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current full_memory:  30\n",
      "current dv_cost_factor:  10\n",
      "current dw_cost_factor:  10\n",
      "current w_cost_factor:  10\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  39\n",
      "[t=40000] alpha=0.046  Q=-22.95  losses: Q1=0.167 Q2=0.222 Pi=22.656\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  -121.61496\n",
      "Cost breakdown:  {'dv_cost': np.float32(81.54793), 'dw_cost': np.float32(19.317186), 'w_cost': np.float32(20.749866)}\n",
      "Ep   40 | steps=1025 | return= -121.61 | buf= 41000\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current full_memory:  30\n",
      "current dv_cost_factor:  10\n",
      "current dw_cost_factor:  10\n",
      "current w_cost_factor:  10\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  40\n",
      "[t=41000] alpha=0.045  Q=-22.38  losses: Q1=0.059 Q2=0.105 Pi=22.654\n",
      "[t=42000] alpha=0.045  Q=-22.72  losses: Q1=0.128 Q2=0.084 Pi=22.530\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  -133.28468\n",
      "Cost breakdown:  {'dv_cost': np.float32(95.15176), 'dw_cost': np.float32(18.675133), 'w_cost': np.float32(19.45779)}\n",
      "Ep   41 | steps=1025 | return= -133.28 | buf= 42025\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current full_memory:  30\n",
      "current dv_cost_factor:  10\n",
      "current dw_cost_factor:  10\n",
      "current w_cost_factor:  10\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  41\n",
      "[t=43000] alpha=0.046  Q=-22.64  losses: Q1=0.111 Q2=0.078 Pi=22.643\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  -125.89301\n",
      "Cost breakdown:  {'dv_cost': np.float32(87.03607), 'dw_cost': np.float32(19.91516), 'w_cost': np.float32(18.941708)}\n",
      "Ep   42 | steps=1025 | return= -125.89 | buf= 43050\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current full_memory:  30\n",
      "current dv_cost_factor:  10\n",
      "current dw_cost_factor:  10\n",
      "current w_cost_factor:  10\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  42\n",
      "[t=44000] alpha=0.045  Q=-22.97  losses: Q1=0.072 Q2=0.059 Pi=22.517\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  -130.43613\n",
      "Cost breakdown:  {'dv_cost': np.float32(91.52682), 'dw_cost': np.float32(19.021355), 'w_cost': np.float32(19.888103)}\n",
      "Ep   43 | steps=1025 | return= -130.44 | buf= 44075\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current full_memory:  30\n",
      "current dv_cost_factor:  10\n",
      "current dw_cost_factor:  10\n",
      "current w_cost_factor:  10\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  43\n",
      "[t=45000] alpha=0.047  Q=-22.52  losses: Q1=0.206 Q2=0.106 Pi=22.545\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  -127.66088\n",
      "Cost breakdown:  {'dv_cost': np.float32(89.22036), 'dw_cost': np.float32(19.544914), 'w_cost': np.float32(18.895634)}\n",
      "Ep   44 | steps=1025 | return= -127.66 | buf= 45100\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current full_memory:  30\n",
      "current dv_cost_factor:  10\n",
      "current dw_cost_factor:  10\n",
      "current w_cost_factor:  10\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  44\n",
      "[t=46000] alpha=0.048  Q=-22.82  losses: Q1=0.395 Q2=0.304 Pi=22.136\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  -118.91572\n",
      "Cost breakdown:  {'dv_cost': np.float32(84.55681), 'dw_cost': np.float32(16.726807), 'w_cost': np.float32(17.63206)}\n",
      "Ep   45 | steps=1025 | return= -118.92 | buf= 46125\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current full_memory:  30\n",
      "current dv_cost_factor:  10\n",
      "current dw_cost_factor:  10\n",
      "current w_cost_factor:  10\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  45\n",
      "[t=47000] alpha=0.048  Q=-22.42  losses: Q1=0.229 Q2=0.258 Pi=22.453\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  -127.3287\n",
      "Cost breakdown:  {'dv_cost': np.float32(91.88635), 'dw_cost': np.float32(18.153885), 'w_cost': np.float32(17.288488)}\n",
      "Ep   46 | steps=1025 | return= -127.33 | buf= 47150\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current full_memory:  30\n",
      "current dv_cost_factor:  10\n",
      "current dw_cost_factor:  10\n",
      "current w_cost_factor:  10\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  46\n",
      "[t=48000] alpha=0.046  Q=-22.74  losses: Q1=0.058 Q2=0.055 Pi=22.474\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  -128.60236\n",
      "Cost breakdown:  {'dv_cost': np.float32(91.326645), 'dw_cost': np.float32(18.29138), 'w_cost': np.float32(18.984451)}\n",
      "Ep   47 | steps=1025 | return= -128.60 | buf= 48175\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current full_memory:  30\n",
      "current dv_cost_factor:  10\n",
      "current dw_cost_factor:  10\n",
      "current w_cost_factor:  10\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  47\n",
      "[t=49000] alpha=0.046  Q=-22.41  losses: Q1=3.840 Q2=3.824 Pi=22.097\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  -127.67752\n",
      "Cost breakdown:  {'dv_cost': np.float32(90.70254), 'dw_cost': np.float32(18.19043), 'w_cost': np.float32(18.784594)}\n",
      "Ep   48 | steps=1025 | return= -127.68 | buf= 49200\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current full_memory:  30\n",
      "current dv_cost_factor:  10\n",
      "current dw_cost_factor:  10\n",
      "current w_cost_factor:  10\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  48\n",
      "17.1 action:  [0.0085, -0.9861] n_targets:  1 reward:  100\n",
      "[t=50000] alpha=0.046  Q=-22.57  losses: Q1=0.374 Q2=0.389 Pi=22.481\n",
      "Saved to RL_models/ATTN_FF/agent_0/\n",
      "TIME before resetting: 79.99999999999973\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current full_memory:  30\n",
      "current dv_cost_factor:  10\n",
      "current dw_cost_factor:  10\n",
      "current w_cost_factor:  10\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  49\n",
      "Eval ep 1/3: return=-83.27\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current full_memory:  30\n",
      "current dv_cost_factor:  10\n",
      "current dw_cost_factor:  10\n",
      "current w_cost_factor:  10\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  50\n",
      "Eval ep 2/3: return=-84.38\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current full_memory:  30\n",
      "current dv_cost_factor:  10\n",
      "current dw_cost_factor:  10\n",
      "current w_cost_factor:  10\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  51\n",
      "Eval ep 3/3: return=-87.14\n",
      "Average return over 3 eps: -84.93\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-84.93156433105469"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 1) Make env wrapper (inherits your MultiFF)\n",
    "env = env_attn_multiff.EnvForAttentionSAC(\n",
    "    num_obs_ff=5,\n",
    "    slot_fields=['valid','d_log','sin','cos','t_start_seen'],  # choose fields by name\n",
    "    recenter=True,                                             # map to [-1,1]\n",
    "    flash_on_interval=0.3, episode_len=1024\n",
    ")\n",
    "\n",
    "# 2) Build trainer\n",
    "agent = attn_sac_ff.AttnSACforMultifirefly(model_folder='RL_models/ATTN_FF/agent_0/',\n",
    "                               num_obs_ff=5, slot_fields=['valid','d_log','sin','cos','t_start_seen'])\n",
    "\n",
    "agent.make_env()  # pass/override env kwargs here if you want\n",
    "agent.make_agent(\n",
    "    k_top=4, d_slot=64, n_heads=2, include_ctx=True,\n",
    "    attn_temperature=1.0, slot_dropout_p=0.05, gumbel_topk=True,\n",
    "    lr=3e-4, gamma=0.99, tau=0.995, target_entropy=-2.0,\n",
    "    batch_size=128, replay_size=200_000, random_steps=2_000, updates_per_step=1,\n",
    "    amp=False, log_every=1000, eval_every=0, save_every=0\n",
    ")\n",
    "\n",
    "# 3) Train\n",
    "agent.regular_training(total_env_steps=50_000)\n",
    "\n",
    "# 4) Save / Eval\n",
    "agent.save_agent()\n",
    "agent.test_agent(num_eps=3, deterministic=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d03ec6f",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9bb9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME before resetting: 0\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current full_memory:  30\n",
      "current dv_cost_factor:  10\n",
      "current dw_cost_factor:  10\n",
      "current w_cost_factor:  10\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  -5399.0127\n",
      "Cost breakdown:  {'dv_cost': np.float32(4860.5903), 'dw_cost': np.float32(328.59384), 'w_cost': np.float32(209.82349)}\n",
      "Ep    1 | steps=1025 | return=-5399.01 | eps_buf=   1\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current full_memory:  30\n",
      "current dv_cost_factor:  10\n",
      "current dw_cost_factor:  10\n",
      "current w_cost_factor:  10\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  1\n",
      "[t=2000] alpha=0.756  losses: Q1=20.869 Q2=18.772 Pi=25.095\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  -4713.7593\n",
      "Cost breakdown:  {'dv_cost': np.float32(4225.106), 'dw_cost': np.float32(287.42172), 'w_cost': np.float32(201.22934)}\n",
      "Ep    2 | steps=1025 | return=-4713.76 | eps_buf=   2\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current full_memory:  30\n",
      "current dv_cost_factor:  10\n",
      "current dw_cost_factor:  10\n",
      "current w_cost_factor:  10\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  2\n",
      "[t=3000] alpha=0.590  losses: Q1=16.683 Q2=14.586 Pi=40.817\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  -3351.044\n",
      "Cost breakdown:  {'dv_cost': np.float32(2848.581), 'dw_cost': np.float32(245.91493), 'w_cost': np.float32(256.5452)}\n",
      "Ep    3 | steps=1025 | return=-3351.04 | eps_buf=   3\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current full_memory:  30\n",
      "current dv_cost_factor:  10\n",
      "current dw_cost_factor:  10\n",
      "current w_cost_factor:  10\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  3\n",
      "[t=4000] alpha=0.480  losses: Q1=15.170 Q2=14.655 Pi=55.205\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  -3119.8425\n",
      "Cost breakdown:  {'dv_cost': np.float32(2601.9702), 'dw_cost': np.float32(273.5881), 'w_cost': np.float32(244.28389)}\n",
      "Ep    4 | steps=1025 | return=-3119.84 | eps_buf=   4\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current full_memory:  30\n",
      "current dv_cost_factor:  10\n",
      "current dw_cost_factor:  10\n",
      "current w_cost_factor:  10\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  4\n",
      "[t=5000] alpha=0.390  losses: Q1=10.432 Q2=9.104 Pi=61.793\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      3\u001b[39m rnn_agent.make_env()\n\u001b[32m      4\u001b[39m rnn_agent.make_agent(\n\u001b[32m      5\u001b[39m     k_top=\u001b[32m4\u001b[39m, d_slot=\u001b[32m64\u001b[39m, include_ctx=\u001b[38;5;28;01mTrue\u001b[39;00m, rnn=\u001b[33m'\u001b[39m\u001b[33mgru\u001b[39m\u001b[33m'\u001b[39m, d_hidden=\u001b[32m256\u001b[39m,\n\u001b[32m      6\u001b[39m     attn_temperature=\u001b[32m1.0\u001b[39m, slot_dropout_p=\u001b[32m0.05\u001b[39m, gumbel_topk=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m      9\u001b[39m     amp=\u001b[38;5;28;01mFalse\u001b[39;00m, log_every=\u001b[32m1000\u001b[39m, eval_every=\u001b[32m0\u001b[39m\n\u001b[32m     10\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43mrnn_agent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mregular_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_env_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50_000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m rnn_agent.save_agent()\n\u001b[32m     13\u001b[39m rnn_agent.test_agent(num_eps=\u001b[32m3\u001b[39m, deterministic=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Multifirefly-Project/multiff_analysis/multiff_code/methods/machine_learning/RL_models/ff_attention/attn_sac_rnn.py:1002\u001b[39m, in \u001b[36mAttnRNNSACforMultifirefly.regular_training\u001b[39m\u001b[34m(self, total_env_steps)\u001b[39m\n\u001b[32m    989\u001b[39m     batch_t = {\n\u001b[32m    990\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mslot_feats_seq\u001b[39m\u001b[33m'\u001b[39m: sf_seq,\n\u001b[32m    991\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mslot_mask_seq\u001b[39m\u001b[33m'\u001b[39m: sm_seq,\n\u001b[32m   (...)\u001b[39m\u001b[32m    998\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mself_feats_seq_next\u001b[39m\u001b[33m'\u001b[39m: ssn_seq,\n\u001b[32m    999\u001b[39m     }\n\u001b[32m   1001\u001b[39m     \u001b[38;5;66;03m# Single SAC update on sequences\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1002\u001b[39m     metrics = \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_t\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m        \u001b[49m\u001b[43mactor\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mactor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq1\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mq1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq2\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mq2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[43m        \u001b[49m\u001b[43mactor_targ\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mactor_targ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq1_targ\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mq1_targ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq2_targ\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mq2_targ\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[43m        \u001b[49m\u001b[43mactor_opt\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mactor_opt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq1_opt\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mq1_opt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq2_opt\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mq2_opt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_alpha\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlog_alpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha_opt\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43malpha_opt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtau\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtau\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_entropy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget_entropy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m        \u001b[49m\u001b[43maction_limits\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction_limits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mburn_in\u001b[49m\u001b[43m=\u001b[49m\u001b[43mburn_in\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_clip\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_clip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mamp\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_amp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[32m   1014\u001b[39m \u001b[38;5;66;03m# 3) EPISODE bookkeeping & logging\u001b[39;00m\n\u001b[32m   1015\u001b[39m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m done \u001b[38;5;129;01mor\u001b[39;00m truncated:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Multifirefly-Project/multiff_analysis/multiff_code/methods/machine_learning/RL_models/ff_attention/attn_sac_rnn.py:555\u001b[39m, in \u001b[36msac_train_step_rnn\u001b[39m\u001b[34m(batch, actor, q1, q2, actor_targ, q1_targ, q2_targ, actor_opt, q1_opt, q2_opt, log_alpha, alpha_opt, gamma, tau, target_entropy, action_limits, burn_in, grad_clip, amp, device)\u001b[39m\n\u001b[32m    553\u001b[39m mu_n, std_n, _, _ = actor(s_tr_n, m_tr_n, ss_tr_n, hx_actor)\n\u001b[32m    554\u001b[39m \u001b[38;5;66;03m# shapes: [B, Ttr, A], [B, Ttr, 1]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m555\u001b[39m a_n, logp_n = \u001b[43msample_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmu_n\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd_n\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_limits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[38;5;66;03m# Target critics evaluate Q(s', a')\u001b[39;00m\n\u001b[32m    558\u001b[39m q1n_seq, _ = q1_targ(s_tr_n, m_tr_n, ss_tr_n, a_n, hx_q1_t)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Multifirefly-Project/multiff_analysis/multiff_code/methods/machine_learning/RL_models/ff_attention/attn_sac_rnn.py:441\u001b[39m, in \u001b[36msample_action\u001b[39m\u001b[34m(mu, std, action_limits)\u001b[39m\n\u001b[32m    438\u001b[39m     std = std.reshape(B * T, A)\n\u001b[32m    440\u001b[39m \u001b[38;5;66;03m# 1) Sample z ~ N(mu, std) using rsample() to enable backprop through randomness\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m441\u001b[39m normal = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdistributions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mNormal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    442\u001b[39m z = normal.rsample()\n\u001b[32m    444\u001b[39m \u001b[38;5;66;03m# 2) Squash via tanh to (-1,1)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/multiff_clean/lib/python3.11/site-packages/torch/distributions/normal.py:59\u001b[39m, in \u001b[36mNormal.__init__\u001b[39m\u001b[34m(self, loc, scale, validate_args)\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     58\u001b[39m     batch_shape = \u001b[38;5;28mself\u001b[39m.loc.size()\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/multiff_clean/lib/python3.11/site-packages/torch/distributions/distribution.py:70\u001b[39m, in \u001b[36mDistribution.__init__\u001b[39m\u001b[34m(self, batch_shape, event_shape, validate_args)\u001b[39m\n\u001b[32m     68\u001b[39m         value = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, param)\n\u001b[32m     69\u001b[39m         valid = constraint.check(value)\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid.all():\n\u001b[32m     71\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     72\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     73\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value.shape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     76\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     77\u001b[39m             )\n\u001b[32m     78\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "rnn_agent = attn_sac_rnn.AttnRNNSACforMultifirefly(model_folder='RL_models/ATTN_RNN/agent_0/',\n",
    "                                      num_obs_ff=5, slot_fields=['valid','d_log','sin','cos','t_start_seen'])\n",
    "rnn_agent.make_env()\n",
    "rnn_agent.make_agent(\n",
    "    k_top=4, d_slot=64, include_ctx=True, rnn='gru', d_hidden=256,\n",
    "    attn_temperature=1.0, slot_dropout_p=0.05, gumbel_topk=True,\n",
    "    lr=3e-4, gamma=0.99, tau=0.995, target_entropy=-2.0,\n",
    "    batch_size=16, seq_len_total=128, burn_in=16, updates_per_step=1,\n",
    "    amp=False, log_every=1000, eval_every=0\n",
    ")\n",
    "rnn_agent.regular_training(total_env_steps=50_000)\n",
    "rnn_agent.save_agent()\n",
    "rnn_agent.test_agent(num_eps=3, deterministic=True)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [
    {
     "file_id": "1FShUys0iUmi3huyQwtaEdyhGivLG2R_5",
     "timestamp": 1681095282536
    }
   ],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "multiff_clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
