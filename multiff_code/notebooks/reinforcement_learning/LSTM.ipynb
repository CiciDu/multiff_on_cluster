{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8446ba7d",
   "metadata": {
    "id": "8446ba7d"
   },
   "source": [
    "Note: all \".to(device)\" are deleted because Apple M1 chip does does support it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83af57b",
   "metadata": {
    "id": "b83af57b"
   },
   "source": [
    "Would recommend using Google Colab instead because it will be much faster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KMRCk6qpFsyH",
   "metadata": {
    "id": "KMRCk6qpFsyH"
   },
   "source": [
    "Source of LSTM codes:\n",
    "https://github.com/quantumiracle/Popular-RL-Algorithms/blob/master/sac_v2_lstm.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93C64_FAEoSJ",
   "metadata": {
    "id": "93C64_FAEoSJ"
   },
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06f9c7f3",
   "metadata": {
    "id": "06f9c7f3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set up logging configuration.\n",
      "Numba isn't available, making a no-op decorator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cicid/miniconda3/envs/multiff_clean/lib/python3.11/site-packages/xgboost/core.py:377: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc >= 2.28) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os, sys\n",
    "for p in [Path.cwd()] + list(Path.cwd().parents):\n",
    "    if p.name == 'Multifirefly-Project':\n",
    "        os.chdir(p)\n",
    "        sys.path.insert(0, str(p / 'multiff_analysis/multiff_code/methods'))\n",
    "        break\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "from data_wrangling import specific_utils, process_monkey_information, base_processing_class\n",
    "from pattern_discovery import pattern_by_trials, pattern_by_points, make_ff_dataframe, ff_dataframe_utils, pattern_by_trials, pattern_by_points, cluster_analysis, organize_patterns_and_features, category_class\n",
    "from decision_making_analysis.cluster_replacement import cluster_replacement_utils, plot_cluster_replacement\n",
    "from decision_making_analysis.decision_making import decision_making_utils, plot_decision_making, intended_targets_classes\n",
    "from decision_making_analysis.GUAT import GUAT_helper_class, GUAT_collect_info_class, GUAT_combine_info_class, add_features_GUAT_and_TAFT\n",
    "from decision_making_analysis import free_selection, replacement, trajectory_info\n",
    "from visualization.matplotlib_tools import plot_trials, plot_polar, additional_plots, plot_behaviors_utils, plot_statistics\n",
    "from visualization.animation import animation_func, animation_utils, animation_class\n",
    "from null_behaviors import sample_null_distributions, show_null_trajectory\n",
    "from machine_learning.ml_methods import regression_utils, classification_utils, prep_ml_data_utils, hyperparam_tuning_class\n",
    "from machine_learning.RL.env_related import env_for_lstm, env_utils, base_env, collect_agent_data, process_agent_data\n",
    "from machine_learning.RL.lstm import GRU_functions, LSTM_functions, LSTM_functions, lstm_for_multiff_class\n",
    "from machine_learning.RL.SB3 import interpret_neural_network, sb3_for_multiff_class, rl_for_multiff_utils, SB3_functions\n",
    "from eye_position_analysis import eye_positions\n",
    "from neural_data_analysis.neural_analysis_tools.model_neural_data import neural_data_modeling\n",
    "from decision_making_analysis.compare_GUAT_and_TAFT import find_GUAT_or_TAFT_trials\n",
    "from machine_learning.RL.SB3 import rl_for_multiff_utils, rl_for_multiff_class\n",
    "\n",
    "import os, sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "from gymnasium import spaces, Env\n",
    "import torch\n",
    "import optuna\n",
    "from numpy import pi\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from torch.linalg import vector_norm\n",
    "from IPython.display import HTML\n",
    "from functools import partial\n",
    "from optuna.pruners import MedianPruner\n",
    "from optuna.samplers import TPESampler\n",
    "import gc\n",
    "from importlib import reload\n",
    "plt.rcParams[\"animation.html\"] = \"html5\"\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "device_idx = 0\n",
    "# device = torch.device(\"cuda:\" + str(device_idx) if torch.cuda.is_available() else \"cpu\")\n",
    "## if using Jupyter Notebook\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "model_folder_name = \"RL_models/LSTM_stored_models/all_agents/gen_0/LSTM_Aug_1_24\"\n",
    "os.makedirs(model_folder_name, exist_ok=True)\n",
    "PLAYER = \"agent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabe4586",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7770be",
   "metadata": {},
   "source": [
    "# streamline everything"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55b14d7",
   "metadata": {},
   "source": [
    "### oct_13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28e8f636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_folder_name: RL_models/LSTM_stored_models/all_agents/oct_13_3/dv0_dw0_w0_memT1\n"
     ]
    }
   ],
   "source": [
    "overall_folder = 'RL_models/LSTM_stored_models/all_agents/oct_13_3/'\n",
    "env_kwargs = {'num_obs_ff': 5,\n",
    "              'add_action_to_obs': True,\n",
    "              'angular_terminal_vel': 1,\n",
    "              \"dt\": 0.1,\n",
    "              \"flash_on_interval\": 0.3,\n",
    "              \"dv_cost_factor\": 0,\n",
    "              \"dw_cost_factor\": 0,\n",
    "              \"w_cost_factor\": 0,\n",
    "              \"max_in_memory_time\": 1,\n",
    "            }   \n",
    "lm = lstm_for_multiff_class.LSTMforMultifirefly(overall_folder=overall_folder,\n",
    "                                                **env_kwargs)\n",
    "\n",
    "# lm.streamline_everything(currentTrial_for_animation=None, num_trials_for_animation=None, duration=[10, 40],\n",
    "#                          best_model_postcurriculum_exists_ok=True)\n",
    "                                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa0eee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cicid/miniconda3/envs/multiff_clean/lib/python3.11/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\n",
      "/user_data/cicid/Multifirefly-Project/multiff_analysis/multiff_code/methods/machine_learning/RL/lstm/LSTM_functions.py:551: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.soft_q_net1.load_state_dict(torch.load(\n",
      "/user_data/cicid/Multifirefly-Project/multiff_analysis/multiff_code/methods/machine_learning/RL/lstm/LSTM_functions.py:553: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.soft_q_net2.load_state_dict(torch.load(\n",
      "/user_data/cicid/Multifirefly-Project/multiff_analysis/multiff_code/methods/machine_learning/RL/lstm/LSTM_functions.py:555: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.policy_net.load_state_dict(torch.load(\n",
      "/user_data/cicid/Multifirefly-Project/multiff_analysis/multiff_code/methods/machine_learning/RL/lstm/lstm_for_multiff_class.py:374: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return lambda b: torch.load(io.BytesIO(b), map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soft Q Network (1,2):  QNetworkLSTM2(\n",
      "  (linear1): Linear(in_features=51, out_features=128, bias=True)\n",
      "  (lstm1): LSTM(128, 128, dropout=0.2)\n",
      "  (linear2): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "Policy Network:  SAC_PolicyNetworkLSTM(\n",
      "  (linear1): Linear(in_features=47, out_features=128, bias=True)\n",
      "  (linear2): Linear(in_features=49, out_features=128, bias=True)\n",
      "  (lstm1): LSTM(128, 128, dropout=0.2)\n",
      "  (linear3): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (mean_linear): Linear(in_features=128, out_features=2, bias=True)\n",
      "  (log_std_linear): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n",
      "Soft Q Network (1,2):  QNetworkLSTM2(\n",
      "  (linear1): Linear(in_features=51, out_features=128, bias=True)\n",
      "  (lstm1): LSTM(128, 128, dropout=0.2)\n",
      "  (linear2): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "Policy Network:  SAC_PolicyNetworkLSTM(\n",
      "  (linear1): Linear(in_features=47, out_features=128, bias=True)\n",
      "  (linear2): Linear(in_features=49, out_features=128, bias=True)\n",
      "  (lstm1): LSTM(128, 128, dropout=0.2)\n",
      "  (linear3): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (mean_linear): Linear(in_features=128, out_features=2, bias=True)\n",
      "  (log_std_linear): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n",
      "Need to train a new best_model_postcurriculum\n",
      "Starting curriculum training\n",
      "Soft Q Network (1,2):  QNetworkLSTM2(\n",
      "  (linear1): Linear(in_features=51, out_features=128, bias=True)\n",
      "  (lstm1): LSTM(128, 128, dropout=0.2)\n",
      "  (linear2): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "Policy Network:  SAC_PolicyNetworkLSTM(\n",
      "  (linear1): Linear(in_features=47, out_features=128, bias=True)\n",
      "  (linear2): Linear(in_features=49, out_features=128, bias=True)\n",
      "  (lstm1): LSTM(128, 128, dropout=0.2)\n",
      "  (linear3): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (mean_linear): Linear(in_features=128, out_features=2, bias=True)\n",
      "  (log_std_linear): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n",
      "length of replay buffer: 63\n",
      "Loaded existing agent: RL_models/LSTM_stored_models/all_agents/oct_13_3/dv0_dw0_w0_memT1/best_model_in_curriculum\n",
      "Loaded best_model_in_curriculum\n",
      "Made env based on env params saved in RL_models/LSTM_stored_models/all_agents/oct_13_3/dv0_dw0_w0_memT1/best_model_in_curriculum\n",
      "Soft Q Network (1,2):  QNetworkLSTM2(\n",
      "  (linear1): Linear(in_features=51, out_features=128, bias=True)\n",
      "  (lstm1): LSTM(128, 128, dropout=0.2)\n",
      "  (linear2): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "Policy Network:  SAC_PolicyNetworkLSTM(\n",
      "  (linear1): Linear(in_features=47, out_features=128, bias=True)\n",
      "  (linear2): Linear(in_features=49, out_features=128, bias=True)\n",
      "  (lstm1): LSTM(128, 128, dropout=0.2)\n",
      "  (linear3): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (mean_linear): Linear(in_features=128, out_features=2, bias=True)\n",
      "  (log_std_linear): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n",
      "length of replay buffer: 63\n",
      "Loaded existing agent: RL_models/LSTM_stored_models/all_agents/oct_13_3/dv0_dw0_w0_memT1/best_model_in_curriculum\n",
      "TIME before resetting: 0\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "28.5 action:  [-0.5474, -0.9829] n_targets:  1 reward:  57.1\n",
      "82.8 action:  [-0.1548, -0.981] n_targets:  1 reward:  51.48\n",
      "95.0 action:  [0.4062, -0.9868] n_targets:  1 reward:  60.17\n",
      "ALPHA (entropy-related):  tensor([0.9782], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.97824]\n",
      "Episode: 0, Episode Reward: 168.74747212727863\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  1\n",
      "72.9 action:  [-0.6346, -0.996] n_targets:  2 reward:  108.42\n",
      "77.1 action:  [0.5757, -0.9897] n_targets:  3 reward:  210.8\n",
      "89.2 action:  [0.0527, -0.9951] n_targets:  1 reward:  82.33\n",
      "ALPHA (entropy-related):  tensor([0.9570], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.97824 0.95695]\n",
      "Episode: 1, Episode Reward: 401.5480651855469\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  2\n",
      "8.1 action:  [0.517, -0.9878] n_targets:  1 reward:  65.71\n",
      "19.9 action:  [-0.2887, -0.9875] n_targets:  1 reward:  55.04\n",
      "56.5 action:  [0.4287, -0.981] n_targets:  1 reward:  60.92\n",
      "101.7 action:  [-0.4129, -0.9877] n_targets:  1 reward:  81.36\n",
      "ALPHA (entropy-related):  tensor([0.9361], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.97824 0.95695 0.93613]\n",
      "Episode: 2, Episode Reward: 263.0246327718099\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  3\n",
      "63.8 action:  [-0.4125, -0.9939] n_targets:  2 reward:  113.55\n",
      "95.8 action:  [0.357, -0.989] n_targets:  1 reward:  70.59\n",
      "ALPHA (entropy-related):  tensor([0.9158], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.97824 0.95695 0.93613 0.91576]\n",
      "Episode: 3, Episode Reward: 184.14054616292316\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  4\n",
      "41.8 action:  [0.4473, -0.9821] n_targets:  1 reward:  60.26\n",
      "47.9 action:  [-0.2985, -0.9855] n_targets:  1 reward:  53.91\n",
      "87.4 action:  [0.4076, -0.9969] n_targets:  3 reward:  191.47\n",
      "ALPHA (entropy-related):  tensor([0.8958], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.97824 0.95695 0.93613 0.91576 0.89583]\n",
      "Episode: 4, Episode Reward: 305.63903554280597\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  5\n",
      "28.2 action:  [0.4256, -0.9951] n_targets:  1 reward:  52.42\n",
      "70.6 action:  [0.4083, -0.9942] n_targets:  3 reward:  195.98\n",
      "ALPHA (entropy-related):  tensor([0.8763], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.97824 0.95695 0.93613 0.91576 0.89583 0.87634]\n",
      "Episode: 5, Episode Reward: 248.40179443359372\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  6\n",
      "3.5 action:  [-0.2571, -0.9829] n_targets:  1 reward:  67.64\n",
      "ALPHA (entropy-related):  tensor([0.8573], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.97824 0.95695 0.93613 0.91576 0.89583 0.87634 0.85727]\n",
      "Episode: 6, Episode Reward: 67.63689422607422\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  7\n",
      "5.1 action:  [-0.5555, -0.9861] n_targets:  2 reward:  149.66\n",
      "18.1 action:  [0.4556, -0.9852] n_targets:  1 reward:  83.37\n",
      "30.4 action:  [0.2321, -0.9893] n_targets:  3 reward:  166.65\n",
      "34.4 action:  [-0.3614, -0.9874] n_targets:  1 reward:  50.42\n",
      "69.4 action:  [-0.403, -0.9961] n_targets:  3 reward:  201.96\n",
      "ALPHA (entropy-related):  tensor([0.8386], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.97824 0.95695 0.93613 0.91576 0.89583 0.87634 0.85727 0.83861]\n",
      "Episode: 7, Episode Reward: 652.0641352335612\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  8\n",
      "36.7 action:  [-0.4482, -0.9891] n_targets:  2 reward:  146.39\n",
      "ALPHA (entropy-related):  tensor([0.8204], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.97824 0.95695 0.93613 0.91576 0.89583 0.87634 0.85727 0.83861 0.82037]\n",
      "Episode: 8, Episode Reward: 146.39271036783853\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  9\n",
      "65.0 action:  [0.581, -0.9861] n_targets:  1 reward:  56.65\n",
      "85.0 action:  [0.2763, -0.9855] n_targets:  3 reward:  201.67\n",
      "ALPHA (entropy-related):  tensor([0.8025], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.97824 0.95695 0.93613 0.91576 0.89583 0.87634 0.85727 0.83861 0.82037\n",
      " 0.80252]\n",
      "Episode: 9, Episode Reward: 258.32256571451825\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  10\n",
      "25.5 action:  [-0.4452, -0.9938] n_targets:  1 reward:  51.82\n",
      "63.2 action:  [-0.2871, -0.9941] n_targets:  1 reward:  57.25\n",
      "70.3 action:  [-0.2439, -0.9827] n_targets:  1 reward:  59.12\n",
      "ALPHA (entropy-related):  tensor([0.7851], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.95695 0.93613 0.91576 0.89583 0.87634 0.85727 0.83861 0.82037 0.80252\n",
      " 0.78506]\n",
      "Episode: 10, Episode Reward: 168.18805948893228\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  11\n",
      "64.1 action:  [-0.0235, -0.9844] n_targets:  2 reward:  126.37\n",
      "94.3 action:  [-0.3464, -0.9836] n_targets:  3 reward:  234.52\n",
      "ALPHA (entropy-related):  tensor([0.7680], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.93613 0.91576 0.89583 0.87634 0.85727 0.83861 0.82037 0.80252 0.78506\n",
      " 0.76797]\n",
      "Episode: 11, Episode Reward: 360.8937072753906\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  12\n",
      "73.7 action:  [0.0149, -0.981] n_targets:  1 reward:  74.47\n",
      "ALPHA (entropy-related):  tensor([0.7513], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.91576 0.89583 0.87634 0.85727 0.83861 0.82037 0.80252 0.78506 0.76797\n",
      " 0.75126]\n",
      "Episode: 12, Episode Reward: 74.46531931559244\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  13\n",
      "44.5 action:  [-0.1659, -0.9856] n_targets:  2 reward:  155.89\n",
      "47.8 action:  [0.0263, -0.9875] n_targets:  1 reward:  59.55\n",
      "ALPHA (entropy-related):  tensor([0.7349], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.89583 0.87634 0.85727 0.83861 0.82037 0.80252 0.78506 0.76797 0.75126\n",
      " 0.73492]\n",
      "Episode: 13, Episode Reward: 215.4372431437174\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  14\n",
      "16.2 action:  [-0.5415, -0.9937] n_targets:  3 reward:  197.45\n",
      "ALPHA (entropy-related):  tensor([0.7189], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.87634 0.85727 0.83861 0.82037 0.80252 0.78506 0.76797 0.75126 0.73492\n",
      " 0.71893]\n",
      "Episode: 14, Episode Reward: 197.4535929361979\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  15\n",
      "3.4 action:  [-0.3613, -0.9817] n_targets:  3 reward:  192.03\n",
      "60.7 action:  [0.2687, -0.9901] n_targets:  1 reward:  56.88\n",
      "ALPHA (entropy-related):  tensor([0.7033], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.85727 0.83861 0.82037 0.80252 0.78506 0.76797 0.75126 0.73492 0.71893\n",
      " 0.70328]\n",
      "Episode: 15, Episode Reward: 248.90988667805988\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  16\n",
      "6.9 action:  [0.1539, -0.9807] n_targets:  1 reward:  55.4\n",
      "67.8 action:  [0.079, -0.9828] n_targets:  1 reward:  69.88\n",
      "73.4 action:  [0.2798, -0.9825] n_targets:  1 reward:  60.33\n",
      "88.7 action:  [-0.3995, -0.9825] n_targets:  2 reward:  120.88\n",
      "92.7 action:  [0.3088, -0.9935] n_targets:  1 reward:  67.11\n",
      "ALPHA (entropy-related):  tensor([0.6880], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.83861 0.82037 0.80252 0.78506 0.76797 0.75126 0.73492 0.71893 0.70328\n",
      " 0.68798]\n",
      "Episode: 16, Episode Reward: 373.59176381429035\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  17\n",
      "71.1 action:  [0.1785, -0.9956] n_targets:  1 reward:  59.83\n",
      "92.6 action:  [0.6351, -0.9817] n_targets:  2 reward:  143.6\n",
      "ALPHA (entropy-related):  tensor([0.6730], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.82037 0.80252 0.78506 0.76797 0.75126 0.73492 0.71893 0.70328 0.68798\n",
      " 0.67301]\n",
      "Episode: 17, Episode Reward: 203.4300587972005\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  18\n",
      "7.6 action:  [-0.0565, -0.9933] n_targets:  2 reward:  109.46\n",
      "17.4 action:  [0.2974, -0.9873] n_targets:  2 reward:  118.64\n",
      "80.7 action:  [-0.4938, -0.9837] n_targets:  2 reward:  132.56\n",
      "ALPHA (entropy-related):  tensor([0.6584], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.80252 0.78506 0.76797 0.75126 0.73492 0.71893 0.70328 0.68798 0.67301\n",
      " 0.65837]\n",
      "Episode: 18, Episode Reward: 360.66593933105463\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  19\n",
      "37.6 action:  [-0.2639, -0.987] n_targets:  2 reward:  129.15\n",
      "ALPHA (entropy-related):  tensor([0.6441], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.78506 0.76797 0.75126 0.73492 0.71893 0.70328 0.68798 0.67301 0.65837\n",
      " 0.64405]\n",
      "Episode: 19, Episode Reward: 129.1482442220052\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  20\n",
      "0.5 action:  [-0.532, -0.9933] n_targets:  2 reward:  147.76\n",
      "51.2 action:  [-0.36, -0.9935] n_targets:  2 reward:  108.66\n",
      "55.1 action:  [0.1798, -0.9924] n_targets:  2 reward:  174.91\n",
      "64.9 action:  [0.3275, -0.991] n_targets:  2 reward:  144.23\n",
      "98.7 action:  [0.2132, -0.9927] n_targets:  1 reward:  84.82\n",
      "ALPHA (entropy-related):  tensor([0.6300], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.76797 0.75126 0.73492 0.71893 0.70328 0.68798 0.67301 0.65837 0.64405\n",
      " 0.63004]\n",
      "Last 100 ALPHA: [0.97824 0.95695 0.93613 0.91576 0.89583 0.87634 0.85727 0.83861 0.82037\n",
      " 0.80252 0.78506 0.76797 0.75126 0.73492 0.71893 0.70328 0.68798 0.67301\n",
      " 0.65837 0.64405 0.63004]\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  21\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  22\n",
      "Best average reward: -9999, Current average reward: 0.0\n",
      "Best average reward = 0.0\n",
      "Best model saved at episode 20 to RL_models/LSTM_stored_models/all_agents/oct_13_3/dv0_dw0_w0_memT1/best_model_postcurriculum\n",
      "Evaluation rewards: [0.0]\n",
      "Episode: 20, Episode Reward: 660.3791325887044\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  23\n",
      "1.5 action:  [0.4585, -0.9841] n_targets:  2 reward:  124.97\n",
      "72.0 action:  [0.3043, -0.994] n_targets:  1 reward:  60.89\n",
      "ALPHA (entropy-related):  tensor([0.6163], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.75126 0.73492 0.71893 0.70328 0.68798 0.67301 0.65837 0.64405 0.63004\n",
      " 0.61633]\n",
      "Episode: 21, Episode Reward: 185.86222330729166\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  24\n",
      "45.1 action:  [-0.1848, -0.9891] n_targets:  2 reward:  135.85\n",
      "66.0 action:  [-0.6117, -0.9807] n_targets:  2 reward:  150.03\n",
      "101.3 action:  [-0.0166, -0.9889] n_targets:  2 reward:  142.3\n",
      "ALPHA (entropy-related):  tensor([0.6029], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.73492 0.71893 0.70328 0.68798 0.67301 0.65837 0.64405 0.63004 0.61633\n",
      " 0.60293]\n",
      "Episode: 22, Episode Reward: 428.1848907470703\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  25\n",
      "3.4 action:  [0.355, -0.9912] n_targets:  1 reward:  77.08\n",
      "40.0 action:  [0.4938, -0.9864] n_targets:  2 reward:  148.2\n",
      "ALPHA (entropy-related):  tensor([0.5898], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.71893 0.70328 0.68798 0.67301 0.65837 0.64405 0.63004 0.61633 0.60293\n",
      " 0.58981]\n",
      "Episode: 23, Episode Reward: 225.27607472737628\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  26\n",
      "5.6 action:  [0.2148, -0.9892] n_targets:  2 reward:  132.32\n",
      "71.5 action:  [-0.0376, -0.9845] n_targets:  3 reward:  261.29\n",
      "96.7 action:  [-0.3906, -0.9873] n_targets:  2 reward:  120.45\n",
      "ALPHA (entropy-related):  tensor([0.5770], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.70328 0.68798 0.67301 0.65837 0.64405 0.63004 0.61633 0.60293 0.58981\n",
      " 0.57698]\n",
      "Episode: 24, Episode Reward: 514.0588150024414\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  27\n",
      "8.7 action:  [0.3385, -0.9809] n_targets:  1 reward:  63.33\n",
      "96.7 action:  [-0.2938, -0.9904] n_targets:  1 reward:  74.6\n",
      "ALPHA (entropy-related):  tensor([0.5644], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.68798 0.67301 0.65837 0.64405 0.63004 0.61633 0.60293 0.58981 0.57698\n",
      " 0.56443]\n",
      "Episode: 25, Episode Reward: 137.92284647623697\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  28\n",
      "37.2 action:  [-0.297, -0.9885] n_targets:  1 reward:  75.8\n",
      "37.4 action:  [0.062, -0.9817] n_targets:  1 reward:  58.44\n",
      "ALPHA (entropy-related):  tensor([0.5521], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.67301 0.65837 0.64405 0.63004 0.61633 0.60293 0.58981 0.57698 0.56443\n",
      " 0.55215]\n",
      "Episode: 26, Episode Reward: 134.248415629069\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  29\n",
      "14.5 action:  [-0.457, -0.9885] n_targets:  1 reward:  69.54\n",
      "26.8 action:  [0.3915, -0.9927] n_targets:  2 reward:  114.4\n",
      "ALPHA (entropy-related):  tensor([0.5401], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.65837 0.64405 0.63004 0.61633 0.60293 0.58981 0.57698 0.56443 0.55215\n",
      " 0.54014]\n",
      "Episode: 27, Episode Reward: 183.94552103678382\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  30\n",
      "10.0 action:  [-0.4958, -0.9829] n_targets:  1 reward:  73.19\n",
      "89.4 action:  [-0.2137, -0.9845] n_targets:  1 reward:  50.46\n",
      "ALPHA (entropy-related):  tensor([0.5284], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.64405 0.63004 0.61633 0.60293 0.58981 0.57698 0.56443 0.55215 0.54014\n",
      " 0.52839]\n",
      "Episode: 28, Episode Reward: 123.65608723958333\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  31\n",
      "27.8 action:  [0.1749, -0.989] n_targets:  2 reward:  149.39\n",
      "74.7 action:  [0.2642, -0.9829] n_targets:  2 reward:  101.68\n",
      "ALPHA (entropy-related):  tensor([0.5169], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.63004 0.61633 0.60293 0.58981 0.57698 0.56443 0.55215 0.54014 0.52839\n",
      " 0.5169 ]\n",
      "Episode: 29, Episode Reward: 251.07544962565103\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  32\n",
      "0.6 action:  [-0.2643, -0.9861] n_targets:  3 reward:  193.27\n",
      "6.7 action:  [-0.5026, -0.9857] n_targets:  4 reward:  239.64\n",
      "22.1 action:  [-0.555, -0.9804] n_targets:  3 reward:  212.78\n",
      "28.7 action:  [0.2326, -0.9853] n_targets:  1 reward:  72.55\n",
      "36.4 action:  [0.5823, -0.9834] n_targets:  1 reward:  65.83\n",
      "65.3 action:  [-0.1326, -0.9951] n_targets:  1 reward:  67.45\n",
      "69.3 action:  [0.1331, -0.981] n_targets:  2 reward:  163.11\n",
      "ALPHA (entropy-related):  tensor([0.5057], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.61633 0.60293 0.58981 0.57698 0.56443 0.55215 0.54014 0.52839 0.5169\n",
      " 0.50566]\n",
      "Episode: 30, Episode Reward: 1014.6316019694009\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  33\n",
      "58.7 action:  [-0.0451, -0.9884] n_targets:  2 reward:  159.51\n",
      "ALPHA (entropy-related):  tensor([0.4947], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.60293 0.58981 0.57698 0.56443 0.55215 0.54014 0.52839 0.5169  0.50566\n",
      " 0.49466]\n",
      "Episode: 31, Episode Reward: 159.5080108642578\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  34\n",
      "48.1 action:  [0.6116, -0.9873] n_targets:  3 reward:  219.37\n",
      "58.3 action:  [-0.3258, -0.9967] n_targets:  1 reward:  72.22\n",
      "ALPHA (entropy-related):  tensor([0.4839], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.58981 0.57698 0.56443 0.55215 0.54014 0.52839 0.5169  0.50566 0.49466\n",
      " 0.4839 ]\n",
      "Episode: 32, Episode Reward: 291.5975570678711\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  35\n",
      "45.0 action:  [0.4973, -0.9928] n_targets:  2 reward:  135.26\n",
      "ALPHA (entropy-related):  tensor([0.4734], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.57698 0.56443 0.55215 0.54014 0.52839 0.5169  0.50566 0.49466 0.4839\n",
      " 0.47338]\n",
      "Episode: 33, Episode Reward: 135.25663248697916\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  36\n",
      "ALPHA (entropy-related):  tensor([0.4631], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.56443 0.55215 0.54014 0.52839 0.5169  0.50566 0.49466 0.4839  0.47338\n",
      " 0.46308]\n",
      "Episode: 34, Episode Reward: 0.0\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  37\n",
      "63.9 action:  [0.4366, -0.9913] n_targets:  2 reward:  148.68\n",
      "70.8 action:  [-0.53, -0.9901] n_targets:  2 reward:  117.49\n",
      "ALPHA (entropy-related):  tensor([0.4530], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.55215 0.54014 0.52839 0.5169  0.50566 0.49466 0.4839  0.47338 0.46308\n",
      " 0.45302]\n",
      "Episode: 35, Episode Reward: 266.17878214518225\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  38\n",
      "24.2 action:  [0.1631, -0.9878] n_targets:  1 reward:  50.05\n",
      "ALPHA (entropy-related):  tensor([0.4432], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.54014 0.52839 0.5169  0.50566 0.49466 0.4839  0.47338 0.46308 0.45302\n",
      " 0.44317]\n",
      "Episode: 36, Episode Reward: 50.05391438802083\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  39\n",
      "1.7 action:  [-0.0501, -0.9877] n_targets:  1 reward:  66.39\n",
      "12.1 action:  [-0.4299, -0.9948] n_targets:  1 reward:  68.78\n",
      "32.2 action:  [-0.241, -0.9877] n_targets:  2 reward:  117.06\n",
      "ALPHA (entropy-related):  tensor([0.4335], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.52839 0.5169  0.50566 0.49466 0.4839  0.47338 0.46308 0.45302 0.44317\n",
      " 0.43353]\n",
      "Episode: 37, Episode Reward: 252.22763315836588\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  40\n",
      "ALPHA (entropy-related):  tensor([0.4241], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.5169  0.50566 0.49466 0.4839  0.47338 0.46308 0.45302 0.44317 0.43353\n",
      " 0.4241 ]\n",
      "Episode: 38, Episode Reward: 0.0\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  41\n",
      "68.2 action:  [-0.4793, -0.9869] n_targets:  2 reward:  137.71\n",
      "ALPHA (entropy-related):  tensor([0.4149], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.50566 0.49466 0.4839  0.47338 0.46308 0.45302 0.44317 0.43353 0.4241\n",
      " 0.41488]\n",
      "Episode: 39, Episode Reward: 137.7105763753255\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  42\n",
      "9.2 action:  [-0.0988, -0.9883] n_targets:  2 reward:  124.04\n",
      "25.4 action:  [-0.0353, -0.9899] n_targets:  3 reward:  205.99\n",
      "53.4 action:  [-0.5564, -0.9914] n_targets:  4 reward:  298.89\n",
      "81.5 action:  [-0.4277, -0.9934] n_targets:  2 reward:  129.22\n",
      "97.5 action:  [-0.2366, -0.981] n_targets:  1 reward:  52.87\n",
      "ALPHA (entropy-related):  tensor([0.4059], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.49466 0.4839  0.47338 0.46308 0.45302 0.44317 0.43353 0.4241  0.41488\n",
      " 0.40586]\n",
      "Last 100 ALPHA: [0.97824 0.95695 0.93613 0.91576 0.89583 0.87634 0.85727 0.83861 0.82037\n",
      " 0.80252 0.78506 0.76797 0.75126 0.73492 0.71893 0.70328 0.68798 0.67301\n",
      " 0.65837 0.64405 0.63004 0.61633 0.60293 0.58981 0.57698 0.56443 0.55215\n",
      " 0.54014 0.52839 0.5169  0.50566 0.49466 0.4839  0.47338 0.46308 0.45302\n",
      " 0.44317 0.43353 0.4241  0.41488 0.40586]\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  43\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  44\n",
      "Best average reward: 0.0, Current average reward: 0.0\n",
      "Evaluation rewards: [0.0, 0.0]\n",
      "Episode: 40, Episode Reward: 811.0029144287109\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  45\n",
      "23.3 action:  [-0.4714, -0.9883] n_targets:  1 reward:  88.39\n",
      "ALPHA (entropy-related):  tensor([0.3970], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.4839  0.47338 0.46308 0.45302 0.44317 0.43353 0.4241  0.41488 0.40586\n",
      " 0.39704]\n",
      "Episode: 41, Episode Reward: 88.38922373453777\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  46\n",
      "47.6 action:  [-0.5115, -0.986] n_targets:  1 reward:  59.92\n",
      "54.9 action:  [-0.5041, -0.9802] n_targets:  3 reward:  168.68\n",
      "ALPHA (entropy-related):  tensor([0.3884], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.47338 0.46308 0.45302 0.44317 0.43353 0.4241  0.41488 0.40586 0.39704\n",
      " 0.3884 ]\n",
      "Episode: 42, Episode Reward: 228.60440572102863\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  47\n",
      "12.4 action:  [-0.2869, -0.9821] n_targets:  1 reward:  73.0\n",
      "13.8 action:  [0.2377, -0.9832] n_targets:  1 reward:  54.11\n",
      "46.1 action:  [-0.5032, -0.994] n_targets:  4 reward:  298.79\n",
      "70.8 action:  [-0.6281, -0.9869] n_targets:  2 reward:  136.66\n",
      "79.6 action:  [0.0423, -0.9838] n_targets:  2 reward:  127.7\n",
      "94.4 action:  [0.4152, -0.9883] n_targets:  1 reward:  64.2\n",
      "ALPHA (entropy-related):  tensor([0.3800], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.46308 0.45302 0.44317 0.43353 0.4241  0.41488 0.40586 0.39704 0.3884\n",
      " 0.37997]\n",
      "Episode: 43, Episode Reward: 754.4695841471353\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  48\n",
      "8.7 action:  [-0.6377, -0.9926] n_targets:  1 reward:  72.8\n",
      "20.9 action:  [-0.4158, -0.9808] n_targets:  1 reward:  73.81\n",
      "26.9 action:  [0.1642, -0.9894] n_targets:  1 reward:  73.25\n",
      "59.6 action:  [-0.3891, -0.9962] n_targets:  2 reward:  139.08\n",
      "ALPHA (entropy-related):  tensor([0.3717], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.45302 0.44317 0.43353 0.4241  0.41488 0.40586 0.39704 0.3884  0.37997\n",
      " 0.37171]\n",
      "Episode: 44, Episode Reward: 358.95187632242835\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  49\n",
      "26.4 action:  [0.0914, -0.9897] n_targets:  2 reward:  126.46\n",
      "47.5 action:  [0.5831, -0.9836] n_targets:  1 reward:  83.39\n",
      "95.9 action:  [0.3442, -0.9851] n_targets:  1 reward:  56.54\n",
      "97.6 action:  [0.4881, -0.9914] n_targets:  3 reward:  234.9\n",
      "ALPHA (entropy-related):  tensor([0.3636], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.44317 0.43353 0.4241  0.41488 0.40586 0.39704 0.3884  0.37997 0.37171\n",
      " 0.36363]\n",
      "Episode: 45, Episode Reward: 501.28599421183264\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  50\n",
      "26.0 action:  [-0.3387, -0.9826] n_targets:  2 reward:  137.61\n",
      "56.7 action:  [-0.2169, -0.9843] n_targets:  1 reward:  72.75\n",
      "81.1 action:  [-0.6308, -0.9857] n_targets:  1 reward:  63.94\n",
      "ALPHA (entropy-related):  tensor([0.3557], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.43353 0.4241  0.41488 0.40586 0.39704 0.3884  0.37997 0.37171 0.36363\n",
      " 0.35573]\n",
      "Episode: 46, Episode Reward: 274.29704538981116\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  51\n",
      "3.2 action:  [-0.0577, -0.9944] n_targets:  2 reward:  151.09\n",
      "10.9 action:  [0.2541, -0.989] n_targets:  1 reward:  83.97\n",
      "20.6 action:  [0.0296, -0.9813] n_targets:  1 reward:  82.74\n",
      "37.0 action:  [-0.0355, -0.9866] n_targets:  1 reward:  57.47\n",
      "67.2 action:  [0.3229, -0.9876] n_targets:  1 reward:  50.32\n",
      "91.8 action:  [0.6192, -0.9989] n_targets:  1 reward:  56.63\n",
      "94.4 action:  [0.3344, -0.9876] n_targets:  1 reward:  59.47\n",
      "ALPHA (entropy-related):  tensor([0.3480], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.4241  0.41488 0.40586 0.39704 0.3884  0.37997 0.37171 0.36363 0.35573\n",
      " 0.34801]\n",
      "Episode: 47, Episode Reward: 541.6931660970051\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  52\n",
      "41.3 action:  [0.3994, -0.9843] n_targets:  1 reward:  51.59\n",
      "98.4 action:  [0.4687, -0.9819] n_targets:  1 reward:  66.11\n",
      "ALPHA (entropy-related):  tensor([0.3405], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.41488 0.40586 0.39704 0.3884  0.37997 0.37171 0.36363 0.35573 0.34801\n",
      " 0.34045]\n",
      "Episode: 48, Episode Reward: 117.7080612182617\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  53\n",
      "23.1 action:  [-0.5878, -0.9902] n_targets:  2 reward:  126.01\n",
      "28.9 action:  [0.4783, -0.9894] n_targets:  1 reward:  63.37\n",
      "33.4 action:  [-0.2514, -0.991] n_targets:  1 reward:  80.3\n",
      "53.6 action:  [0.1479, -0.9878] n_targets:  2 reward:  142.25\n",
      "54.2 action:  [0.618, -0.9844] n_targets:  2 reward:  123.67\n",
      "69.6 action:  [-0.3018, -0.9868] n_targets:  1 reward:  73.74\n",
      "73.5 action:  [0.5332, -0.9905] n_targets:  2 reward:  110.52\n",
      "85.4 action:  [0.3481, -0.9917] n_targets:  1 reward:  72.03\n",
      "86.4 action:  [0.3114, -0.9902] n_targets:  1 reward:  53.9\n",
      "ALPHA (entropy-related):  tensor([0.3331], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.40586 0.39704 0.3884  0.37997 0.37171 0.36363 0.35573 0.34801 0.34045\n",
      " 0.33308]\n",
      "Episode: 49, Episode Reward: 845.789628346761\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  54\n",
      "48.7 action:  [-0.5002, -0.9935] n_targets:  1 reward:  56.36\n",
      "52.8 action:  [-0.5585, -0.9962] n_targets:  1 reward:  54.51\n",
      "90.3 action:  [0.2004, -0.9842] n_targets:  1 reward:  53.67\n",
      "ALPHA (entropy-related):  tensor([0.3259], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.39704 0.3884  0.37997 0.37171 0.36363 0.35573 0.34801 0.34045 0.33308\n",
      " 0.32586]\n",
      "Episode: 50, Episode Reward: 164.53906249999997\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  55\n",
      "7.6 action:  [0.2706, -0.9836] n_targets:  1 reward:  65.36\n",
      "30.3 action:  [0.5799, -0.9907] n_targets:  2 reward:  107.35\n",
      "73.0 action:  [0.1022, -0.9946] n_targets:  1 reward:  70.58\n",
      "85.3 action:  [-0.1487, -0.9849] n_targets:  1 reward:  63.97\n",
      "91.1 action:  [0.0701, -0.98] n_targets:  1 reward:  61.95\n",
      "ALPHA (entropy-related):  tensor([0.3188], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.3884  0.37997 0.37171 0.36363 0.35573 0.34801 0.34045 0.33308 0.32586\n",
      " 0.31878]\n",
      "Episode: 51, Episode Reward: 369.20302073160804\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  56\n",
      "2.2 action:  [-0.0798, -0.9964] n_targets:  2 reward:  152.5\n",
      "11.1 action:  [-0.3738, -0.9836] n_targets:  2 reward:  136.86\n",
      "34.1 action:  [-0.4669, -0.9896] n_targets:  4 reward:  274.35\n",
      "77.4 action:  [-0.5989, -0.9855] n_targets:  2 reward:  121.93\n",
      "ALPHA (entropy-related):  tensor([0.3119], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.37997 0.37171 0.36363 0.35573 0.34801 0.34045 0.33308 0.32586 0.31878\n",
      " 0.31189]\n",
      "Episode: 52, Episode Reward: 685.6416015625\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  57\n",
      "8.0 action:  [-0.1808, -0.9807] n_targets:  1 reward:  54.01\n",
      "15.8 action:  [0.5422, -0.98] n_targets:  2 reward:  124.68\n",
      "44.3 action:  [-0.5019, -0.986] n_targets:  1 reward:  68.04\n",
      "48.5 action:  [0.2528, -0.9868] n_targets:  1 reward:  56.48\n",
      "60.2 action:  [0.5114, -0.9941] n_targets:  3 reward:  188.85\n",
      "73.3 action:  [-0.1436, -0.9946] n_targets:  1 reward:  55.1\n",
      "95.1 action:  [0.2326, -0.9966] n_targets:  1 reward:  86.52\n",
      "100.5 action:  [0.5179, -0.9805] n_targets:  2 reward:  117.21\n",
      "ALPHA (entropy-related):  tensor([0.3051], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.37171 0.36363 0.35573 0.34801 0.34045 0.33308 0.32586 0.31878 0.31189\n",
      " 0.30515]\n",
      "Episode: 53, Episode Reward: 750.8939412434895\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  58\n",
      "4.8 action:  [0.2255, -0.9807] n_targets:  1 reward:  67.0\n",
      "5.2 action:  [-0.6039, -0.9906] n_targets:  1 reward:  52.21\n",
      "21.8 action:  [0.6134, -0.9807] n_targets:  1 reward:  65.72\n",
      "47.9 action:  [0.356, -0.9904] n_targets:  3 reward:  235.62\n",
      "67.0 action:  [-0.4986, -0.9966] n_targets:  3 reward:  231.22\n",
      "72.3 action:  [-0.4558, -0.9865] n_targets:  1 reward:  61.91\n",
      "ALPHA (entropy-related):  tensor([0.2985], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.36363 0.35573 0.34801 0.34045 0.33308 0.32586 0.31878 0.31189 0.30515\n",
      " 0.29854]\n",
      "Episode: 54, Episode Reward: 713.6802825927734\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  59\n",
      "42.3 action:  [-0.3162, -0.9811] n_targets:  1 reward:  62.39\n",
      "43.2 action:  [-0.2165, -0.9863] n_targets:  1 reward:  69.52\n",
      "49.0 action:  [0.2841, -0.9803] n_targets:  1 reward:  69.0\n",
      "53.9 action:  [-0.3573, -0.9821] n_targets:  1 reward:  74.96\n",
      "ALPHA (entropy-related):  tensor([0.2921], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.35573 0.34801 0.34045 0.33308 0.32586 0.31878 0.31189 0.30515 0.29854\n",
      " 0.29209]\n",
      "Episode: 55, Episode Reward: 275.87270609537757\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  60\n",
      "38.9 action:  [-0.5115, -0.987] n_targets:  1 reward:  52.7\n",
      "81.7 action:  [0.538, -0.9801] n_targets:  3 reward:  232.12\n",
      "ALPHA (entropy-related):  tensor([0.2858], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.34801 0.34045 0.33308 0.32586 0.31878 0.31189 0.30515 0.29854 0.29209\n",
      " 0.28578]\n",
      "Episode: 56, Episode Reward: 284.8251444498698\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  61\n",
      "13.3 action:  [0.4395, -0.988] n_targets:  2 reward:  158.75\n",
      "18.0 action:  [0.6173, -0.9828] n_targets:  1 reward:  68.47\n",
      "49.9 action:  [0.4155, -0.9898] n_targets:  3 reward:  194.75\n",
      "68.2 action:  [0.2456, -0.9927] n_targets:  1 reward:  67.85\n",
      "74.1 action:  [0.4809, -0.985] n_targets:  1 reward:  61.47\n",
      "ALPHA (entropy-related):  tensor([0.2796], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.34045 0.33308 0.32586 0.31878 0.31189 0.30515 0.29854 0.29209 0.28578\n",
      " 0.27959]\n",
      "Episode: 57, Episode Reward: 551.2872695922852\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  62\n",
      "5.9 action:  [-0.0653, -0.9844] n_targets:  1 reward:  68.87\n",
      "19.1 action:  [0.4393, -0.9863] n_targets:  1 reward:  57.14\n",
      "27.6 action:  [-0.2302, -0.9857] n_targets:  1 reward:  51.38\n",
      "48.3 action:  [-0.0376, -0.9871] n_targets:  2 reward:  126.14\n",
      "56.1 action:  [-0.491, -0.9984] n_targets:  1 reward:  71.14\n",
      "62.9 action:  [-0.104, -0.9812] n_targets:  2 reward:  175.14\n",
      "78.3 action:  [-0.6214, -0.9883] n_targets:  1 reward:  79.55\n",
      "88.4 action:  [-0.3351, -0.9924] n_targets:  1 reward:  76.12\n",
      "91.1 action:  [-0.6382, -0.9833] n_targets:  1 reward:  65.86\n",
      "92.0 action:  [-0.5132, -0.9952] n_targets:  2 reward:  122.23\n",
      "97.2 action:  [0.0696, -0.9938] n_targets:  3 reward:  180.79\n",
      "ALPHA (entropy-related):  tensor([0.2736], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.33308 0.32586 0.31878 0.31189 0.30515 0.29854 0.29209 0.28578 0.27959\n",
      " 0.27356]\n",
      "Episode: 58, Episode Reward: 1074.3744862874348\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  63\n",
      "4.2 action:  [0.0488, -0.9887] n_targets:  2 reward:  154.12\n",
      "7.3 action:  [0.2738, -0.9941] n_targets:  1 reward:  57.36\n",
      "9.0 action:  [-0.3474, -0.985] n_targets:  2 reward:  153.15\n",
      "33.8 action:  [0.1432, -0.9936] n_targets:  2 reward:  110.2\n",
      "49.6 action:  [-0.2048, -0.9878] n_targets:  1 reward:  90.72\n",
      "51.3 action:  [-0.5748, -0.9838] n_targets:  2 reward:  138.9\n",
      "53.6 action:  [-0.49, -0.9817] n_targets:  2 reward:  149.31\n",
      "60.9 action:  [0.434, -0.9875] n_targets:  1 reward:  78.93\n",
      "67.2 action:  [-0.6218, -0.9835] n_targets:  1 reward:  84.75\n",
      "74.6 action:  [-0.3099, -0.9847] n_targets:  1 reward:  51.37\n",
      "76.0 action:  [-0.5208, -0.9965] n_targets:  5 reward:  323.16\n",
      "77.9 action:  [-0.1633, -0.982] n_targets:  1 reward:  64.14\n",
      "82.9 action:  [0.5122, -0.9861] n_targets:  2 reward:  128.13\n",
      "101.7 action:  [-0.3203, -0.9881] n_targets:  1 reward:  51.91\n",
      "ALPHA (entropy-related):  tensor([0.2677], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.32586 0.31878 0.31189 0.30515 0.29854 0.29209 0.28578 0.27959 0.27356\n",
      " 0.26766]\n",
      "Episode: 59, Episode Reward: 1636.1564044952393\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  64\n",
      "14.9 action:  [0.6241, -0.992] n_targets:  1 reward:  61.0\n",
      "15.6 action:  [0.2348, -0.9806] n_targets:  1 reward:  63.66\n",
      "23.2 action:  [0.1614, -0.9862] n_targets:  1 reward:  57.48\n",
      "25.7 action:  [-0.2501, -0.9803] n_targets:  2 reward:  162.84\n",
      "45.6 action:  [0.1149, -0.9838] n_targets:  1 reward:  51.17\n",
      "64.6 action:  [-0.0389, -0.9855] n_targets:  2 reward:  109.22\n",
      "ALPHA (entropy-related):  tensor([0.2619], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.31878 0.31189 0.30515 0.29854 0.29209 0.28578 0.27959 0.27356 0.26766\n",
      " 0.26192]\n",
      "Last 100 ALPHA: [0.97824 0.95695 0.93613 0.91576 0.89583 0.87634 0.85727 0.83861 0.82037\n",
      " 0.80252 0.78506 0.76797 0.75126 0.73492 0.71893 0.70328 0.68798 0.67301\n",
      " 0.65837 0.64405 0.63004 0.61633 0.60293 0.58981 0.57698 0.56443 0.55215\n",
      " 0.54014 0.52839 0.5169  0.50566 0.49466 0.4839  0.47338 0.46308 0.45302\n",
      " 0.44317 0.43353 0.4241  0.41488 0.40586 0.39704 0.3884  0.37997 0.37171\n",
      " 0.36363 0.35573 0.34801 0.34045 0.33308 0.32586 0.31878 0.31189 0.30515\n",
      " 0.29854 0.29209 0.28578 0.27959 0.27356 0.26766 0.26192]\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  65\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  66\n",
      "Best average reward: 0.0, Current average reward: 0.0\n",
      "Evaluation rewards: [0.0, 0.0, 0.0]\n",
      "Episode: 60, Episode Reward: 505.3677673339843\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  67\n",
      "24.5 action:  [-0.6346, -0.9914] n_targets:  1 reward:  86.05\n",
      "25.3 action:  [-0.16, -0.9822] n_targets:  1 reward:  65.4\n",
      "28.9 action:  [0.3432, -0.9903] n_targets:  2 reward:  152.04\n",
      "32.6 action:  [-0.1666, -0.9838] n_targets:  1 reward:  73.78\n",
      "35.6 action:  [-0.342, -0.9838] n_targets:  1 reward:  54.69\n",
      "44.7 action:  [0.4645, -0.9892] n_targets:  1 reward:  80.19\n",
      "45.6 action:  [-0.3138, -0.9921] n_targets:  1 reward:  74.13\n",
      "57.1 action:  [-0.1433, -0.9881] n_targets:  3 reward:  192.65\n",
      "71.5 action:  [-0.4793, -0.9837] n_targets:  2 reward:  152.5\n",
      "93.8 action:  [-0.329, -0.985] n_targets:  1 reward:  55.23\n",
      "96.3 action:  [0.0727, -0.999] n_targets:  2 reward:  113.57\n",
      "101.5 action:  [-0.0378, -0.9811] n_targets:  2 reward:  129.01\n",
      "ALPHA (entropy-related):  tensor([0.2563], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.31189 0.30515 0.29854 0.29209 0.28578 0.27959 0.27356 0.26766 0.26192\n",
      " 0.25628]\n",
      "Episode: 61, Episode Reward: 1229.2302525838215\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  68\n",
      "6.0 action:  [-0.3641, -0.9939] n_targets:  2 reward:  124.57\n",
      "11.7 action:  [-0.5066, -0.9842] n_targets:  4 reward:  278.52\n",
      "19.1 action:  [-0.432, -0.985] n_targets:  1 reward:  65.92\n",
      "23.5 action:  [0.3561, -0.9902] n_targets:  1 reward:  79.19\n",
      "31.6 action:  [-0.3128, -0.9864] n_targets:  1 reward:  53.46\n",
      "35.1 action:  [0.2596, -0.9946] n_targets:  1 reward:  61.78\n",
      "42.1 action:  [0.2324, -0.9865] n_targets:  2 reward:  116.87\n",
      "44.5 action:  [-0.2729, -0.9803] n_targets:  2 reward:  113.75\n",
      "50.4 action:  [-0.2991, -0.991] n_targets:  1 reward:  59.66\n",
      "59.5 action:  [-0.465, -0.984] n_targets:  3 reward:  202.12\n",
      "67.4 action:  [-0.1858, -0.9963] n_targets:  1 reward:  61.24\n",
      "89.5 action:  [-0.0719, -0.9898] n_targets:  2 reward:  151.88\n",
      "93.5 action:  [0.5723, -0.9912] n_targets:  1 reward:  63.32\n",
      "96.3 action:  [0.2731, -0.9918] n_targets:  1 reward:  98.56\n",
      "ALPHA (entropy-related):  tensor([0.2508], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.30515 0.29854 0.29209 0.28578 0.27959 0.27356 0.26766 0.26192 0.25628\n",
      " 0.25081]\n",
      "Episode: 62, Episode Reward: 1530.846745808919\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  69\n",
      "2.0 action:  [0.1101, -0.9864] n_targets:  1 reward:  82.19\n",
      "24.4 action:  [0.2501, -0.9936] n_targets:  1 reward:  52.53\n",
      "46.3 action:  [0.1956, -0.9816] n_targets:  1 reward:  50.4\n",
      "49.1 action:  [-0.341, -0.9834] n_targets:  2 reward:  155.47\n",
      "64.9 action:  [-0.372, -0.9827] n_targets:  3 reward:  204.97\n",
      "69.4 action:  [0.5772, -0.9978] n_targets:  3 reward:  201.94\n",
      "70.9 action:  [0.1537, -0.9943] n_targets:  2 reward:  161.54\n",
      "87.4 action:  [-0.5592, -0.993] n_targets:  1 reward:  58.16\n",
      "98.7 action:  [0.4656, -0.9826] n_targets:  1 reward:  58.93\n",
      "ALPHA (entropy-related):  tensor([0.2454], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.29854 0.29209 0.28578 0.27959 0.27356 0.26766 0.26192 0.25628 0.25081\n",
      " 0.24544]\n",
      "Episode: 63, Episode Reward: 1026.1328468322752\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  70\n",
      "3.9 action:  [0.2301, -0.9901] n_targets:  2 reward:  124.0\n",
      "6.3 action:  [-0.5907, -0.9833] n_targets:  1 reward:  60.15\n",
      "31.0 action:  [-0.4541, -0.987] n_targets:  3 reward:  224.24\n",
      "35.9 action:  [0.6361, -0.9875] n_targets:  1 reward:  58.44\n",
      "41.5 action:  [-0.341, -0.9923] n_targets:  2 reward:  156.74\n",
      "50.4 action:  [-0.4009, -0.9933] n_targets:  1 reward:  67.49\n",
      "51.7 action:  [0.5804, -0.989] n_targets:  1 reward:  59.42\n",
      "58.5 action:  [0.2468, -0.9945] n_targets:  3 reward:  206.35\n",
      "67.1 action:  [0.394, -0.9801] n_targets:  1 reward:  60.1\n",
      "72.7 action:  [-0.0299, -0.9889] n_targets:  3 reward:  204.95\n",
      "83.1 action:  [0.4812, -0.992] n_targets:  2 reward:  119.3\n",
      "85.8 action:  [0.0615, -0.9935] n_targets:  1 reward:  68.22\n",
      "ALPHA (entropy-related):  tensor([0.2402], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.29209 0.28578 0.27959 0.27356 0.26766 0.26192 0.25628 0.25081 0.24544\n",
      " 0.24022]\n",
      "Episode: 64, Episode Reward: 1409.3995997111\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  71\n",
      "4.4 action:  [0.379, -0.9898] n_targets:  1 reward:  52.5\n",
      "13.8 action:  [0.4061, -0.9917] n_targets:  1 reward:  58.99\n",
      "18.8 action:  [-0.2026, -0.9868] n_targets:  2 reward:  137.51\n",
      "20.5 action:  [-0.3308, -0.9889] n_targets:  4 reward:  227.2\n",
      "30.8 action:  [0.4056, -0.9816] n_targets:  4 reward:  276.67\n",
      "33.3 action:  [0.059, -0.9888] n_targets:  2 reward:  130.75\n",
      "46.2 action:  [-0.3605, -0.9939] n_targets:  2 reward:  122.42\n",
      "55.8 action:  [-0.3283, -0.9857] n_targets:  2 reward:  151.32\n",
      "61.9 action:  [-0.6081, -0.9949] n_targets:  2 reward:  133.45\n",
      "64.2 action:  [0.0816, -0.9974] n_targets:  3 reward:  250.09\n",
      "65.4 action:  [0.4126, -0.9967] n_targets:  2 reward:  138.52\n",
      "67.2 action:  [0.3099, -0.9928] n_targets:  1 reward:  55.15\n",
      "71.8 action:  [-0.373, -0.9937] n_targets:  1 reward:  51.37\n",
      "74.5 action:  [-0.3734, -0.9936] n_targets:  2 reward:  145.61\n",
      "78.8 action:  [0.1731, -0.9983] n_targets:  1 reward:  66.46\n",
      "82.1 action:  [0.2172, -0.9966] n_targets:  2 reward:  127.19\n",
      "85.8 action:  [0.3795, -0.9904] n_targets:  3 reward:  183.93\n",
      "87.4 action:  [0.4913, -0.9872] n_targets:  1 reward:  65.27\n",
      "97.3 action:  [-0.3812, -0.9823] n_targets:  1 reward:  56.32\n",
      "ALPHA (entropy-related):  tensor([0.2352], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.28578 0.27959 0.27356 0.26766 0.26192 0.25628 0.25081 0.24544 0.24022\n",
      " 0.23516]\n",
      "Episode: 65, Episode Reward: 2430.7058334350586\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  72\n",
      "9.3 action:  [-0.003, -0.9875] n_targets:  1 reward:  85.35\n",
      "19.8 action:  [0.1317, -0.9904] n_targets:  1 reward:  82.35\n",
      "21.7 action:  [-0.4464, -0.9808] n_targets:  2 reward:  121.91\n",
      "28.3 action:  [0.0419, -0.9973] n_targets:  1 reward:  55.78\n",
      "63.8 action:  [-0.2601, -0.9904] n_targets:  1 reward:  60.1\n",
      "71.0 action:  [0.3356, -0.9918] n_targets:  1 reward:  56.81\n",
      "77.7 action:  [0.1269, -0.9841] n_targets:  2 reward:  120.74\n",
      "84.8 action:  [0.2941, -0.9939] n_targets:  1 reward:  58.0\n",
      "90.2 action:  [0.2328, -0.9921] n_targets:  2 reward:  146.85\n",
      "92.6 action:  [0.0989, -0.9892] n_targets:  3 reward:  190.89\n",
      "99.2 action:  [0.3682, -0.98] n_targets:  2 reward:  146.36\n",
      "ALPHA (entropy-related):  tensor([0.2303], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.27959 0.27356 0.26766 0.26192 0.25628 0.25081 0.24544 0.24022 0.23516\n",
      " 0.23025]\n",
      "Episode: 66, Episode Reward: 1125.1520156860352\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  73\n",
      "2.4 action:  [-0.2446, -0.9869] n_targets:  1 reward:  75.61\n",
      "8.5 action:  [-0.2096, -0.9828] n_targets:  1 reward:  55.23\n",
      "10.3 action:  [0.2053, -0.9842] n_targets:  2 reward:  129.34\n",
      "14.1 action:  [0.3655, -0.9936] n_targets:  2 reward:  135.45\n",
      "19.1 action:  [0.2998, -0.9904] n_targets:  3 reward:  179.29\n",
      "25.4 action:  [0.4661, -0.9897] n_targets:  1 reward:  60.38\n",
      "31.5 action:  [0.1575, -0.9934] n_targets:  1 reward:  61.41\n",
      "35.0 action:  [-0.0751, -0.9873] n_targets:  1 reward:  64.89\n",
      "51.5 action:  [-0.5153, -0.9972] n_targets:  2 reward:  110.08\n",
      "60.4 action:  [0.4503, -0.9837] n_targets:  3 reward:  220.83\n",
      "67.3 action:  [-0.3553, -0.9851] n_targets:  1 reward:  66.78\n",
      "74.8 action:  [-0.3877, -0.9888] n_targets:  2 reward:  150.01\n",
      "80.8 action:  [-0.2368, -0.9814] n_targets:  2 reward:  134.56\n",
      "97.5 action:  [-0.6098, -0.9888] n_targets:  1 reward:  78.1\n",
      "ALPHA (entropy-related):  tensor([0.2255], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.27356 0.26766 0.26192 0.25628 0.25081 0.24544 0.24022 0.23516 0.23025\n",
      " 0.22546]\n",
      "Episode: 67, Episode Reward: 1521.970883687337\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  74\n",
      "2.8 action:  [0.3166, -0.9928] n_targets:  4 reward:  261.85\n",
      "5.7 action:  [-0.0325, -0.9922] n_targets:  2 reward:  143.41\n",
      "9.9 action:  [0.0699, -0.9815] n_targets:  1 reward:  62.62\n",
      "21.6 action:  [-0.5793, -0.9958] n_targets:  1 reward:  67.24\n",
      "25.9 action:  [-0.4446, -0.9976] n_targets:  1 reward:  74.61\n",
      "31.6 action:  [-0.3503, -0.9929] n_targets:  2 reward:  158.19\n",
      "38.0 action:  [-0.5219, -0.9939] n_targets:  1 reward:  62.83\n",
      "42.5 action:  [0.2223, -0.9853] n_targets:  2 reward:  155.6\n",
      "43.8 action:  [-0.0859, -0.993] n_targets:  1 reward:  73.8\n",
      "55.6 action:  [-0.0314, -0.9934] n_targets:  4 reward:  226.99\n",
      "59.7 action:  [-0.4517, -0.9911] n_targets:  1 reward:  63.94\n",
      "70.6 action:  [-0.3086, -0.9876] n_targets:  1 reward:  50.05\n",
      "73.8 action:  [0.3756, -0.9891] n_targets:  1 reward:  80.34\n",
      "77.6 action:  [-0.275, -0.9868] n_targets:  1 reward:  53.72\n",
      "80.0 action:  [0.3515, -0.9801] n_targets:  1 reward:  58.23\n",
      "84.2 action:  [-0.3529, -0.9801] n_targets:  2 reward:  119.92\n",
      "94.8 action:  [-0.4052, -0.9874] n_targets:  1 reward:  62.9\n",
      "100.0 action:  [0.6079, -0.9904] n_targets:  1 reward:  65.08\n",
      "ALPHA (entropy-related):  tensor([0.2207], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.26766 0.26192 0.25628 0.25081 0.24544 0.24022 0.23516 0.23025 0.22546\n",
      " 0.22071]\n",
      "Episode: 68, Episode Reward: 1841.3161570231118\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  75\n",
      "3.3 action:  [-0.0379, -0.9898] n_targets:  1 reward:  90.91\n",
      "21.8 action:  [0.3427, -0.9968] n_targets:  1 reward:  65.92\n",
      "29.1 action:  [0.1525, -0.998] n_targets:  1 reward:  56.99\n",
      "33.1 action:  [-0.5831, -0.9958] n_targets:  1 reward:  71.85\n",
      "34.4 action:  [0.0431, -0.9911] n_targets:  2 reward:  133.75\n",
      "41.0 action:  [0.612, -0.9882] n_targets:  2 reward:  148.82\n",
      "53.4 action:  [0.584, -0.9863] n_targets:  2 reward:  132.22\n",
      "61.2 action:  [-0.5866, -0.9909] n_targets:  2 reward:  117.23\n",
      "63.6 action:  [0.1766, -0.9919] n_targets:  2 reward:  134.64\n",
      "66.2 action:  [-0.5464, -0.9865] n_targets:  1 reward:  73.04\n",
      "72.6 action:  [-0.4795, -0.9898] n_targets:  1 reward:  58.24\n",
      "83.2 action:  [0.4706, -0.9846] n_targets:  2 reward:  128.67\n",
      "87.2 action:  [-0.5458, -0.9923] n_targets:  3 reward:  203.03\n",
      "89.1 action:  [0.1139, -0.9961] n_targets:  1 reward:  72.68\n",
      "ALPHA (entropy-related):  tensor([0.2161], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.26192 0.25628 0.25081 0.24544 0.24022 0.23516 0.23025 0.22546 0.22071\n",
      " 0.21614]\n",
      "Episode: 69, Episode Reward: 1488.0070069630938\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  76\n",
      "4.0 action:  [-0.1334, -0.9817] n_targets:  1 reward:  62.13\n",
      "4.8 action:  [-0.3325, -0.9845] n_targets:  1 reward:  50.23\n",
      "7.3 action:  [0.0604, -0.9975] n_targets:  1 reward:  77.92\n",
      "8.6 action:  [0.2322, -0.9892] n_targets:  2 reward:  119.81\n",
      "11.4 action:  [-0.0534, -0.9813] n_targets:  2 reward:  140.05\n",
      "13.4 action:  [-0.316, -0.9958] n_targets:  1 reward:  81.97\n",
      "16.2 action:  [-0.1663, -0.9949] n_targets:  2 reward:  119.43\n",
      "26.3 action:  [0.4524, -0.9841] n_targets:  1 reward:  64.47\n",
      "30.1 action:  [0.1556, -0.9919] n_targets:  1 reward:  57.25\n",
      "32.6 action:  [-0.2343, -0.9818] n_targets:  3 reward:  223.47\n",
      "34.9 action:  [-0.1635, -0.9907] n_targets:  2 reward:  148.65\n",
      "37.6 action:  [-0.3441, -0.9961] n_targets:  1 reward:  53.21\n",
      "41.5 action:  [-0.3684, -0.9978] n_targets:  1 reward:  83.27\n",
      "43.8 action:  [0.3608, -0.9842] n_targets:  1 reward:  56.38\n",
      "47.8 action:  [0.2935, -0.981] n_targets:  3 reward:  199.86\n",
      "49.2 action:  [-0.1312, -0.9867] n_targets:  1 reward:  69.73\n",
      "49.9 action:  [0.4837, -0.9956] n_targets:  1 reward:  55.25\n",
      "51.9 action:  [-0.3871, -0.9982] n_targets:  1 reward:  61.86\n",
      "52.6 action:  [0.2427, -0.9912] n_targets:  1 reward:  62.21\n",
      "54.0 action:  [-0.5773, -0.9896] n_targets:  1 reward:  52.81\n",
      "57.3 action:  [0.1157, -0.9868] n_targets:  1 reward:  53.94\n",
      "60.6 action:  [-0.0067, -0.9913] n_targets:  2 reward:  120.41\n",
      "62.4 action:  [0.4867, -0.9849] n_targets:  1 reward:  69.61\n",
      "65.8 action:  [-0.1597, -0.9815] n_targets:  1 reward:  65.17\n",
      "66.5 action:  [-0.2653, -0.9952] n_targets:  1 reward:  53.92\n",
      "72.9 action:  [-0.0397, -0.9902] n_targets:  1 reward:  61.83\n",
      "76.6 action:  [0.5155, -0.9859] n_targets:  1 reward:  58.42\n",
      "77.8 action:  [-0.4219, -0.9949] n_targets:  2 reward:  125.32\n",
      "80.5 action:  [-0.405, -0.9884] n_targets:  2 reward:  102.25\n",
      "86.4 action:  [0.36, -0.9853] n_targets:  1 reward:  54.81\n",
      "89.6 action:  [0.2081, -0.994] n_targets:  2 reward:  136.54\n",
      "92.0 action:  [-0.2284, -0.9967] n_targets:  2 reward:  150.93\n",
      "93.2 action:  [-0.384, -0.9974] n_targets:  1 reward:  55.41\n",
      "97.8 action:  [0.5881, -0.9949] n_targets:  1 reward:  60.56\n",
      "100.8 action:  [0.4238, -0.9946] n_targets:  1 reward:  52.51\n",
      "ALPHA (entropy-related):  tensor([0.2118], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.25628 0.25081 0.24544 0.24022 0.23516 0.23025 0.22546 0.22071 0.21614\n",
      " 0.21183]\n",
      "Episode: 70, Episode Reward: 3061.5764414469395\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  77\n",
      "2.6 action:  [0.3836, -0.9841] n_targets:  3 reward:  233.88\n",
      "3.9 action:  [-0.1265, -0.9994] n_targets:  1 reward:  51.44\n",
      "6.0 action:  [0.0489, -0.991] n_targets:  2 reward:  139.34\n",
      "6.9 action:  [-0.0964, -0.9912] n_targets:  1 reward:  58.85\n",
      "12.1 action:  [0.2451, -0.9914] n_targets:  2 reward:  113.54\n",
      "14.7 action:  [0.1754, -0.988] n_targets:  2 reward:  147.98\n",
      "21.1 action:  [-0.3544, -0.986] n_targets:  1 reward:  72.91\n",
      "28.1 action:  [0.1297, -0.9802] n_targets:  2 reward:  130.24\n",
      "34.2 action:  [0.4947, -0.9868] n_targets:  1 reward:  55.38\n",
      "44.4 action:  [-0.5906, -0.9837] n_targets:  1 reward:  68.72\n",
      "53.1 action:  [-0.6175, -0.9895] n_targets:  4 reward:  269.19\n",
      "56.5 action:  [0.2896, -0.993] n_targets:  2 reward:  123.37\n",
      "71.3 action:  [-0.135, -0.9855] n_targets:  2 reward:  125.94\n",
      "73.1 action:  [0.2699, -0.9906] n_targets:  1 reward:  55.39\n",
      "83.8 action:  [-0.4605, -0.9926] n_targets:  1 reward:  79.75\n",
      "87.4 action:  [0.5329, -0.992] n_targets:  1 reward:  59.14\n",
      "99.2 action:  [-0.5942, -0.9833] n_targets:  1 reward:  68.18\n",
      "101.6 action:  [-0.2166, -0.9869] n_targets:  1 reward:  75.18\n",
      "ALPHA (entropy-related):  tensor([0.2075], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.25081 0.24544 0.24022 0.23516 0.23025 0.22546 0.22071 0.21614 0.21183\n",
      " 0.20748]\n",
      "Episode: 71, Episode Reward: 1928.4088185628254\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  78\n",
      "2.3 action:  [-0.6104, -0.9823] n_targets:  1 reward:  56.71\n",
      "7.8 action:  [0.0857, -0.9954] n_targets:  2 reward:  127.0\n",
      "11.3 action:  [-0.3929, -0.9921] n_targets:  1 reward:  52.56\n",
      "15.6 action:  [0.5742, -0.9972] n_targets:  1 reward:  63.02\n",
      "18.0 action:  [0.0689, -0.987] n_targets:  1 reward:  50.66\n",
      "23.0 action:  [-0.2993, -0.9925] n_targets:  2 reward:  142.28\n",
      "26.2 action:  [0.555, -0.9951] n_targets:  1 reward:  62.41\n",
      "35.4 action:  [-0.367, -0.9961] n_targets:  1 reward:  51.1\n",
      "40.0 action:  [-0.1015, -0.9883] n_targets:  1 reward:  67.87\n",
      "44.6 action:  [-0.2124, -0.9843] n_targets:  1 reward:  51.91\n",
      "48.0 action:  [0.2067, -0.9857] n_targets:  1 reward:  51.58\n",
      "54.0 action:  [0.1317, -0.9848] n_targets:  1 reward:  53.87\n",
      "55.0 action:  [0.4773, -0.9947] n_targets:  2 reward:  107.1\n",
      "60.0 action:  [0.316, -0.9973] n_targets:  1 reward:  52.97\n",
      "62.5 action:  [-0.2883, -0.995] n_targets:  1 reward:  66.08\n",
      "62.8 action:  [-0.2288, -0.9966] n_targets:  1 reward:  50.51\n",
      "69.4 action:  [-0.1997, -0.99] n_targets:  1 reward:  53.87\n",
      "71.9 action:  [-0.157, -0.9944] n_targets:  2 reward:  134.86\n",
      "92.3 action:  [0.3424, -0.9825] n_targets:  3 reward:  192.19\n",
      "92.7 action:  [-0.5537, -0.9936] n_targets:  1 reward:  50.42\n",
      "ALPHA (entropy-related):  tensor([0.2033], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.24544 0.24022 0.23516 0.23025 0.22546 0.22071 0.21614 0.21183 0.20748\n",
      " 0.20331]\n",
      "Episode: 72, Episode Reward: 1538.9746093749998\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  79\n",
      "2.0 action:  [-0.4311, -0.9847] n_targets:  1 reward:  61.02\n",
      "7.0 action:  [-0.0617, -0.98] n_targets:  2 reward:  123.31\n",
      "9.6 action:  [-0.2003, -0.9856] n_targets:  1 reward:  70.48\n",
      "10.5 action:  [0.3712, -0.9984] n_targets:  1 reward:  57.23\n",
      "18.6 action:  [-0.3451, -0.9945] n_targets:  1 reward:  63.36\n",
      "26.8 action:  [-0.0453, -0.9935] n_targets:  1 reward:  56.13\n",
      "27.7 action:  [0.5433, -0.9963] n_targets:  1 reward:  56.16\n",
      "50.2 action:  [0.1324, -0.9959] n_targets:  3 reward:  190.82\n",
      "57.7 action:  [-0.5658, -0.9878] n_targets:  2 reward:  179.83\n",
      "61.2 action:  [-0.0145, -0.9805] n_targets:  2 reward:  144.86\n",
      "77.1 action:  [-0.2715, -0.9966] n_targets:  1 reward:  50.75\n",
      "78.1 action:  [-0.1894, -0.9929] n_targets:  2 reward:  133.89\n",
      "78.9 action:  [-0.1743, -0.9937] n_targets:  1 reward:  77.4\n",
      "80.3 action:  [0.437, -0.9869] n_targets:  2 reward:  108.48\n",
      "83.1 action:  [0.6272, -0.9953] n_targets:  3 reward:  194.36\n",
      "91.5 action:  [0.1829, -0.9942] n_targets:  1 reward:  51.72\n",
      "ALPHA (entropy-related):  tensor([0.1992], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.24022 0.23516 0.23025 0.22546 0.22071 0.21614 0.21183 0.20748 0.20331\n",
      " 0.19923]\n",
      "Episode: 73, Episode Reward: 1619.7905337015786\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  80\n",
      "1.1 action:  [0.3418, -0.9832] n_targets:  3 reward:  187.96\n",
      "16.1 action:  [-0.1017, -0.9806] n_targets:  2 reward:  104.07\n",
      "32.2 action:  [-0.1095, -0.9893] n_targets:  1 reward:  50.84\n",
      "38.2 action:  [0.3246, -0.9947] n_targets:  1 reward:  51.52\n",
      "38.9 action:  [0.5163, -0.9951] n_targets:  1 reward:  51.59\n",
      "41.0 action:  [0.2845, -0.9982] n_targets:  1 reward:  60.26\n",
      "43.5 action:  [-0.2658, -0.9952] n_targets:  2 reward:  121.78\n",
      "44.8 action:  [0.1456, -0.9903] n_targets:  1 reward:  51.19\n",
      "46.1 action:  [0.0105, -0.9844] n_targets:  1 reward:  64.85\n",
      "48.8 action:  [-0.5488, -0.9923] n_targets:  1 reward:  54.58\n",
      "54.8 action:  [-0.4687, -0.993] n_targets:  1 reward:  78.77\n",
      "66.1 action:  [0.3023, -0.989] n_targets:  1 reward:  81.31\n",
      "69.4 action:  [-0.0181, -0.9853] n_targets:  1 reward:  52.82\n",
      "71.1 action:  [0.0433, -0.9944] n_targets:  2 reward:  114.38\n",
      "77.2 action:  [-0.3871, -0.9888] n_targets:  2 reward:  134.01\n",
      "78.9 action:  [-0.1851, -0.9943] n_targets:  2 reward:  134.23\n",
      "80.2 action:  [0.2848, -0.9937] n_targets:  1 reward:  54.36\n",
      "82.8 action:  [-0.3699, -0.9826] n_targets:  1 reward:  66.35\n",
      "84.6 action:  [0.5908, -0.9951] n_targets:  1 reward:  67.33\n",
      "88.3 action:  [-0.1377, -0.994] n_targets:  1 reward:  59.59\n",
      "90.6 action:  [0.1811, -0.9838] n_targets:  1 reward:  57.37\n",
      "93.7 action:  [0.0074, -0.9978] n_targets:  1 reward:  84.98\n",
      "99.4 action:  [-0.0703, -0.9853] n_targets:  3 reward:  222.47\n",
      "100.7 action:  [0.6202, -0.9982] n_targets:  1 reward:  59.51\n",
      "102.3 action:  [-0.2395, -0.9994] n_targets:  1 reward:  57.57\n",
      "ALPHA (entropy-related):  tensor([0.1954], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.23516 0.23025 0.22546 0.22071 0.21614 0.21183 0.20748 0.20331 0.19923\n",
      " 0.19545]\n",
      "Episode: 74, Episode Reward: 2123.70903523763\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  81\n",
      "0.2 action:  [0.2708, -0.9985] n_targets:  1 reward:  64.55\n",
      "1.2 action:  [0.2011, -0.9967] n_targets:  1 reward:  51.32\n",
      "6.9 action:  [-0.1566, -0.9988] n_targets:  3 reward:  175.9\n",
      "10.3 action:  [-0.4706, -0.9841] n_targets:  1 reward:  60.16\n",
      "12.1 action:  [-0.1301, -0.9986] n_targets:  2 reward:  142.47\n",
      "13.8 action:  [-0.3465, -0.9873] n_targets:  1 reward:  59.96\n",
      "28.8 action:  [0.3587, -0.9869] n_targets:  2 reward:  149.23\n",
      "32.4 action:  [0.6232, -0.9823] n_targets:  1 reward:  57.57\n",
      "44.3 action:  [0.54, -0.9886] n_targets:  2 reward:  112.01\n",
      "48.4 action:  [0.388, -0.9911] n_targets:  2 reward:  174.73\n",
      "57.8 action:  [0.0594, -0.9803] n_targets:  1 reward:  60.56\n",
      "59.7 action:  [0.3036, -0.9883] n_targets:  1 reward:  50.52\n",
      "61.0 action:  [0.2077, -0.9963] n_targets:  1 reward:  60.14\n",
      "61.2 action:  [0.6221, -0.9928] n_targets:  1 reward:  54.48\n",
      "69.3 action:  [0.4117, -0.987] n_targets:  1 reward:  50.0\n",
      "82.9 action:  [-0.6167, -0.998] n_targets:  1 reward:  65.11\n",
      "83.3 action:  [-0.4816, -0.9895] n_targets:  1 reward:  52.81\n",
      "92.1 action:  [0.2886, -0.9981] n_targets:  1 reward:  89.12\n",
      "95.3 action:  [0.2304, -0.9927] n_targets:  2 reward:  101.45\n",
      "ALPHA (entropy-related):  tensor([0.1918], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.23025 0.22546 0.22071 0.21614 0.21183 0.20748 0.20331 0.19923 0.19545\n",
      " 0.19182]\n",
      "Episode: 75, Episode Reward: 1632.091702779134\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  82\n",
      "11.9 action:  [-0.0192, -0.9845] n_targets:  1 reward:  76.86\n",
      "16.9 action:  [-0.1248, -0.9845] n_targets:  1 reward:  53.13\n",
      "20.4 action:  [0.2939, -0.993] n_targets:  1 reward:  57.4\n",
      "25.1 action:  [0.1833, -0.9878] n_targets:  1 reward:  89.66\n",
      "26.5 action:  [-0.0177, -0.9851] n_targets:  1 reward:  56.74\n",
      "28.0 action:  [-0.5552, -0.982] n_targets:  1 reward:  59.49\n",
      "33.4 action:  [-0.5359, -0.9977] n_targets:  1 reward:  60.93\n",
      "37.7 action:  [0.4402, -0.9812] n_targets:  1 reward:  55.37\n",
      "39.3 action:  [0.1143, -0.9811] n_targets:  1 reward:  62.09\n",
      "44.8 action:  [0.151, -0.9889] n_targets:  1 reward:  50.55\n",
      "46.9 action:  [0.6231, -0.9971] n_targets:  1 reward:  54.66\n",
      "51.6 action:  [0.3871, -0.9832] n_targets:  2 reward:  119.44\n",
      "61.7 action:  [0.0008, -0.9808] n_targets:  1 reward:  50.7\n",
      "62.2 action:  [0.353, -0.9869] n_targets:  1 reward:  57.74\n",
      "64.2 action:  [0.4733, -0.9842] n_targets:  1 reward:  81.75\n",
      "68.5 action:  [0.2657, -0.999] n_targets:  3 reward:  167.69\n",
      "71.0 action:  [-0.4966, -0.9925] n_targets:  1 reward:  78.18\n",
      "71.7 action:  [-0.0793, -0.986] n_targets:  1 reward:  53.13\n",
      "73.4 action:  [0.5825, -0.996] n_targets:  1 reward:  51.11\n",
      "74.8 action:  [0.5763, -0.9902] n_targets:  1 reward:  63.69\n",
      "85.0 action:  [0.0578, -0.9873] n_targets:  1 reward:  74.81\n",
      "89.7 action:  [0.6313, -0.9826] n_targets:  1 reward:  54.03\n",
      "94.5 action:  [0.3757, -0.9827] n_targets:  1 reward:  57.21\n",
      "97.7 action:  [0.3658, -0.984] n_targets:  3 reward:  197.12\n",
      "ALPHA (entropy-related):  tensor([0.1884], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.22546 0.22071 0.21614 0.21183 0.20748 0.20331 0.19923 0.19545 0.19182\n",
      " 0.1884 ]\n",
      "Episode: 76, Episode Reward: 1783.4895509084065\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  83\n",
      "4.5 action:  [0.0519, -0.9979] n_targets:  1 reward:  59.6\n",
      "4.9 action:  [0.3738, -0.9802] n_targets:  1 reward:  55.31\n",
      "8.3 action:  [0.1902, -0.9814] n_targets:  1 reward:  53.14\n",
      "8.9 action:  [0.4289, -0.983] n_targets:  1 reward:  55.64\n",
      "13.2 action:  [-0.0699, -0.9941] n_targets:  1 reward:  53.77\n",
      "21.2 action:  [0.2106, -0.9936] n_targets:  1 reward:  78.04\n",
      "28.3 action:  [0.2771, -0.9828] n_targets:  3 reward:  216.67\n",
      "29.6 action:  [0.038, -0.9915] n_targets:  1 reward:  71.44\n",
      "33.0 action:  [-0.3104, -0.9924] n_targets:  1 reward:  68.77\n",
      "39.2 action:  [-0.3216, -0.9855] n_targets:  2 reward:  102.86\n",
      "48.9 action:  [-0.0126, -0.9995] n_targets:  1 reward:  56.72\n",
      "50.2 action:  [0.2946, -0.9823] n_targets:  1 reward:  78.29\n",
      "51.7 action:  [0.0334, -0.9918] n_targets:  1 reward:  67.78\n",
      "55.5 action:  [0.2909, -0.9805] n_targets:  1 reward:  65.09\n",
      "58.1 action:  [-0.2054, -0.9953] n_targets:  3 reward:  183.28\n",
      "59.5 action:  [0.0765, -0.994] n_targets:  1 reward:  52.12\n",
      "63.2 action:  [0.6365, -0.9978] n_targets:  1 reward:  66.18\n",
      "68.5 action:  [-0.5451, -0.9854] n_targets:  2 reward:  116.15\n",
      "68.8 action:  [0.4379, -0.9951] n_targets:  1 reward:  64.23\n",
      "71.9 action:  [-0.6156, -0.9805] n_targets:  1 reward:  52.22\n",
      "75.2 action:  [0.6233, -0.9949] n_targets:  1 reward:  72.9\n",
      "76.8 action:  [-0.4518, -0.9894] n_targets:  1 reward:  94.31\n",
      "78.5 action:  [0.2048, -0.9953] n_targets:  3 reward:  199.05\n",
      "80.3 action:  [-0.3365, -0.9823] n_targets:  1 reward:  73.43\n",
      "81.3 action:  [-0.4084, -0.987] n_targets:  1 reward:  50.27\n",
      "82.8 action:  [0.3263, -0.9841] n_targets:  1 reward:  56.76\n",
      "87.9 action:  [0.3678, -0.9928] n_targets:  1 reward:  96.99\n",
      "89.0 action:  [0.6023, -0.9833] n_targets:  1 reward:  78.59\n",
      "91.8 action:  [0.5409, -0.996] n_targets:  1 reward:  61.7\n",
      "97.0 action:  [-0.1662, -0.9838] n_targets:  1 reward:  69.6\n",
      "97.3 action:  [0.621, -0.9811] n_targets:  1 reward:  55.32\n",
      "ALPHA (entropy-related):  tensor([0.1848], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.22071 0.21614 0.21183 0.20748 0.20331 0.19923 0.19545 0.19182 0.1884\n",
      " 0.18483]\n",
      "Episode: 77, Episode Reward: 2526.1969248453775\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  84\n",
      "1.7 action:  [-0.0567, -0.9964] n_targets:  1 reward:  50.67\n",
      "4.3 action:  [-0.1315, -0.9862] n_targets:  4 reward:  265.36\n",
      "4.4 action:  [-0.4699, -0.9826] n_targets:  1 reward:  72.44\n",
      "7.2 action:  [0.5902, -0.9985] n_targets:  1 reward:  52.09\n",
      "10.2 action:  [-0.0417, -0.998] n_targets:  5 reward:  323.88\n",
      "16.0 action:  [-0.049, -0.9864] n_targets:  1 reward:  53.68\n",
      "16.6 action:  [-0.4793, -0.9961] n_targets:  1 reward:  50.16\n",
      "20.3 action:  [0.1636, -0.9813] n_targets:  1 reward:  52.55\n",
      "32.5 action:  [-0.5887, -0.9845] n_targets:  1 reward:  60.15\n",
      "42.6 action:  [-0.1581, -0.9829] n_targets:  1 reward:  53.03\n",
      "44.4 action:  [-0.1651, -0.9878] n_targets:  2 reward:  115.43\n",
      "50.0 action:  [-0.0012, -0.9859] n_targets:  1 reward:  50.49\n",
      "69.7 action:  [-0.5093, -0.9876] n_targets:  1 reward:  51.79\n",
      "90.9 action:  [0.2769, -0.9896] n_targets:  2 reward:  173.15\n",
      "99.2 action:  [-0.1836, -0.9964] n_targets:  1 reward:  51.69\n",
      "ALPHA (entropy-related):  tensor([0.1814], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.21614 0.21183 0.20748 0.20331 0.19923 0.19545 0.19182 0.1884  0.18483\n",
      " 0.18145]\n",
      "Episode: 78, Episode Reward: 1476.575635274251\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  85\n",
      "5.3 action:  [0.5224, -0.9966] n_targets:  4 reward:  267.3\n",
      "14.1 action:  [0.1718, -0.9893] n_targets:  1 reward:  59.44\n",
      "22.2 action:  [0.5771, -0.9846] n_targets:  1 reward:  84.32\n",
      "25.9 action:  [0.2954, -0.9898] n_targets:  1 reward:  94.27\n",
      "28.1 action:  [-0.1806, -0.9935] n_targets:  1 reward:  54.43\n",
      "35.3 action:  [0.0964, -0.9822] n_targets:  1 reward:  59.08\n",
      "45.9 action:  [-0.3736, -0.986] n_targets:  1 reward:  65.99\n",
      "47.5 action:  [-0.5517, -0.9824] n_targets:  1 reward:  58.75\n",
      "49.3 action:  [-0.5228, -0.9991] n_targets:  2 reward:  142.26\n",
      "52.7 action:  [-0.1649, -0.9828] n_targets:  1 reward:  56.94\n",
      "53.9 action:  [0.0328, -0.9821] n_targets:  1 reward:  63.89\n",
      "60.7 action:  [-0.0458, -0.9834] n_targets:  1 reward:  79.88\n",
      "63.8 action:  [0.4847, -0.9978] n_targets:  2 reward:  152.19\n",
      "65.8 action:  [0.5072, -0.9981] n_targets:  1 reward:  75.47\n",
      "67.7 action:  [0.5723, -0.9996] n_targets:  1 reward:  57.07\n",
      "74.3 action:  [0.108, -0.9907] n_targets:  3 reward:  213.35\n",
      "76.7 action:  [0.4102, -0.9918] n_targets:  2 reward:  132.49\n",
      "80.2 action:  [0.2548, -0.9831] n_targets:  1 reward:  62.46\n",
      "86.2 action:  [0.1002, -0.9947] n_targets:  2 reward:  192.81\n",
      "93.1 action:  [0.1545, -0.9949] n_targets:  1 reward:  52.43\n",
      "94.5 action:  [0.0184, -0.9895] n_targets:  1 reward:  52.1\n",
      "ALPHA (entropy-related):  tensor([0.1782], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.21183 0.20748 0.20331 0.19923 0.19545 0.19182 0.1884  0.18483 0.18145\n",
      " 0.17817]\n",
      "Episode: 79, Episode Reward: 2076.9137287139893\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  86\n",
      "10.2 action:  [-0.2808, -0.9855] n_targets:  1 reward:  65.62\n",
      "14.7 action:  [-0.4203, -0.9907] n_targets:  1 reward:  60.59\n",
      "16.8 action:  [0.3479, -0.9978] n_targets:  1 reward:  76.26\n",
      "28.3 action:  [-0.2283, -0.9919] n_targets:  1 reward:  50.15\n",
      "49.1 action:  [-0.3646, -0.9831] n_targets:  3 reward:  181.61\n",
      "53.5 action:  [-0.5676, -0.9948] n_targets:  1 reward:  52.7\n",
      "65.8 action:  [-0.4581, -0.9828] n_targets:  1 reward:  62.97\n",
      "68.5 action:  [0.1632, -0.9894] n_targets:  1 reward:  52.07\n",
      "76.4 action:  [-0.3612, -0.9869] n_targets:  1 reward:  68.7\n",
      "78.0 action:  [0.5497, -0.9838] n_targets:  2 reward:  111.56\n",
      "90.7 action:  [0.4705, -0.9846] n_targets:  1 reward:  53.89\n",
      "92.5 action:  [0.1639, -0.9893] n_targets:  1 reward:  56.82\n",
      "94.6 action:  [-0.3319, -0.9955] n_targets:  2 reward:  117.4\n",
      "98.6 action:  [-0.3798, -0.9829] n_targets:  2 reward:  130.31\n",
      "ALPHA (entropy-related):  tensor([0.1748], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.20748 0.20331 0.19923 0.19545 0.19182 0.1884  0.18483 0.18145 0.17817\n",
      " 0.17481]\n",
      "Last 100 ALPHA: [0.97824 0.95695 0.93613 0.91576 0.89583 0.87634 0.85727 0.83861 0.82037\n",
      " 0.80252 0.78506 0.76797 0.75126 0.73492 0.71893 0.70328 0.68798 0.67301\n",
      " 0.65837 0.64405 0.63004 0.61633 0.60293 0.58981 0.57698 0.56443 0.55215\n",
      " 0.54014 0.52839 0.5169  0.50566 0.49466 0.4839  0.47338 0.46308 0.45302\n",
      " 0.44317 0.43353 0.4241  0.41488 0.40586 0.39704 0.3884  0.37997 0.37171\n",
      " 0.36363 0.35573 0.34801 0.34045 0.33308 0.32586 0.31878 0.31189 0.30515\n",
      " 0.29854 0.29209 0.28578 0.27959 0.27356 0.26766 0.26192 0.25628 0.25081\n",
      " 0.24544 0.24022 0.23516 0.23025 0.22546 0.22071 0.21614 0.21183 0.20748\n",
      " 0.20331 0.19923 0.19545 0.19182 0.1884  0.18483 0.18145 0.17817 0.17481]\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  87\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  88\n",
      "Best average reward: 0.0, Current average reward: 0.0\n",
      "Evaluation rewards: [0.0, 0.0, 0.0, 0.0]\n",
      "Episode: 80, Episode Reward: 1140.6406784057617\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  89\n",
      "9.3 action:  [-0.2786, -0.9932] n_targets:  1 reward:  60.72\n",
      "11.4 action:  [-0.2933, -0.9807] n_targets:  3 reward:  210.82\n",
      "14.4 action:  [0.5618, -0.9812] n_targets:  1 reward:  83.84\n",
      "16.9 action:  [0.4077, -0.9986] n_targets:  1 reward:  82.2\n",
      "19.0 action:  [-0.2105, -0.9924] n_targets:  1 reward:  58.55\n",
      "20.9 action:  [-0.4165, -0.9979] n_targets:  2 reward:  135.78\n",
      "25.0 action:  [0.1707, -0.9802] n_targets:  1 reward:  60.94\n",
      "25.5 action:  [0.3959, -0.9921] n_targets:  1 reward:  51.44\n",
      "47.6 action:  [0.0591, -0.996] n_targets:  1 reward:  71.77\n",
      "50.9 action:  [-0.1687, -0.9888] n_targets:  1 reward:  53.73\n",
      "53.5 action:  [0.1771, -0.9921] n_targets:  1 reward:  54.88\n",
      "61.9 action:  [-0.0225, -0.9847] n_targets:  1 reward:  53.17\n",
      "69.9 action:  [0.3008, -0.9873] n_targets:  1 reward:  62.04\n",
      "74.3 action:  [-0.2042, -0.9916] n_targets:  1 reward:  65.87\n",
      "75.8 action:  [0.0443, -0.9842] n_targets:  1 reward:  50.69\n",
      "84.8 action:  [-0.4382, -0.9895] n_targets:  1 reward:  56.53\n",
      "99.2 action:  [-0.3992, -0.9966] n_targets:  1 reward:  55.64\n",
      "ALPHA (entropy-related):  tensor([0.1716], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.20331 0.19923 0.19545 0.19182 0.1884  0.18483 0.18145 0.17817 0.17481\n",
      " 0.17161]\n",
      "Episode: 81, Episode Reward: 1268.6010754903155\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  90\n",
      "3.7 action:  [-0.6058, -0.9908] n_targets:  1 reward:  50.04\n",
      "10.1 action:  [0.0836, -0.9884] n_targets:  1 reward:  54.76\n",
      "21.6 action:  [0.2988, -0.9902] n_targets:  1 reward:  75.97\n",
      "22.4 action:  [-0.2847, -0.995] n_targets:  1 reward:  52.8\n",
      "25.7 action:  [0.0254, -0.9935] n_targets:  1 reward:  68.58\n",
      "29.8 action:  [0.3604, -0.9899] n_targets:  1 reward:  50.62\n",
      "35.1 action:  [-0.0825, -0.9976] n_targets:  1 reward:  51.67\n",
      "40.9 action:  [0.5679, -0.9847] n_targets:  1 reward:  69.87\n",
      "52.5 action:  [0.3387, -0.9886] n_targets:  1 reward:  51.35\n",
      "57.5 action:  [0.0237, -0.9965] n_targets:  1 reward:  50.12\n",
      "61.1 action:  [-0.0203, -0.9856] n_targets:  1 reward:  52.03\n",
      "67.0 action:  [0.1988, -0.9936] n_targets:  1 reward:  54.9\n",
      "ALPHA (entropy-related):  tensor([0.1688], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.19923 0.19545 0.19182 0.1884  0.18483 0.18145 0.17817 0.17481 0.17161\n",
      " 0.16875]\n",
      "Episode: 82, Episode Reward: 682.712781270345\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  91\n",
      "0.4 action:  [-0.2386, -0.9919] n_targets:  1 reward:  84.19\n",
      "23.0 action:  [-0.2464, -0.9839] n_targets:  1 reward:  50.07\n",
      "77.1 action:  [0.1172, -0.9823] n_targets:  1 reward:  57.8\n",
      "82.6 action:  [-0.3912, -0.9934] n_targets:  1 reward:  52.52\n",
      "84.1 action:  [-0.0013, -0.9921] n_targets:  1 reward:  62.28\n",
      "88.0 action:  [0.5448, -0.991] n_targets:  1 reward:  57.41\n",
      "91.4 action:  [0.3615, -0.9884] n_targets:  1 reward:  52.03\n",
      "95.4 action:  [-0.1856, -0.9851] n_targets:  3 reward:  168.97\n",
      "101.7 action:  [0.4316, -0.9959] n_targets:  1 reward:  50.32\n",
      "ALPHA (entropy-related):  tensor([0.1661], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.19545 0.19182 0.1884  0.18483 0.18145 0.17817 0.17481 0.17161 0.16875\n",
      " 0.16612]\n",
      "Episode: 83, Episode Reward: 635.5842603047688\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  92\n",
      "0.1 action:  [0.4417, -0.9943] n_targets:  1 reward:  55.32\n",
      "1.6 action:  [0.3267, -0.9993] n_targets:  2 reward:  118.72\n",
      "5.1 action:  [0.2285, -0.9874] n_targets:  1 reward:  51.35\n",
      "13.3 action:  [-0.3327, -0.9977] n_targets:  1 reward:  69.13\n",
      "14.6 action:  [0.3002, -0.9867] n_targets:  1 reward:  72.65\n",
      "21.2 action:  [0.5129, -0.9953] n_targets:  1 reward:  51.1\n",
      "38.3 action:  [-0.2939, -0.9887] n_targets:  4 reward:  252.75\n",
      "47.1 action:  [0.6396, -0.9931] n_targets:  1 reward:  52.14\n",
      "51.2 action:  [0.4533, -0.994] n_targets:  1 reward:  51.1\n",
      "52.2 action:  [0.4832, -0.9963] n_targets:  1 reward:  52.57\n",
      "61.0 action:  [-0.176, -0.9809] n_targets:  2 reward:  122.36\n",
      "64.4 action:  [-0.0477, -0.9978] n_targets:  1 reward:  50.89\n",
      "71.6 action:  [-0.4231, -0.9844] n_targets:  1 reward:  50.79\n",
      "74.2 action:  [-0.5363, -0.9941] n_targets:  1 reward:  59.71\n",
      "77.0 action:  [0.0777, -0.9963] n_targets:  1 reward:  57.18\n",
      "79.4 action:  [0.3387, -0.9801] n_targets:  2 reward:  107.78\n",
      "83.9 action:  [0.3291, -0.9891] n_targets:  1 reward:  88.58\n",
      "86.6 action:  [-0.2885, -0.9938] n_targets:  2 reward:  114.99\n",
      "92.3 action:  [-0.1732, -0.9844] n_targets:  1 reward:  53.16\n",
      "95.9 action:  [-0.202, -0.9806] n_targets:  1 reward:  63.36\n",
      "99.7 action:  [-0.4773, -0.9884] n_targets:  2 reward:  126.91\n",
      "ALPHA (entropy-related):  tensor([0.1633], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.19182 0.1884  0.18483 0.18145 0.17817 0.17481 0.17161 0.16875 0.16612\n",
      " 0.16328]\n",
      "Episode: 84, Episode Reward: 1722.5399703979492\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  93\n",
      "2.9 action:  [0.0984, -0.9969] n_targets:  1 reward:  50.88\n",
      "5.2 action:  [0.0942, -0.9956] n_targets:  1 reward:  51.31\n",
      "13.8 action:  [0.3358, -0.9865] n_targets:  1 reward:  62.47\n",
      "15.9 action:  [0.576, -0.983] n_targets:  1 reward:  55.55\n",
      "18.0 action:  [-0.2639, -0.9958] n_targets:  1 reward:  53.68\n",
      "21.5 action:  [0.4992, -0.9811] n_targets:  1 reward:  60.83\n",
      "24.8 action:  [0.2902, -0.9957] n_targets:  1 reward:  65.35\n",
      "29.8 action:  [0.0433, -0.9894] n_targets:  1 reward:  65.07\n",
      "31.0 action:  [0.6012, -0.9873] n_targets:  1 reward:  51.32\n",
      "36.3 action:  [-0.254, -0.9876] n_targets:  1 reward:  54.37\n",
      "37.5 action:  [0.3877, -0.991] n_targets:  1 reward:  55.67\n",
      "40.8 action:  [0.5937, -0.9933] n_targets:  1 reward:  50.56\n",
      "52.9 action:  [-0.198, -0.9963] n_targets:  1 reward:  64.33\n",
      "55.8 action:  [0.1788, -0.9903] n_targets:  1 reward:  58.3\n",
      "62.1 action:  [0.3237, -0.9926] n_targets:  2 reward:  160.53\n",
      "63.2 action:  [-0.1104, -0.984] n_targets:  1 reward:  53.05\n",
      "63.6 action:  [0.4577, -0.9918] n_targets:  1 reward:  53.25\n",
      "65.6 action:  [0.4645, -0.9976] n_targets:  2 reward:  149.45\n",
      "72.1 action:  [-0.5505, -0.9979] n_targets:  1 reward:  50.66\n",
      "77.7 action:  [0.1101, -0.9984] n_targets:  1 reward:  55.51\n",
      "80.4 action:  [0.19, -0.9955] n_targets:  1 reward:  58.87\n",
      "88.3 action:  [0.5229, -0.9943] n_targets:  1 reward:  68.48\n",
      "88.6 action:  [-0.1794, -0.9845] n_targets:  2 reward:  100.04\n",
      "89.9 action:  [0.2111, -0.9841] n_targets:  1 reward:  57.53\n",
      "91.4 action:  [0.1461, -0.9857] n_targets:  2 reward:  103.46\n",
      "93.7 action:  [0.6037, -0.9973] n_targets:  1 reward:  53.78\n",
      "95.5 action:  [-0.0463, -0.9859] n_targets:  1 reward:  53.51\n",
      "ALPHA (entropy-related):  tensor([0.1604], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.1884  0.18483 0.18145 0.17817 0.17481 0.17161 0.16875 0.16612 0.16328\n",
      " 0.16045]\n",
      "Episode: 85, Episode Reward: 1817.7848917643228\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  94\n",
      "1.0 action:  [0.3797, -0.9944] n_targets:  2 reward:  137.52\n",
      "7.5 action:  [0.5042, -0.9982] n_targets:  1 reward:  59.07\n",
      "8.1 action:  [-0.0918, -0.9975] n_targets:  1 reward:  52.07\n",
      "13.5 action:  [-0.5279, -0.9876] n_targets:  1 reward:  51.86\n",
      "16.6 action:  [0.227, -0.9924] n_targets:  1 reward:  62.47\n",
      "20.1 action:  [-0.4851, -0.9862] n_targets:  2 reward:  127.48\n",
      "23.1 action:  [-0.209, -0.995] n_targets:  1 reward:  53.25\n",
      "36.0 action:  [0.4314, -0.9855] n_targets:  1 reward:  50.07\n",
      "44.8 action:  [-0.1941, -0.9923] n_targets:  3 reward:  212.87\n",
      "48.7 action:  [0.5689, -0.9816] n_targets:  1 reward:  70.91\n",
      "52.6 action:  [0.4966, -0.9872] n_targets:  1 reward:  50.04\n",
      "55.0 action:  [-0.0568, -0.9993] n_targets:  2 reward:  108.33\n",
      "58.9 action:  [0.3296, -0.9984] n_targets:  1 reward:  55.73\n",
      "61.0 action:  [0.1946, -0.9868] n_targets:  1 reward:  50.4\n",
      "62.4 action:  [0.0028, -0.9955] n_targets:  1 reward:  58.46\n",
      "73.2 action:  [0.1835, -0.9864] n_targets:  1 reward:  51.43\n",
      "76.0 action:  [-0.4153, -0.992] n_targets:  1 reward:  57.04\n",
      "77.8 action:  [-0.0751, -0.9974] n_targets:  1 reward:  52.09\n",
      "85.0 action:  [0.0775, -0.9927] n_targets:  1 reward:  68.6\n",
      "89.2 action:  [-0.3797, -0.9845] n_targets:  1 reward:  53.84\n",
      "93.4 action:  [0.3469, -0.9933] n_targets:  1 reward:  81.64\n",
      "97.5 action:  [0.1583, -0.997] n_targets:  1 reward:  56.56\n",
      "98.2 action:  [-0.5348, -0.9958] n_targets:  1 reward:  59.56\n",
      "101.5 action:  [-0.5335, -0.9846] n_targets:  1 reward:  53.49\n",
      "ALPHA (entropy-related):  tensor([0.1577], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.18483 0.18145 0.17817 0.17481 0.17161 0.16875 0.16612 0.16328 0.16045\n",
      " 0.15774]\n",
      "Episode: 86, Episode Reward: 1734.783822377523\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  95\n",
      "0.2 action:  [0.0357, -0.9876] n_targets:  2 reward:  114.04\n",
      "4.7 action:  [-0.3506, -0.9975] n_targets:  1 reward:  61.8\n",
      "6.5 action:  [-0.0084, -0.9805] n_targets:  1 reward:  54.33\n",
      "9.3 action:  [-0.1387, -0.9893] n_targets:  1 reward:  59.25\n",
      "16.2 action:  [-0.1499, -0.9918] n_targets:  1 reward:  55.92\n",
      "24.9 action:  [0.4146, -0.9943] n_targets:  1 reward:  64.34\n",
      "26.0 action:  [0.324, -0.9891] n_targets:  1 reward:  53.12\n",
      "36.3 action:  [0.6067, -0.987] n_targets:  1 reward:  50.88\n",
      "37.4 action:  [-0.2924, -0.9971] n_targets:  1 reward:  56.16\n",
      "88.8 action:  [-0.6119, -0.9975] n_targets:  1 reward:  52.05\n",
      "94.0 action:  [0.2323, -0.9977] n_targets:  1 reward:  69.5\n",
      "ALPHA (entropy-related):  tensor([0.1552], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.18145 0.17817 0.17481 0.17161 0.16875 0.16612 0.16328 0.16045 0.15774\n",
      " 0.15521]\n",
      "Episode: 87, Episode Reward: 691.3861668904622\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  96\n",
      "1.0 action:  [0.3208, -0.9922] n_targets:  1 reward:  51.72\n",
      "5.7 action:  [0.3436, -0.9917] n_targets:  1 reward:  51.4\n",
      "18.4 action:  [-0.1198, -0.9896] n_targets:  1 reward:  50.2\n",
      "23.8 action:  [0.5356, -0.9926] n_targets:  1 reward:  58.91\n",
      "25.6 action:  [0.4748, -0.9881] n_targets:  1 reward:  51.76\n",
      "27.5 action:  [-0.0602, -0.9898] n_targets:  1 reward:  51.2\n",
      "33.5 action:  [0.3791, -0.9916] n_targets:  1 reward:  54.93\n",
      "47.7 action:  [-0.3751, -0.9883] n_targets:  2 reward:  127.0\n",
      "50.5 action:  [0.2823, -0.9987] n_targets:  1 reward:  52.83\n",
      "61.4 action:  [-0.0418, -0.996] n_targets:  1 reward:  50.42\n",
      "63.3 action:  [-0.3164, -0.9866] n_targets:  1 reward:  55.24\n",
      "67.2 action:  [0.2172, -0.988] n_targets:  1 reward:  51.9\n",
      "101.2 action:  [0.1815, -0.9822] n_targets:  1 reward:  53.1\n",
      "ALPHA (entropy-related):  tensor([0.1530], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.17817 0.17481 0.17161 0.16875 0.16612 0.16328 0.16045 0.15774 0.15521\n",
      " 0.15304]\n",
      "Episode: 88, Episode Reward: 760.60573832194\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  97\n",
      "9.2 action:  [-0.0934, -0.9848] n_targets:  1 reward:  58.04\n",
      "10.1 action:  [0.2473, -0.9814] n_targets:  1 reward:  50.03\n",
      "29.6 action:  [-0.2497, -0.996] n_targets:  1 reward:  51.1\n",
      "32.1 action:  [0.3703, -0.992] n_targets:  1 reward:  60.51\n",
      "39.4 action:  [-0.3318, -0.9985] n_targets:  1 reward:  53.24\n",
      "40.0 action:  [-0.1785, -0.9897] n_targets:  1 reward:  50.89\n",
      "45.6 action:  [0.4531, -0.9834] n_targets:  1 reward:  52.82\n",
      "48.2 action:  [-0.2511, -0.9983] n_targets:  1 reward:  51.27\n",
      "62.0 action:  [0.1945, -0.9801] n_targets:  2 reward:  103.57\n",
      "69.3 action:  [0.0427, -0.994] n_targets:  1 reward:  54.08\n",
      "80.7 action:  [0.6393, -0.9898] n_targets:  1 reward:  53.73\n",
      "85.7 action:  [-0.0176, -0.9947] n_targets:  1 reward:  62.25\n",
      "102.2 action:  [0.6079, -0.9848] n_targets:  1 reward:  51.45\n",
      "ALPHA (entropy-related):  tensor([0.1510], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.17481 0.17161 0.16875 0.16612 0.16328 0.16045 0.15774 0.15521 0.15304\n",
      " 0.15099]\n",
      "Episode: 89, Episode Reward: 752.9830678304036\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  98\n",
      "0.9 action:  [0.5252, -0.9827] n_targets:  2 reward:  153.6\n",
      "7.3 action:  [0.3721, -0.9891] n_targets:  1 reward:  70.06\n",
      "21.7 action:  [-0.2905, -0.9979] n_targets:  1 reward:  53.21\n",
      "27.0 action:  [0.557, -0.9991] n_targets:  2 reward:  105.87\n",
      "42.0 action:  [0.0042, -0.9802] n_targets:  1 reward:  59.5\n",
      "44.0 action:  [-0.4766, -0.9871] n_targets:  1 reward:  73.89\n",
      "46.4 action:  [-0.1138, -0.989] n_targets:  1 reward:  55.97\n",
      "47.3 action:  [0.5642, -0.9864] n_targets:  1 reward:  52.59\n",
      "51.2 action:  [-0.5321, -0.9889] n_targets:  2 reward:  132.33\n",
      "59.2 action:  [-0.5374, -0.997] n_targets:  1 reward:  54.6\n",
      "67.1 action:  [-0.3926, -0.9958] n_targets:  2 reward:  124.6\n",
      "67.4 action:  [0.2376, -0.997] n_targets:  1 reward:  50.49\n",
      "69.5 action:  [0.3133, -0.9962] n_targets:  1 reward:  61.93\n",
      "77.5 action:  [-0.1996, -0.9963] n_targets:  1 reward:  58.59\n",
      "80.1 action:  [-0.5706, -0.9927] n_targets:  1 reward:  55.32\n",
      "86.8 action:  [-0.6044, -0.9961] n_targets:  1 reward:  58.08\n",
      "94.6 action:  [0.5824, -0.9838] n_targets:  1 reward:  78.92\n",
      "97.2 action:  [-0.5079, -0.9929] n_targets:  1 reward:  61.2\n",
      "97.7 action:  [-0.4852, -0.9891] n_targets:  1 reward:  51.56\n",
      "101.3 action:  [-0.0829, -0.9958] n_targets:  1 reward:  97.04\n",
      "102.0 action:  [0.3815, -0.9957] n_targets:  1 reward:  54.79\n",
      "ALPHA (entropy-related):  tensor([0.1487], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.17161 0.16875 0.16612 0.16328 0.16045 0.15774 0.15521 0.15304 0.15099\n",
      " 0.14874]\n",
      "Episode: 90, Episode Reward: 1564.1334756215413\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  99\n",
      "1.2 action:  [-0.1091, -0.9803] n_targets:  1 reward:  54.54\n",
      "4.0 action:  [-0.377, -0.9996] n_targets:  1 reward:  53.56\n",
      "11.0 action:  [0.6202, -0.9995] n_targets:  1 reward:  81.36\n",
      "12.4 action:  [0.5844, -0.991] n_targets:  1 reward:  52.19\n",
      "14.2 action:  [0.0463, -0.9947] n_targets:  1 reward:  71.84\n",
      "19.7 action:  [-0.1717, -0.9977] n_targets:  1 reward:  50.08\n",
      "25.5 action:  [0.5437, -0.9907] n_targets:  2 reward:  102.87\n",
      "42.3 action:  [-0.607, -0.9895] n_targets:  1 reward:  51.19\n",
      "44.3 action:  [-0.5919, -0.9941] n_targets:  1 reward:  68.49\n",
      "45.9 action:  [-0.4191, -0.9979] n_targets:  1 reward:  51.34\n",
      "56.6 action:  [0.5787, -0.9889] n_targets:  3 reward:  188.43\n",
      "57.2 action:  [0.5351, -0.9969] n_targets:  1 reward:  63.07\n",
      "60.2 action:  [-0.0616, -0.9894] n_targets:  1 reward:  51.46\n",
      "76.6 action:  [0.3494, -0.9805] n_targets:  1 reward:  55.2\n",
      "79.2 action:  [0.0073, -0.9943] n_targets:  1 reward:  55.79\n",
      "84.1 action:  [0.1192, -0.9863] n_targets:  1 reward:  52.1\n",
      "85.1 action:  [0.2029, -0.9914] n_targets:  1 reward:  54.02\n",
      "91.0 action:  [0.5189, -0.9913] n_targets:  1 reward:  50.69\n",
      "97.4 action:  [-0.6342, -0.9998] n_targets:  1 reward:  52.29\n",
      "98.9 action:  [-0.2376, -0.9938] n_targets:  1 reward:  52.87\n",
      "101.8 action:  [-0.0679, -0.9957] n_targets:  1 reward:  54.14\n",
      "ALPHA (entropy-related):  tensor([0.1461], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.16875 0.16612 0.16328 0.16045 0.15774 0.15521 0.15304 0.15099 0.14874\n",
      " 0.14611]\n",
      "Episode: 91, Episode Reward: 1367.5149701436358\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  100\n",
      "0.1 action:  [-0.2591, -0.9956] n_targets:  2 reward:  187.73\n",
      "9.0 action:  [-0.5905, -0.9812] n_targets:  1 reward:  50.15\n",
      "12.7 action:  [-0.1095, -0.9817] n_targets:  1 reward:  50.41\n",
      "18.4 action:  [-0.2158, -0.9981] n_targets:  1 reward:  51.11\n",
      "20.6 action:  [0.1819, -0.9885] n_targets:  1 reward:  53.85\n",
      "43.6 action:  [0.0727, -0.9938] n_targets:  1 reward:  50.08\n",
      "93.2 action:  [0.3372, -0.9807] n_targets:  1 reward:  50.34\n",
      "95.0 action:  [0.2903, -0.9935] n_targets:  1 reward:  54.11\n",
      "99.9 action:  [-0.1254, -0.9877] n_targets:  1 reward:  57.88\n",
      "101.3 action:  [-0.6041, -0.9858] n_targets:  1 reward:  62.94\n",
      "ALPHA (entropy-related):  tensor([0.1439], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.16612 0.16328 0.16045 0.15774 0.15521 0.15304 0.15099 0.14874 0.14611\n",
      " 0.14394]\n",
      "Episode: 92, Episode Reward: 668.597349802653\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  101\n",
      "0.2 action:  [0.5293, -0.993] n_targets:  1 reward:  78.41\n",
      "6.9 action:  [-0.3829, -0.9933] n_targets:  1 reward:  50.23\n",
      "12.8 action:  [0.3234, -0.9916] n_targets:  3 reward:  187.01\n",
      "14.3 action:  [-0.2884, -0.9939] n_targets:  1 reward:  50.43\n",
      "20.6 action:  [0.1052, -0.9862] n_targets:  1 reward:  69.94\n",
      "23.2 action:  [0.2252, -0.9901] n_targets:  1 reward:  55.8\n",
      "25.9 action:  [0.5467, -0.9865] n_targets:  1 reward:  53.96\n",
      "32.0 action:  [-0.4997, -0.9834] n_targets:  1 reward:  58.25\n",
      "35.4 action:  [-0.2946, -0.9954] n_targets:  1 reward:  64.01\n",
      "47.1 action:  [-0.4206, -0.9937] n_targets:  1 reward:  58.18\n",
      "80.2 action:  [0.41, -0.9971] n_targets:  1 reward:  52.27\n",
      "86.5 action:  [-0.6193, -0.9988] n_targets:  1 reward:  54.14\n",
      "92.7 action:  [0.5891, -0.9948] n_targets:  1 reward:  51.53\n",
      "102.4 action:  [-0.1978, -0.9958] n_targets:  1 reward:  52.34\n",
      "ALPHA (entropy-related):  tensor([0.1419], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.16328 0.16045 0.15774 0.15521 0.15304 0.15099 0.14874 0.14611 0.14394\n",
      " 0.14189]\n",
      "Episode: 93, Episode Reward: 936.504564921061\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  102\n",
      "26.7 action:  [0.2978, -0.9989] n_targets:  1 reward:  54.95\n",
      "28.5 action:  [0.2659, -0.9823] n_targets:  1 reward:  52.86\n",
      "54.5 action:  [0.2134, -0.9948] n_targets:  1 reward:  50.39\n",
      "62.1 action:  [-0.5771, -0.9917] n_targets:  1 reward:  50.82\n",
      "63.3 action:  [-0.5247, -0.986] n_targets:  1 reward:  52.6\n",
      "69.1 action:  [-0.2634, -0.9843] n_targets:  1 reward:  51.31\n",
      "86.5 action:  [0.6319, -0.9978] n_targets:  1 reward:  54.72\n",
      "92.2 action:  [-0.1401, -0.9918] n_targets:  1 reward:  52.72\n",
      "101.9 action:  [0.4084, -0.9835] n_targets:  1 reward:  54.97\n",
      "ALPHA (entropy-related):  tensor([0.1400], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.16045 0.15774 0.15521 0.15304 0.15099 0.14874 0.14611 0.14394 0.14189\n",
      " 0.13997]\n",
      "Episode: 94, Episode Reward: 475.34801228841144\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  103\n",
      "1.3 action:  [0.3738, -0.9898] n_targets:  1 reward:  63.66\n",
      "5.0 action:  [-0.1452, -0.9947] n_targets:  1 reward:  53.8\n",
      "8.0 action:  [-0.3443, -0.998] n_targets:  1 reward:  50.82\n",
      "11.6 action:  [-0.4557, -0.9853] n_targets:  1 reward:  57.12\n",
      "24.2 action:  [0.1961, -0.9827] n_targets:  1 reward:  50.18\n",
      "27.6 action:  [-0.2023, -0.9871] n_targets:  1 reward:  55.35\n",
      "42.4 action:  [-0.1253, -0.9899] n_targets:  1 reward:  50.81\n",
      "45.3 action:  [-0.5374, -0.9812] n_targets:  1 reward:  57.12\n",
      "67.9 action:  [-0.0834, -0.9874] n_targets:  1 reward:  62.04\n",
      "79.5 action:  [0.4948, -0.9924] n_targets:  1 reward:  52.54\n",
      "86.5 action:  [-0.5069, -0.9987] n_targets:  1 reward:  50.64\n",
      "ALPHA (entropy-related):  tensor([0.1381], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.15774 0.15521 0.15304 0.15099 0.14874 0.14611 0.14394 0.14189 0.13997\n",
      " 0.13811]\n",
      "Episode: 95, Episode Reward: 604.0855077107747\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  104\n",
      "0.8 action:  [0.0792, -0.9958] n_targets:  1 reward:  53.27\n",
      "10.1 action:  [-0.0244, -0.9887] n_targets:  1 reward:  50.12\n",
      "30.0 action:  [0.0184, -0.9851] n_targets:  1 reward:  50.65\n",
      "36.4 action:  [-0.0123, -0.9977] n_targets:  1 reward:  50.29\n",
      "61.8 action:  [0.2417, -0.998] n_targets:  1 reward:  51.06\n",
      "64.2 action:  [0.4659, -0.9962] n_targets:  1 reward:  51.06\n",
      "67.8 action:  [-0.5756, -0.9949] n_targets:  2 reward:  104.74\n",
      "77.4 action:  [-0.2744, -0.9826] n_targets:  1 reward:  51.61\n",
      "87.0 action:  [0.1438, -0.9975] n_targets:  1 reward:  50.46\n",
      "90.3 action:  [-0.1934, -0.9831] n_targets:  1 reward:  51.06\n",
      "ALPHA (entropy-related):  tensor([0.1364], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.15521 0.15304 0.15099 0.14874 0.14611 0.14394 0.14189 0.13997 0.13811\n",
      " 0.13644]\n",
      "Episode: 96, Episode Reward: 564.3130849202473\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  105\n",
      "9.9 action:  [-0.2812, -0.9814] n_targets:  1 reward:  53.68\n",
      "12.1 action:  [-0.5601, -0.9985] n_targets:  1 reward:  50.9\n",
      "21.0 action:  [0.601, -0.9962] n_targets:  1 reward:  52.08\n",
      "22.3 action:  [-0.3826, -0.9874] n_targets:  1 reward:  51.94\n",
      "35.0 action:  [-0.2902, -0.9894] n_targets:  1 reward:  52.49\n",
      "35.4 action:  [-0.429, -0.9923] n_targets:  1 reward:  50.63\n",
      "73.2 action:  [0.6368, -0.9979] n_targets:  1 reward:  57.34\n",
      "79.4 action:  [0.3117, -0.987] n_targets:  1 reward:  56.22\n",
      "82.5 action:  [-0.1866, -0.9887] n_targets:  1 reward:  51.39\n",
      "96.1 action:  [-0.3081, -0.995] n_targets:  1 reward:  50.97\n",
      "ALPHA (entropy-related):  tensor([0.1350], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.15304 0.15099 0.14874 0.14611 0.14394 0.14189 0.13997 0.13811 0.13644\n",
      " 0.13501]\n",
      "Episode: 97, Episode Reward: 527.6357676188151\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  106\n",
      "5.2 action:  [-0.0994, -0.9992] n_targets:  2 reward:  107.41\n",
      "35.9 action:  [-0.4575, -0.9972] n_targets:  1 reward:  51.09\n",
      "43.8 action:  [0.3495, -0.9965] n_targets:  1 reward:  50.22\n",
      "44.3 action:  [0.02, -0.9861] n_targets:  2 reward:  102.05\n",
      "61.7 action:  [0.3639, -0.9819] n_targets:  1 reward:  51.35\n",
      "63.8 action:  [-0.2528, -0.9813] n_targets:  1 reward:  51.22\n",
      "80.4 action:  [0.1437, -0.9855] n_targets:  1 reward:  52.7\n",
      "ALPHA (entropy-related):  tensor([0.1335], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.15099 0.14874 0.14611 0.14394 0.14189 0.13997 0.13811 0.13644 0.13501\n",
      " 0.13345]\n",
      "Episode: 98, Episode Reward: 466.0417022705078\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  107\n",
      "0.1 action:  [0.151, -0.9848] n_targets:  1 reward:  51.2\n",
      "29.0 action:  [-0.4169, -0.991] n_targets:  1 reward:  51.27\n",
      "31.2 action:  [-0.0481, -0.9944] n_targets:  1 reward:  51.14\n",
      "31.8 action:  [0.0658, -0.9827] n_targets:  1 reward:  53.49\n",
      "38.4 action:  [-0.6141, -0.9929] n_targets:  1 reward:  53.26\n",
      "71.2 action:  [-0.4974, -0.9825] n_targets:  1 reward:  56.01\n",
      "77.7 action:  [0.3724, -0.986] n_targets:  1 reward:  50.02\n",
      "79.8 action:  [0.1192, -0.9867] n_targets:  1 reward:  53.25\n",
      "89.3 action:  [0.5222, -0.9858] n_targets:  1 reward:  88.84\n",
      "92.7 action:  [0.5427, -0.9906] n_targets:  1 reward:  50.33\n",
      "95.9 action:  [-0.1136, -0.9884] n_targets:  1 reward:  50.52\n",
      "101.5 action:  [-0.02, -0.9804] n_targets:  1 reward:  51.16\n",
      "ALPHA (entropy-related):  tensor([0.1317], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.14874 0.14611 0.14394 0.14189 0.13997 0.13811 0.13644 0.13501 0.13345\n",
      " 0.13168]\n",
      "Episode: 99, Episode Reward: 660.4941253662109\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  108\n",
      "0.4 action:  [0.3476, -0.9807] n_targets:  1 reward:  88.82\n",
      "16.2 action:  [0.6154, -0.9824] n_targets:  1 reward:  52.16\n",
      "30.9 action:  [-0.5891, -0.9966] n_targets:  1 reward:  50.31\n",
      "33.2 action:  [0.0941, -0.981] n_targets:  1 reward:  55.71\n",
      "68.2 action:  [-0.4406, -0.9981] n_targets:  1 reward:  59.44\n",
      "ALPHA (entropy-related):  tensor([0.1299], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.14611 0.14394 0.14189 0.13997 0.13811 0.13644 0.13501 0.13345 0.13168\n",
      " 0.12991]\n",
      "Last 100 ALPHA: [0.95695 0.93613 0.91576 0.89583 0.87634 0.85727 0.83861 0.82037 0.80252\n",
      " 0.78506 0.76797 0.75126 0.73492 0.71893 0.70328 0.68798 0.67301 0.65837\n",
      " 0.64405 0.63004 0.61633 0.60293 0.58981 0.57698 0.56443 0.55215 0.54014\n",
      " 0.52839 0.5169  0.50566 0.49466 0.4839  0.47338 0.46308 0.45302 0.44317\n",
      " 0.43353 0.4241  0.41488 0.40586 0.39704 0.3884  0.37997 0.37171 0.36363\n",
      " 0.35573 0.34801 0.34045 0.33308 0.32586 0.31878 0.31189 0.30515 0.29854\n",
      " 0.29209 0.28578 0.27959 0.27356 0.26766 0.26192 0.25628 0.25081 0.24544\n",
      " 0.24022 0.23516 0.23025 0.22546 0.22071 0.21614 0.21183 0.20748 0.20331\n",
      " 0.19923 0.19545 0.19182 0.1884  0.18483 0.18145 0.17817 0.17481 0.17161\n",
      " 0.16875 0.16612 0.16328 0.16045 0.15774 0.15521 0.15304 0.15099 0.14874\n",
      " 0.14611 0.14394 0.14189 0.13997 0.13811 0.13644 0.13501 0.13345 0.13168\n",
      " 0.12991]\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  109\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  110\n",
      "Best average reward: 0.0, Current average reward: 0.0\n",
      "Evaluation rewards: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Episode: 100, Episode Reward: 306.44009526570636\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  111\n",
      "0.1 action:  [0.6342, -0.9969] n_targets:  1 reward:  54.21\n",
      "17.5 action:  [0.4507, -0.9968] n_targets:  1 reward:  59.66\n",
      "30.0 action:  [0.0692, -0.9964] n_targets:  2 reward:  104.89\n",
      "35.8 action:  [0.2969, -0.9948] n_targets:  1 reward:  64.54\n",
      "42.6 action:  [0.3131, -0.9835] n_targets:  1 reward:  52.18\n",
      "48.5 action:  [0.5175, -0.9859] n_targets:  1 reward:  50.44\n",
      "86.0 action:  [-0.1521, -0.9917] n_targets:  1 reward:  50.03\n",
      "ALPHA (entropy-related):  tensor([0.1286], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.14394 0.14189 0.13997 0.13811 0.13644 0.13501 0.13345 0.13168 0.12991\n",
      " 0.12857]\n",
      "Episode: 101, Episode Reward: 435.9573160807292\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  112\n",
      "0.6 action:  [0.2316, -0.9871] n_targets:  1 reward:  61.29\n",
      "11.1 action:  [-0.3085, -0.9828] n_targets:  1 reward:  52.95\n",
      "25.3 action:  [-0.3118, -0.9811] n_targets:  1 reward:  51.91\n",
      "97.4 action:  [0.2875, -0.9811] n_targets:  1 reward:  50.71\n",
      "101.6 action:  [0.0936, -0.9859] n_targets:  1 reward:  50.5\n",
      "102.1 action:  [-0.0227, -0.9852] n_targets:  1 reward:  91.18\n",
      "ALPHA (entropy-related):  tensor([0.1271], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.14189 0.13997 0.13811 0.13644 0.13501 0.13345 0.13168 0.12991 0.12857\n",
      " 0.12711]\n",
      "Episode: 102, Episode Reward: 358.5423914591471\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  113\n",
      "0.3 action:  [0.0984, -0.9844] n_targets:  2 reward:  144.02\n",
      "11.8 action:  [-0.0659, -0.9865] n_targets:  1 reward:  50.78\n",
      "15.7 action:  [-0.1482, -0.9879] n_targets:  1 reward:  50.0\n",
      "21.3 action:  [-0.5811, -0.9857] n_targets:  1 reward:  57.47\n",
      "27.4 action:  [0.4361, -0.9949] n_targets:  1 reward:  54.12\n",
      "33.8 action:  [-0.3238, -0.9966] n_targets:  1 reward:  51.46\n",
      "52.1 action:  [0.036, -0.9829] n_targets:  1 reward:  55.02\n",
      "55.9 action:  [0.0921, -0.9886] n_targets:  1 reward:  50.68\n",
      "64.2 action:  [-0.1505, -0.998] n_targets:  1 reward:  61.89\n",
      "67.7 action:  [0.2804, -0.9812] n_targets:  1 reward:  51.52\n",
      "86.5 action:  [0.3416, -0.9865] n_targets:  1 reward:  51.85\n",
      "92.2 action:  [0.1217, -0.9978] n_targets:  1 reward:  51.66\n",
      "95.2 action:  [0.2783, -0.9952] n_targets:  1 reward:  55.04\n",
      "98.8 action:  [-0.4005, -0.999] n_targets:  1 reward:  50.98\n",
      "99.1 action:  [-0.1449, -0.987] n_targets:  1 reward:  52.58\n",
      "ALPHA (entropy-related):  tensor([0.1255], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.13997 0.13811 0.13644 0.13501 0.13345 0.13168 0.12991 0.12857 0.12711\n",
      " 0.12548]\n",
      "Episode: 103, Episode Reward: 889.0756276448567\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  114\n",
      "6.3 action:  [0.0426, -0.9971] n_targets:  1 reward:  55.29\n",
      "14.7 action:  [-0.2118, -0.9953] n_targets:  1 reward:  53.84\n",
      "15.4 action:  [-0.2997, -0.9958] n_targets:  1 reward:  50.24\n",
      "20.9 action:  [-0.0436, -0.9878] n_targets:  1 reward:  54.14\n",
      "31.9 action:  [0.2074, -0.9896] n_targets:  1 reward:  57.25\n",
      "33.2 action:  [-0.1593, -0.9828] n_targets:  1 reward:  50.81\n",
      "59.0 action:  [0.0981, -0.9822] n_targets:  1 reward:  54.68\n",
      "59.8 action:  [-0.0902, -0.9874] n_targets:  2 reward:  105.39\n",
      "60.6 action:  [0.6174, -0.9926] n_targets:  1 reward:  52.33\n",
      "61.5 action:  [0.276, -0.9877] n_targets:  1 reward:  52.35\n",
      "65.2 action:  [-0.0013, -0.9951] n_targets:  2 reward:  108.12\n",
      "67.2 action:  [0.3457, -0.9861] n_targets:  1 reward:  52.01\n",
      "72.0 action:  [-0.2139, -0.9928] n_targets:  1 reward:  50.04\n",
      "95.6 action:  [-0.3053, -0.9912] n_targets:  1 reward:  50.83\n",
      "ALPHA (entropy-related):  tensor([0.1239], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.13811 0.13644 0.13501 0.13345 0.13168 0.12991 0.12857 0.12711 0.12548\n",
      " 0.12385]\n",
      "Episode: 104, Episode Reward: 847.3115895589192\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  115\n",
      "27.5 action:  [0.4061, -0.9943] n_targets:  1 reward:  51.64\n",
      "32.5 action:  [0.3237, -0.9804] n_targets:  1 reward:  50.04\n",
      "99.3 action:  [-0.2227, -0.9959] n_targets:  1 reward:  50.47\n",
      "ALPHA (entropy-related):  tensor([0.1223], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.13644 0.13501 0.13345 0.13168 0.12991 0.12857 0.12711 0.12548 0.12385\n",
      " 0.12229]\n",
      "Episode: 105, Episode Reward: 152.15508524576822\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  116\n",
      "0.2 action:  [0.2166, -0.9845] n_targets:  2 reward:  141.99\n",
      "4.3 action:  [-0.5881, -0.9914] n_targets:  1 reward:  51.06\n",
      "ALPHA (entropy-related):  tensor([0.1209], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.13501 0.13345 0.13168 0.12991 0.12857 0.12711 0.12548 0.12385 0.12229\n",
      " 0.12085]\n",
      "Episode: 106, Episode Reward: 193.053227742513\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  117\n",
      "0.3 action:  [0.349, -0.9895] n_targets:  1 reward:  96.25\n",
      "12.4 action:  [0.0637, -0.9935] n_targets:  1 reward:  50.7\n",
      "12.7 action:  [-0.0356, -0.9945] n_targets:  1 reward:  51.37\n",
      "17.1 action:  [-0.2115, -0.995] n_targets:  1 reward:  51.96\n",
      "27.0 action:  [0.3526, -0.9836] n_targets:  1 reward:  52.89\n",
      "33.2 action:  [-0.1428, -0.9844] n_targets:  1 reward:  51.75\n",
      "ALPHA (entropy-related):  tensor([0.1195], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.13345 0.13168 0.12991 0.12857 0.12711 0.12548 0.12385 0.12229 0.12085\n",
      " 0.1195 ]\n",
      "Episode: 107, Episode Reward: 354.9234972000122\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  118\n",
      "71.9 action:  [-0.3658, -0.9927] n_targets:  1 reward:  51.94\n",
      "77.6 action:  [0.579, -0.9901] n_targets:  1 reward:  53.27\n",
      "82.0 action:  [0.2584, -0.9834] n_targets:  1 reward:  51.68\n",
      "87.3 action:  [-0.3773, -0.9837] n_targets:  1 reward:  59.71\n",
      "89.3 action:  [-0.0931, -0.9964] n_targets:  1 reward:  54.49\n",
      "91.5 action:  [-0.1117, -0.9913] n_targets:  1 reward:  54.16\n",
      "ALPHA (entropy-related):  tensor([0.1182], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.13168 0.12991 0.12857 0.12711 0.12548 0.12385 0.12229 0.12085 0.1195\n",
      " 0.1182 ]\n",
      "Episode: 108, Episode Reward: 325.2433980305989\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  119\n",
      "0.1 action:  [0.0407, -0.9922] n_targets:  1 reward:  65.31\n",
      "10.9 action:  [-0.0013, -0.9943] n_targets:  1 reward:  52.36\n",
      "38.5 action:  [-0.3133, -0.9981] n_targets:  1 reward:  50.28\n",
      "42.7 action:  [0.1715, -0.9884] n_targets:  1 reward:  50.45\n",
      "85.3 action:  [0.4416, -0.9837] n_targets:  1 reward:  51.03\n",
      "92.0 action:  [-0.0958, -0.9815] n_targets:  1 reward:  50.88\n",
      "ALPHA (entropy-related):  tensor([0.1170], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.12991 0.12857 0.12711 0.12548 0.12385 0.12229 0.12085 0.1195  0.1182\n",
      " 0.117  ]\n",
      "Episode: 109, Episode Reward: 320.30767822265625\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  120\n",
      "0.2 action:  [-0.0486, -0.999] n_targets:  4 reward:  313.52\n",
      "0.8 action:  [-0.0129, -0.9885] n_targets:  1 reward:  52.69\n",
      "48.0 action:  [0.4453, -0.991] n_targets:  1 reward:  50.0\n",
      "60.8 action:  [0.0361, -0.993] n_targets:  1 reward:  51.47\n",
      "77.1 action:  [0.305, -0.9982] n_targets:  1 reward:  52.37\n",
      "81.3 action:  [-0.3311, -0.9943] n_targets:  1 reward:  52.17\n",
      "89.4 action:  [0.2313, -0.9901] n_targets:  1 reward:  51.09\n",
      "97.9 action:  [-0.1968, -0.9821] n_targets:  1 reward:  50.67\n",
      "ALPHA (entropy-related):  tensor([0.1159], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.12857 0.12711 0.12548 0.12385 0.12229 0.12085 0.1195  0.1182  0.117\n",
      " 0.11586]\n",
      "Episode: 110, Episode Reward: 673.9830373128254\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  121\n",
      "0.6 action:  [-0.1344, -0.995] n_targets:  1 reward:  65.82\n",
      "8.5 action:  [0.4067, -0.9991] n_targets:  1 reward:  51.8\n",
      "13.2 action:  [-0.1203, -0.9908] n_targets:  1 reward:  56.3\n",
      "18.9 action:  [-0.0517, -0.9972] n_targets:  1 reward:  51.53\n",
      "27.4 action:  [0.3358, -0.9842] n_targets:  1 reward:  51.78\n",
      "42.4 action:  [-0.6054, -0.9965] n_targets:  1 reward:  50.46\n",
      "86.0 action:  [-0.1389, -0.9858] n_targets:  1 reward:  52.79\n",
      "ALPHA (entropy-related):  tensor([0.1146], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.12711 0.12548 0.12385 0.12229 0.12085 0.1195  0.1182  0.117   0.11586\n",
      " 0.11458]\n",
      "Episode: 111, Episode Reward: 380.4912439982096\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  122\n",
      "0.7 action:  [0.4607, -0.9811] n_targets:  1 reward:  86.2\n",
      "46.6 action:  [0.1913, -0.9831] n_targets:  1 reward:  50.6\n",
      "53.7 action:  [0.0924, -0.9913] n_targets:  1 reward:  50.21\n",
      "74.2 action:  [-0.1515, -0.9955] n_targets:  1 reward:  50.54\n",
      "79.1 action:  [-0.1696, -0.9937] n_targets:  1 reward:  50.59\n",
      "ALPHA (entropy-related):  tensor([0.1134], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.12548 0.12385 0.12229 0.12085 0.1195  0.1182  0.117   0.11586 0.11458\n",
      " 0.11337]\n",
      "Episode: 112, Episode Reward: 288.1268107096354\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  123\n",
      "69.9 action:  [0.0482, -0.9949] n_targets:  1 reward:  50.62\n",
      "85.4 action:  [-0.5652, -0.9917] n_targets:  1 reward:  50.12\n",
      "97.3 action:  [-0.4142, -0.9841] n_targets:  1 reward:  50.53\n",
      "98.2 action:  [-0.1937, -0.98] n_targets:  1 reward:  50.8\n",
      "ALPHA (entropy-related):  tensor([0.1121], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.12385 0.12229 0.12085 0.1195  0.1182  0.117   0.11586 0.11458 0.11337\n",
      " 0.11211]\n",
      "Episode: 113, Episode Reward: 202.07416280110675\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  124\n",
      "10.0 action:  [-0.1437, -0.9874] n_targets:  1 reward:  52.13\n",
      "76.7 action:  [0.1073, -0.992] n_targets:  1 reward:  51.02\n",
      "ALPHA (entropy-related):  tensor([0.1109], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.12229 0.12085 0.1195  0.1182  0.117   0.11586 0.11458 0.11337 0.11211\n",
      " 0.11092]\n",
      "Episode: 114, Episode Reward: 103.14804585774738\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  125\n",
      "0.1 action:  [0.3694, -0.9948] n_targets:  1 reward:  80.22\n",
      "66.4 action:  [0.0324, -0.9976] n_targets:  1 reward:  50.45\n",
      "73.8 action:  [0.075, -0.9858] n_targets:  1 reward:  53.05\n",
      "ALPHA (entropy-related):  tensor([0.1099], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.12085 0.1195  0.1182  0.117   0.11586 0.11458 0.11337 0.11211 0.11092\n",
      " 0.10986]\n",
      "Episode: 115, Episode Reward: 183.7199592590332\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  126\n",
      "0.6 action:  [0.2511, -0.9917] n_targets:  3 reward:  184.95\n",
      "4.3 action:  [0.2755, -0.9942] n_targets:  1 reward:  50.98\n",
      "7.9 action:  [0.0661, -0.9859] n_targets:  1 reward:  50.1\n",
      "8.4 action:  [-0.3465, -0.9968] n_targets:  1 reward:  50.32\n",
      "10.7 action:  [0.4527, -0.9841] n_targets:  1 reward:  51.4\n",
      "13.2 action:  [0.2223, -0.9947] n_targets:  2 reward:  108.2\n",
      "13.8 action:  [-0.3397, -0.9955] n_targets:  1 reward:  50.67\n",
      "18.5 action:  [0.0746, -0.9812] n_targets:  1 reward:  51.17\n",
      "37.5 action:  [0.6004, -0.988] n_targets:  1 reward:  50.43\n",
      "40.1 action:  [-0.0103, -0.9967] n_targets:  1 reward:  50.57\n",
      "43.4 action:  [0.2544, -0.9954] n_targets:  1 reward:  50.59\n",
      "47.0 action:  [0.0343, -0.9895] n_targets:  1 reward:  55.98\n",
      "47.3 action:  [-0.2537, -0.9939] n_targets:  1 reward:  50.43\n",
      "ALPHA (entropy-related):  tensor([0.1086], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.1195  0.1182  0.117   0.11586 0.11458 0.11337 0.11211 0.11092 0.10986\n",
      " 0.10857]\n",
      "Episode: 116, Episode Reward: 855.785176595052\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  127\n",
      "0.2 action:  [0.1737, -0.9868] n_targets:  2 reward:  134.61\n",
      "7.2 action:  [0.569, -0.9852] n_targets:  1 reward:  52.62\n",
      "12.9 action:  [0.4638, -0.9992] n_targets:  1 reward:  50.43\n",
      "21.7 action:  [-0.3658, -0.9836] n_targets:  1 reward:  55.86\n",
      "26.3 action:  [0.0012, -0.9883] n_targets:  2 reward:  107.76\n",
      "37.7 action:  [-0.0847, -0.9917] n_targets:  1 reward:  50.03\n",
      "45.0 action:  [0.231, -0.9833] n_targets:  1 reward:  51.0\n",
      "48.9 action:  [0.1005, -0.9951] n_targets:  1 reward:  50.81\n",
      "58.4 action:  [-0.3688, -0.9931] n_targets:  1 reward:  51.16\n",
      "60.2 action:  [-0.5143, -0.9945] n_targets:  1 reward:  51.96\n",
      "70.7 action:  [0.2228, -0.9957] n_targets:  1 reward:  52.85\n",
      "72.1 action:  [-0.0445, -0.9933] n_targets:  1 reward:  51.98\n",
      "73.6 action:  [0.0127, -0.9937] n_targets:  1 reward:  50.2\n",
      "ALPHA (entropy-related):  tensor([0.1072], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.1182  0.117   0.11586 0.11458 0.11337 0.11211 0.11092 0.10986 0.10857\n",
      " 0.10725]\n",
      "Episode: 117, Episode Reward: 811.2462361653645\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  128\n",
      "8.3 action:  [0.2057, -0.9842] n_targets:  1 reward:  57.56\n",
      "12.0 action:  [0.2696, -0.9974] n_targets:  1 reward:  52.17\n",
      "28.3 action:  [-0.1, -0.9987] n_targets:  1 reward:  51.2\n",
      "48.2 action:  [0.3846, -0.9859] n_targets:  1 reward:  52.14\n",
      "56.7 action:  [-0.2538, -0.9917] n_targets:  1 reward:  50.68\n",
      "69.7 action:  [0.184, -0.9832] n_targets:  1 reward:  50.76\n",
      "ALPHA (entropy-related):  tensor([0.1064], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.117   0.11586 0.11458 0.11337 0.11211 0.11092 0.10986 0.10857 0.10725\n",
      " 0.10635]\n",
      "Episode: 118, Episode Reward: 314.5133743286133\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  129\n",
      "0.4 action:  [-0.1878, -0.9934] n_targets:  4 reward:  285.91\n",
      "1.9 action:  [0.2702, -0.9899] n_targets:  1 reward:  52.26\n",
      "25.7 action:  [-0.1319, -0.9874] n_targets:  1 reward:  51.25\n",
      "41.4 action:  [-0.0267, -0.9924] n_targets:  1 reward:  53.14\n",
      "43.3 action:  [-0.307, -0.9836] n_targets:  1 reward:  50.33\n",
      "57.8 action:  [-0.5439, -0.9892] n_targets:  1 reward:  50.54\n",
      "59.4 action:  [0.412, -0.9909] n_targets:  1 reward:  55.93\n",
      "64.2 action:  [0.1234, -0.9846] n_targets:  1 reward:  51.81\n",
      "76.0 action:  [0.379, -0.9827] n_targets:  1 reward:  54.59\n",
      "94.6 action:  [-0.1435, -0.9969] n_targets:  1 reward:  52.57\n",
      "95.5 action:  [-0.3891, -0.9894] n_targets:  1 reward:  56.82\n",
      "96.1 action:  [0.4294, -0.9897] n_targets:  1 reward:  50.01\n",
      "ALPHA (entropy-related):  tensor([0.1056], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.11586 0.11458 0.11337 0.11211 0.11092 0.10986 0.10857 0.10725 0.10635\n",
      " 0.10559]\n",
      "Episode: 119, Episode Reward: 865.166035970052\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  130\n",
      "0.1 action:  [-0.0449, -0.999] n_targets:  1 reward:  72.38\n",
      "4.2 action:  [0.058, -0.9879] n_targets:  1 reward:  52.13\n",
      "8.4 action:  [-0.0828, -0.9895] n_targets:  1 reward:  53.0\n",
      "17.0 action:  [-0.4491, -0.9995] n_targets:  1 reward:  53.22\n",
      "22.0 action:  [0.5482, -0.9955] n_targets:  1 reward:  50.31\n",
      "23.4 action:  [0.1243, -0.9992] n_targets:  1 reward:  51.15\n",
      "24.6 action:  [0.1151, -0.9829] n_targets:  1 reward:  50.87\n",
      "33.7 action:  [-0.4414, -0.9978] n_targets:  1 reward:  54.83\n",
      "50.1 action:  [-0.2497, -0.9929] n_targets:  1 reward:  50.06\n",
      "60.3 action:  [0.195, -0.9934] n_targets:  1 reward:  51.31\n",
      "66.9 action:  [-0.3066, -0.9862] n_targets:  1 reward:  50.76\n",
      "78.8 action:  [0.1907, -0.9978] n_targets:  1 reward:  50.57\n",
      "88.6 action:  [-0.3676, -0.993] n_targets:  1 reward:  50.47\n",
      "92.9 action:  [-0.1958, -0.9989] n_targets:  1 reward:  50.56\n",
      "95.8 action:  [0.6138, -0.9868] n_targets:  1 reward:  53.1\n",
      "ALPHA (entropy-related):  tensor([0.1049], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.11458 0.11337 0.11211 0.11092 0.10986 0.10857 0.10725 0.10635 0.10559\n",
      " 0.10488]\n",
      "Last 100 ALPHA: [0.61633 0.60293 0.58981 0.57698 0.56443 0.55215 0.54014 0.52839 0.5169\n",
      " 0.50566 0.49466 0.4839  0.47338 0.46308 0.45302 0.44317 0.43353 0.4241\n",
      " 0.41488 0.40586 0.39704 0.3884  0.37997 0.37171 0.36363 0.35573 0.34801\n",
      " 0.34045 0.33308 0.32586 0.31878 0.31189 0.30515 0.29854 0.29209 0.28578\n",
      " 0.27959 0.27356 0.26766 0.26192 0.25628 0.25081 0.24544 0.24022 0.23516\n",
      " 0.23025 0.22546 0.22071 0.21614 0.21183 0.20748 0.20331 0.19923 0.19545\n",
      " 0.19182 0.1884  0.18483 0.18145 0.17817 0.17481 0.17161 0.16875 0.16612\n",
      " 0.16328 0.16045 0.15774 0.15521 0.15304 0.15099 0.14874 0.14611 0.14394\n",
      " 0.14189 0.13997 0.13811 0.13644 0.13501 0.13345 0.13168 0.12991 0.12857\n",
      " 0.12711 0.12548 0.12385 0.12229 0.12085 0.1195  0.1182  0.117   0.11586\n",
      " 0.11458 0.11337 0.11211 0.11092 0.10986 0.10857 0.10725 0.10635 0.10559\n",
      " 0.10488]\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  131\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  132\n",
      "Best average reward: 0.0, Current average reward: 0.0\n",
      "Evaluation rewards: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Episode: 120, Episode Reward: 794.713409423828\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  133\n",
      "0.3 action:  [0.2669, -0.9908] n_targets:  3 reward:  179.23\n",
      "2.2 action:  [-0.3487, -0.9946] n_targets:  1 reward:  52.63\n",
      "18.6 action:  [0.006, -0.9969] n_targets:  1 reward:  50.09\n",
      "43.5 action:  [-0.3472, -0.9899] n_targets:  1 reward:  53.95\n",
      "51.9 action:  [0.5543, -0.9963] n_targets:  1 reward:  50.63\n",
      "57.3 action:  [0.5175, -0.9903] n_targets:  1 reward:  50.68\n",
      "61.0 action:  [0.0332, -0.992] n_targets:  1 reward:  51.36\n",
      "ALPHA (entropy-related):  tensor([0.1041], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.11337 0.11211 0.11092 0.10986 0.10857 0.10725 0.10635 0.10559 0.10488\n",
      " 0.10409]\n",
      "Episode: 121, Episode Reward: 488.56411743164057\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  134\n",
      "0.1 action:  [0.2361, -0.986] n_targets:  1 reward:  81.84\n",
      "0.6 action:  [0.0459, -0.9979] n_targets:  1 reward:  51.02\n",
      "30.1 action:  [0.5307, -0.9836] n_targets:  1 reward:  50.92\n",
      "85.4 action:  [-0.1421, -0.9931] n_targets:  1 reward:  50.71\n",
      "87.3 action:  [0.54, -0.9921] n_targets:  1 reward:  50.31\n",
      "ALPHA (entropy-related):  tensor([0.1034], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.11211 0.11092 0.10986 0.10857 0.10725 0.10635 0.10559 0.10488 0.10409\n",
      " 0.10341]\n",
      "Episode: 122, Episode Reward: 284.7988370259602\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  135\n",
      "0.4 action:  [-0.2219, -0.9859] n_targets:  1 reward:  83.91\n",
      "7.2 action:  [0.2788, -0.9889] n_targets:  1 reward:  50.89\n",
      "14.7 action:  [0.1107, -0.9885] n_targets:  2 reward:  122.01\n",
      "23.0 action:  [0.2645, -0.9911] n_targets:  1 reward:  53.4\n",
      "39.9 action:  [0.1393, -0.9861] n_targets:  1 reward:  50.76\n",
      "41.6 action:  [-0.2746, -0.9918] n_targets:  1 reward:  50.25\n",
      "46.2 action:  [-0.101, -0.9831] n_targets:  1 reward:  50.0\n",
      "65.0 action:  [0.108, -0.9814] n_targets:  1 reward:  53.92\n",
      "66.0 action:  [0.3741, -0.9809] n_targets:  1 reward:  53.04\n",
      "70.9 action:  [0.1821, -0.9996] n_targets:  1 reward:  52.25\n",
      "89.6 action:  [-0.0174, -0.9879] n_targets:  1 reward:  52.27\n",
      "ALPHA (entropy-related):  tensor([0.1024], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.11092 0.10986 0.10857 0.10725 0.10635 0.10559 0.10488 0.10409 0.10341\n",
      " 0.10238]\n",
      "Episode: 123, Episode Reward: 672.6996078491211\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  136\n",
      "5.3 action:  [-0.1623, -0.9864] n_targets:  1 reward:  52.83\n",
      "8.2 action:  [-0.2465, -0.988] n_targets:  1 reward:  54.47\n",
      "27.3 action:  [0.1937, -0.9857] n_targets:  1 reward:  50.59\n",
      "35.8 action:  [-0.1202, -0.987] n_targets:  1 reward:  52.22\n",
      "41.2 action:  [0.4548, -0.9837] n_targets:  2 reward:  106.53\n",
      "84.8 action:  [0.5846, -0.9948] n_targets:  1 reward:  57.27\n",
      "96.9 action:  [-0.2892, -0.9946] n_targets:  1 reward:  50.12\n",
      "101.5 action:  [0.5511, -0.981] n_targets:  1 reward:  51.43\n",
      "ALPHA (entropy-related):  tensor([0.1013], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.10986 0.10857 0.10725 0.10635 0.10559 0.10488 0.10409 0.10341 0.10238\n",
      " 0.10134]\n",
      "Episode: 124, Episode Reward: 475.4609171549479\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  137\n",
      "0.2 action:  [0.0404, -0.985] n_targets:  1 reward:  86.75\n",
      "2.2 action:  [-0.5985, -0.9997] n_targets:  1 reward:  52.76\n",
      "13.3 action:  [-0.2033, -0.985] n_targets:  1 reward:  50.25\n",
      "15.6 action:  [-0.0407, -0.9884] n_targets:  1 reward:  51.03\n",
      "19.6 action:  [-0.3221, -0.9946] n_targets:  2 reward:  126.19\n",
      "22.5 action:  [0.3649, -0.9997] n_targets:  1 reward:  54.75\n",
      "25.8 action:  [0.3588, -0.9847] n_targets:  1 reward:  57.02\n",
      "34.2 action:  [0.1654, -0.9979] n_targets:  1 reward:  53.53\n",
      "38.2 action:  [-0.218, -0.9983] n_targets:  3 reward:  182.22\n",
      "39.1 action:  [0.3268, -0.9856] n_targets:  3 reward:  187.72\n",
      "41.4 action:  [-0.4919, -0.9997] n_targets:  1 reward:  64.79\n",
      "44.7 action:  [0.0461, -0.9982] n_targets:  1 reward:  68.17\n",
      "46.5 action:  [0.1412, -0.9963] n_targets:  2 reward:  100.91\n",
      "46.7 action:  [-0.158, -0.9879] n_targets:  1 reward:  50.02\n",
      "49.2 action:  [0.1701, -0.9802] n_targets:  2 reward:  134.17\n",
      "52.3 action:  [0.3695, -0.9886] n_targets:  1 reward:  55.18\n",
      "56.8 action:  [-0.5153, -0.9995] n_targets:  1 reward:  53.87\n",
      "61.7 action:  [0.3581, -0.9925] n_targets:  1 reward:  56.19\n",
      "69.1 action:  [-0.4262, -0.9924] n_targets:  1 reward:  55.99\n",
      "70.7 action:  [-0.0943, -0.9996] n_targets:  1 reward:  50.26\n",
      "72.4 action:  [0.1404, -0.9929] n_targets:  1 reward:  56.06\n",
      "89.7 action:  [0.5549, -0.9901] n_targets:  1 reward:  72.87\n",
      "93.7 action:  [0.2921, -0.9887] n_targets:  2 reward:  110.38\n",
      "96.3 action:  [0.2781, -0.9965] n_targets:  1 reward:  57.54\n",
      "102.4 action:  [-0.2114, -0.9921] n_targets:  2 reward:  105.19\n",
      "ALPHA (entropy-related):  tensor([0.0997], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.10857 0.10725 0.10635 0.10559 0.10488 0.10409 0.10341 0.10238 0.10134\n",
      " 0.09973]\n",
      "Episode: 125, Episode Reward: 1993.801751454671\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  138\n",
      "0.4 action:  [-0.2791, -0.9901] n_targets:  2 reward:  134.07\n",
      "1.1 action:  [-0.4457, -0.9909] n_targets:  1 reward:  50.12\n",
      "2.5 action:  [0.0787, -0.9943] n_targets:  1 reward:  52.79\n",
      "11.3 action:  [-0.4458, -0.9892] n_targets:  1 reward:  52.22\n",
      "12.4 action:  [0.3626, -0.9869] n_targets:  1 reward:  56.23\n",
      "13.2 action:  [-0.4417, -0.9919] n_targets:  1 reward:  50.11\n",
      "14.0 action:  [-0.4709, -0.9992] n_targets:  1 reward:  50.17\n",
      "32.4 action:  [0.0089, -0.9935] n_targets:  1 reward:  50.41\n",
      "33.2 action:  [-0.0357, -0.9988] n_targets:  3 reward:  163.32\n",
      "39.3 action:  [-0.1242, -0.9923] n_targets:  1 reward:  50.82\n",
      "40.5 action:  [0.0871, -0.9921] n_targets:  1 reward:  52.76\n",
      "43.2 action:  [0.3273, -0.9888] n_targets:  1 reward:  70.58\n",
      "50.1 action:  [-0.3553, -0.9826] n_targets:  1 reward:  52.85\n",
      "51.4 action:  [-0.1563, -0.998] n_targets:  1 reward:  52.7\n",
      "53.5 action:  [0.1121, -0.9902] n_targets:  1 reward:  52.7\n",
      "53.9 action:  [0.172, -0.998] n_targets:  1 reward:  51.55\n",
      "59.4 action:  [0.0065, -0.9853] n_targets:  1 reward:  69.64\n",
      "64.6 action:  [0.109, -0.9959] n_targets:  1 reward:  52.72\n",
      "68.0 action:  [-0.1755, -0.998] n_targets:  2 reward:  133.87\n",
      "68.5 action:  [-0.2734, -0.9807] n_targets:  1 reward:  58.46\n",
      "77.9 action:  [0.5151, -0.9891] n_targets:  1 reward:  67.84\n",
      "80.6 action:  [0.4743, -0.9807] n_targets:  1 reward:  51.07\n",
      "87.6 action:  [-0.0074, -0.9943] n_targets:  2 reward:  130.46\n",
      "88.2 action:  [0.4352, -0.9918] n_targets:  1 reward:  50.76\n",
      "90.5 action:  [-0.3493, -0.9864] n_targets:  1 reward:  51.69\n",
      "98.8 action:  [0.0156, -0.9955] n_targets:  1 reward:  52.49\n",
      "101.8 action:  [0.5143, -0.9987] n_targets:  1 reward:  51.58\n",
      "ALPHA (entropy-related):  tensor([0.0980], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.10725 0.10635 0.10559 0.10488 0.10409 0.10341 0.10238 0.10134 0.09973\n",
      " 0.09803]\n",
      "Episode: 126, Episode Reward: 1813.965393066406\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  139\n",
      "3.1 action:  [0.0573, -0.9958] n_targets:  2 reward:  130.29\n",
      "4.8 action:  [-0.0971, -0.9991] n_targets:  1 reward:  63.75\n",
      "6.9 action:  [0.2791, -0.9859] n_targets:  1 reward:  60.63\n",
      "7.8 action:  [-0.0249, -0.9971] n_targets:  1 reward:  50.57\n",
      "10.8 action:  [0.012, -0.9961] n_targets:  1 reward:  60.99\n",
      "11.6 action:  [0.5923, -0.9987] n_targets:  1 reward:  65.86\n",
      "14.4 action:  [0.1642, -0.9931] n_targets:  1 reward:  53.72\n",
      "18.1 action:  [0.4865, -0.9979] n_targets:  1 reward:  51.31\n",
      "19.6 action:  [-0.1866, -0.9982] n_targets:  1 reward:  55.54\n",
      "21.6 action:  [0.2524, -0.9973] n_targets:  1 reward:  52.67\n",
      "36.1 action:  [-0.3027, -0.996] n_targets:  1 reward:  62.97\n",
      "38.4 action:  [-0.5241, -0.9917] n_targets:  1 reward:  60.19\n",
      "39.6 action:  [0.5012, -0.9987] n_targets:  1 reward:  54.36\n",
      "42.6 action:  [0.2604, -0.9815] n_targets:  1 reward:  57.87\n",
      "44.8 action:  [0.1798, -0.9931] n_targets:  2 reward:  111.93\n",
      "53.0 action:  [-0.4866, -0.9949] n_targets:  1 reward:  71.28\n",
      "54.7 action:  [0.5056, -0.9936] n_targets:  1 reward:  58.32\n",
      "62.4 action:  [0.2111, -0.9999] n_targets:  1 reward:  76.68\n",
      "70.3 action:  [-0.0166, -0.9943] n_targets:  1 reward:  71.45\n",
      "73.4 action:  [0.0664, -0.9809] n_targets:  1 reward:  56.77\n",
      "74.8 action:  [-0.2198, -0.9918] n_targets:  1 reward:  56.32\n",
      "75.8 action:  [0.1804, -0.9918] n_targets:  1 reward:  54.25\n",
      "83.1 action:  [0.2241, -0.9887] n_targets:  5 reward:  305.67\n",
      "83.4 action:  [-0.4977, -0.9813] n_targets:  2 reward:  111.6\n",
      "85.3 action:  [-0.4586, -0.9996] n_targets:  1 reward:  51.69\n",
      "88.6 action:  [0.2056, -0.994] n_targets:  4 reward:  264.0\n",
      "93.9 action:  [-0.249, -0.9964] n_targets:  1 reward:  51.15\n",
      "96.1 action:  [0.0972, -0.9978] n_targets:  1 reward:  67.02\n",
      "96.6 action:  [-0.3795, -0.9905] n_targets:  1 reward:  50.2\n",
      "98.4 action:  [-0.4828, -0.9958] n_targets:  1 reward:  80.57\n",
      "100.2 action:  [0.3088, -0.9911] n_targets:  1 reward:  69.52\n",
      "ALPHA (entropy-related):  tensor([0.0963], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.10635 0.10559 0.10488 0.10409 0.10341 0.10238 0.10134 0.09973 0.09803\n",
      " 0.09629]\n",
      "Episode: 127, Episode Reward: 2489.1590258280435\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  140\n",
      "0.4 action:  [0.0699, -0.9907] n_targets:  2 reward:  156.95\n",
      "1.6 action:  [0.3889, -0.9994] n_targets:  1 reward:  63.49\n",
      "2.7 action:  [0.3987, -0.9988] n_targets:  2 reward:  136.75\n",
      "4.4 action:  [-0.417, -0.9982] n_targets:  1 reward:  65.91\n",
      "5.1 action:  [0.2659, -0.9998] n_targets:  1 reward:  53.96\n",
      "7.0 action:  [-0.0353, -0.9914] n_targets:  2 reward:  106.82\n",
      "8.0 action:  [0.1122, -0.9829] n_targets:  1 reward:  74.87\n",
      "9.7 action:  [0.5273, -0.992] n_targets:  1 reward:  55.16\n",
      "11.2 action:  [-0.0234, -0.9948] n_targets:  2 reward:  162.38\n",
      "12.2 action:  [0.39, -0.9972] n_targets:  1 reward:  55.28\n",
      "14.4 action:  [0.0531, -0.9914] n_targets:  1 reward:  56.89\n",
      "17.9 action:  [-0.2461, -0.9967] n_targets:  1 reward:  52.65\n",
      "20.3 action:  [0.1554, -0.9982] n_targets:  1 reward:  58.42\n",
      "22.8 action:  [0.3806, -0.984] n_targets:  2 reward:  137.55\n",
      "24.1 action:  [-0.2662, -0.9812] n_targets:  1 reward:  59.75\n",
      "25.6 action:  [-0.534, -0.993] n_targets:  1 reward:  55.65\n",
      "27.6 action:  [-0.4418, -0.9972] n_targets:  1 reward:  50.28\n",
      "31.0 action:  [-0.4239, -0.9909] n_targets:  1 reward:  55.94\n",
      "37.5 action:  [-0.2117, -0.9839] n_targets:  1 reward:  50.11\n",
      "50.3 action:  [0.244, -0.994] n_targets:  1 reward:  52.67\n",
      "53.6 action:  [-0.0939, -0.9997] n_targets:  1 reward:  51.46\n",
      "83.4 action:  [-0.1701, -0.9834] n_targets:  1 reward:  51.34\n",
      "84.8 action:  [-0.2492, -0.9959] n_targets:  1 reward:  53.99\n",
      "92.9 action:  [0.3983, -0.9981] n_targets:  2 reward:  108.54\n",
      "93.8 action:  [-0.1575, -0.9988] n_targets:  2 reward:  118.51\n",
      "96.7 action:  [0.11, -0.9916] n_targets:  1 reward:  56.69\n",
      "ALPHA (entropy-related):  tensor([0.0948], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.10559 0.10488 0.10409 0.10341 0.10238 0.10134 0.09973 0.09803 0.09629\n",
      " 0.09479]\n",
      "Episode: 128, Episode Reward: 2002.0028635660806\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  141\n",
      "1.1 action:  [-0.255, -0.9827] n_targets:  3 reward:  204.07\n",
      "12.8 action:  [0.1971, -0.9933] n_targets:  1 reward:  63.22\n",
      "17.4 action:  [0.1757, -0.9923] n_targets:  1 reward:  57.72\n",
      "21.0 action:  [0.2417, -0.9953] n_targets:  1 reward:  53.11\n",
      "25.4 action:  [0.5247, -0.9873] n_targets:  1 reward:  64.54\n",
      "33.0 action:  [0.2935, -0.9881] n_targets:  1 reward:  53.72\n",
      "35.3 action:  [0.501, -0.9962] n_targets:  1 reward:  52.89\n",
      "38.4 action:  [0.2901, -0.9913] n_targets:  1 reward:  51.17\n",
      "38.7 action:  [0.5136, -0.9915] n_targets:  1 reward:  52.73\n",
      "40.3 action:  [-0.3589, -0.998] n_targets:  1 reward:  51.04\n",
      "45.4 action:  [0.3695, -0.9922] n_targets:  1 reward:  53.59\n",
      "47.5 action:  [0.2708, -0.9916] n_targets:  1 reward:  53.53\n",
      "54.5 action:  [0.0086, -0.9861] n_targets:  1 reward:  52.47\n",
      "55.1 action:  [0.3043, -0.9995] n_targets:  1 reward:  55.07\n",
      "80.7 action:  [-0.0037, -0.9977] n_targets:  1 reward:  66.25\n",
      "81.2 action:  [-0.2126, -0.9954] n_targets:  1 reward:  56.79\n",
      "81.6 action:  [-0.1821, -0.9902] n_targets:  1 reward:  57.8\n",
      "84.0 action:  [0.4419, -0.9893] n_targets:  2 reward:  147.34\n",
      "85.1 action:  [0.0904, -0.9884] n_targets:  1 reward:  63.76\n",
      "85.9 action:  [0.1411, -0.9943] n_targets:  1 reward:  71.64\n",
      "88.3 action:  [0.31, -0.9947] n_targets:  1 reward:  58.17\n",
      "90.1 action:  [-0.1149, -0.9891] n_targets:  1 reward:  87.21\n",
      "93.1 action:  [0.0441, -0.9956] n_targets:  1 reward:  50.57\n",
      "101.2 action:  [0.1786, -0.9967] n_targets:  2 reward:  109.64\n",
      "ALPHA (entropy-related):  tensor([0.0936], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.10488 0.10409 0.10341 0.10238 0.10134 0.09973 0.09803 0.09629 0.09479\n",
      " 0.09357]\n",
      "Episode: 129, Episode Reward: 1688.0407905578613\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  142\n",
      "0.5 action:  [0.0435, -0.9947] n_targets:  1 reward:  61.25\n",
      "4.5 action:  [0.2548, -0.9844] n_targets:  1 reward:  56.06\n",
      "9.5 action:  [-0.3136, -0.9938] n_targets:  1 reward:  50.2\n",
      "10.1 action:  [0.2613, -0.9992] n_targets:  1 reward:  51.07\n",
      "13.7 action:  [-0.3873, -0.9935] n_targets:  1 reward:  50.65\n",
      "16.1 action:  [-0.1443, -0.9987] n_targets:  1 reward:  63.97\n",
      "26.3 action:  [0.0928, -0.9929] n_targets:  1 reward:  51.72\n",
      "28.1 action:  [-0.1501, -0.9992] n_targets:  1 reward:  61.12\n",
      "28.9 action:  [-0.3138, -0.9873] n_targets:  1 reward:  50.27\n",
      "31.7 action:  [0.2892, -0.999] n_targets:  2 reward:  113.66\n",
      "52.0 action:  [-0.4433, -0.9843] n_targets:  1 reward:  59.33\n",
      "54.0 action:  [0.0932, -0.9885] n_targets:  1 reward:  50.17\n",
      "54.7 action:  [0.3036, -0.9882] n_targets:  1 reward:  50.14\n",
      "58.5 action:  [0.4783, -0.9971] n_targets:  1 reward:  50.13\n",
      "60.3 action:  [0.0911, -0.9949] n_targets:  1 reward:  66.94\n",
      "63.5 action:  [-0.6286, -0.9974] n_targets:  1 reward:  59.0\n",
      "66.2 action:  [0.6388, -0.9971] n_targets:  1 reward:  70.71\n",
      "67.8 action:  [0.248, -0.9878] n_targets:  1 reward:  62.17\n",
      "72.5 action:  [0.0665, -0.983] n_targets:  1 reward:  94.77\n",
      "73.4 action:  [0.0285, -0.9976] n_targets:  1 reward:  60.28\n",
      "75.2 action:  [0.3798, -0.9991] n_targets:  1 reward:  53.32\n",
      "77.2 action:  [-0.5274, -0.9906] n_targets:  1 reward:  51.73\n",
      "79.1 action:  [0.1752, -0.9988] n_targets:  2 reward:  134.43\n",
      "89.4 action:  [0.275, -0.9894] n_targets:  2 reward:  141.63\n",
      "92.3 action:  [0.3687, -0.9998] n_targets:  1 reward:  51.33\n",
      "93.4 action:  [0.0618, -0.9935] n_targets:  1 reward:  61.56\n",
      "94.5 action:  [-0.4944, -0.998] n_targets:  1 reward:  52.7\n",
      "97.2 action:  [0.5168, -0.9888] n_targets:  2 reward:  115.17\n",
      "ALPHA (entropy-related):  tensor([0.0923], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.10409 0.10341 0.10238 0.10134 0.09973 0.09803 0.09629 0.09479 0.09357\n",
      " 0.09231]\n",
      "Episode: 130, Episode Reward: 1895.5089801152546\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  143\n",
      "1.5 action:  [0.0072, -0.9958] n_targets:  1 reward:  59.81\n",
      "1.9 action:  [-0.0053, -0.9996] n_targets:  1 reward:  60.61\n",
      "3.5 action:  [-0.0899, -0.9996] n_targets:  1 reward:  63.95\n",
      "7.5 action:  [0.1749, -0.9866] n_targets:  1 reward:  60.78\n",
      "9.1 action:  [-0.1456, -0.9807] n_targets:  2 reward:  108.07\n",
      "9.9 action:  [0.2703, -0.9952] n_targets:  1 reward:  51.29\n",
      "13.4 action:  [-0.0563, -0.9996] n_targets:  1 reward:  52.92\n",
      "13.9 action:  [0.183, -0.9968] n_targets:  1 reward:  50.41\n",
      "21.7 action:  [-0.5847, -0.9988] n_targets:  1 reward:  66.76\n",
      "22.5 action:  [-0.0755, -0.9976] n_targets:  1 reward:  60.3\n",
      "22.9 action:  [-0.6356, -0.9994] n_targets:  1 reward:  58.57\n",
      "26.8 action:  [0.1142, -0.9999] n_targets:  3 reward:  237.5\n",
      "28.1 action:  [-0.1759, -0.9978] n_targets:  2 reward:  152.68\n",
      "37.6 action:  [0.1745, -0.9974] n_targets:  3 reward:  199.67\n",
      "39.7 action:  [-0.5607, -0.9854] n_targets:  1 reward:  69.24\n",
      "40.4 action:  [0.0213, -0.9941] n_targets:  1 reward:  77.93\n",
      "41.5 action:  [-0.2914, -0.9985] n_targets:  2 reward:  113.28\n",
      "42.4 action:  [0.5153, -0.9913] n_targets:  1 reward:  60.23\n",
      "43.2 action:  [0.4213, -0.9989] n_targets:  1 reward:  62.57\n",
      "44.3 action:  [-0.4914, -0.9922] n_targets:  1 reward:  56.48\n",
      "45.3 action:  [0.3825, -0.9993] n_targets:  1 reward:  59.12\n",
      "45.9 action:  [0.1309, -0.9988] n_targets:  1 reward:  83.49\n",
      "46.5 action:  [0.1891, -0.9924] n_targets:  3 reward:  174.26\n",
      "47.6 action:  [0.545, -0.9868] n_targets:  1 reward:  66.19\n",
      "48.6 action:  [-0.3861, -0.9981] n_targets:  1 reward:  70.01\n",
      "49.3 action:  [-0.3076, -0.9913] n_targets:  2 reward:  112.15\n",
      "49.9 action:  [-0.2311, -0.9938] n_targets:  1 reward:  51.05\n",
      "53.6 action:  [0.3557, -0.9988] n_targets:  1 reward:  50.46\n",
      "66.8 action:  [0.4585, -0.9999] n_targets:  1 reward:  53.65\n",
      "70.8 action:  [0.0534, -0.9995] n_targets:  1 reward:  62.41\n",
      "71.4 action:  [-0.1485, -1.0] n_targets:  1 reward:  51.76\n",
      "72.4 action:  [-0.528, -0.9999] n_targets:  1 reward:  54.75\n",
      "74.0 action:  [-0.4396, -0.9995] n_targets:  1 reward:  88.6\n",
      "74.6 action:  [-0.1055, -0.9996] n_targets:  1 reward:  76.1\n",
      "75.8 action:  [0.2135, -0.9992] n_targets:  1 reward:  91.6\n",
      "76.2 action:  [-0.499, -1.0] n_targets:  1 reward:  53.83\n",
      "77.5 action:  [0.4048, -0.9919] n_targets:  1 reward:  76.91\n",
      "80.2 action:  [0.5052, -1.0] n_targets:  1 reward:  58.66\n",
      "80.6 action:  [0.0016, -0.9972] n_targets:  1 reward:  63.7\n",
      "81.0 action:  [0.1454, -0.998] n_targets:  1 reward:  53.46\n",
      "81.5 action:  [-0.4296, -0.9957] n_targets:  1 reward:  61.85\n",
      "83.1 action:  [0.3002, -1.0] n_targets:  2 reward:  115.61\n",
      "85.4 action:  [0.2157, -0.9955] n_targets:  1 reward:  66.63\n",
      "86.7 action:  [-0.0465, -0.9937] n_targets:  2 reward:  129.14\n",
      "88.1 action:  [-0.4814, -0.9966] n_targets:  1 reward:  55.79\n",
      "88.7 action:  [0.1298, -0.9989] n_targets:  1 reward:  59.0\n",
      "89.6 action:  [-0.2094, -0.9995] n_targets:  2 reward:  115.26\n",
      "90.1 action:  [-0.0434, -0.9998] n_targets:  3 reward:  161.82\n",
      "91.4 action:  [-0.4821, -1.0] n_targets:  1 reward:  59.63\n",
      "91.7 action:  [-0.1585, -0.9902] n_targets:  1 reward:  55.75\n",
      "94.5 action:  [-0.3478, -1.0] n_targets:  1 reward:  51.11\n",
      "95.5 action:  [0.2349, -0.9996] n_targets:  1 reward:  50.45\n",
      "95.9 action:  [-0.325, -0.9997] n_targets:  1 reward:  58.6\n",
      "ALPHA (entropy-related):  tensor([0.0908], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.10341 0.10238 0.10134 0.09973 0.09803 0.09629 0.09479 0.09357 0.09231\n",
      " 0.09078]\n",
      "Episode: 131, Episode Reward: 4215.8641929626465\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  144\n",
      "0.1 action:  [0.5914, -0.9917] n_targets:  1 reward:  51.94\n",
      "2.8 action:  [0.1796, -0.9935] n_targets:  1 reward:  52.26\n",
      "4.9 action:  [-0.4057, -0.9979] n_targets:  1 reward:  58.8\n",
      "6.0 action:  [-0.1942, -0.9927] n_targets:  2 reward:  137.18\n",
      "7.4 action:  [-0.3857, -0.9967] n_targets:  1 reward:  60.36\n",
      "10.6 action:  [-0.3829, -0.9997] n_targets:  3 reward:  186.1\n",
      "12.2 action:  [-0.0113, -0.9894] n_targets:  1 reward:  57.66\n",
      "14.4 action:  [-0.3751, -1.0] n_targets:  1 reward:  50.78\n",
      "39.7 action:  [0.0571, -0.9886] n_targets:  1 reward:  51.25\n",
      "52.5 action:  [-0.0559, -0.9945] n_targets:  2 reward:  111.84\n",
      "56.1 action:  [-0.2845, -0.998] n_targets:  1 reward:  59.84\n",
      "57.2 action:  [-0.463, -1.0] n_targets:  1 reward:  54.32\n",
      "58.0 action:  [-0.3581, -0.9854] n_targets:  1 reward:  56.13\n",
      "58.2 action:  [-0.1764, -0.9992] n_targets:  1 reward:  58.78\n",
      "59.3 action:  [-0.1598, -0.9967] n_targets:  1 reward:  51.58\n",
      "63.8 action:  [0.2364, -0.9952] n_targets:  1 reward:  65.09\n",
      "70.8 action:  [0.3973, -0.9948] n_targets:  1 reward:  58.53\n",
      "73.5 action:  [-0.0623, -0.9953] n_targets:  1 reward:  57.92\n",
      "74.6 action:  [-0.0133, -0.9996] n_targets:  2 reward:  135.68\n",
      "78.9 action:  [-0.3542, -0.9999] n_targets:  1 reward:  52.64\n",
      "80.4 action:  [0.0616, -0.9891] n_targets:  3 reward:  247.4\n",
      "81.8 action:  [-0.4419, -0.9982] n_targets:  1 reward:  53.08\n",
      "82.1 action:  [0.0715, -0.9812] n_targets:  1 reward:  55.02\n",
      "87.2 action:  [0.2363, -0.9999] n_targets:  1 reward:  58.0\n",
      "94.0 action:  [0.5317, -0.993] n_targets:  1 reward:  50.08\n",
      "96.6 action:  [-0.0812, -0.9983] n_targets:  1 reward:  70.4\n",
      "100.1 action:  [0.0622, -0.9911] n_targets:  1 reward:  50.54\n",
      "102.1 action:  [-0.3737, -0.9969] n_targets:  1 reward:  52.0\n",
      "ALPHA (entropy-related):  tensor([0.0894], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.10238 0.10134 0.09973 0.09803 0.09629 0.09479 0.09357 0.09231 0.09078\n",
      " 0.08945]\n",
      "Episode: 132, Episode Reward: 2105.2005513509116\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  145\n",
      "11.6 action:  [-0.0769, -0.994] n_targets:  1 reward:  53.47\n",
      "30.4 action:  [-0.0011, -0.9857] n_targets:  1 reward:  66.53\n",
      "38.8 action:  [-0.0527, -0.9929] n_targets:  1 reward:  55.4\n",
      "44.8 action:  [0.1196, -0.9923] n_targets:  1 reward:  51.97\n",
      "46.0 action:  [-0.1057, -0.9915] n_targets:  1 reward:  53.53\n",
      "51.8 action:  [-0.1589, -0.9911] n_targets:  1 reward:  54.2\n",
      "56.6 action:  [-0.2279, -0.9998] n_targets:  1 reward:  50.68\n",
      "61.4 action:  [0.2579, -0.9883] n_targets:  1 reward:  50.33\n",
      "64.6 action:  [-0.1648, -0.9996] n_targets:  1 reward:  62.72\n",
      "76.1 action:  [0.1724, -0.9899] n_targets:  1 reward:  55.0\n",
      "77.9 action:  [0.22, -0.998] n_targets:  1 reward:  61.08\n",
      "81.1 action:  [0.014, -0.9978] n_targets:  3 reward:  166.46\n",
      "82.7 action:  [0.0603, -0.9804] n_targets:  1 reward:  50.48\n",
      "82.9 action:  [-0.212, -0.9868] n_targets:  1 reward:  51.94\n",
      "86.2 action:  [0.1794, -0.9908] n_targets:  1 reward:  55.17\n",
      "92.9 action:  [-0.034, -0.9936] n_targets:  1 reward:  59.1\n",
      "96.9 action:  [0.1406, -0.998] n_targets:  1 reward:  50.12\n",
      "101.2 action:  [0.0218, -0.997] n_targets:  1 reward:  53.26\n",
      "ALPHA (entropy-related):  tensor([0.0884], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.10134 0.09973 0.09803 0.09629 0.09479 0.09357 0.09231 0.09078 0.08945\n",
      " 0.08837]\n",
      "Episode: 133, Episode Reward: 1101.4500452677407\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  146\n",
      "0.1 action:  [0.54, -0.9959] n_targets:  1 reward:  65.88\n",
      "1.9 action:  [0.5233, -0.999] n_targets:  1 reward:  50.8\n",
      "3.3 action:  [-0.1679, -0.9951] n_targets:  1 reward:  52.4\n",
      "23.6 action:  [-0.2899, -0.9951] n_targets:  1 reward:  51.99\n",
      "23.8 action:  [-0.4429, -0.9837] n_targets:  1 reward:  70.75\n",
      "43.8 action:  [0.0422, -0.9898] n_targets:  1 reward:  54.18\n",
      "49.3 action:  [0.3783, -0.9812] n_targets:  1 reward:  50.34\n",
      "52.0 action:  [0.0196, -0.9912] n_targets:  1 reward:  55.67\n",
      "54.2 action:  [0.0847, -0.9983] n_targets:  1 reward:  50.46\n",
      "55.6 action:  [-0.1108, -0.9999] n_targets:  1 reward:  56.32\n",
      "64.2 action:  [0.2515, -0.9924] n_targets:  1 reward:  51.97\n",
      "71.7 action:  [0.3108, -0.9874] n_targets:  1 reward:  55.07\n",
      "76.1 action:  [-0.1195, -0.9926] n_targets:  1 reward:  51.65\n",
      "81.0 action:  [-0.0069, -0.9907] n_targets:  1 reward:  50.33\n",
      "102.0 action:  [-0.5014, -0.9967] n_targets:  1 reward:  51.65\n",
      "ALPHA (entropy-related):  tensor([0.0876], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.09973 0.09803 0.09629 0.09479 0.09357 0.09231 0.09078 0.08945 0.08837\n",
      " 0.08758]\n",
      "Episode: 134, Episode Reward: 819.4783706665037\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  147\n",
      "0.2 action:  [-0.0493, -0.9984] n_targets:  1 reward:  65.47\n",
      "2.2 action:  [-0.0624, -0.9972] n_targets:  1 reward:  50.76\n",
      "9.7 action:  [-0.1172, -0.9916] n_targets:  1 reward:  53.46\n",
      "14.7 action:  [0.2359, -0.9989] n_targets:  1 reward:  53.3\n",
      "20.8 action:  [0.1673, -0.9877] n_targets:  1 reward:  54.69\n",
      "27.3 action:  [-0.0707, -0.9995] n_targets:  1 reward:  50.41\n",
      "30.6 action:  [0.0175, -0.9842] n_targets:  2 reward:  102.04\n",
      "31.7 action:  [0.0378, -0.9828] n_targets:  1 reward:  54.37\n",
      "34.4 action:  [-0.3256, -0.9857] n_targets:  1 reward:  52.84\n",
      "35.2 action:  [0.0677, -0.9934] n_targets:  1 reward:  60.71\n",
      "40.8 action:  [-0.0429, -0.9909] n_targets:  1 reward:  50.63\n",
      "44.4 action:  [0.581, -0.9996] n_targets:  1 reward:  64.99\n",
      "47.5 action:  [0.3195, -0.9895] n_targets:  1 reward:  52.9\n",
      "48.1 action:  [0.1769, -0.9864] n_targets:  1 reward:  54.97\n",
      "55.3 action:  [0.0632, -0.9972] n_targets:  1 reward:  50.97\n",
      "58.8 action:  [-0.2188, -0.9976] n_targets:  1 reward:  51.33\n",
      "61.6 action:  [-0.1867, -0.9984] n_targets:  1 reward:  55.5\n",
      "71.3 action:  [-0.0241, -0.9953] n_targets:  1 reward:  50.29\n",
      "85.5 action:  [-0.268, -0.9958] n_targets:  3 reward:  173.03\n",
      "87.1 action:  [-0.1816, -0.9916] n_targets:  1 reward:  54.98\n",
      "95.8 action:  [-0.3232, -0.9983] n_targets:  1 reward:  52.08\n",
      "98.0 action:  [0.0747, -0.9838] n_targets:  1 reward:  57.32\n",
      "ALPHA (entropy-related):  tensor([0.0869], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.09803 0.09629 0.09479 0.09357 0.09231 0.09078 0.08945 0.08837 0.08758\n",
      " 0.08692]\n",
      "Episode: 135, Episode Reward: 1367.0463790893555\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  148\n",
      "0.6 action:  [0.0767, -0.9997] n_targets:  2 reward:  123.89\n",
      "6.8 action:  [-0.1225, -0.9984] n_targets:  1 reward:  60.83\n",
      "25.6 action:  [-0.2919, -0.9892] n_targets:  1 reward:  51.21\n",
      "34.2 action:  [-0.1042, -0.9853] n_targets:  1 reward:  53.92\n",
      "41.6 action:  [0.1777, -1.0] n_targets:  1 reward:  57.15\n",
      "42.6 action:  [-0.0253, -0.9992] n_targets:  1 reward:  50.07\n",
      "48.6 action:  [0.0391, -0.9991] n_targets:  1 reward:  50.36\n",
      "51.3 action:  [-0.0926, -0.9844] n_targets:  1 reward:  58.25\n",
      "51.9 action:  [0.3036, -0.9985] n_targets:  1 reward:  50.41\n",
      "52.8 action:  [0.2314, -0.9991] n_targets:  1 reward:  51.94\n",
      "55.2 action:  [0.0853, -0.9944] n_targets:  1 reward:  54.77\n",
      "56.4 action:  [0.4183, -0.9863] n_targets:  1 reward:  52.07\n",
      "56.9 action:  [0.6023, -0.9979] n_targets:  1 reward:  64.9\n",
      "58.5 action:  [-0.284, -0.9907] n_targets:  1 reward:  53.63\n",
      "63.7 action:  [0.5127, -0.9983] n_targets:  2 reward:  103.14\n",
      "64.1 action:  [-0.0962, -0.9942] n_targets:  1 reward:  52.81\n",
      "74.4 action:  [-0.1128, -0.9978] n_targets:  1 reward:  50.26\n",
      "75.1 action:  [0.3908, -0.9991] n_targets:  1 reward:  57.74\n",
      "77.5 action:  [0.1106, -0.9991] n_targets:  2 reward:  103.9\n",
      "78.4 action:  [-0.1148, -0.9992] n_targets:  1 reward:  55.32\n",
      "78.7 action:  [0.4633, -0.9953] n_targets:  2 reward:  113.64\n",
      "81.4 action:  [-0.1496, -0.9875] n_targets:  1 reward:  57.12\n",
      "82.5 action:  [-0.2242, -0.9991] n_targets:  1 reward:  55.42\n",
      "84.5 action:  [0.3165, -0.9982] n_targets:  1 reward:  56.06\n",
      "84.7 action:  [-0.3073, -0.9955] n_targets:  1 reward:  51.53\n",
      "85.3 action:  [-0.07, -0.9988] n_targets:  1 reward:  55.01\n",
      "86.8 action:  [-0.4303, -0.9977] n_targets:  1 reward:  50.8\n",
      "88.3 action:  [-0.0979, -0.9971] n_targets:  2 reward:  108.45\n",
      "91.8 action:  [0.1712, -0.9971] n_targets:  1 reward:  56.95\n",
      "92.0 action:  [-0.1562, -0.9967] n_targets:  1 reward:  52.47\n",
      "93.2 action:  [-0.0044, -0.9887] n_targets:  1 reward:  55.02\n",
      "94.3 action:  [-0.1395, -0.9975] n_targets:  1 reward:  52.64\n",
      "ALPHA (entropy-related):  tensor([0.0861], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.09629 0.09479 0.09357 0.09231 0.09078 0.08945 0.08837 0.08758 0.08692\n",
      " 0.08612]\n",
      "Episode: 136, Episode Reward: 2021.7227986653643\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  149\n",
      "0.3 action:  [0.2523, -0.9984] n_targets:  2 reward:  184.65\n",
      "5.0 action:  [0.043, -0.9895] n_targets:  1 reward:  59.65\n",
      "5.5 action:  [-0.2682, -0.9955] n_targets:  1 reward:  52.38\n",
      "5.9 action:  [0.3127, -0.9988] n_targets:  1 reward:  54.77\n",
      "6.5 action:  [-0.1454, -0.9978] n_targets:  1 reward:  53.2\n",
      "6.7 action:  [-0.3178, -0.9992] n_targets:  1 reward:  53.28\n",
      "8.5 action:  [-0.0018, -0.9818] n_targets:  1 reward:  50.66\n",
      "13.6 action:  [-0.0615, -0.9971] n_targets:  1 reward:  54.17\n",
      "14.1 action:  [0.405, -0.9931] n_targets:  1 reward:  58.68\n",
      "17.3 action:  [-0.119, -0.9895] n_targets:  1 reward:  52.6\n",
      "25.9 action:  [0.034, -0.9977] n_targets:  1 reward:  50.16\n",
      "32.7 action:  [0.0329, -0.9997] n_targets:  1 reward:  52.98\n",
      "45.4 action:  [-0.444, -0.9875] n_targets:  1 reward:  50.4\n",
      "54.8 action:  [-0.1247, -0.9945] n_targets:  1 reward:  68.65\n",
      "56.2 action:  [0.0151, -0.9931] n_targets:  1 reward:  60.87\n",
      "57.8 action:  [0.0767, -0.9904] n_targets:  1 reward:  51.58\n",
      "58.0 action:  [0.5588, -0.9977] n_targets:  1 reward:  59.69\n",
      "58.8 action:  [-0.3634, -0.9972] n_targets:  1 reward:  52.53\n",
      "59.6 action:  [0.0393, -0.9865] n_targets:  1 reward:  56.49\n",
      "60.2 action:  [0.015, -0.9951] n_targets:  1 reward:  53.63\n",
      "61.7 action:  [0.0123, -0.9961] n_targets:  1 reward:  51.21\n",
      "63.4 action:  [-0.234, -0.9951] n_targets:  1 reward:  67.45\n",
      "72.1 action:  [-0.2565, -0.9894] n_targets:  1 reward:  65.94\n",
      "74.2 action:  [-0.0505, -0.9939] n_targets:  2 reward:  112.83\n",
      "77.5 action:  [0.396, -0.9941] n_targets:  1 reward:  50.51\n",
      "90.5 action:  [-0.0909, -0.994] n_targets:  1 reward:  51.36\n",
      "90.7 action:  [-0.4001, -0.9928] n_targets:  1 reward:  55.78\n",
      "90.9 action:  [0.2713, -0.9964] n_targets:  1 reward:  53.63\n",
      "93.5 action:  [-0.2103, -0.9944] n_targets:  1 reward:  56.75\n",
      "97.4 action:  [0.4402, -0.9984] n_targets:  1 reward:  51.14\n",
      "99.3 action:  [-0.2105, -0.9898] n_targets:  1 reward:  51.28\n",
      "ALPHA (entropy-related):  tensor([0.0854], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.09479 0.09357 0.09231 0.09078 0.08945 0.08837 0.08758 0.08692 0.08612\n",
      " 0.08535]\n",
      "Episode: 137, Episode Reward: 1898.9037551879878\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  150\n",
      "7.3 action:  [0.0871, -0.9946] n_targets:  1 reward:  56.39\n",
      "7.6 action:  [-0.209, -0.9964] n_targets:  1 reward:  58.45\n",
      "9.4 action:  [-0.0365, -0.9909] n_targets:  2 reward:  119.95\n",
      "10.7 action:  [-0.0176, -0.99] n_targets:  1 reward:  58.25\n",
      "11.1 action:  [-0.0247, -0.9905] n_targets:  1 reward:  50.52\n",
      "12.1 action:  [-0.3933, -0.9931] n_targets:  1 reward:  50.11\n",
      "12.6 action:  [-0.0962, -0.9963] n_targets:  1 reward:  56.06\n",
      "14.6 action:  [-0.4188, -0.9856] n_targets:  1 reward:  51.6\n",
      "15.2 action:  [-0.1351, -0.9831] n_targets:  1 reward:  56.0\n",
      "16.0 action:  [-0.4398, -0.9972] n_targets:  1 reward:  51.6\n",
      "17.8 action:  [-0.042, -0.9862] n_targets:  2 reward:  106.84\n",
      "24.5 action:  [0.0558, -0.9952] n_targets:  1 reward:  74.05\n",
      "26.1 action:  [-0.431, -0.9828] n_targets:  1 reward:  64.35\n",
      "26.9 action:  [0.2142, -0.9833] n_targets:  1 reward:  50.67\n",
      "27.6 action:  [-0.319, -0.9912] n_targets:  1 reward:  54.64\n",
      "30.3 action:  [0.196, -0.9949] n_targets:  1 reward:  52.29\n",
      "32.5 action:  [0.0738, -0.9944] n_targets:  2 reward:  116.29\n",
      "32.9 action:  [0.0129, -0.9854] n_targets:  1 reward:  65.52\n",
      "34.1 action:  [0.0873, -0.9943] n_targets:  1 reward:  50.34\n",
      "34.8 action:  [0.2636, -0.9917] n_targets:  1 reward:  51.44\n",
      "37.9 action:  [-0.2585, -0.9968] n_targets:  1 reward:  50.24\n",
      "39.9 action:  [-0.4036, -0.9899] n_targets:  1 reward:  52.1\n",
      "41.2 action:  [0.4583, -0.9907] n_targets:  1 reward:  50.77\n",
      "46.0 action:  [0.1881, -0.985] n_targets:  1 reward:  54.86\n",
      "46.4 action:  [-0.2008, -0.9903] n_targets:  1 reward:  63.18\n",
      "46.8 action:  [0.2403, -0.9952] n_targets:  2 reward:  105.1\n",
      "50.4 action:  [-0.0204, -0.9858] n_targets:  2 reward:  115.74\n",
      "56.4 action:  [-0.5165, -0.9867] n_targets:  1 reward:  52.83\n",
      "61.7 action:  [0.4644, -0.9848] n_targets:  1 reward:  56.98\n",
      "63.7 action:  [0.4453, -0.9879] n_targets:  1 reward:  52.92\n",
      "67.7 action:  [0.2337, -0.9986] n_targets:  1 reward:  53.14\n",
      "68.3 action:  [0.3795, -0.9939] n_targets:  1 reward:  58.91\n",
      "68.7 action:  [-0.277, -0.9959] n_targets:  2 reward:  132.4\n",
      "70.3 action:  [0.3901, -0.9965] n_targets:  1 reward:  57.16\n",
      "72.1 action:  [-0.1001, -0.9983] n_targets:  1 reward:  53.84\n",
      "72.9 action:  [-0.1337, -0.9974] n_targets:  1 reward:  60.68\n",
      "76.6 action:  [-0.3338, -0.9953] n_targets:  1 reward:  65.41\n",
      "77.6 action:  [0.2403, -0.9945] n_targets:  1 reward:  58.1\n",
      "78.1 action:  [0.1649, -0.9961] n_targets:  1 reward:  55.02\n",
      "78.3 action:  [0.0772, -0.9989] n_targets:  2 reward:  110.85\n",
      "79.2 action:  [-0.5505, -0.9961] n_targets:  1 reward:  58.38\n",
      "80.2 action:  [0.2866, -0.9945] n_targets:  1 reward:  60.35\n",
      "83.2 action:  [0.071, -0.9845] n_targets:  1 reward:  64.11\n",
      "85.8 action:  [0.5201, -0.9888] n_targets:  1 reward:  52.17\n",
      "89.7 action:  [0.2454, -0.9991] n_targets:  1 reward:  50.15\n",
      "90.3 action:  [0.0177, -0.9866] n_targets:  1 reward:  51.73\n",
      "94.5 action:  [-0.1783, -0.9887] n_targets:  1 reward:  50.59\n",
      "94.7 action:  [-0.5572, -0.9966] n_targets:  1 reward:  52.56\n",
      "94.9 action:  [0.3262, -0.9963] n_targets:  1 reward:  58.22\n",
      "97.4 action:  [0.1445, -0.9988] n_targets:  1 reward:  56.72\n",
      "99.9 action:  [-0.0855, -0.9874] n_targets:  1 reward:  74.56\n",
      "100.1 action:  [0.2915, -0.9908] n_targets:  1 reward:  58.46\n",
      "101.6 action:  [-0.1372, -0.9971] n_targets:  2 reward:  119.34\n",
      "ALPHA (entropy-related):  tensor([0.0847], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.09357 0.09231 0.09078 0.08945 0.08837 0.08758 0.08692 0.08612 0.08535\n",
      " 0.08469]\n",
      "Episode: 138, Episode Reward: 3462.9093653361\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  151\n",
      "0.6 action:  [0.4658, -0.9857] n_targets:  1 reward:  63.5\n",
      "1.2 action:  [0.2716, -0.998] n_targets:  1 reward:  52.5\n",
      "2.6 action:  [0.5453, -0.9985] n_targets:  1 reward:  50.22\n",
      "3.4 action:  [-0.3163, -0.9989] n_targets:  1 reward:  55.82\n",
      "4.6 action:  [0.3284, -0.9947] n_targets:  1 reward:  52.39\n",
      "5.4 action:  [0.3453, -0.9996] n_targets:  1 reward:  51.19\n",
      "9.0 action:  [-0.1051, -0.9992] n_targets:  2 reward:  114.18\n",
      "12.4 action:  [0.3246, -0.9978] n_targets:  1 reward:  56.09\n",
      "12.6 action:  [0.362, -0.9938] n_targets:  1 reward:  54.91\n",
      "16.8 action:  [0.1536, -0.9811] n_targets:  1 reward:  51.15\n",
      "17.4 action:  [0.2516, -0.9953] n_targets:  1 reward:  55.76\n",
      "21.6 action:  [-0.2801, -0.999] n_targets:  1 reward:  60.6\n",
      "23.2 action:  [-0.0137, -0.9996] n_targets:  1 reward:  66.17\n",
      "23.4 action:  [0.1291, -0.9966] n_targets:  1 reward:  56.04\n",
      "25.0 action:  [0.0545, -0.9989] n_targets:  2 reward:  115.62\n",
      "25.6 action:  [-0.5022, -0.9989] n_targets:  1 reward:  59.28\n",
      "27.4 action:  [0.3822, -0.992] n_targets:  1 reward:  50.45\n",
      "27.6 action:  [0.006, -0.9976] n_targets:  1 reward:  54.88\n",
      "30.4 action:  [0.0923, -0.9994] n_targets:  1 reward:  53.25\n",
      "31.0 action:  [0.0932, -0.9965] n_targets:  1 reward:  51.35\n",
      "31.4 action:  [0.1271, -0.9895] n_targets:  1 reward:  57.27\n",
      "32.8 action:  [-0.1831, -0.997] n_targets:  1 reward:  56.29\n",
      "34.0 action:  [0.5289, -0.9999] n_targets:  1 reward:  55.5\n",
      "35.2 action:  [0.2789, -0.9998] n_targets:  2 reward:  121.15\n",
      "39.4 action:  [-0.2366, -0.9944] n_targets:  1 reward:  55.29\n",
      "40.8 action:  [0.1394, -0.9971] n_targets:  1 reward:  70.03\n",
      "41.2 action:  [0.3533, -0.9998] n_targets:  2 reward:  140.14\n",
      "42.2 action:  [0.231, -0.9993] n_targets:  1 reward:  50.62\n",
      "43.8 action:  [0.1425, -0.9982] n_targets:  1 reward:  59.29\n",
      "45.4 action:  [0.3727, -0.9999] n_targets:  1 reward:  51.63\n",
      "48.0 action:  [0.2635, -0.998] n_targets:  1 reward:  51.93\n",
      "48.4 action:  [0.0963, -0.9997] n_targets:  1 reward:  54.5\n",
      "49.2 action:  [-0.2211, -0.9978] n_targets:  1 reward:  57.6\n",
      "49.4 action:  [-0.0303, -0.9993] n_targets:  3 reward:  162.74\n",
      "49.6 action:  [0.3017, -0.9958] n_targets:  1 reward:  50.79\n",
      "50.0 action:  [0.1094, -0.9979] n_targets:  1 reward:  50.76\n",
      "51.6 action:  [0.2938, -0.9989] n_targets:  2 reward:  111.88\n",
      "54.0 action:  [0.0509, -0.9937] n_targets:  1 reward:  51.5\n",
      "59.8 action:  [0.2848, -0.99] n_targets:  1 reward:  54.27\n",
      "63.8 action:  [0.3394, -0.9862] n_targets:  1 reward:  58.88\n",
      "64.0 action:  [-0.0413, -0.9983] n_targets:  1 reward:  62.35\n",
      "65.0 action:  [0.2114, -0.9993] n_targets:  1 reward:  51.64\n",
      "65.4 action:  [0.1574, -0.9955] n_targets:  1 reward:  58.64\n",
      "65.8 action:  [-0.0941, -0.9889] n_targets:  1 reward:  57.77\n",
      "66.2 action:  [0.1299, -0.9979] n_targets:  1 reward:  51.55\n",
      "67.2 action:  [0.3294, -0.9976] n_targets:  1 reward:  51.75\n",
      "67.4 action:  [0.4336, -0.9993] n_targets:  1 reward:  50.34\n",
      "72.2 action:  [-0.0999, -0.9881] n_targets:  1 reward:  56.4\n",
      "73.8 action:  [-0.1171, -0.9997] n_targets:  1 reward:  78.93\n",
      "77.2 action:  [-0.175, -0.998] n_targets:  1 reward:  55.3\n",
      "81.4 action:  [-0.2105, -0.9937] n_targets:  1 reward:  51.04\n",
      "84.0 action:  [-0.1464, -0.9945] n_targets:  1 reward:  54.58\n",
      "84.8 action:  [0.1197, -0.998] n_targets:  2 reward:  111.82\n",
      "85.2 action:  [-0.1645, -0.9886] n_targets:  1 reward:  66.43\n",
      "87.6 action:  [0.1932, -0.9901] n_targets:  1 reward:  50.23\n",
      "88.6 action:  [-0.0406, -0.9921] n_targets:  1 reward:  59.8\n",
      "94.0 action:  [-0.359, -0.9936] n_targets:  1 reward:  53.36\n",
      "95.2 action:  [-0.0773, -0.9831] n_targets:  1 reward:  50.03\n",
      "96.0 action:  [-0.1704, -0.9849] n_targets:  2 reward:  133.74\n",
      "96.4 action:  [-0.068, -0.9978] n_targets:  1 reward:  55.31\n",
      "99.6 action:  [0.0553, -0.9985] n_targets:  1 reward:  58.07\n",
      "99.8 action:  [0.1764, -0.9847] n_targets:  1 reward:  51.3\n",
      "100.2 action:  [-0.0981, -0.9911] n_targets:  1 reward:  52.98\n",
      "100.4 action:  [-0.0633, -0.9885] n_targets:  1 reward:  57.45\n",
      "100.6 action:  [-0.3442, -0.9952] n_targets:  1 reward:  51.76\n",
      "ALPHA (entropy-related):  tensor([0.0841], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.09231 0.09078 0.08945 0.08837 0.08758 0.08692 0.08612 0.08535 0.08469\n",
      " 0.08412]\n",
      "Episode: 139, Episode Reward: 4179.962830861409\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  152\n",
      "0.5 action:  [-0.3563, -0.9861] n_targets:  1 reward:  55.33\n",
      "2.1 action:  [-0.2428, -0.9964] n_targets:  1 reward:  55.05\n",
      "3.3 action:  [-0.2172, -0.9825] n_targets:  2 reward:  134.16\n",
      "4.7 action:  [-0.2076, -0.9966] n_targets:  1 reward:  53.78\n",
      "5.3 action:  [0.1124, -0.999] n_targets:  1 reward:  52.57\n",
      "6.5 action:  [-0.0509, -0.9884] n_targets:  1 reward:  56.16\n",
      "7.3 action:  [-0.1941, -0.9816] n_targets:  2 reward:  122.66\n",
      "7.5 action:  [-0.3413, -0.994] n_targets:  1 reward:  54.37\n",
      "8.5 action:  [0.0757, -0.9893] n_targets:  1 reward:  58.44\n",
      "10.7 action:  [0.2173, -0.9874] n_targets:  1 reward:  56.68\n",
      "14.9 action:  [-0.0354, -0.9818] n_targets:  2 reward:  118.81\n",
      "15.1 action:  [0.0917, -0.9855] n_targets:  2 reward:  105.15\n",
      "17.1 action:  [-0.15, -0.9876] n_targets:  1 reward:  52.85\n",
      "17.3 action:  [-0.3585, -0.9887] n_targets:  1 reward:  61.42\n",
      "17.7 action:  [-0.1033, -0.9916] n_targets:  1 reward:  50.82\n",
      "18.3 action:  [-0.0289, -0.9835] n_targets:  1 reward:  86.91\n",
      "18.7 action:  [-0.2357, -0.9818] n_targets:  1 reward:  54.78\n",
      "20.5 action:  [-0.0273, -0.9951] n_targets:  1 reward:  60.96\n",
      "21.3 action:  [-0.0063, -0.9975] n_targets:  1 reward:  57.04\n",
      "22.7 action:  [0.0024, -0.9941] n_targets:  2 reward:  106.85\n",
      "22.9 action:  [0.0055, -0.9906] n_targets:  1 reward:  53.24\n",
      "23.1 action:  [0.2442, -0.9804] n_targets:  1 reward:  51.39\n",
      "24.3 action:  [-0.2666, -0.991] n_targets:  1 reward:  58.35\n",
      "26.1 action:  [-0.0341, -0.9932] n_targets:  1 reward:  56.02\n",
      "26.5 action:  [0.3898, -0.9993] n_targets:  1 reward:  56.34\n",
      "27.1 action:  [-0.1341, -0.9835] n_targets:  1 reward:  60.06\n",
      "31.8 action:  [-0.2167, -0.984] n_targets:  1 reward:  62.05\n",
      "32.2 action:  [-0.1296, -0.9964] n_targets:  1 reward:  59.11\n",
      "32.6 action:  [0.0229, -0.9948] n_targets:  1 reward:  57.59\n",
      "34.5 action:  [-0.1222, -0.9919] n_targets:  1 reward:  52.0\n",
      "35.1 action:  [-0.3987, -0.9828] n_targets:  1 reward:  50.04\n",
      "35.7 action:  [0.1492, -0.9905] n_targets:  1 reward:  72.55\n",
      "36.5 action:  [0.0973, -0.9933] n_targets:  1 reward:  56.95\n",
      "37.8 action:  [0.0335, -0.9841] n_targets:  1 reward:  57.85\n",
      "38.0 action:  [0.3686, -0.9963] n_targets:  1 reward:  51.79\n",
      "38.2 action:  [-0.0874, -0.992] n_targets:  1 reward:  54.49\n",
      "38.6 action:  [-0.0494, -0.9834] n_targets:  1 reward:  54.42\n",
      "41.0 action:  [0.3011, -0.9862] n_targets:  2 reward:  140.98\n",
      "41.4 action:  [-0.2968, -0.9984] n_targets:  1 reward:  59.83\n",
      "42.8 action:  [-0.3974, -0.9947] n_targets:  1 reward:  59.8\n",
      "43.4 action:  [0.3545, -0.9992] n_targets:  1 reward:  75.14\n",
      "43.6 action:  [0.0322, -0.9984] n_targets:  1 reward:  53.27\n",
      "45.0 action:  [-0.0899, -0.9957] n_targets:  1 reward:  69.27\n",
      "46.6 action:  [-0.0731, -0.9844] n_targets:  1 reward:  52.06\n",
      "47.4 action:  [-0.174, -0.9955] n_targets:  1 reward:  56.62\n",
      "48.0 action:  [-0.152, -0.9888] n_targets:  1 reward:  52.54\n",
      "48.8 action:  [0.1811, -0.9839] n_targets:  1 reward:  56.89\n",
      "50.8 action:  [0.0453, -0.9949] n_targets:  1 reward:  57.08\n",
      "51.0 action:  [0.1653, -0.996] n_targets:  1 reward:  56.17\n",
      "52.7 action:  [0.3787, -0.9954] n_targets:  1 reward:  50.02\n",
      "53.3 action:  [0.4355, -0.9836] n_targets:  1 reward:  54.32\n",
      "54.6 action:  [0.24, -0.9874] n_targets:  1 reward:  51.46\n",
      "55.6 action:  [0.1504, -0.9951] n_targets:  1 reward:  57.42\n",
      "56.2 action:  [0.0143, -0.9958] n_targets:  1 reward:  51.55\n",
      "63.0 action:  [0.0722, -0.9831] n_targets:  1 reward:  61.63\n",
      "64.0 action:  [0.0883, -0.9848] n_targets:  1 reward:  55.79\n",
      "64.2 action:  [0.0947, -0.9955] n_targets:  1 reward:  55.32\n",
      "64.4 action:  [0.0167, -0.9941] n_targets:  1 reward:  55.99\n",
      "65.6 action:  [0.501, -0.9922] n_targets:  2 reward:  111.06\n",
      "68.3 action:  [0.3227, -0.9948] n_targets:  1 reward:  58.33\n",
      "68.7 action:  [-0.2026, -0.9951] n_targets:  1 reward:  52.27\n",
      "69.5 action:  [-0.1163, -0.9888] n_targets:  1 reward:  50.14\n",
      "71.2 action:  [0.1712, -0.9929] n_targets:  1 reward:  56.58\n",
      "73.5 action:  [0.0964, -0.9957] n_targets:  2 reward:  142.42\n",
      "75.0 action:  [0.3141, -0.9882] n_targets:  1 reward:  51.27\n",
      "75.4 action:  [-0.0387, -0.9936] n_targets:  1 reward:  65.33\n",
      "75.6 action:  [-0.0375, -0.9911] n_targets:  1 reward:  58.79\n",
      "75.8 action:  [-0.2017, -0.9821] n_targets:  1 reward:  51.34\n",
      "76.0 action:  [0.3783, -0.9848] n_targets:  2 reward:  113.36\n",
      "77.5 action:  [0.2569, -0.986] n_targets:  2 reward:  157.37\n",
      "77.7 action:  [0.5317, -0.9916] n_targets:  1 reward:  62.12\n",
      "78.3 action:  [0.3548, -0.9972] n_targets:  1 reward:  58.21\n",
      "78.7 action:  [0.1399, -0.9866] n_targets:  1 reward:  52.14\n",
      "79.8 action:  [0.3794, -0.9969] n_targets:  1 reward:  65.92\n",
      "80.8 action:  [0.0525, -0.9947] n_targets:  1 reward:  54.31\n",
      "83.4 action:  [0.0896, -0.9924] n_targets:  1 reward:  52.31\n",
      "83.8 action:  [0.4248, -0.9954] n_targets:  2 reward:  113.35\n",
      "86.3 action:  [-0.164, -0.9915] n_targets:  1 reward:  53.01\n",
      "91.4 action:  [0.5389, -0.99] n_targets:  1 reward:  81.83\n",
      "93.1 action:  [0.2178, -0.9969] n_targets:  1 reward:  58.43\n",
      "94.1 action:  [-0.3996, -0.9917] n_targets:  1 reward:  62.29\n",
      "94.5 action:  [0.0914, -0.9963] n_targets:  2 reward:  113.24\n",
      "95.9 action:  [0.3565, -0.9951] n_targets:  1 reward:  58.14\n",
      "96.3 action:  [0.1524, -0.9946] n_targets:  1 reward:  52.76\n",
      "100.8 action:  [0.3596, -0.9824] n_targets:  1 reward:  56.51\n",
      "ALPHA (entropy-related):  tensor([0.0837], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.09078 0.08945 0.08837 0.08758 0.08692 0.08612 0.08535 0.08469 0.08412\n",
      " 0.0837 ]\n",
      "Last 100 ALPHA: [0.39704 0.3884  0.37997 0.37171 0.36363 0.35573 0.34801 0.34045 0.33308\n",
      " 0.32586 0.31878 0.31189 0.30515 0.29854 0.29209 0.28578 0.27959 0.27356\n",
      " 0.26766 0.26192 0.25628 0.25081 0.24544 0.24022 0.23516 0.23025 0.22546\n",
      " 0.22071 0.21614 0.21183 0.20748 0.20331 0.19923 0.19545 0.19182 0.1884\n",
      " 0.18483 0.18145 0.17817 0.17481 0.17161 0.16875 0.16612 0.16328 0.16045\n",
      " 0.15774 0.15521 0.15304 0.15099 0.14874 0.14611 0.14394 0.14189 0.13997\n",
      " 0.13811 0.13644 0.13501 0.13345 0.13168 0.12991 0.12857 0.12711 0.12548\n",
      " 0.12385 0.12229 0.12085 0.1195  0.1182  0.117   0.11586 0.11458 0.11337\n",
      " 0.11211 0.11092 0.10986 0.10857 0.10725 0.10635 0.10559 0.10488 0.10409\n",
      " 0.10341 0.10238 0.10134 0.09973 0.09803 0.09629 0.09479 0.09357 0.09231\n",
      " 0.09078 0.08945 0.08837 0.08758 0.08692 0.08612 0.08535 0.08469 0.08412\n",
      " 0.0837 ]\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  153\n",
      "0.2 action:  [0.077, -0.9908] n_targets:  1 reward:  50.26\n",
      "2.0 action:  [0.094, -0.9891] n_targets:  1 reward:  53.77\n",
      "4.2 action:  [0.0723, -0.9882] n_targets:  1 reward:  54.25\n",
      "7.2 action:  [0.0869, -0.9906] n_targets:  1 reward:  52.71\n",
      "7.4 action:  [0.0851, -0.9906] n_targets:  1 reward:  52.04\n",
      "8.0 action:  [0.0853, -0.9885] n_targets:  1 reward:  54.0\n",
      "8.2 action:  [0.1009, -0.9873] n_targets:  1 reward:  52.82\n",
      "8.6 action:  [0.0925, -0.9879] n_targets:  1 reward:  51.08\n",
      "9.4 action:  [0.0992, -0.9878] n_targets:  1 reward:  56.63\n",
      "10.4 action:  [0.1012, -0.9867] n_targets:  1 reward:  55.29\n",
      "13.0 action:  [0.0886, -0.9865] n_targets:  1 reward:  54.16\n",
      "15.6 action:  [0.0837, -0.989] n_targets:  1 reward:  52.7\n",
      "16.8 action:  [0.06, -0.9915] n_targets:  1 reward:  51.46\n",
      "17.8 action:  [0.0887, -0.9884] n_targets:  1 reward:  52.25\n",
      "20.8 action:  [0.0691, -0.9932] n_targets:  1 reward:  50.24\n",
      "21.4 action:  [0.0865, -0.9875] n_targets:  1 reward:  59.85\n",
      "22.0 action:  [0.0847, -0.9878] n_targets:  1 reward:  52.93\n",
      "23.0 action:  [0.0852, -0.9909] n_targets:  1 reward:  50.79\n",
      "23.2 action:  [0.094, -0.9873] n_targets:  1 reward:  52.81\n",
      "23.4 action:  [0.0958, -0.9874] n_targets:  1 reward:  54.48\n",
      "23.6 action:  [0.0818, -0.9895] n_targets:  1 reward:  50.65\n",
      "24.4 action:  [0.0598, -0.9933] n_targets:  1 reward:  50.14\n",
      "25.6 action:  [0.0701, -0.9887] n_targets:  1 reward:  50.13\n",
      "26.2 action:  [0.0737, -0.9905] n_targets:  1 reward:  51.88\n",
      "28.2 action:  [0.0808, -0.9824] n_targets:  1 reward:  58.7\n",
      "28.6 action:  [0.0697, -0.9833] n_targets:  1 reward:  54.47\n",
      "35.8 action:  [0.0514, -0.9913] n_targets:  2 reward:  128.54\n",
      "36.0 action:  [0.0836, -0.9908] n_targets:  1 reward:  58.52\n",
      "36.4 action:  [0.0824, -0.9892] n_targets:  1 reward:  60.51\n",
      "36.8 action:  [0.0777, -0.9894] n_targets:  1 reward:  57.87\n",
      "38.2 action:  [0.0695, -0.9916] n_targets:  1 reward:  52.17\n",
      "39.2 action:  [0.0675, -0.9929] n_targets:  1 reward:  53.3\n",
      "41.2 action:  [0.0748, -0.9908] n_targets:  1 reward:  57.46\n",
      "43.0 action:  [0.0879, -0.9888] n_targets:  1 reward:  57.63\n",
      "45.8 action:  [0.1087, -0.9856] n_targets:  1 reward:  54.11\n",
      "59.8 action:  [0.0948, -0.9855] n_targets:  2 reward:  105.4\n",
      "72.4 action:  [0.0856, -0.9901] n_targets:  1 reward:  61.05\n",
      "72.8 action:  [0.0932, -0.9882] n_targets:  1 reward:  54.95\n",
      "74.2 action:  [0.0895, -0.9894] n_targets:  1 reward:  56.93\n",
      "74.4 action:  [0.0878, -0.9904] n_targets:  1 reward:  50.56\n",
      "74.8 action:  [0.0872, -0.989] n_targets:  1 reward:  50.42\n",
      "76.4 action:  [0.0822, -0.9907] n_targets:  1 reward:  61.54\n",
      "76.8 action:  [0.0731, -0.9928] n_targets:  1 reward:  54.65\n",
      "77.0 action:  [0.0697, -0.9932] n_targets:  1 reward:  58.74\n",
      "77.2 action:  [0.0839, -0.9893] n_targets:  2 reward:  108.69\n",
      "77.8 action:  [0.0918, -0.9886] n_targets:  1 reward:  50.83\n",
      "78.8 action:  [0.0735, -0.9894] n_targets:  2 reward:  106.61\n",
      "79.0 action:  [0.0804, -0.9914] n_targets:  1 reward:  54.62\n",
      "79.2 action:  [0.0693, -0.993] n_targets:  1 reward:  50.17\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  154\n",
      "0.1 action:  [0.1225, -0.984] n_targets:  3 reward:  215.0\n",
      "0.7 action:  [0.0833, -0.9916] n_targets:  2 reward:  108.97\n",
      "1.1 action:  [0.0709, -0.993] n_targets:  1 reward:  57.13\n",
      "2.7 action:  [0.0865, -0.9901] n_targets:  1 reward:  57.58\n",
      "3.3 action:  [0.0883, -0.988] n_targets:  1 reward:  57.01\n",
      "4.9 action:  [0.0799, -0.992] n_targets:  1 reward:  51.25\n",
      "5.1 action:  [0.0797, -0.9919] n_targets:  1 reward:  53.86\n",
      "5.7 action:  [0.0537, -0.9942] n_targets:  1 reward:  60.18\n",
      "5.9 action:  [0.0498, -0.9943] n_targets:  1 reward:  50.46\n",
      "8.5 action:  [0.0783, -0.9917] n_targets:  1 reward:  57.77\n",
      "8.9 action:  [0.0768, -0.9921] n_targets:  1 reward:  52.04\n",
      "22.8 action:  [0.0933, -0.9858] n_targets:  1 reward:  55.54\n",
      "24.2 action:  [0.0892, -0.9909] n_targets:  2 reward:  146.72\n",
      "27.8 action:  [0.0646, -0.9886] n_targets:  1 reward:  50.4\n",
      "28.2 action:  [0.0861, -0.9884] n_targets:  1 reward:  51.49\n",
      "28.6 action:  [0.0977, -0.9877] n_targets:  1 reward:  61.86\n",
      "28.8 action:  [0.0981, -0.9879] n_targets:  1 reward:  50.51\n",
      "29.0 action:  [0.088, -0.9877] n_targets:  1 reward:  58.55\n",
      "29.2 action:  [0.0889, -0.9877] n_targets:  1 reward:  55.19\n",
      "31.0 action:  [0.0621, -0.9936] n_targets:  2 reward:  106.47\n",
      "32.0 action:  [0.062, -0.9889] n_targets:  1 reward:  54.22\n",
      "34.6 action:  [0.076, -0.9927] n_targets:  1 reward:  51.1\n",
      "37.2 action:  [0.0354, -0.9908] n_targets:  1 reward:  62.11\n",
      "41.2 action:  [0.034, -0.9913] n_targets:  2 reward:  106.5\n",
      "61.2 action:  [0.0348, -0.9904] n_targets:  1 reward:  63.64\n",
      "64.6 action:  [0.0853, -0.9867] n_targets:  1 reward:  52.2\n",
      "66.8 action:  [0.0835, -0.9858] n_targets:  1 reward:  53.66\n",
      "67.2 action:  [0.0888, -0.9882] n_targets:  1 reward:  53.4\n",
      "69.2 action:  [0.0893, -0.9895] n_targets:  1 reward:  58.38\n",
      "69.4 action:  [0.0869, -0.9897] n_targets:  1 reward:  58.82\n",
      "69.6 action:  [0.0872, -0.9895] n_targets:  1 reward:  59.77\n",
      "69.8 action:  [0.0766, -0.9886] n_targets:  1 reward:  53.13\n",
      "71.6 action:  [0.082, -0.9896] n_targets:  1 reward:  50.06\n",
      "76.4 action:  [0.0712, -0.993] n_targets:  1 reward:  51.13\n",
      "77.0 action:  [0.09, -0.9888] n_targets:  1 reward:  53.6\n",
      "78.2 action:  [0.0991, -0.9873] n_targets:  1 reward:  51.33\n",
      "78.8 action:  [0.0939, -0.9879] n_targets:  1 reward:  53.2\n",
      "79.0 action:  [0.0916, -0.9881] n_targets:  1 reward:  59.33\n",
      "79.2 action:  [0.0939, -0.9876] n_targets:  1 reward:  52.97\n",
      "79.8 action:  [0.0771, -0.9881] n_targets:  2 reward:  112.1\n",
      "80.4 action:  [0.0844, -0.9883] n_targets:  1 reward:  55.35\n",
      "80.6 action:  [0.0864, -0.9883] n_targets:  1 reward:  55.46\n",
      "81.0 action:  [0.0708, -0.9894] n_targets:  1 reward:  50.52\n",
      "83.0 action:  [0.0685, -0.9933] n_targets:  1 reward:  54.57\n",
      "85.4 action:  [0.092, -0.9889] n_targets:  1 reward:  52.28\n",
      "85.8 action:  [0.0957, -0.9884] n_targets:  1 reward:  51.67\n",
      "86.2 action:  [0.0821, -0.9883] n_targets:  1 reward:  58.2\n",
      "87.2 action:  [0.0924, -0.99] n_targets:  1 reward:  50.96\n",
      "87.8 action:  [0.0915, -0.9889] n_targets:  4 reward:  233.25\n",
      "88.4 action:  [0.08, -0.9914] n_targets:  2 reward:  104.99\n",
      "89.4 action:  [0.0605, -0.9931] n_targets:  1 reward:  55.2\n",
      "91.8 action:  [0.0817, -0.9918] n_targets:  1 reward:  54.08\n",
      "92.6 action:  [0.0716, -0.9927] n_targets:  1 reward:  53.61\n",
      "93.2 action:  [0.0863, -0.9897] n_targets:  1 reward:  53.27\n",
      "93.6 action:  [0.0803, -0.99] n_targets:  1 reward:  53.7\n",
      "95.0 action:  [0.0902, -0.9885] n_targets:  1 reward:  58.5\n",
      "95.2 action:  [0.0825, -0.992] n_targets:  1 reward:  51.41\n",
      "95.6 action:  [0.0748, -0.9878] n_targets:  1 reward:  55.16\n",
      "96.8 action:  [0.0689, -0.9933] n_targets:  1 reward:  57.13\n",
      "97.0 action:  [0.0682, -0.9934] n_targets:  1 reward:  56.7\n",
      "98.0 action:  [0.0964, -0.9874] n_targets:  1 reward:  52.76\n",
      "98.4 action:  [0.0874, -0.9904] n_targets:  1 reward:  54.98\n",
      "99.2 action:  [0.0912, -0.9879] n_targets:  1 reward:  50.84\n",
      "99.8 action:  [0.077, -0.9888] n_targets:  1 reward:  56.36\n",
      "100.0 action:  [0.1048, -0.9859] n_targets:  1 reward:  53.3\n",
      "100.2 action:  [0.1011, -0.9858] n_targets:  1 reward:  59.52\n",
      "100.4 action:  [0.1054, -0.9864] n_targets:  2 reward:  111.98\n",
      "101.0 action:  [0.0862, -0.9885] n_targets:  1 reward:  54.75\n",
      "Best average reward: 0.0, Current average reward: 3682.463589986165\n",
      "Best average reward = 3682.463589986165\n",
      "Best model saved at episode 140 to RL_models/LSTM_stored_models/all_agents/oct_13_3/dv0_dw0_w0_memT1/best_model_postcurriculum\n",
      "reward_threshold: 1433.6000000000004\n",
      "Soft Q Network (1,2):  QNetworkLSTM2(\n",
      "  (linear1): Linear(in_features=51, out_features=128, bias=True)\n",
      "  (lstm1): LSTM(128, 128, dropout=0.2)\n",
      "  (linear2): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "Policy Network:  SAC_PolicyNetworkLSTM(\n",
      "  (linear1): Linear(in_features=47, out_features=128, bias=True)\n",
      "  (linear2): Linear(in_features=49, out_features=128, bias=True)\n",
      "  (lstm1): LSTM(128, 128, dropout=0.2)\n",
      "  (linear3): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (mean_linear): Linear(in_features=128, out_features=2, bias=True)\n",
      "  (log_std_linear): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n",
      "length of replay buffer: 204\n",
      "Loaded existing agent: RL_models/LSTM_stored_models/all_agents/oct_13_3/dv0_dw0_w0_memT1/best_model_postcurriculum\n",
      "Collecting new agent data......\n",
      "TIME before resetting: 0\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "0.1 action:  [0.0429, -0.9919] n_targets:  2 reward:  200\n",
      "0.7 action:  [0.0934, -0.9862] n_targets:  1 reward:  100\n",
      "2.9 action:  [0.0682, -0.9933] n_targets:  1 reward:  100\n",
      "5.7 action:  [0.098, -0.985] n_targets:  1 reward:  100\n",
      "7.7 action:  [0.0798, -0.992] n_targets:  1 reward:  100\n",
      "8.9 action:  [0.0701, -0.9929] n_targets:  1 reward:  100\n",
      "10.7 action:  [0.1013, -0.9866] n_targets:  1 reward:  100\n",
      "10.9 action:  [0.1002, -0.9868] n_targets:  1 reward:  100\n",
      "11.3 action:  [0.0676, -0.9927] n_targets:  2 reward:  200\n",
      "15.4 action:  [0.0812, -0.991] n_targets:  2 reward:  200\n",
      "16.0 action:  [0.0964, -0.9885] n_targets:  1 reward:  100\n",
      "16.4 action:  [0.099, -0.9872] n_targets:  1 reward:  100\n",
      "16.6 action:  [0.0854, -0.9904] n_targets:  1 reward:  100\n",
      "17.2 action:  [0.082, -0.9911] n_targets:  1 reward:  100\n",
      "18.2 action:  [0.0876, -0.9896] n_targets:  1 reward:  100\n",
      "18.6 action:  [0.0874, -0.9896] n_targets:  1 reward:  100\n",
      "18.8 action:  [0.0871, -0.9897] n_targets:  1 reward:  100\n",
      "19.2 action:  [0.09, -0.9885] n_targets:  2 reward:  200\n",
      "21.2 action:  [0.0797, -0.9901] n_targets:  1 reward:  100\n",
      "21.4 action:  [0.0669, -0.9933] n_targets:  1 reward:  100\n",
      "21.6 action:  [0.0918, -0.9878] n_targets:  2 reward:  200\n",
      "22.6 action:  [0.0771, -0.9904] n_targets:  1 reward:  100\n",
      "22.8 action:  [0.0799, -0.9902] n_targets:  1 reward:  100\n",
      "24.2 action:  [0.0872, -0.9887] n_targets:  1 reward:  100\n",
      "25.2 action:  [0.0673, -0.993] n_targets:  1 reward:  100\n",
      "25.4 action:  [0.0721, -0.9924] n_targets:  2 reward:  200\n",
      "26.0 action:  [0.0881, -0.9904] n_targets:  1 reward:  100\n",
      "50.2 action:  [0.0258, -0.9909] n_targets:  3 reward:  300\n",
      "52.0 action:  [0.0975, -0.9884] n_targets:  1 reward:  100\n",
      "52.6 action:  [0.0794, -0.9914] n_targets:  1 reward:  100\n",
      "54.2 action:  [0.0911, -0.9895] n_targets:  1 reward:  100\n",
      "56.0 action:  [0.088, -0.9888] n_targets:  1 reward:  100\n",
      "57.2 action:  [0.0729, -0.9922] n_targets:  1 reward:  100\n",
      "57.4 action:  [0.0676, -0.993] n_targets:  1 reward:  100\n",
      "57.6 action:  [0.0659, -0.9931] n_targets:  1 reward:  100\n",
      "63.0 action:  [0.079, -0.9895] n_targets:  1 reward:  100\n",
      "63.2 action:  [0.0947, -0.9874] n_targets:  1 reward:  100\n",
      "63.4 action:  [0.094, -0.9883] n_targets:  1 reward:  100\n",
      "70.7 action:  [0.049, -0.9918] n_targets:  1 reward:  100\n",
      "71.9 action:  [0.0677, -0.9921] n_targets:  1 reward:  100\n",
      "72.3 action:  [0.0816, -0.9883] n_targets:  1 reward:  100\n",
      "72.5 action:  [0.083, -0.9878] n_targets:  1 reward:  100\n",
      "73.1 action:  [0.0817, -0.9914] n_targets:  1 reward:  100\n",
      "73.9 action:  [0.0861, -0.9906] n_targets:  1 reward:  100\n",
      "74.5 action:  [0.048, -0.9944] n_targets:  1 reward:  100\n",
      "75.5 action:  [0.0825, -0.9915] n_targets:  1 reward:  100\n",
      "77.3 action:  [0.1007, -0.9872] n_targets:  1 reward:  100\n",
      "79.5 action:  [0.065, -0.9934] n_targets:  1 reward:  100\n",
      "81.5 action:  [0.0749, -0.9881] n_targets:  2 reward:  200\n",
      "82.7 action:  [0.0693, -0.9868] n_targets:  1 reward:  100\n",
      "83.9 action:  [0.0873, -0.9848] n_targets:  1 reward:  100\n",
      "94.9 action:  [0.0697, -0.9932] n_targets:  1 reward:  100\n",
      "2025-10-13 22:59:42,235 - INFO - Step: 1000 / 8000\n",
      "100.5 action:  [0.0709, -0.9929] n_targets:  1 reward:  100\n",
      "104.7 action:  [0.0929, -0.9874] n_targets:  1 reward:  100\n",
      "105.1 action:  [0.077, -0.9916] n_targets:  1 reward:  100\n",
      "105.3 action:  [0.0611, -0.9932] n_targets:  1 reward:  100\n",
      "105.5 action:  [0.0656, -0.9929] n_targets:  1 reward:  100\n",
      "105.9 action:  [0.0687, -0.9909] n_targets:  1 reward:  100\n",
      "108.0 action:  [0.0891, -0.9861] n_targets:  1 reward:  100\n",
      "108.6 action:  [0.0831, -0.9862] n_targets:  1 reward:  100\n",
      "116.3 action:  [0.0808, -0.991] n_targets:  2 reward:  200\n",
      "117.5 action:  [0.0761, -0.9916] n_targets:  1 reward:  100\n",
      "121.5 action:  [0.0877, -0.9901] n_targets:  1 reward:  100\n",
      "124.1 action:  [0.0861, -0.9896] n_targets:  2 reward:  200\n",
      "124.9 action:  [0.0751, -0.992] n_targets:  2 reward:  200\n",
      "125.1 action:  [0.0715, -0.993] n_targets:  1 reward:  100\n",
      "125.5 action:  [0.0556, -0.994] n_targets:  2 reward:  200\n",
      "126.3 action:  [0.0931, -0.9877] n_targets:  1 reward:  100\n",
      "126.7 action:  [0.0696, -0.9876] n_targets:  1 reward:  100\n",
      "140.3 action:  [0.0926, -0.9858] n_targets:  1 reward:  100\n",
      "140.8 action:  [0.1007, -0.9882] n_targets:  4 reward:  400\n",
      "141.2 action:  [0.09, -0.9895] n_targets:  1 reward:  100\n",
      "142.4 action:  [0.0914, -0.9897] n_targets:  1 reward:  100\n",
      "144.0 action:  [0.073, -0.9928] n_targets:  1 reward:  100\n",
      "144.6 action:  [0.096, -0.9883] n_targets:  1 reward:  100\n",
      "147.4 action:  [0.0672, -0.9931] n_targets:  1 reward:  100\n",
      "149.8 action:  [0.0757, -0.9906] n_targets:  1 reward:  100\n",
      "153.0 action:  [0.0795, -0.9829] n_targets:  1 reward:  100\n",
      "170.5 action:  [0.0929, -0.986] n_targets:  1 reward:  100\n",
      "171.7 action:  [0.1006, -0.9872] n_targets:  1 reward:  100\n",
      "172.7 action:  [0.0875, -0.9879] n_targets:  1 reward:  100\n",
      "173.5 action:  [0.0957, -0.9884] n_targets:  1 reward:  100\n",
      "175.5 action:  [0.0864, -0.9887] n_targets:  1 reward:  100\n",
      "182.7 action:  [0.104, -0.987] n_targets:  1 reward:  100\n",
      "2025-10-13 22:59:47,667 - INFO - Step: 2000 / 8000\n",
      "204.8 action:  [0.1018, -0.9863] n_targets:  1 reward:  100\n",
      "205.8 action:  [0.0936, -0.9881] n_targets:  1 reward:  100\n",
      "212.0 action:  [0.0765, -0.9918] n_targets:  1 reward:  100\n",
      "233.2 action:  [0.0865, -0.9874] n_targets:  1 reward:  100\n",
      "236.8 action:  [0.082, -0.991] n_targets:  1 reward:  100\n",
      "238.6 action:  [0.0755, -0.9909] n_targets:  1 reward:  100\n",
      "240.4 action:  [0.088, -0.9884] n_targets:  1 reward:  100\n",
      "242.0 action:  [0.0768, -0.9923] n_targets:  1 reward:  100\n",
      "242.4 action:  [0.0782, -0.992] n_targets:  1 reward:  100\n",
      "242.6 action:  [0.0921, -0.9899] n_targets:  2 reward:  200\n",
      "254.0 action:  [0.0624, -0.9897] n_targets:  1 reward:  100\n",
      "254.4 action:  [0.0828, -0.9918] n_targets:  1 reward:  100\n",
      "255.6 action:  [0.0889, -0.9888] n_targets:  1 reward:  100\n",
      "256.0 action:  [0.0808, -0.9915] n_targets:  1 reward:  100\n",
      "256.4 action:  [0.0717, -0.9929] n_targets:  1 reward:  100\n",
      "261.8 action:  [0.084, -0.9888] n_targets:  1 reward:  100\n",
      "263.4 action:  [0.082, -0.9909] n_targets:  1 reward:  100\n",
      "263.6 action:  [0.0821, -0.991] n_targets:  1 reward:  100\n",
      "263.8 action:  [0.0774, -0.9906] n_targets:  1 reward:  100\n",
      "264.8 action:  [0.0934, -0.9893] n_targets:  1 reward:  100\n",
      "265.0 action:  [0.0814, -0.9914] n_targets:  1 reward:  100\n",
      "265.2 action:  [0.0873, -0.99] n_targets:  1 reward:  100\n",
      "266.0 action:  [0.0851, -0.9888] n_targets:  1 reward:  100\n",
      "266.4 action:  [0.1019, -0.9859] n_targets:  2 reward:  200\n",
      "281.0 action:  [0.0915, -0.986] n_targets:  1 reward:  100\n",
      "282.8 action:  [0.0835, -0.9885] n_targets:  1 reward:  100\n",
      "290.6 action:  [0.0803, -0.9906] n_targets:  1 reward:  100\n",
      "291.0 action:  [0.0824, -0.9887] n_targets:  1 reward:  100\n",
      "292.0 action:  [0.0942, -0.9894] n_targets:  1 reward:  100\n",
      "292.2 action:  [0.0914, -0.9881] n_targets:  2 reward:  200\n",
      "293.2 action:  [0.0937, -0.9867] n_targets:  1 reward:  100\n",
      "293.4 action:  [0.1021, -0.9862] n_targets:  1 reward:  100\n",
      "293.8 action:  [0.1022, -0.9868] n_targets:  1 reward:  100\n",
      "294.6 action:  [0.106, -0.9875] n_targets:  1 reward:  100\n",
      "2025-10-13 22:59:52,890 - INFO - Step: 3000 / 8000\n",
      "312.3 action:  [0.0758, -0.9907] n_targets:  1 reward:  100\n",
      "313.1 action:  [0.087, -0.9889] n_targets:  1 reward:  100\n",
      "313.3 action:  [0.0797, -0.9912] n_targets:  1 reward:  100\n",
      "314.1 action:  [0.0774, -0.9918] n_targets:  1 reward:  100\n",
      "314.3 action:  [0.0669, -0.9932] n_targets:  1 reward:  100\n",
      "314.5 action:  [0.0724, -0.9926] n_targets:  1 reward:  100\n",
      "315.9 action:  [0.0721, -0.9927] n_targets:  1 reward:  100\n",
      "317.9 action:  [0.0726, -0.9882] n_targets:  1 reward:  100\n",
      "319.9 action:  [0.0901, -0.9891] n_targets:  1 reward:  100\n",
      "320.3 action:  [0.0831, -0.9898] n_targets:  1 reward:  100\n",
      "320.9 action:  [0.0872, -0.9896] n_targets:  1 reward:  100\n",
      "322.9 action:  [0.0653, -0.9935] n_targets:  2 reward:  200\n",
      "323.1 action:  [0.0768, -0.9895] n_targets:  2 reward:  200\n",
      "325.7 action:  [0.1036, -0.9846] n_targets:  2 reward:  200\n",
      "341.4 action:  [0.0495, -0.9921] n_targets:  1 reward:  100\n",
      "342.0 action:  [0.0824, -0.9898] n_targets:  2 reward:  200\n",
      "342.2 action:  [0.0748, -0.9913] n_targets:  1 reward:  100\n",
      "342.4 action:  [0.0818, -0.9898] n_targets:  2 reward:  200\n",
      "345.8 action:  [0.0699, -0.993] n_targets:  1 reward:  100\n",
      "347.0 action:  [0.0843, -0.9905] n_targets:  1 reward:  100\n",
      "349.6 action:  [0.0767, -0.9917] n_targets:  1 reward:  100\n",
      "350.8 action:  [0.0803, -0.9897] n_targets:  1 reward:  100\n",
      "353.6 action:  [0.0761, -0.9929] n_targets:  1 reward:  100\n",
      "354.0 action:  [0.0585, -0.9871] n_targets:  1 reward:  100\n",
      "367.7 action:  [0.0429, -0.9903] n_targets:  1 reward:  100\n",
      "368.7 action:  [0.0851, -0.9906] n_targets:  1 reward:  100\n",
      "378.9 action:  [0.0425, -0.9923] n_targets:  1 reward:  100\n",
      "393.4 action:  [0.079, -0.9891] n_targets:  1 reward:  100\n",
      "395.6 action:  [0.0774, -0.9883] n_targets:  1 reward:  100\n",
      "396.4 action:  [0.0928, -0.9883] n_targets:  1 reward:  100\n",
      "396.6 action:  [0.0903, -0.9876] n_targets:  1 reward:  100\n",
      "397.6 action:  [0.0842, -0.9907] n_targets:  1 reward:  100\n",
      "2025-10-13 22:59:58,816 - INFO - Step: 4000 / 8000\n",
      "433.0 action:  [0.0833, -0.991] n_targets:  2 reward:  200\n",
      "433.8 action:  [0.0769, -0.992] n_targets:  1 reward:  100\n",
      "434.4 action:  [0.0867, -0.9892] n_targets:  1 reward:  100\n",
      "459.2 action:  [0.0176, -0.9894] n_targets:  1 reward:  100\n",
      "461.9 action:  [0.0519, -0.9918] n_targets:  1 reward:  100\n",
      "462.9 action:  [0.0919, -0.9897] n_targets:  1 reward:  100\n",
      "464.5 action:  [0.064, -0.9936] n_targets:  1 reward:  100\n",
      "480.2 action:  [0.0623, -0.9899] n_targets:  1 reward:  100\n",
      "493.3 action:  [0.0645, -0.9934] n_targets:  1 reward:  100\n",
      "497.5 action:  [0.0721, -0.9928] n_targets:  1 reward:  100\n",
      "2025-10-13 23:00:04,800 - INFO - Step: 5000 / 8000\n",
      "502.5 action:  [0.06, -0.9913] n_targets:  1 reward:  100\n",
      "529.4 action:  [0.0935, -0.987] n_targets:  1 reward:  100\n",
      "531.8 action:  [0.0441, -0.9944] n_targets:  1 reward:  100\n",
      "538.2 action:  [0.0455, -0.992] n_targets:  1 reward:  100\n",
      "545.8 action:  [0.0546, -0.9929] n_targets:  2 reward:  200\n",
      "546.0 action:  [0.0874, -0.9895] n_targets:  1 reward:  100\n",
      "548.0 action:  [0.0706, -0.993] n_targets:  1 reward:  100\n",
      "574.3 action:  [0.0744, -0.9917] n_targets:  1 reward:  100\n",
      "575.1 action:  [0.085, -0.9893] n_targets:  1 reward:  100\n",
      "575.9 action:  [0.0948, -0.9884] n_targets:  1 reward:  100\n",
      "576.5 action:  [0.0657, -0.9935] n_targets:  1 reward:  100\n",
      "578.1 action:  [0.0826, -0.9896] n_targets:  1 reward:  100\n",
      "2025-10-13 23:00:09,719 - INFO - Step: 6000 / 8000\n",
      "610.4 action:  [0.0842, -0.9888] n_targets:  1 reward:  100\n",
      "611.6 action:  [0.0725, -0.993] n_targets:  1 reward:  100\n",
      "618.6 action:  [0.0826, -0.9895] n_targets:  1 reward:  100\n",
      "619.0 action:  [0.0877, -0.9878] n_targets:  1 reward:  100\n",
      "620.2 action:  [0.0235, -0.9915] n_targets:  2 reward:  200\n",
      "630.4 action:  [0.0561, -0.9916] n_targets:  3 reward:  300\n",
      "630.8 action:  [0.0873, -0.9879] n_targets:  2 reward:  200\n",
      "631.6 action:  [0.0922, -0.9873] n_targets:  2 reward:  200\n",
      "632.4 action:  [0.0647, -0.9934] n_targets:  1 reward:  100\n",
      "634.6 action:  [0.0808, -0.9896] n_targets:  1 reward:  100\n",
      "634.8 action:  [0.0888, -0.9864] n_targets:  1 reward:  100\n",
      "635.6 action:  [0.0705, -0.9921] n_targets:  1 reward:  100\n",
      "638.4 action:  [0.0823, -0.9883] n_targets:  1 reward:  100\n",
      "639.8 action:  [0.0942, -0.9901] n_targets:  1 reward:  100\n",
      "640.6 action:  [0.0823, -0.9906] n_targets:  1 reward:  100\n",
      "641.4 action:  [0.0962, -0.9886] n_targets:  1 reward:  100\n",
      "642.0 action:  [0.0792, -0.9922] n_targets:  1 reward:  100\n",
      "642.2 action:  [0.0616, -0.9936] n_targets:  1 reward:  100\n",
      "642.4 action:  [0.0715, -0.9931] n_targets:  1 reward:  100\n",
      "642.8 action:  [0.104, -0.9887] n_targets:  1 reward:  100\n",
      "643.0 action:  [0.1111, -0.9846] n_targets:  1 reward:  100\n",
      "643.6 action:  [0.0175, -0.9832] n_targets:  1 reward:  100\n",
      "659.0 action:  [0.058, -0.9917] n_targets:  1 reward:  100\n",
      "659.2 action:  [0.0944, -0.9878] n_targets:  1 reward:  100\n",
      "660.2 action:  [0.0875, -0.99] n_targets:  2 reward:  200\n",
      "660.4 action:  [0.0813, -0.9905] n_targets:  1 reward:  100\n",
      "670.6 action:  [0.0833, -0.9912] n_targets:  1 reward:  100\n",
      "685.7 action:  [0.0521, -0.99] n_targets:  1 reward:  100\n",
      "685.9 action:  [0.0942, -0.9854] n_targets:  1 reward:  100\n",
      "686.1 action:  [0.1011, -0.9856] n_targets:  1 reward:  100\n",
      "686.3 action:  [0.1017, -0.9848] n_targets:  1 reward:  100\n",
      "690.3 action:  [0.0755, -0.9906] n_targets:  1 reward:  100\n",
      "690.7 action:  [0.0719, -0.9925] n_targets:  1 reward:  100\n",
      "2025-10-13 23:00:14,620 - INFO - Step: 7000 / 8000\n",
      "709.4 action:  [0.0593, -0.9916] n_targets:  1 reward:  100\n",
      "709.6 action:  [0.0959, -0.9888] n_targets:  1 reward:  100\n",
      "710.0 action:  [0.0854, -0.99] n_targets:  2 reward:  200\n",
      "710.6 action:  [0.0809, -0.9915] n_targets:  1 reward:  100\n",
      "713.2 action:  [0.0634, -0.9936] n_targets:  1 reward:  100\n",
      "714.0 action:  [0.0878, -0.9902] n_targets:  1 reward:  100\n",
      "715.6 action:  [0.0819, -0.9885] n_targets:  1 reward:  100\n",
      "716.0 action:  [0.0767, -0.9912] n_targets:  1 reward:  100\n",
      "717.2 action:  [0.0715, -0.9914] n_targets:  1 reward:  100\n",
      "747.4 action:  [0.0728, -0.9929] n_targets:  1 reward:  100\n",
      "748.6 action:  [0.0891, -0.9877] n_targets:  1 reward:  100\n",
      "755.8 action:  [0.0926, -0.9885] n_targets:  1 reward:  100\n",
      "756.6 action:  [0.0856, -0.9891] n_targets:  1 reward:  100\n",
      "757.8 action:  [0.0949, -0.9874] n_targets:  1 reward:  100\n",
      "780.4 action:  [0.0793, -0.9908] n_targets:  1 reward:  100\n",
      "780.6 action:  [0.0844, -0.9886] n_targets:  1 reward:  100\n",
      "780.8 action:  [0.0852, -0.9893] n_targets:  1 reward:  100\n",
      "781.0 action:  [0.0723, -0.9928] n_targets:  1 reward:  100\n",
      "785.2 action:  [0.0795, -0.9904] n_targets:  1 reward:  100\n",
      "789.6 action:  [0.0755, -0.9927] n_targets:  1 reward:  100\n",
      "789.8 action:  [0.0896, -0.9902] n_targets:  1 reward:  100\n",
      "791.4 action:  [0.0859, -0.9874] n_targets:  1 reward:  100\n",
      "791.6 action:  [0.0856, -0.9859] n_targets:  1 reward:  100\n",
      "792.4 action:  [0.0294, -0.99] n_targets:  1 reward:  100\n",
      "800.0 action:  [0.0943, -0.9887] n_targets:  1 reward:  100\n",
      "2025-10-13 23:00:20,634 - INFO - Firefly capture rate: 0.2875\n",
      "Warnings: currently, only ff in obs at each step are used in ff_dataframe. All ff are labeled 'visible' regardless of their actual time since last visible.\n",
      "It is possible that the LSTM agent has the memory of ff in the past, but the code needs to be modified to reflect that. For planning analysis, info of in-memory ff is not needed.\n",
      "made ff_dataframe\n",
      "Number of frames is: 298\n",
      "Number of frames for the animation is: 298\n",
      "Saving animation as: RL_models/LSTM_stored_models/all_agents/oct_13_3/dv0_dw0_w0_memT1/best_model_postcurriculum/no_cost__7-32_rate_0.58.mp4\n",
      "2025-10-13 23:00:22,093 - INFO - Animation.save using <class 'matplotlib.animation.FFMpegWriter'>\n",
      "2025-10-13 23:00:22,094 - INFO - MovieWriter._run: running command: ffmpeg -f rawvideo -vcodec rawvideo -s 640x480 -pix_fmt rgba -framerate 10 -loglevel error -i pipe: -vcodec h264 -pix_fmt yuv420p -y RL_models/LSTM_stored_models/all_agents/oct_13_3/dv0_dw0_w0_memT1/best_model_postcurriculum/no_cost__7-32_rate_0.58.mp4\n",
      "Animation is saved at: RL_models/LSTM_stored_models/all_agents/oct_13_3/dv0_dw0_w0_memT1/best_model_postcurriculum/no_cost__7-32_rate_0.58.mp4\n",
      "Rendering animation ......\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  155\n",
      "0.2 action:  [-0.0521, -0.9876] n_targets:  1 reward:  66.13\n",
      "2.7 action:  [0.0877, -0.9938] n_targets:  1 reward:  53.93\n",
      "3.4 action:  [-0.1982, -0.9968] n_targets:  1 reward:  57.44\n",
      "4.7 action:  [0.2768, -0.9946] n_targets:  1 reward:  64.59\n",
      "6.4 action:  [0.2864, -0.9816] n_targets:  1 reward:  50.84\n",
      "12.0 action:  [0.5194, -0.9955] n_targets:  1 reward:  57.13\n",
      "17.1 action:  [-0.0036, -0.9906] n_targets:  2 reward:  129.75\n",
      "17.5 action:  [0.2395, -0.9924] n_targets:  2 reward:  124.13\n",
      "19.9 action:  [-0.1376, -0.9913] n_targets:  1 reward:  56.2\n",
      "21.1 action:  [-0.2975, -0.9955] n_targets:  2 reward:  132.18\n",
      "21.9 action:  [-0.2491, -0.9948] n_targets:  2 reward:  121.81\n",
      "23.4 action:  [-0.368, -0.9921] n_targets:  1 reward:  50.91\n",
      "24.4 action:  [0.4975, -0.9979] n_targets:  4 reward:  247.28\n",
      "28.0 action:  [-0.3991, -0.9955] n_targets:  1 reward:  52.84\n",
      "28.4 action:  [-0.2934, -0.9947] n_targets:  1 reward:  58.55\n",
      "31.1 action:  [-0.0916, -0.9943] n_targets:  1 reward:  76.24\n",
      "33.7 action:  [-0.1154, -0.984] n_targets:  2 reward:  127.75\n",
      "34.5 action:  [-0.152, -0.991] n_targets:  1 reward:  53.96\n",
      "34.8 action:  [0.0005, -0.9991] n_targets:  1 reward:  53.43\n",
      "36.4 action:  [0.0529, -0.9812] n_targets:  3 reward:  172.07\n",
      "37.0 action:  [0.2679, -0.9963] n_targets:  1 reward:  72.44\n",
      "38.8 action:  [0.1073, -0.9954] n_targets:  1 reward:  68.57\n",
      "39.0 action:  [0.1668, -0.9958] n_targets:  2 reward:  108.43\n",
      "40.8 action:  [-0.016, -0.9892] n_targets:  3 reward:  171.8\n",
      "41.6 action:  [-0.041, -0.9892] n_targets:  1 reward:  65.65\n",
      "47.0 action:  [-0.2129, -0.9874] n_targets:  2 reward:  174.51\n",
      "49.7 action:  [0.1647, -0.9867] n_targets:  3 reward:  241.66\n",
      "49.9 action:  [0.3869, -0.9823] n_targets:  1 reward:  51.72\n",
      "51.9 action:  [-0.1582, -0.9934] n_targets:  1 reward:  51.96\n",
      "53.3 action:  [-0.0119, -0.9991] n_targets:  2 reward:  150.8\n",
      "56.8 action:  [-0.06, -0.9981] n_targets:  1 reward:  69.68\n",
      "58.0 action:  [-0.4611, -0.9985] n_targets:  1 reward:  52.26\n",
      "62.7 action:  [0.2498, -0.9928] n_targets:  1 reward:  60.56\n",
      "64.5 action:  [-0.3205, -0.9882] n_targets:  2 reward:  110.36\n",
      "66.0 action:  [-0.1702, -0.9925] n_targets:  1 reward:  53.43\n",
      "67.2 action:  [0.0861, -0.9989] n_targets:  1 reward:  67.12\n",
      "68.4 action:  [0.2084, -0.9965] n_targets:  1 reward:  62.37\n",
      "69.2 action:  [-0.4756, -0.9889] n_targets:  4 reward:  253.28\n",
      "72.5 action:  [0.2744, -0.9857] n_targets:  1 reward:  69.54\n",
      "73.4 action:  [0.6385, -0.9874] n_targets:  2 reward:  122.99\n",
      "74.2 action:  [-0.5545, -0.9971] n_targets:  1 reward:  80.96\n",
      "74.5 action:  [0.396, -0.9951] n_targets:  1 reward:  53.68\n",
      "75.4 action:  [-0.4759, -0.9919] n_targets:  1 reward:  58.51\n",
      "76.6 action:  [0.2447, -0.9871] n_targets:  1 reward:  58.31\n",
      "82.4 action:  [0.5255, -0.9856] n_targets:  2 reward:  135.8\n",
      "82.7 action:  [-0.2425, -0.9908] n_targets:  2 reward:  118.86\n",
      "84.0 action:  [-0.0748, -0.9965] n_targets:  2 reward:  122.84\n",
      "85.5 action:  [0.2684, -0.9988] n_targets:  3 reward:  220.17\n",
      "88.0 action:  [0.0994, -0.9862] n_targets:  1 reward:  82.18\n",
      "88.6 action:  [-0.449, -0.9805] n_targets:  1 reward:  75.68\n",
      "89.7 action:  [0.4277, -0.9861] n_targets:  1 reward:  71.92\n",
      "90.0 action:  [-0.4626, -0.989] n_targets:  1 reward:  61.27\n",
      "93.2 action:  [0.0598, -0.9973] n_targets:  2 reward:  138.5\n",
      "94.9 action:  [0.2043, -0.9933] n_targets:  1 reward:  59.27\n",
      "97.6 action:  [-0.1693, -0.9982] n_targets:  1 reward:  52.47\n",
      "101.5 action:  [-0.2466, -0.9997] n_targets:  1 reward:  54.51\n",
      "ALPHA (entropy-related):  tensor([0.9779], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.97788]\n",
      "Episode: 0, Episode Reward: 5331.245836893717\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  156\n",
      "3.1 action:  [0.3694, -0.9866] n_targets:  1 reward:  95.32\n",
      "4.2 action:  [0.0115, -0.9811] n_targets:  2 reward:  136.42\n",
      "6.7 action:  [-0.558, -0.9901] n_targets:  3 reward:  191.46\n",
      "12.5 action:  [-0.3508, -0.9993] n_targets:  2 reward:  155.96\n",
      "13.8 action:  [0.2641, -0.9944] n_targets:  2 reward:  128.04\n",
      "14.5 action:  [-0.2454, -0.9913] n_targets:  1 reward:  68.14\n",
      "17.4 action:  [-0.5997, -0.9976] n_targets:  2 reward:  163.0\n",
      "18.0 action:  [-0.2649, -0.9878] n_targets:  1 reward:  59.8\n",
      "20.4 action:  [0.0651, -0.9996] n_targets:  3 reward:  193.0\n",
      "26.3 action:  [-0.5212, -0.9936] n_targets:  3 reward:  225.03\n",
      "27.1 action:  [0.0932, -0.9868] n_targets:  1 reward:  56.85\n",
      "28.4 action:  [0.4039, -0.9834] n_targets:  2 reward:  149.24\n",
      "30.8 action:  [-0.5621, -0.9808] n_targets:  1 reward:  83.21\n",
      "33.5 action:  [-0.2097, -0.9829] n_targets:  1 reward:  50.45\n",
      "40.1 action:  [-0.287, -0.9918] n_targets:  2 reward:  153.27\n",
      "40.5 action:  [-0.1646, -0.9815] n_targets:  1 reward:  65.02\n",
      "58.2 action:  [0.3424, -0.9877] n_targets:  1 reward:  85.42\n",
      "70.6 action:  [0.4146, -0.9997] n_targets:  4 reward:  260.0\n",
      "79.9 action:  [-0.5729, -0.9936] n_targets:  3 reward:  206.8\n",
      "83.7 action:  [0.5492, -0.9933] n_targets:  2 reward:  107.13\n",
      "92.0 action:  [-0.3688, -0.9995] n_targets:  1 reward:  56.01\n",
      "95.8 action:  [0.0631, -0.9819] n_targets:  2 reward:  122.45\n",
      "ALPHA (entropy-related):  tensor([0.9550], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.97788 0.95496]\n",
      "Episode: 1, Episode Reward: 2812.0150257746377\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  157\n",
      "17.4 action:  [0.1981, -0.9803] n_targets:  1 reward:  55.78\n",
      "26.2 action:  [-0.2298, -0.9979] n_targets:  1 reward:  50.59\n",
      "30.0 action:  [-0.0439, -0.9867] n_targets:  2 reward:  137.05\n",
      "43.7 action:  [-0.3878, -0.992] n_targets:  3 reward:  184.39\n",
      "57.2 action:  [-0.5009, -0.9838] n_targets:  1 reward:  55.95\n",
      "59.2 action:  [-0.1565, -0.9997] n_targets:  2 reward:  126.0\n",
      "63.9 action:  [-0.3959, -0.981] n_targets:  1 reward:  56.23\n",
      "67.0 action:  [0.337, -0.9898] n_targets:  2 reward:  137.97\n",
      "68.4 action:  [-0.2551, -0.9974] n_targets:  1 reward:  68.14\n",
      "81.7 action:  [-0.22, -0.9842] n_targets:  1 reward:  76.28\n",
      "99.5 action:  [0.0932, -0.9973] n_targets:  1 reward:  51.21\n",
      "ALPHA (entropy-related):  tensor([0.9322], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.97788 0.95496 0.93215]\n",
      "Episode: 2, Episode Reward: 999.5739466349283\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  158\n",
      "8.7 action:  [-0.4038, -0.9974] n_targets:  4 reward:  257.91\n",
      "10.9 action:  [0.6069, -0.9996] n_targets:  3 reward:  219.75\n",
      "27.2 action:  [-0.0392, -0.9882] n_targets:  2 reward:  137.58\n",
      "39.0 action:  [-0.1149, -0.9875] n_targets:  1 reward:  59.8\n",
      "43.8 action:  [0.6028, -0.9891] n_targets:  1 reward:  84.5\n",
      "46.6 action:  [-0.4512, -0.9863] n_targets:  2 reward:  171.26\n",
      "55.3 action:  [0.0262, -0.9861] n_targets:  2 reward:  135.52\n",
      "72.7 action:  [0.057, -0.9864] n_targets:  3 reward:  192.39\n",
      "81.9 action:  [-0.3017, -0.9904] n_targets:  1 reward:  50.26\n",
      "90.7 action:  [0.3749, -0.9885] n_targets:  1 reward:  52.7\n",
      "ALPHA (entropy-related):  tensor([0.9100], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.97788 0.95496 0.93215 0.90999]\n",
      "Episode: 3, Episode Reward: 1361.6805547078447\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  159\n",
      "10.0 action:  [0.1409, -0.9871] n_targets:  1 reward:  89.99\n",
      "26.2 action:  [0.1421, -0.98] n_targets:  1 reward:  64.58\n",
      "27.7 action:  [0.1525, -0.9841] n_targets:  1 reward:  58.91\n",
      "29.1 action:  [-0.0007, -0.9864] n_targets:  3 reward:  204.44\n",
      "31.5 action:  [-0.4528, -0.9814] n_targets:  1 reward:  50.15\n",
      "62.2 action:  [-0.4409, -0.9877] n_targets:  3 reward:  164.8\n",
      "65.2 action:  [-0.1564, -0.9804] n_targets:  3 reward:  208.5\n",
      "76.3 action:  [0.4473, -0.9994] n_targets:  2 reward:  106.97\n",
      "90.8 action:  [0.2272, -0.9936] n_targets:  1 reward:  62.54\n",
      "ALPHA (entropy-related):  tensor([0.8885], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.97788 0.95496 0.93215 0.90999 0.88852]\n",
      "Episode: 4, Episode Reward: 1010.8758455912272\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  160\n",
      "7.8 action:  [0.278, -0.9845] n_targets:  1 reward:  72.1\n",
      "23.9 action:  [-0.0819, -0.9828] n_targets:  2 reward:  138.02\n",
      "30.5 action:  [0.3509, -0.9961] n_targets:  1 reward:  55.91\n",
      "42.3 action:  [0.1047, -0.9918] n_targets:  1 reward:  88.9\n",
      "44.5 action:  [-0.5411, -0.9876] n_targets:  2 reward:  125.17\n",
      "50.5 action:  [0.4358, -0.9886] n_targets:  2 reward:  134.15\n",
      "61.6 action:  [0.6296, -0.9908] n_targets:  3 reward:  203.0\n",
      "91.5 action:  [0.282, -0.9965] n_targets:  1 reward:  80.19\n",
      "93.7 action:  [0.4412, -0.9918] n_targets:  2 reward:  107.58\n",
      "ALPHA (entropy-related):  tensor([0.8679], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.97788 0.95496 0.93215 0.90999 0.88852 0.86787]\n",
      "Episode: 5, Episode Reward: 1005.0211575826008\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  161\n",
      "16.8 action:  [-0.2758, -0.9873] n_targets:  1 reward:  51.9\n",
      "21.6 action:  [0.247, -0.9869] n_targets:  1 reward:  69.66\n",
      "27.4 action:  [0.0277, -0.9893] n_targets:  1 reward:  70.33\n",
      "34.9 action:  [0.5986, -0.9805] n_targets:  1 reward:  59.38\n",
      "ALPHA (entropy-related):  tensor([0.8479], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.97788 0.95496 0.93215 0.90999 0.88852 0.86787 0.84794]\n",
      "Episode: 6, Episode Reward: 251.28029378255206\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  162\n",
      "27.9 action:  [0.3887, -0.9864] n_targets:  1 reward:  59.57\n",
      "29.5 action:  [-0.0718, -0.9801] n_targets:  3 reward:  196.27\n",
      "42.5 action:  [-0.1675, -0.9802] n_targets:  3 reward:  181.35\n",
      "ALPHA (entropy-related):  tensor([0.8285], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.97788 0.95496 0.93215 0.90999 0.88852 0.86787 0.84794 0.82852]\n",
      "Episode: 7, Episode Reward: 437.19023386637366\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  163\n",
      "10.4 action:  [0.1195, -0.984] n_targets:  1 reward:  64.58\n",
      "28.2 action:  [-0.6394, -0.9969] n_targets:  2 reward:  185.18\n",
      "49.8 action:  [-0.3416, -0.997] n_targets:  1 reward:  50.22\n",
      "ALPHA (entropy-related):  tensor([0.8096], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.97788 0.95496 0.93215 0.90999 0.88852 0.86787 0.84794 0.82852 0.80965]\n",
      "Episode: 8, Episode Reward: 299.9755846659342\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  164\n",
      "17.8 action:  [-0.0943, -0.9929] n_targets:  3 reward:  224.18\n",
      "25.0 action:  [-0.2853, -0.9921] n_targets:  1 reward:  66.81\n",
      "26.9 action:  [0.4838, -0.9888] n_targets:  2 reward:  134.67\n",
      "50.2 action:  [-0.4426, -0.996] n_targets:  1 reward:  58.85\n",
      "92.0 action:  [0.3967, -0.9886] n_targets:  3 reward:  175.91\n",
      "ALPHA (entropy-related):  tensor([0.7913], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.97788 0.95496 0.93215 0.90999 0.88852 0.86787 0.84794 0.82852 0.80965\n",
      " 0.79129]\n",
      "Episode: 9, Episode Reward: 660.4145406087239\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  165\n",
      "20.5 action:  [-0.0669, -0.9903] n_targets:  1 reward:  55.74\n",
      "ALPHA (entropy-related):  tensor([0.7734], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.95496 0.93215 0.90999 0.88852 0.86787 0.84794 0.82852 0.80965 0.79129\n",
      " 0.7734 ]\n",
      "Episode: 10, Episode Reward: 55.74063618977864\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  166\n",
      "27.7 action:  [0.0735, -0.9948] n_targets:  2 reward:  140.94\n",
      "66.9 action:  [-0.1515, -0.9976] n_targets:  3 reward:  180.63\n",
      "83.0 action:  [-0.4199, -0.9889] n_targets:  2 reward:  134.79\n",
      "ALPHA (entropy-related):  tensor([0.7560], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.93215 0.90999 0.88852 0.86787 0.84794 0.82852 0.80965 0.79129 0.7734\n",
      " 0.75598]\n",
      "Episode: 11, Episode Reward: 456.3561248779297\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  167\n",
      "33.6 action:  [-0.2485, -0.9893] n_targets:  2 reward:  137.03\n",
      "56.2 action:  [0.5819, -0.9911] n_targets:  3 reward:  188.66\n",
      "68.0 action:  [0.0016, -0.9944] n_targets:  1 reward:  63.84\n",
      "93.3 action:  [-0.5501, -0.9914] n_targets:  1 reward:  65.04\n",
      "95.5 action:  [-0.3845, -0.9852] n_targets:  1 reward:  58.9\n",
      "ALPHA (entropy-related):  tensor([0.7390], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.90999 0.88852 0.86787 0.84794 0.82852 0.80965 0.79129 0.7734  0.75598\n",
      " 0.73901]\n",
      "Episode: 12, Episode Reward: 513.4603500366211\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  168\n",
      "42.4 action:  [-0.0185, -0.9972] n_targets:  1 reward:  63.79\n",
      "59.6 action:  [0.0001, -0.9837] n_targets:  2 reward:  130.77\n",
      "66.0 action:  [0.2359, -0.9977] n_targets:  1 reward:  54.7\n",
      "ALPHA (entropy-related):  tensor([0.7225], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.88852 0.86787 0.84794 0.82852 0.80965 0.79129 0.7734  0.75598 0.73901\n",
      " 0.72248]\n",
      "Episode: 13, Episode Reward: 249.2651646931966\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  169\n",
      "38.1 action:  [0.278, -0.9835] n_targets:  1 reward:  54.96\n",
      "53.1 action:  [0.0684, -0.9843] n_targets:  1 reward:  63.74\n",
      "53.4 action:  [0.5885, -0.9878] n_targets:  1 reward:  50.52\n",
      "ALPHA (entropy-related):  tensor([0.7064], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.86787 0.84794 0.82852 0.80965 0.79129 0.7734  0.75598 0.73901 0.72248\n",
      " 0.70635]\n",
      "Episode: 14, Episode Reward: 169.21831512451172\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  170\n",
      "6.8 action:  [-0.6024, -0.9926] n_targets:  1 reward:  55.08\n",
      "7.4 action:  [-0.187, -0.9971] n_targets:  1 reward:  55.54\n",
      "28.6 action:  [0.5397, -0.9994] n_targets:  1 reward:  54.21\n",
      "49.6 action:  [0.5545, -0.9956] n_targets:  1 reward:  61.65\n",
      "80.4 action:  [-0.3789, -0.9884] n_targets:  1 reward:  61.71\n",
      "ALPHA (entropy-related):  tensor([0.6906], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.84794 0.82852 0.80965 0.79129 0.7734  0.75598 0.73901 0.72248 0.70635\n",
      " 0.69064]\n",
      "Last 100 ALPHA: [0.97788 0.95496 0.93215 0.90999 0.88852 0.86787 0.84794 0.82852 0.80965\n",
      " 0.79129 0.7734  0.75598 0.73901 0.72248 0.70635 0.69064]\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  171\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  172\n",
      "Best average reward: -9999, Current average reward: 0.0\n",
      "Best average reward = 0.0\n",
      "Best model saved at episode 15 to None\n",
      "Evaluation rewards: [0.0]\n",
      "Episode: 15, Episode Reward: 288.18461354573566\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  173\n",
      "11.4 action:  [-0.5332, -0.9853] n_targets:  1 reward:  60.89\n",
      "30.5 action:  [0.5144, -0.9954] n_targets:  2 reward:  139.56\n",
      "52.9 action:  [-0.5283, -0.9845] n_targets:  4 reward:  275.9\n",
      "54.6 action:  [0.5071, -0.9947] n_targets:  1 reward:  58.51\n",
      "88.4 action:  [0.5532, -0.9912] n_targets:  1 reward:  86.03\n",
      "94.2 action:  [0.3085, -0.9868] n_targets:  1 reward:  75.96\n",
      "ALPHA (entropy-related):  tensor([0.6753], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.82852 0.80965 0.79129 0.7734  0.75598 0.73901 0.72248 0.70635 0.69064\n",
      " 0.67531]\n",
      "Episode: 16, Episode Reward: 696.8519287109375\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  174\n",
      "2.7 action:  [0.129, -0.9974] n_targets:  4 reward:  246.37\n",
      "47.3 action:  [-0.0784, -0.9821] n_targets:  1 reward:  69.47\n",
      "49.4 action:  [0.1317, -0.9867] n_targets:  1 reward:  65.79\n",
      "93.2 action:  [0.066, -0.9929] n_targets:  1 reward:  74.86\n",
      "ALPHA (entropy-related):  tensor([0.6604], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.80965 0.79129 0.7734  0.75598 0.73901 0.72248 0.70635 0.69064 0.67531\n",
      " 0.66036]\n",
      "Episode: 17, Episode Reward: 456.4832153320312\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  175\n",
      "1.8 action:  [-0.5463, -0.9955] n_targets:  1 reward:  54.97\n",
      "9.1 action:  [-0.2851, -0.9831] n_targets:  1 reward:  50.1\n",
      "29.4 action:  [0.6095, -0.9944] n_targets:  1 reward:  50.7\n",
      "41.9 action:  [0.4258, -0.9841] n_targets:  1 reward:  58.44\n",
      "68.4 action:  [-0.4984, -0.9802] n_targets:  2 reward:  123.31\n",
      "72.2 action:  [0.6199, -0.9852] n_targets:  1 reward:  82.53\n",
      "76.3 action:  [-0.1707, -0.9882] n_targets:  2 reward:  126.36\n",
      "86.6 action:  [0.0641, -0.9949] n_targets:  2 reward:  138.06\n",
      "ALPHA (entropy-related):  tensor([0.6458], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.79129 0.7734  0.75598 0.73901 0.72248 0.70635 0.69064 0.67531 0.66036\n",
      " 0.64578]\n",
      "Episode: 18, Episode Reward: 684.4785690307616\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  176\n",
      "25.3 action:  [-0.098, -0.9832] n_targets:  1 reward:  59.44\n",
      "31.0 action:  [-0.3758, -0.9806] n_targets:  2 reward:  143.96\n",
      "49.5 action:  [0.475, -0.9989] n_targets:  1 reward:  75.16\n",
      "52.7 action:  [-0.035, -0.9987] n_targets:  1 reward:  56.98\n",
      "57.7 action:  [-0.1509, -0.9946] n_targets:  1 reward:  53.61\n",
      "67.9 action:  [-0.2759, -0.989] n_targets:  2 reward:  134.63\n",
      "74.2 action:  [0.1176, -0.9884] n_targets:  2 reward:  141.74\n",
      "77.2 action:  [0.6106, -0.9863] n_targets:  1 reward:  68.46\n",
      "97.7 action:  [0.0014, -0.9897] n_targets:  1 reward:  51.88\n",
      "ALPHA (entropy-related):  tensor([0.6315], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.7734  0.75598 0.73901 0.72248 0.70635 0.69064 0.67531 0.66036 0.64578\n",
      " 0.63155]\n",
      "Episode: 19, Episode Reward: 785.8636500040689\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  177\n",
      "5.2 action:  [-0.2542, -0.9826] n_targets:  1 reward:  74.3\n",
      "10.4 action:  [-0.405, -0.9842] n_targets:  4 reward:  262.63\n",
      "40.5 action:  [-0.0823, -0.9918] n_targets:  1 reward:  54.33\n",
      "54.1 action:  [-0.558, -0.9861] n_targets:  1 reward:  75.93\n",
      "75.9 action:  [0.221, -0.9969] n_targets:  3 reward:  209.52\n",
      "83.4 action:  [0.3443, -0.9835] n_targets:  1 reward:  57.44\n",
      "93.2 action:  [-0.464, -0.9955] n_targets:  1 reward:  74.02\n",
      "ALPHA (entropy-related):  tensor([0.6177], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.75598 0.73901 0.72248 0.70635 0.69064 0.67531 0.66036 0.64578 0.63155\n",
      " 0.61769]\n",
      "Episode: 20, Episode Reward: 808.1648915608723\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  178\n",
      "0.1 action:  [0.3936, -0.998] n_targets:  1 reward:  65.08\n",
      "0.5 action:  [0.273, -0.9985] n_targets:  1 reward:  50.48\n",
      "18.5 action:  [0.3175, -0.9912] n_targets:  1 reward:  66.02\n",
      "24.6 action:  [0.4234, -0.9946] n_targets:  3 reward:  158.71\n",
      "47.3 action:  [-0.02, -0.9962] n_targets:  2 reward:  137.75\n",
      "49.1 action:  [0.451, -0.9959] n_targets:  2 reward:  133.46\n",
      "55.4 action:  [0.3278, -0.9843] n_targets:  1 reward:  59.16\n",
      "66.9 action:  [-0.5849, -0.985] n_targets:  1 reward:  68.66\n",
      "83.9 action:  [0.4884, -0.9966] n_targets:  1 reward:  69.84\n",
      "85.2 action:  [0.0837, -0.9831] n_targets:  1 reward:  69.65\n",
      "ALPHA (entropy-related):  tensor([0.6042], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.73901 0.72248 0.70635 0.69064 0.67531 0.66036 0.64578 0.63155 0.61769\n",
      " 0.60419]\n",
      "Episode: 21, Episode Reward: 878.8270848592122\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  179\n",
      "4.4 action:  [-0.2118, -0.9926] n_targets:  1 reward:  68.79\n",
      "12.1 action:  [-0.3625, -0.9854] n_targets:  1 reward:  50.99\n",
      "21.1 action:  [0.2174, -0.9944] n_targets:  1 reward:  59.06\n",
      "30.9 action:  [-0.4625, -0.9905] n_targets:  2 reward:  160.0\n",
      "36.7 action:  [0.3478, -0.9839] n_targets:  1 reward:  56.54\n",
      "45.2 action:  [-0.0656, -0.9861] n_targets:  3 reward:  220.95\n",
      "53.6 action:  [0.1696, -0.9992] n_targets:  4 reward:  260.78\n",
      "59.8 action:  [0.4042, -0.9934] n_targets:  1 reward:  51.4\n",
      "60.3 action:  [-0.31, -0.9974] n_targets:  2 reward:  127.85\n",
      "65.8 action:  [-0.1695, -0.9911] n_targets:  1 reward:  53.17\n",
      "69.7 action:  [0.3894, -0.9956] n_targets:  2 reward:  135.27\n",
      "72.6 action:  [0.574, -0.9955] n_targets:  1 reward:  57.14\n",
      "80.7 action:  [-0.5367, -0.9864] n_targets:  2 reward:  159.32\n",
      "87.7 action:  [-0.4594, -0.9861] n_targets:  1 reward:  70.12\n",
      "90.8 action:  [0.5249, -0.9992] n_targets:  1 reward:  63.64\n",
      "91.5 action:  [-0.2934, -0.9822] n_targets:  1 reward:  74.39\n",
      "96.5 action:  [-0.013, -0.9927] n_targets:  1 reward:  55.5\n",
      "97.8 action:  [-0.2392, -0.9859] n_targets:  1 reward:  90.57\n",
      "99.4 action:  [0.4624, -0.997] n_targets:  1 reward:  55.74\n",
      "102.3 action:  [-0.1781, -0.9804] n_targets:  2 reward:  139.66\n",
      "ALPHA (entropy-related):  tensor([0.5911], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.72248 0.70635 0.69064 0.67531 0.66036 0.64578 0.63155 0.61769 0.60419\n",
      " 0.59106]\n",
      "Episode: 22, Episode Reward: 2010.8612925211587\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  180\n",
      "2.7 action:  [-0.5774, -0.981] n_targets:  1 reward:  83.42\n",
      "7.9 action:  [0.4954, -0.9839] n_targets:  2 reward:  147.36\n",
      "12.0 action:  [-0.3687, -0.9826] n_targets:  2 reward:  105.56\n",
      "14.3 action:  [-0.4797, -0.9996] n_targets:  3 reward:  160.46\n",
      "16.8 action:  [0.3937, -0.9917] n_targets:  3 reward:  259.76\n",
      "22.0 action:  [0.0055, -0.9907] n_targets:  1 reward:  54.84\n",
      "24.2 action:  [0.2102, -0.9941] n_targets:  1 reward:  67.46\n",
      "25.6 action:  [0.486, -0.985] n_targets:  1 reward:  77.12\n",
      "28.6 action:  [0.5492, -0.9957] n_targets:  1 reward:  57.29\n",
      "35.2 action:  [-0.6224, -0.9922] n_targets:  2 reward:  153.19\n",
      "37.4 action:  [0.5524, -0.988] n_targets:  1 reward:  69.08\n",
      "41.0 action:  [0.1693, -0.9804] n_targets:  1 reward:  68.19\n",
      "45.3 action:  [-0.0396, -0.9892] n_targets:  2 reward:  147.58\n",
      "47.3 action:  [0.4479, -0.9807] n_targets:  1 reward:  88.65\n",
      "49.2 action:  [0.3481, -0.9811] n_targets:  2 reward:  123.95\n",
      "55.2 action:  [-0.5821, -0.9843] n_targets:  3 reward:  163.34\n",
      "68.3 action:  [0.4958, -0.9812] n_targets:  1 reward:  67.0\n",
      "70.6 action:  [-0.5097, -0.9971] n_targets:  1 reward:  52.04\n",
      "80.9 action:  [0.1866, -0.9929] n_targets:  1 reward:  74.09\n",
      "87.8 action:  [0.475, -0.9984] n_targets:  2 reward:  148.06\n",
      "90.6 action:  [-0.44, -0.9934] n_targets:  1 reward:  62.32\n",
      "ALPHA (entropy-related):  tensor([0.5783], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.70635 0.69064 0.67531 0.66036 0.64578 0.63155 0.61769 0.60419 0.59106\n",
      " 0.5783 ]\n",
      "Episode: 23, Episode Reward: 2230.7510706583657\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  181\n",
      "1.4 action:  [-0.4095, -0.9975] n_targets:  4 reward:  255.7\n",
      "2.5 action:  [-0.4675, -0.9956] n_targets:  2 reward:  137.34\n",
      "7.1 action:  [0.3231, -0.9854] n_targets:  1 reward:  76.36\n",
      "7.6 action:  [-0.1016, -0.9983] n_targets:  1 reward:  60.47\n",
      "11.8 action:  [0.3641, -0.9944] n_targets:  2 reward:  134.68\n",
      "15.4 action:  [0.0579, -0.9975] n_targets:  1 reward:  69.5\n",
      "20.4 action:  [-0.1584, -0.9964] n_targets:  1 reward:  79.23\n",
      "24.7 action:  [-0.2094, -0.9878] n_targets:  1 reward:  60.51\n",
      "32.5 action:  [0.2079, -0.9929] n_targets:  1 reward:  52.89\n",
      "39.6 action:  [0.385, -0.9924] n_targets:  1 reward:  84.62\n",
      "40.0 action:  [0.5263, -0.9815] n_targets:  1 reward:  61.61\n",
      "66.4 action:  [0.5931, -0.9825] n_targets:  1 reward:  67.23\n",
      "69.6 action:  [-0.1849, -0.9911] n_targets:  1 reward:  64.3\n",
      "70.6 action:  [-0.0954, -0.9882] n_targets:  2 reward:  146.27\n",
      "85.5 action:  [-0.5145, -0.983] n_targets:  1 reward:  85.45\n",
      "93.9 action:  [-0.3184, -0.9994] n_targets:  1 reward:  54.48\n",
      "99.7 action:  [0.5471, -0.9971] n_targets:  2 reward:  128.47\n",
      "100.7 action:  [-0.2475, -0.999] n_targets:  1 reward:  52.25\n",
      "ALPHA (entropy-related):  tensor([0.5658], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.69064 0.67531 0.66036 0.64578 0.63155 0.61769 0.60419 0.59106 0.5783\n",
      " 0.5658 ]\n",
      "Episode: 24, Episode Reward: 1671.3821818033853\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  182\n",
      "9.8 action:  [-0.0378, -0.9906] n_targets:  2 reward:  132.64\n",
      "17.5 action:  [-0.6014, -0.9819] n_targets:  2 reward:  110.89\n",
      "27.8 action:  [0.6063, -0.9891] n_targets:  1 reward:  62.92\n",
      "28.3 action:  [-0.5019, -0.9984] n_targets:  1 reward:  53.13\n",
      "30.7 action:  [0.4976, -0.9927] n_targets:  2 reward:  136.66\n",
      "32.1 action:  [-0.4023, -0.9991] n_targets:  2 reward:  118.64\n",
      "47.0 action:  [0.3422, -0.995] n_targets:  1 reward:  68.09\n",
      "62.4 action:  [0.2443, -0.999] n_targets:  1 reward:  59.56\n",
      "63.5 action:  [0.0187, -0.9982] n_targets:  2 reward:  168.4\n",
      "64.3 action:  [0.4074, -0.9945] n_targets:  1 reward:  51.65\n",
      "72.4 action:  [0.1975, -0.9947] n_targets:  1 reward:  52.59\n",
      "74.5 action:  [-0.0071, -0.9936] n_targets:  1 reward:  51.74\n",
      "91.0 action:  [0.3599, -0.994] n_targets:  3 reward:  169.47\n",
      "93.0 action:  [0.0557, -0.9922] n_targets:  1 reward:  74.84\n",
      "97.6 action:  [-0.5602, -0.999] n_targets:  1 reward:  65.48\n",
      "98.6 action:  [0.5202, -0.981] n_targets:  2 reward:  126.16\n",
      "ALPHA (entropy-related):  tensor([0.5537], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.67531 0.66036 0.64578 0.63155 0.61769 0.60419 0.59106 0.5783  0.5658\n",
      " 0.55365]\n",
      "Episode: 25, Episode Reward: 1502.8658142089844\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  183\n",
      "0.4 action:  [0.4392, -0.9958] n_targets:  2 reward:  127.91\n",
      "1.0 action:  [0.5418, -0.9908] n_targets:  1 reward:  51.07\n",
      "9.3 action:  [0.3118, -0.9913] n_targets:  2 reward:  132.72\n",
      "11.9 action:  [-0.4494, -0.9881] n_targets:  3 reward:  206.11\n",
      "14.2 action:  [-0.2122, -0.9833] n_targets:  1 reward:  58.16\n",
      "25.2 action:  [-0.207, -0.9938] n_targets:  1 reward:  70.54\n",
      "38.2 action:  [-0.0242, -0.9934] n_targets:  1 reward:  56.43\n",
      "43.3 action:  [-0.119, -0.993] n_targets:  1 reward:  89.1\n",
      "45.0 action:  [0.4961, -0.9905] n_targets:  1 reward:  86.04\n",
      "47.9 action:  [-0.5498, -0.9991] n_targets:  2 reward:  144.34\n",
      "55.6 action:  [-0.1562, -0.9954] n_targets:  1 reward:  51.44\n",
      "67.8 action:  [0.2328, -0.9956] n_targets:  2 reward:  166.15\n",
      "81.1 action:  [0.4675, -0.9897] n_targets:  2 reward:  171.54\n",
      "85.5 action:  [0.2474, -0.9905] n_targets:  1 reward:  85.35\n",
      "86.7 action:  [0.3255, -0.993] n_targets:  2 reward:  151.53\n",
      "89.7 action:  [0.4354, -0.9938] n_targets:  2 reward:  159.62\n",
      "94.2 action:  [0.3893, -0.9966] n_targets:  1 reward:  58.65\n",
      "95.0 action:  [0.0603, -0.992] n_targets:  2 reward:  117.54\n",
      "97.9 action:  [0.5937, -0.9941] n_targets:  1 reward:  52.64\n",
      "101.2 action:  [-0.1935, -0.9923] n_targets:  2 reward:  136.32\n",
      "ALPHA (entropy-related):  tensor([0.5418], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.66036 0.64578 0.63155 0.61769 0.60419 0.59106 0.5783  0.5658  0.55365\n",
      " 0.54182]\n",
      "Episode: 26, Episode Reward: 2173.1981862386065\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  184\n",
      "9.2 action:  [0.2744, -0.9912] n_targets:  1 reward:  56.77\n",
      "9.9 action:  [0.0181, -0.9839] n_targets:  1 reward:  54.99\n",
      "11.5 action:  [-0.5021, -0.981] n_targets:  1 reward:  53.86\n",
      "12.2 action:  [-0.4084, -0.9892] n_targets:  1 reward:  72.1\n",
      "14.6 action:  [-0.2836, -0.9983] n_targets:  1 reward:  73.97\n",
      "20.9 action:  [0.1699, -0.9881] n_targets:  4 reward:  284.04\n",
      "25.5 action:  [-0.2199, -0.9938] n_targets:  1 reward:  51.86\n",
      "31.8 action:  [-0.2987, -0.9972] n_targets:  1 reward:  55.72\n",
      "36.1 action:  [-0.0776, -0.9971] n_targets:  1 reward:  71.51\n",
      "40.5 action:  [0.1927, -0.9991] n_targets:  3 reward:  242.43\n",
      "49.8 action:  [0.3531, -0.9976] n_targets:  2 reward:  155.53\n",
      "57.1 action:  [-0.1867, -0.9906] n_targets:  1 reward:  66.88\n",
      "67.2 action:  [-0.2259, -0.988] n_targets:  3 reward:  196.85\n",
      "70.7 action:  [-0.274, -0.9839] n_targets:  1 reward:  51.38\n",
      "73.0 action:  [-0.0307, -0.9966] n_targets:  1 reward:  71.53\n",
      "80.4 action:  [0.6385, -0.9824] n_targets:  1 reward:  65.17\n",
      "ALPHA (entropy-related):  tensor([0.5302], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.64578 0.63155 0.61769 0.60419 0.59106 0.5783  0.5658  0.55365 0.54182\n",
      " 0.53022]\n",
      "Episode: 27, Episode Reward: 1624.5878804524737\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  185\n",
      "12.4 action:  [0.1502, -0.9932] n_targets:  1 reward:  65.84\n",
      "15.9 action:  [0.385, -0.9802] n_targets:  1 reward:  56.27\n",
      "16.8 action:  [0.4643, -0.9956] n_targets:  2 reward:  142.56\n",
      "19.7 action:  [-0.3357, -0.984] n_targets:  3 reward:  232.92\n",
      "27.5 action:  [-0.2855, -0.9844] n_targets:  1 reward:  75.56\n",
      "27.9 action:  [-0.0105, -0.9995] n_targets:  1 reward:  51.55\n",
      "29.9 action:  [-0.3443, -0.9829] n_targets:  2 reward:  113.16\n",
      "33.5 action:  [0.6304, -0.9905] n_targets:  2 reward:  122.82\n",
      "36.3 action:  [-0.2977, -0.9905] n_targets:  2 reward:  120.85\n",
      "39.6 action:  [-0.2422, -0.9905] n_targets:  1 reward:  71.75\n",
      "42.3 action:  [0.232, -0.9974] n_targets:  1 reward:  63.66\n",
      "43.6 action:  [-0.1438, -0.99] n_targets:  1 reward:  52.93\n",
      "49.7 action:  [0.116, -0.9957] n_targets:  1 reward:  65.91\n",
      "52.7 action:  [0.2527, -0.9899] n_targets:  1 reward:  78.95\n",
      "55.3 action:  [-0.3586, -0.9822] n_targets:  1 reward:  58.42\n",
      "56.6 action:  [0.5724, -0.9897] n_targets:  1 reward:  56.12\n",
      "57.9 action:  [-0.2397, -0.9893] n_targets:  1 reward:  60.43\n",
      "59.5 action:  [0.4948, -0.9867] n_targets:  1 reward:  50.29\n",
      "62.6 action:  [0.3832, -0.9973] n_targets:  1 reward:  79.18\n",
      "63.5 action:  [0.1406, -0.992] n_targets:  1 reward:  62.55\n",
      "69.7 action:  [-0.0216, -0.9834] n_targets:  3 reward:  220.93\n",
      "79.7 action:  [0.1139, -0.9855] n_targets:  2 reward:  145.02\n",
      "81.1 action:  [0.2707, -0.9976] n_targets:  2 reward:  138.49\n",
      "98.6 action:  [0.0608, -0.9906] n_targets:  3 reward:  206.66\n",
      "100.1 action:  [-0.638, -0.9961] n_targets:  1 reward:  65.89\n",
      "101.9 action:  [-0.4553, -0.9965] n_targets:  1 reward:  60.68\n",
      "ALPHA (entropy-related):  tensor([0.5190], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.63155 0.61769 0.60419 0.59106 0.5783  0.5658  0.55365 0.54182 0.53022\n",
      " 0.51899]\n",
      "Episode: 28, Episode Reward: 2519.381554921468\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  186\n",
      "1.0 action:  [-0.3197, -0.9974] n_targets:  1 reward:  58.43\n",
      "6.5 action:  [0.0406, -0.9817] n_targets:  1 reward:  74.25\n",
      "12.0 action:  [-0.5249, -0.9909] n_targets:  2 reward:  127.59\n",
      "15.3 action:  [-0.3577, -0.9998] n_targets:  2 reward:  149.78\n",
      "16.8 action:  [-0.0365, -0.982] n_targets:  1 reward:  50.78\n",
      "22.0 action:  [0.0928, -0.993] n_targets:  2 reward:  165.25\n",
      "24.9 action:  [0.0038, -0.9968] n_targets:  1 reward:  50.32\n",
      "25.7 action:  [-0.1856, -0.9986] n_targets:  1 reward:  70.66\n",
      "27.2 action:  [-0.0458, -0.991] n_targets:  2 reward:  133.23\n",
      "30.3 action:  [0.2094, -0.9906] n_targets:  2 reward:  120.89\n",
      "33.2 action:  [-0.0232, -0.9911] n_targets:  1 reward:  72.79\n",
      "34.1 action:  [-0.2186, -0.9935] n_targets:  2 reward:  137.01\n",
      "39.6 action:  [0.5008, -0.9879] n_targets:  2 reward:  149.63\n",
      "43.4 action:  [0.062, -0.9826] n_targets:  1 reward:  50.1\n",
      "46.7 action:  [0.5481, -0.9851] n_targets:  2 reward:  146.59\n",
      "48.3 action:  [0.6264, -0.9977] n_targets:  2 reward:  156.22\n",
      "48.9 action:  [0.3493, -0.9901] n_targets:  1 reward:  56.06\n",
      "59.1 action:  [-0.1981, -0.9848] n_targets:  1 reward:  65.21\n",
      "59.9 action:  [0.3465, -0.9932] n_targets:  1 reward:  77.36\n",
      "64.6 action:  [-0.4282, -0.9951] n_targets:  1 reward:  65.28\n",
      "67.2 action:  [-0.5325, -0.9852] n_targets:  3 reward:  202.92\n",
      "74.1 action:  [0.1067, -0.9802] n_targets:  1 reward:  91.92\n",
      "76.2 action:  [0.5134, -0.9944] n_targets:  2 reward:  141.98\n",
      "80.6 action:  [0.3452, -0.9897] n_targets:  3 reward:  194.37\n",
      "83.4 action:  [0.5998, -0.997] n_targets:  3 reward:  212.45\n",
      "93.2 action:  [0.1124, -0.9855] n_targets:  3 reward:  211.49\n",
      "96.7 action:  [0.3478, -0.9883] n_targets:  1 reward:  55.02\n",
      "97.9 action:  [0.3367, -0.9898] n_targets:  2 reward:  131.23\n",
      "100.3 action:  [-0.4962, -0.9814] n_targets:  1 reward:  88.37\n",
      "ALPHA (entropy-related):  tensor([0.5081], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.61769 0.60419 0.59106 0.5783  0.5658  0.55365 0.54182 0.53022 0.51899\n",
      " 0.50808]\n",
      "Episode: 29, Episode Reward: 3307.1759897867837\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  187\n",
      "2.2 action:  [-0.4649, -0.9922] n_targets:  3 reward:  221.58\n",
      "7.5 action:  [-0.2801, -0.9887] n_targets:  2 reward:  118.69\n",
      "12.5 action:  [-0.5573, -0.9949] n_targets:  1 reward:  53.16\n",
      "13.9 action:  [-0.6268, -0.996] n_targets:  1 reward:  68.04\n",
      "14.1 action:  [-0.4489, -0.9959] n_targets:  1 reward:  56.59\n",
      "17.7 action:  [-0.514, -0.9987] n_targets:  1 reward:  69.35\n",
      "26.5 action:  [0.4636, -0.9826] n_targets:  4 reward:  260.91\n",
      "36.1 action:  [-0.0372, -0.9834] n_targets:  1 reward:  88.9\n",
      "43.2 action:  [-0.1909, -0.9994] n_targets:  3 reward:  200.27\n",
      "46.7 action:  [-0.2842, -0.9855] n_targets:  2 reward:  151.42\n",
      "47.6 action:  [-0.3854, -0.9801] n_targets:  1 reward:  81.04\n",
      "54.9 action:  [-0.2235, -0.9992] n_targets:  3 reward:  219.67\n",
      "60.7 action:  [0.2817, -0.9996] n_targets:  2 reward:  156.66\n",
      "62.8 action:  [-0.12, -0.9897] n_targets:  2 reward:  159.86\n",
      "69.1 action:  [-0.0895, -0.9955] n_targets:  1 reward:  55.51\n",
      "69.8 action:  [0.0997, -0.9919] n_targets:  1 reward:  54.92\n",
      "79.3 action:  [0.1085, -0.9842] n_targets:  2 reward:  132.29\n",
      "83.1 action:  [0.246, -0.9953] n_targets:  1 reward:  52.12\n",
      "84.2 action:  [-0.0634, -0.9919] n_targets:  2 reward:  125.61\n",
      "86.4 action:  [0.2623, -0.9997] n_targets:  1 reward:  68.64\n",
      "89.8 action:  [0.1655, -0.9801] n_targets:  1 reward:  64.59\n",
      "100.6 action:  [-0.542, -0.9829] n_targets:  3 reward:  198.14\n",
      "ALPHA (entropy-related):  tensor([0.4975], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.60419 0.59106 0.5783  0.5658  0.55365 0.54182 0.53022 0.51899 0.50808\n",
      " 0.49747]\n",
      "Last 100 ALPHA: [0.97788 0.95496 0.93215 0.90999 0.88852 0.86787 0.84794 0.82852 0.80965\n",
      " 0.79129 0.7734  0.75598 0.73901 0.72248 0.70635 0.69064 0.67531 0.66036\n",
      " 0.64578 0.63155 0.61769 0.60419 0.59106 0.5783  0.5658  0.55365 0.54182\n",
      " 0.53022 0.51899 0.50808 0.49747]\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  188\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  189\n",
      "Best average reward: 0.0, Current average reward: 0.0\n",
      "Evaluation rewards: [0.0, 0.0]\n",
      "Episode: 30, Episode Reward: 2657.9632987976074\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  190\n",
      "0.1 action:  [-0.081, -0.9933] n_targets:  2 reward:  109.56\n",
      "8.6 action:  [-0.5406, -0.9961] n_targets:  2 reward:  129.94\n",
      "11.5 action:  [-0.3187, -0.9994] n_targets:  1 reward:  74.46\n",
      "16.6 action:  [-0.5863, -0.9993] n_targets:  2 reward:  131.61\n",
      "23.9 action:  [-0.4085, -0.9987] n_targets:  1 reward:  55.08\n",
      "25.3 action:  [-0.6086, -0.9913] n_targets:  1 reward:  58.44\n",
      "28.4 action:  [0.3201, -0.9924] n_targets:  2 reward:  166.72\n",
      "32.5 action:  [0.4636, -0.9937] n_targets:  2 reward:  159.28\n",
      "33.7 action:  [-0.0477, -0.9948] n_targets:  2 reward:  153.88\n",
      "34.6 action:  [0.1429, -0.9981] n_targets:  2 reward:  107.73\n",
      "37.3 action:  [-0.1016, -0.9872] n_targets:  1 reward:  58.6\n",
      "42.2 action:  [-0.0155, -0.9993] n_targets:  2 reward:  125.93\n",
      "43.1 action:  [0.3934, -0.9977] n_targets:  1 reward:  60.13\n",
      "45.1 action:  [-0.1451, -0.9898] n_targets:  3 reward:  230.79\n",
      "47.2 action:  [-0.1939, -0.9814] n_targets:  2 reward:  147.95\n",
      "48.7 action:  [0.0595, -0.991] n_targets:  2 reward:  119.2\n",
      "49.5 action:  [-0.2057, -0.9855] n_targets:  2 reward:  119.16\n",
      "51.8 action:  [0.1931, -0.989] n_targets:  1 reward:  57.45\n",
      "54.9 action:  [-0.2436, -0.9884] n_targets:  2 reward:  125.24\n",
      "62.8 action:  [-0.1968, -0.9958] n_targets:  5 reward:  320.36\n",
      "65.4 action:  [0.1268, -0.986] n_targets:  2 reward:  140.65\n",
      "68.3 action:  [0.6259, -0.9812] n_targets:  1 reward:  75.33\n",
      "71.3 action:  [0.104, -0.999] n_targets:  1 reward:  50.4\n",
      "74.9 action:  [0.5008, -0.985] n_targets:  1 reward:  54.29\n",
      "77.7 action:  [-0.2714, -0.9962] n_targets:  1 reward:  53.68\n",
      "81.0 action:  [0.0262, -0.9826] n_targets:  3 reward:  213.89\n",
      "83.3 action:  [-0.3784, -0.9889] n_targets:  1 reward:  84.2\n",
      "85.2 action:  [-0.4359, -0.9896] n_targets:  1 reward:  60.72\n",
      "86.6 action:  [0.1217, -0.9995] n_targets:  3 reward:  208.57\n",
      "90.3 action:  [0.4535, -0.9971] n_targets:  2 reward:  112.1\n",
      "99.2 action:  [-0.0533, -0.9906] n_targets:  1 reward:  57.98\n",
      "100.0 action:  [0.4498, -0.9861] n_targets:  1 reward:  58.04\n",
      "101.3 action:  [-0.0408, -0.9821] n_targets:  1 reward:  74.79\n",
      "ALPHA (entropy-related):  tensor([0.4872], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.59106 0.5783  0.5658  0.55365 0.54182 0.53022 0.51899 0.50808 0.49747\n",
      " 0.48716]\n",
      "Episode: 31, Episode Reward: 3756.1522343953447\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  191\n",
      "1.7 action:  [0.2861, -0.9809] n_targets:  1 reward:  63.89\n",
      "1.9 action:  [0.17, -0.9936] n_targets:  1 reward:  51.54\n",
      "3.9 action:  [0.2134, -0.9972] n_targets:  1 reward:  69.19\n",
      "6.6 action:  [-0.0726, -0.9895] n_targets:  2 reward:  134.58\n",
      "9.6 action:  [0.444, -0.9935] n_targets:  3 reward:  184.87\n",
      "10.5 action:  [0.5233, -0.9956] n_targets:  1 reward:  64.55\n",
      "12.1 action:  [-0.2364, -0.9968] n_targets:  1 reward:  72.39\n",
      "14.0 action:  [0.0716, -0.993] n_targets:  1 reward:  56.54\n",
      "15.6 action:  [0.2617, -0.9931] n_targets:  5 reward:  368.82\n",
      "16.6 action:  [-0.0876, -0.9827] n_targets:  1 reward:  51.66\n",
      "27.3 action:  [-0.5591, -0.9801] n_targets:  1 reward:  52.74\n",
      "29.8 action:  [-0.4483, -0.9876] n_targets:  3 reward:  165.31\n",
      "32.0 action:  [0.521, -0.9904] n_targets:  3 reward:  230.93\n",
      "35.1 action:  [-0.0964, -0.9973] n_targets:  2 reward:  150.73\n",
      "40.3 action:  [-0.1153, -0.9981] n_targets:  1 reward:  53.34\n",
      "43.4 action:  [-0.2517, -0.9815] n_targets:  1 reward:  52.71\n",
      "45.0 action:  [-0.4663, -0.9849] n_targets:  4 reward:  257.51\n",
      "46.1 action:  [-0.5939, -0.9906] n_targets:  1 reward:  58.41\n",
      "47.0 action:  [0.5116, -0.9988] n_targets:  2 reward:  150.28\n",
      "49.3 action:  [-0.5055, -0.9891] n_targets:  1 reward:  64.59\n",
      "55.3 action:  [-0.3673, -0.9932] n_targets:  3 reward:  174.99\n",
      "56.0 action:  [0.2441, -0.998] n_targets:  1 reward:  53.15\n",
      "67.7 action:  [0.4746, -0.9985] n_targets:  1 reward:  50.49\n",
      "69.7 action:  [-0.5768, -0.9855] n_targets:  1 reward:  79.4\n",
      "71.6 action:  [0.4008, -0.9865] n_targets:  2 reward:  159.51\n",
      "74.8 action:  [0.5819, -0.9837] n_targets:  1 reward:  66.4\n",
      "76.7 action:  [0.0083, -0.9941] n_targets:  1 reward:  59.57\n",
      "84.6 action:  [-0.1106, -0.9971] n_targets:  2 reward:  141.11\n",
      "88.8 action:  [-0.3872, -0.9919] n_targets:  4 reward:  287.92\n",
      "ALPHA (entropy-related):  tensor([0.4770], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.5783  0.5658  0.55365 0.54182 0.53022 0.51899 0.50808 0.49747 0.48716\n",
      " 0.47703]\n",
      "Episode: 32, Episode Reward: 3427.1205952962237\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  192\n",
      "6.4 action:  [0.0681, -0.9996] n_targets:  1 reward:  76.28\n",
      "8.4 action:  [0.2687, -0.9925] n_targets:  3 reward:  222.27\n",
      "10.1 action:  [0.1173, -0.9878] n_targets:  2 reward:  145.29\n",
      "13.7 action:  [0.2735, -0.9975] n_targets:  1 reward:  50.18\n",
      "14.5 action:  [0.0699, -0.9823] n_targets:  1 reward:  50.08\n",
      "18.2 action:  [-0.2726, -0.9843] n_targets:  1 reward:  56.37\n",
      "24.8 action:  [-0.2379, -0.9801] n_targets:  1 reward:  66.51\n",
      "26.9 action:  [-0.1225, -0.9963] n_targets:  2 reward:  110.44\n",
      "32.7 action:  [0.3314, -0.9978] n_targets:  1 reward:  56.85\n",
      "35.2 action:  [-0.2951, -0.9984] n_targets:  3 reward:  204.7\n",
      "36.4 action:  [-0.6094, -0.9994] n_targets:  1 reward:  51.3\n",
      "44.7 action:  [0.2626, -0.9889] n_targets:  1 reward:  88.03\n",
      "51.2 action:  [-0.1709, -0.9923] n_targets:  1 reward:  67.56\n",
      "52.1 action:  [0.4316, -0.9838] n_targets:  3 reward:  198.57\n",
      "54.5 action:  [-0.5223, -0.9835] n_targets:  1 reward:  50.4\n",
      "57.0 action:  [0.5462, -0.9976] n_targets:  1 reward:  57.29\n",
      "60.5 action:  [-0.3475, -0.9965] n_targets:  1 reward:  61.53\n",
      "68.5 action:  [-0.533, -0.9881] n_targets:  1 reward:  73.59\n",
      "74.0 action:  [0.5082, -0.9884] n_targets:  1 reward:  69.91\n",
      "77.3 action:  [0.4372, -0.9877] n_targets:  3 reward:  166.83\n",
      "78.8 action:  [0.5277, -0.9919] n_targets:  2 reward:  136.25\n",
      "85.7 action:  [0.1807, -0.9991] n_targets:  4 reward:  297.35\n",
      "88.8 action:  [0.3463, -0.9822] n_targets:  1 reward:  50.41\n",
      "90.9 action:  [-0.1718, -0.9951] n_targets:  1 reward:  51.44\n",
      "96.4 action:  [-0.0768, -0.981] n_targets:  4 reward:  272.18\n",
      "97.7 action:  [-0.6108, -0.9973] n_targets:  1 reward:  82.43\n",
      "100.9 action:  [-0.1148, -0.999] n_targets:  2 reward:  109.75\n",
      "101.3 action:  [-0.6131, -0.9981] n_targets:  1 reward:  55.74\n",
      "ALPHA (entropy-related):  tensor([0.4672], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.5658  0.55365 0.54182 0.53022 0.51899 0.50808 0.49747 0.48716 0.47703\n",
      " 0.4672 ]\n",
      "Episode: 33, Episode Reward: 2979.5611966451006\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  193\n",
      "0.7 action:  [0.4576, -0.99] n_targets:  1 reward:  57.47\n",
      "5.9 action:  [-0.3604, -0.9871] n_targets:  4 reward:  255.1\n",
      "8.9 action:  [0.3204, -0.9942] n_targets:  3 reward:  193.66\n",
      "12.1 action:  [0.416, -0.9863] n_targets:  2 reward:  151.08\n",
      "13.5 action:  [0.1479, -0.9845] n_targets:  1 reward:  62.95\n",
      "16.0 action:  [0.3618, -0.9945] n_targets:  3 reward:  193.07\n",
      "17.9 action:  [0.3125, -0.9853] n_targets:  3 reward:  186.1\n",
      "22.2 action:  [0.1832, -0.9959] n_targets:  2 reward:  135.17\n",
      "30.7 action:  [-0.3316, -0.9871] n_targets:  1 reward:  73.2\n",
      "32.2 action:  [-0.116, -0.9812] n_targets:  1 reward:  51.83\n",
      "34.1 action:  [-0.4638, -0.9825] n_targets:  2 reward:  111.25\n",
      "37.3 action:  [0.3324, -0.9854] n_targets:  1 reward:  62.93\n",
      "40.3 action:  [0.0036, -0.9969] n_targets:  1 reward:  58.76\n",
      "40.5 action:  [-0.0757, -0.9975] n_targets:  1 reward:  51.43\n",
      "51.6 action:  [0.5028, -0.9854] n_targets:  1 reward:  79.31\n",
      "54.6 action:  [0.6319, -0.997] n_targets:  2 reward:  149.69\n",
      "56.8 action:  [0.0876, -0.9877] n_targets:  1 reward:  55.26\n",
      "57.9 action:  [0.2925, -0.9985] n_targets:  1 reward:  67.46\n",
      "59.5 action:  [0.003, -0.9903] n_targets:  1 reward:  57.04\n",
      "60.6 action:  [0.0066, -0.995] n_targets:  2 reward:  120.16\n",
      "63.2 action:  [-0.1859, -0.9906] n_targets:  1 reward:  66.22\n",
      "64.0 action:  [-0.2619, -0.991] n_targets:  2 reward:  128.98\n",
      "66.4 action:  [0.1591, -0.9976] n_targets:  1 reward:  52.41\n",
      "68.8 action:  [0.4501, -0.9961] n_targets:  2 reward:  140.69\n",
      "72.1 action:  [-0.3812, -0.997] n_targets:  2 reward:  113.45\n",
      "74.0 action:  [0.0155, -0.9814] n_targets:  3 reward:  188.11\n",
      "75.9 action:  [-0.1836, -0.9941] n_targets:  2 reward:  106.75\n",
      "77.9 action:  [0.2081, -0.9904] n_targets:  1 reward:  54.54\n",
      "78.5 action:  [-0.5028, -0.9864] n_targets:  1 reward:  51.85\n",
      "81.6 action:  [-0.4773, -0.9886] n_targets:  1 reward:  61.64\n",
      "82.8 action:  [-0.0953, -0.9994] n_targets:  1 reward:  61.28\n",
      "85.1 action:  [-0.2778, -0.9959] n_targets:  3 reward:  193.43\n",
      "85.5 action:  [0.0994, -0.9987] n_targets:  1 reward:  66.85\n",
      "87.8 action:  [-0.6375, -0.9979] n_targets:  1 reward:  56.58\n",
      "90.6 action:  [-0.0307, -0.9872] n_targets:  2 reward:  132.55\n",
      "91.9 action:  [0.4811, -0.9898] n_targets:  2 reward:  123.28\n",
      "94.1 action:  [-0.4115, -0.9955] n_targets:  1 reward:  55.25\n",
      "98.5 action:  [0.0525, -0.9931] n_targets:  1 reward:  57.11\n",
      "99.1 action:  [-0.541, -0.9983] n_targets:  1 reward:  70.02\n",
      "ALPHA (entropy-related):  tensor([0.4577], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.55365 0.54182 0.53022 0.51899 0.50808 0.49747 0.48716 0.47703 0.4672\n",
      " 0.45771]\n",
      "Episode: 34, Episode Reward: 3953.876613616943\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  194\n",
      "5.6 action:  [0.6349, -0.9891] n_targets:  1 reward:  64.87\n",
      "13.0 action:  [0.5931, -0.9831] n_targets:  2 reward:  130.6\n",
      "15.3 action:  [-0.1324, -0.9971] n_targets:  1 reward:  63.27\n",
      "16.2 action:  [-0.0108, -0.9883] n_targets:  2 reward:  139.46\n",
      "22.5 action:  [-0.1032, -0.9985] n_targets:  1 reward:  63.74\n",
      "23.5 action:  [0.2881, -0.9957] n_targets:  2 reward:  165.88\n",
      "25.5 action:  [-0.3275, -0.9922] n_targets:  1 reward:  70.32\n",
      "26.9 action:  [0.122, -0.9987] n_targets:  3 reward:  164.11\n",
      "28.5 action:  [-0.5618, -0.9932] n_targets:  2 reward:  161.53\n",
      "31.8 action:  [-0.0355, -0.995] n_targets:  1 reward:  65.93\n",
      "37.0 action:  [-0.2488, -0.9897] n_targets:  1 reward:  77.19\n",
      "40.1 action:  [0.3683, -0.9985] n_targets:  1 reward:  77.11\n",
      "40.9 action:  [0.1411, -0.996] n_targets:  1 reward:  51.46\n",
      "43.8 action:  [-0.0831, -0.997] n_targets:  1 reward:  50.27\n",
      "51.1 action:  [0.1086, -0.9898] n_targets:  1 reward:  50.88\n",
      "53.9 action:  [0.0159, -0.986] n_targets:  1 reward:  50.23\n",
      "56.1 action:  [-0.1298, -0.9996] n_targets:  1 reward:  78.31\n",
      "56.5 action:  [-0.344, -0.9844] n_targets:  1 reward:  50.46\n",
      "63.9 action:  [-0.0266, -0.9956] n_targets:  3 reward:  213.62\n",
      "65.7 action:  [-0.5973, -0.9817] n_targets:  2 reward:  118.08\n",
      "68.0 action:  [-0.1555, -0.9959] n_targets:  2 reward:  141.52\n",
      "70.1 action:  [-0.1426, -0.9876] n_targets:  2 reward:  149.11\n",
      "72.6 action:  [0.0842, -0.9961] n_targets:  1 reward:  60.81\n",
      "76.2 action:  [0.64, -0.9818] n_targets:  1 reward:  50.14\n",
      "82.3 action:  [-0.2221, -0.9983] n_targets:  2 reward:  130.82\n",
      "90.4 action:  [0.1012, -0.9851] n_targets:  1 reward:  51.63\n",
      "96.2 action:  [0.3944, -0.9968] n_targets:  1 reward:  54.79\n",
      "97.0 action:  [-0.087, -0.9923] n_targets:  1 reward:  75.77\n",
      "98.4 action:  [0.576, -0.9921] n_targets:  1 reward:  86.86\n",
      "100.7 action:  [0.3532, -0.9894] n_targets:  1 reward:  73.44\n",
      "101.5 action:  [0.4795, -0.9969] n_targets:  1 reward:  52.91\n",
      "ALPHA (entropy-related):  tensor([0.4483], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.54182 0.53022 0.51899 0.50808 0.49747 0.48716 0.47703 0.4672  0.45771\n",
      " 0.44833]\n",
      "Episode: 35, Episode Reward: 2835.099655151367\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  195\n",
      "2.3 action:  [0.1142, -0.9918] n_targets:  1 reward:  56.63\n",
      "5.7 action:  [-0.0201, -0.9839] n_targets:  1 reward:  56.48\n",
      "8.4 action:  [0.5961, -0.9878] n_targets:  1 reward:  87.12\n",
      "10.9 action:  [-0.4431, -0.9877] n_targets:  1 reward:  87.11\n",
      "17.3 action:  [0.3645, -0.986] n_targets:  1 reward:  77.06\n",
      "19.5 action:  [-0.0473, -0.9942] n_targets:  1 reward:  68.9\n",
      "19.9 action:  [0.3209, -0.9988] n_targets:  1 reward:  57.54\n",
      "24.6 action:  [-0.4007, -0.9837] n_targets:  1 reward:  62.98\n",
      "25.2 action:  [0.5186, -0.9963] n_targets:  2 reward:  146.48\n",
      "25.6 action:  [-0.079, -0.9896] n_targets:  1 reward:  61.53\n",
      "27.1 action:  [0.2117, -0.9988] n_targets:  1 reward:  51.07\n",
      "29.8 action:  [0.041, -0.9844] n_targets:  1 reward:  52.76\n",
      "34.3 action:  [-0.2446, -0.9824] n_targets:  1 reward:  66.0\n",
      "37.9 action:  [0.3811, -0.9918] n_targets:  1 reward:  62.64\n",
      "45.8 action:  [-0.181, -0.9903] n_targets:  1 reward:  56.57\n",
      "48.0 action:  [0.2604, -0.9818] n_targets:  1 reward:  53.55\n",
      "51.4 action:  [0.4018, -0.9867] n_targets:  2 reward:  147.12\n",
      "67.2 action:  [0.1322, -0.9822] n_targets:  2 reward:  138.09\n",
      "70.0 action:  [-0.4092, -0.9978] n_targets:  2 reward:  145.09\n",
      "72.2 action:  [-0.3296, -0.9838] n_targets:  1 reward:  56.18\n",
      "80.0 action:  [-0.3582, -0.983] n_targets:  2 reward:  132.68\n",
      "84.6 action:  [-0.4298, -0.9803] n_targets:  3 reward:  209.71\n",
      "86.4 action:  [0.1679, -0.987] n_targets:  2 reward:  152.14\n",
      "88.9 action:  [-0.1409, -0.9813] n_targets:  1 reward:  59.43\n",
      "89.3 action:  [-0.0252, -0.992] n_targets:  1 reward:  60.75\n",
      "91.0 action:  [0.2262, -0.9878] n_targets:  2 reward:  134.64\n",
      "97.9 action:  [0.1061, -0.9907] n_targets:  2 reward:  140.22\n",
      "99.7 action:  [0.5769, -0.9982] n_targets:  1 reward:  56.2\n",
      "100.3 action:  [-0.2423, -0.9907] n_targets:  1 reward:  64.27\n",
      "102.3 action:  [0.5567, -0.989] n_targets:  1 reward:  52.09\n",
      "ALPHA (entropy-related):  tensor([0.4393], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.53022 0.51899 0.50808 0.49747 0.48716 0.47703 0.4672  0.45771 0.44833\n",
      " 0.43925]\n",
      "Episode: 36, Episode Reward: 2653.0470390319824\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  196\n",
      "3.2 action:  [0.2704, -0.9934] n_targets:  1 reward:  55.02\n",
      "3.8 action:  [0.3172, -0.9969] n_targets:  1 reward:  64.86\n",
      "6.8 action:  [0.4863, -0.9923] n_targets:  1 reward:  78.87\n",
      "7.7 action:  [0.1552, -0.9932] n_targets:  1 reward:  84.68\n",
      "9.8 action:  [0.4223, -0.9807] n_targets:  1 reward:  59.22\n",
      "12.3 action:  [-0.3697, -0.9937] n_targets:  1 reward:  53.04\n",
      "15.8 action:  [0.4681, -0.9993] n_targets:  1 reward:  84.23\n",
      "23.3 action:  [-0.4878, -0.9933] n_targets:  3 reward:  242.88\n",
      "25.3 action:  [-0.2466, -0.9913] n_targets:  1 reward:  55.78\n",
      "28.8 action:  [0.304, -0.9943] n_targets:  1 reward:  84.84\n",
      "30.0 action:  [0.2046, -0.9847] n_targets:  1 reward:  57.08\n",
      "33.3 action:  [0.0095, -0.9913] n_targets:  2 reward:  109.14\n",
      "33.9 action:  [-0.582, -0.9902] n_targets:  1 reward:  52.26\n",
      "35.9 action:  [0.5825, -0.9911] n_targets:  2 reward:  139.02\n",
      "37.6 action:  [-0.1973, -0.9996] n_targets:  1 reward:  51.45\n",
      "39.2 action:  [-0.5494, -0.9821] n_targets:  4 reward:  275.98\n",
      "40.9 action:  [0.1869, -0.9857] n_targets:  1 reward:  51.55\n",
      "43.1 action:  [0.2437, -0.9962] n_targets:  1 reward:  53.01\n",
      "51.7 action:  [0.027, -0.9869] n_targets:  1 reward:  52.78\n",
      "55.1 action:  [-0.3466, -0.9906] n_targets:  1 reward:  98.04\n",
      "57.5 action:  [-0.1671, -0.9933] n_targets:  3 reward:  223.33\n",
      "58.3 action:  [0.219, -0.9914] n_targets:  1 reward:  55.48\n",
      "62.9 action:  [0.2996, -0.9827] n_targets:  1 reward:  50.34\n",
      "65.1 action:  [0.135, -0.986] n_targets:  1 reward:  86.27\n",
      "65.3 action:  [0.0173, -0.9829] n_targets:  1 reward:  51.05\n",
      "66.6 action:  [-0.2047, -0.9817] n_targets:  1 reward:  53.7\n",
      "70.4 action:  [0.5729, -0.9966] n_targets:  2 reward:  106.8\n",
      "71.2 action:  [-0.5107, -0.998] n_targets:  1 reward:  64.94\n",
      "72.4 action:  [0.5718, -0.9985] n_targets:  1 reward:  59.02\n",
      "73.6 action:  [0.4697, -0.988] n_targets:  1 reward:  56.45\n",
      "75.2 action:  [0.4042, -0.9907] n_targets:  1 reward:  67.58\n",
      "78.2 action:  [-0.2317, -0.9911] n_targets:  1 reward:  87.48\n",
      "79.1 action:  [0.4095, -0.9942] n_targets:  1 reward:  53.54\n",
      "83.0 action:  [0.5355, -0.9903] n_targets:  1 reward:  62.06\n",
      "85.6 action:  [-0.1973, -0.9898] n_targets:  1 reward:  53.97\n",
      "89.3 action:  [0.5598, -0.9803] n_targets:  1 reward:  54.9\n",
      "94.4 action:  [-0.0876, -0.9959] n_targets:  1 reward:  74.4\n",
      "96.6 action:  [0.142, -0.983] n_targets:  1 reward:  74.07\n",
      "97.7 action:  [0.3457, -0.9885] n_targets:  2 reward:  111.09\n",
      "100.0 action:  [0.6198, -0.9924] n_targets:  2 reward:  124.18\n",
      "102.2 action:  [0.461, -0.9957] n_targets:  1 reward:  54.26\n",
      "ALPHA (entropy-related):  tensor([0.4304], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.51899 0.50808 0.49747 0.48716 0.47703 0.4672  0.45771 0.44833 0.43925\n",
      " 0.43039]\n",
      "Episode: 37, Episode Reward: 3428.660981973012\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  197\n",
      "1.6 action:  [-0.0333, -0.9871] n_targets:  1 reward:  51.91\n",
      "4.9 action:  [0.0944, -0.9986] n_targets:  1 reward:  76.19\n",
      "6.1 action:  [0.5688, -0.9847] n_targets:  1 reward:  87.07\n",
      "13.4 action:  [0.4348, -0.9916] n_targets:  1 reward:  50.82\n",
      "18.2 action:  [-0.3671, -0.9877] n_targets:  1 reward:  61.6\n",
      "23.9 action:  [0.1614, -0.9947] n_targets:  1 reward:  75.2\n",
      "26.7 action:  [-0.4001, -0.9885] n_targets:  4 reward:  249.77\n",
      "36.3 action:  [0.5145, -0.996] n_targets:  3 reward:  186.73\n",
      "38.1 action:  [-0.2062, -0.9939] n_targets:  1 reward:  51.97\n",
      "38.6 action:  [-0.538, -0.9815] n_targets:  1 reward:  68.37\n",
      "39.2 action:  [0.2134, -0.9868] n_targets:  1 reward:  56.55\n",
      "40.2 action:  [-0.0015, -0.9968] n_targets:  1 reward:  66.48\n",
      "42.8 action:  [-0.2485, -0.9843] n_targets:  1 reward:  77.16\n",
      "43.4 action:  [0.381, -0.9844] n_targets:  1 reward:  60.34\n",
      "45.5 action:  [-0.5873, -0.9876] n_targets:  1 reward:  50.72\n",
      "46.7 action:  [-0.2095, -0.9929] n_targets:  1 reward:  98.02\n",
      "49.8 action:  [0.4958, -0.9984] n_targets:  1 reward:  61.18\n",
      "53.2 action:  [0.072, -0.9952] n_targets:  3 reward:  218.41\n",
      "57.4 action:  [-0.2832, -0.9857] n_targets:  2 reward:  142.48\n",
      "59.7 action:  [0.4601, -0.9926] n_targets:  2 reward:  139.92\n",
      "61.5 action:  [0.3588, -0.9993] n_targets:  1 reward:  68.77\n",
      "62.7 action:  [-0.4092, -0.9868] n_targets:  1 reward:  62.42\n",
      "70.1 action:  [0.5776, -0.9969] n_targets:  4 reward:  283.31\n",
      "71.2 action:  [0.0584, -0.9922] n_targets:  1 reward:  72.2\n",
      "76.1 action:  [-0.5532, -0.9997] n_targets:  1 reward:  53.79\n",
      "82.0 action:  [0.5276, -0.9987] n_targets:  3 reward:  202.66\n",
      "85.2 action:  [-0.3868, -0.9895] n_targets:  1 reward:  53.23\n",
      "85.8 action:  [0.2809, -0.9926] n_targets:  1 reward:  60.21\n",
      "86.4 action:  [0.4642, -0.9807] n_targets:  1 reward:  62.28\n",
      "87.2 action:  [0.3481, -0.9935] n_targets:  1 reward:  55.17\n",
      "90.2 action:  [0.4372, -0.9965] n_targets:  1 reward:  64.75\n",
      "91.0 action:  [0.012, -0.9946] n_targets:  1 reward:  68.37\n",
      "91.7 action:  [-0.4775, -0.9974] n_targets:  1 reward:  60.16\n",
      "93.0 action:  [-0.363, -0.9869] n_targets:  1 reward:  52.98\n",
      "98.2 action:  [0.4291, -0.9876] n_targets:  1 reward:  70.97\n",
      "100.5 action:  [-0.026, -0.9946] n_targets:  1 reward:  65.09\n",
      "ALPHA (entropy-related):  tensor([0.4217], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.50808 0.49747 0.48716 0.47703 0.4672  0.45771 0.44833 0.43925 0.43039\n",
      " 0.42165]\n",
      "Episode: 38, Episode Reward: 3287.2350937525425\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  198\n",
      "0.1 action:  [-0.5665, -0.9944] n_targets:  1 reward:  58.36\n",
      "7.2 action:  [0.5042, -0.9965] n_targets:  1 reward:  70.68\n",
      "9.9 action:  [0.2997, -0.9891] n_targets:  1 reward:  67.43\n",
      "12.6 action:  [0.0184, -0.9912] n_targets:  2 reward:  115.45\n",
      "13.1 action:  [-0.3127, -0.984] n_targets:  1 reward:  52.54\n",
      "16.0 action:  [-0.1508, -0.9876] n_targets:  1 reward:  79.94\n",
      "18.3 action:  [-0.007, -0.9907] n_targets:  2 reward:  134.35\n",
      "20.1 action:  [-0.2665, -0.9858] n_targets:  3 reward:  196.44\n",
      "20.5 action:  [0.0574, -0.9934] n_targets:  1 reward:  55.76\n",
      "23.4 action:  [-0.0129, -0.9927] n_targets:  1 reward:  58.63\n",
      "25.7 action:  [-0.5756, -0.9971] n_targets:  2 reward:  110.72\n",
      "28.3 action:  [0.4332, -0.9922] n_targets:  2 reward:  140.5\n",
      "31.1 action:  [0.2168, -0.9941] n_targets:  4 reward:  284.29\n",
      "33.2 action:  [-0.1096, -0.9868] n_targets:  1 reward:  69.29\n",
      "37.2 action:  [0.5467, -0.9871] n_targets:  1 reward:  50.96\n",
      "38.1 action:  [-0.0366, -0.9993] n_targets:  1 reward:  56.58\n",
      "42.4 action:  [0.0306, -0.9976] n_targets:  1 reward:  51.89\n",
      "51.7 action:  [0.2844, -0.9816] n_targets:  1 reward:  52.45\n",
      "52.6 action:  [-0.6382, -0.9894] n_targets:  1 reward:  53.94\n",
      "54.1 action:  [0.4362, -0.9854] n_targets:  1 reward:  72.55\n",
      "55.4 action:  [-0.2372, -0.9952] n_targets:  1 reward:  70.78\n",
      "59.7 action:  [-0.2287, -0.9856] n_targets:  1 reward:  55.87\n",
      "65.7 action:  [0.2084, -0.9937] n_targets:  1 reward:  59.89\n",
      "69.3 action:  [-0.4917, -0.9904] n_targets:  2 reward:  148.83\n",
      "70.7 action:  [-0.1693, -0.9879] n_targets:  1 reward:  65.22\n",
      "71.8 action:  [-0.1732, -0.9939] n_targets:  1 reward:  56.04\n",
      "73.5 action:  [-0.4774, -0.9953] n_targets:  1 reward:  52.21\n",
      "74.4 action:  [-0.3225, -0.9904] n_targets:  1 reward:  85.88\n",
      "82.5 action:  [0.0817, -0.9837] n_targets:  1 reward:  65.94\n",
      "91.2 action:  [-0.4319, -0.9948] n_targets:  2 reward:  120.77\n",
      "91.9 action:  [-0.1475, -0.9946] n_targets:  3 reward:  155.09\n",
      "93.6 action:  [0.2372, -0.9944] n_targets:  1 reward:  54.49\n",
      "94.2 action:  [-0.0264, -0.9923] n_targets:  1 reward:  52.93\n",
      "95.1 action:  [0.419, -0.9964] n_targets:  1 reward:  56.0\n",
      "97.7 action:  [0.1584, -0.9926] n_targets:  3 reward:  199.96\n",
      "99.5 action:  [-0.0153, -0.9955] n_targets:  1 reward:  61.37\n",
      "ALPHA (entropy-related):  tensor([0.4131], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.49747 0.48716 0.47703 0.4672  0.45771 0.44833 0.43925 0.43039 0.42165\n",
      " 0.41309]\n",
      "Episode: 39, Episode Reward: 3193.9933471679688\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  199\n",
      "0.1 action:  [0.1285, -0.9896] n_targets:  1 reward:  85.97\n",
      "3.1 action:  [-0.5455, -0.989] n_targets:  2 reward:  116.9\n",
      "3.4 action:  [0.3207, -0.9974] n_targets:  1 reward:  50.58\n",
      "5.9 action:  [0.5937, -0.9936] n_targets:  1 reward:  64.47\n",
      "9.9 action:  [0.2302, -0.9836] n_targets:  2 reward:  108.07\n",
      "11.8 action:  [-0.0114, -0.9877] n_targets:  1 reward:  57.52\n",
      "16.2 action:  [-0.4284, -0.9949] n_targets:  1 reward:  80.02\n",
      "16.7 action:  [-0.3664, -0.992] n_targets:  1 reward:  53.61\n",
      "17.8 action:  [-0.0249, -0.9911] n_targets:  2 reward:  123.33\n",
      "22.0 action:  [-0.0284, -0.9955] n_targets:  2 reward:  129.17\n",
      "22.6 action:  [0.3184, -0.9938] n_targets:  1 reward:  56.87\n",
      "28.5 action:  [-0.3524, -0.9918] n_targets:  5 reward:  321.37\n",
      "29.3 action:  [0.288, -0.9909] n_targets:  1 reward:  55.27\n",
      "31.6 action:  [0.261, -0.9824] n_targets:  2 reward:  113.21\n",
      "32.2 action:  [0.319, -0.9837] n_targets:  1 reward:  55.73\n",
      "37.4 action:  [0.1688, -0.9956] n_targets:  1 reward:  57.44\n",
      "40.0 action:  [0.104, -0.9954] n_targets:  1 reward:  72.08\n",
      "41.2 action:  [-0.3925, -0.9976] n_targets:  1 reward:  67.21\n",
      "41.7 action:  [0.5959, -0.9913] n_targets:  1 reward:  72.02\n",
      "45.9 action:  [0.6254, -0.9986] n_targets:  2 reward:  145.87\n",
      "47.4 action:  [0.6348, -0.9903] n_targets:  3 reward:  180.86\n",
      "50.8 action:  [-0.3901, -0.9809] n_targets:  2 reward:  181.98\n",
      "51.4 action:  [-0.2797, -0.9992] n_targets:  2 reward:  130.41\n",
      "53.7 action:  [0.0432, -0.9924] n_targets:  1 reward:  81.29\n",
      "54.8 action:  [0.3103, -0.9913] n_targets:  1 reward:  65.32\n",
      "59.3 action:  [-0.6164, -0.9815] n_targets:  1 reward:  76.41\n",
      "65.1 action:  [-0.2576, -0.9852] n_targets:  2 reward:  135.45\n",
      "67.9 action:  [0.2738, -0.9961] n_targets:  1 reward:  82.82\n",
      "70.1 action:  [-0.0794, -0.9985] n_targets:  3 reward:  230.76\n",
      "70.5 action:  [0.6342, -0.9961] n_targets:  2 reward:  104.15\n",
      "71.7 action:  [-0.3751, -0.9816] n_targets:  2 reward:  144.91\n",
      "83.2 action:  [0.3013, -0.9989] n_targets:  1 reward:  61.7\n",
      "85.9 action:  [-0.3717, -0.9979] n_targets:  2 reward:  143.56\n",
      "91.0 action:  [0.0154, -0.9917] n_targets:  2 reward:  131.55\n",
      "91.6 action:  [0.4203, -0.9979] n_targets:  1 reward:  51.33\n",
      "92.2 action:  [0.2445, -0.9866] n_targets:  3 reward:  171.05\n",
      "94.8 action:  [0.5819, -0.9956] n_targets:  4 reward:  264.95\n",
      "100.6 action:  [0.3673, -0.9942] n_targets:  1 reward:  53.43\n",
      "ALPHA (entropy-related):  tensor([0.4047], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.48716 0.47703 0.4672  0.45771 0.44833 0.43925 0.43039 0.42165 0.41309\n",
      " 0.40474]\n",
      "Episode: 40, Episode Reward: 4178.62940343221\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  200\n",
      "1.3 action:  [-0.4414, -0.9993] n_targets:  1 reward:  54.66\n",
      "2.5 action:  [0.1247, -0.9877] n_targets:  1 reward:  60.94\n",
      "3.9 action:  [0.1508, -0.9953] n_targets:  1 reward:  72.41\n",
      "7.3 action:  [0.3175, -0.9834] n_targets:  3 reward:  244.43\n",
      "10.3 action:  [0.1362, -0.9942] n_targets:  1 reward:  65.19\n",
      "13.5 action:  [0.2573, -0.9822] n_targets:  1 reward:  53.99\n",
      "15.0 action:  [-0.2738, -0.9865] n_targets:  1 reward:  63.92\n",
      "17.7 action:  [0.5334, -0.9978] n_targets:  1 reward:  62.15\n",
      "17.9 action:  [-0.5545, -0.9956] n_targets:  1 reward:  53.37\n",
      "21.6 action:  [-0.5419, -0.9943] n_targets:  1 reward:  61.22\n",
      "25.8 action:  [-0.6156, -0.9829] n_targets:  1 reward:  74.63\n",
      "30.7 action:  [0.3938, -0.9958] n_targets:  1 reward:  69.45\n",
      "32.6 action:  [-0.1025, -0.9953] n_targets:  1 reward:  54.9\n",
      "38.9 action:  [0.4785, -0.9991] n_targets:  1 reward:  79.54\n",
      "40.6 action:  [-0.0445, -0.9831] n_targets:  1 reward:  60.53\n",
      "41.2 action:  [0.1666, -0.9898] n_targets:  2 reward:  132.06\n",
      "43.9 action:  [-0.2805, -0.9944] n_targets:  1 reward:  56.27\n",
      "45.8 action:  [0.2428, -0.9938] n_targets:  2 reward:  101.56\n",
      "47.0 action:  [0.4712, -0.9994] n_targets:  3 reward:  200.05\n",
      "47.2 action:  [-0.0496, -0.9954] n_targets:  1 reward:  55.01\n",
      "47.8 action:  [-0.4983, -0.99] n_targets:  1 reward:  67.92\n",
      "50.7 action:  [0.0298, -0.9864] n_targets:  2 reward:  141.91\n",
      "52.0 action:  [0.1829, -0.9998] n_targets:  1 reward:  80.19\n",
      "53.7 action:  [-0.194, -0.9872] n_targets:  1 reward:  77.25\n",
      "54.3 action:  [-0.3646, -0.9974] n_targets:  1 reward:  58.11\n",
      "58.1 action:  [-0.5806, -0.9996] n_targets:  1 reward:  68.86\n",
      "58.5 action:  [-0.0438, -0.9874] n_targets:  1 reward:  63.4\n",
      "59.8 action:  [0.3819, -0.9905] n_targets:  1 reward:  77.16\n",
      "61.2 action:  [0.5475, -0.9873] n_targets:  2 reward:  143.91\n",
      "62.4 action:  [-0.1032, -0.9811] n_targets:  2 reward:  116.36\n",
      "62.8 action:  [0.0294, -0.9981] n_targets:  1 reward:  51.24\n",
      "63.2 action:  [-0.4869, -0.9914] n_targets:  1 reward:  50.61\n",
      "63.6 action:  [0.0634, -0.9872] n_targets:  2 reward:  109.96\n",
      "67.9 action:  [-0.4005, -0.9825] n_targets:  2 reward:  113.27\n",
      "69.2 action:  [-0.0824, -0.9885] n_targets:  1 reward:  58.42\n",
      "70.7 action:  [0.1815, -0.9854] n_targets:  2 reward:  137.96\n",
      "74.4 action:  [0.4319, -0.9981] n_targets:  1 reward:  54.57\n",
      "78.0 action:  [0.1311, -0.9866] n_targets:  2 reward:  130.3\n",
      "78.6 action:  [0.3231, -0.9905] n_targets:  1 reward:  52.87\n",
      "80.8 action:  [0.081, -0.9954] n_targets:  2 reward:  106.99\n",
      "89.2 action:  [-0.1636, -0.9926] n_targets:  2 reward:  128.75\n",
      "92.3 action:  [0.2845, -0.9851] n_targets:  1 reward:  54.53\n",
      "93.0 action:  [-0.0499, -0.9903] n_targets:  1 reward:  59.77\n",
      "98.1 action:  [0.5397, -0.9906] n_targets:  1 reward:  60.01\n",
      "101.1 action:  [0.1297, -0.9867] n_targets:  1 reward:  51.52\n",
      "ALPHA (entropy-related):  tensor([0.3965], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.47703 0.4672  0.45771 0.44833 0.43925 0.43039 0.42165 0.41309 0.40474\n",
      " 0.39652]\n",
      "Episode: 41, Episode Reward: 3792.110582987467\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  201\n",
      "0.5 action:  [0.3622, -0.9927] n_targets:  3 reward:  189.9\n",
      "2.9 action:  [-0.0442, -0.9909] n_targets:  1 reward:  69.29\n",
      "5.7 action:  [-0.2266, -0.9958] n_targets:  2 reward:  167.22\n",
      "7.5 action:  [-0.2835, -0.9815] n_targets:  1 reward:  69.92\n",
      "8.4 action:  [0.5236, -0.995] n_targets:  2 reward:  118.64\n",
      "11.4 action:  [0.2196, -0.9937] n_targets:  1 reward:  71.63\n",
      "17.1 action:  [-0.5214, -0.9929] n_targets:  2 reward:  138.79\n",
      "17.5 action:  [0.0509, -0.9992] n_targets:  1 reward:  54.49\n",
      "18.3 action:  [-0.0633, -0.9899] n_targets:  1 reward:  74.5\n",
      "20.6 action:  [0.0243, -0.995] n_targets:  1 reward:  52.89\n",
      "23.8 action:  [0.1899, -0.997] n_targets:  1 reward:  53.92\n",
      "25.2 action:  [-0.4252, -0.9982] n_targets:  3 reward:  174.96\n",
      "26.6 action:  [0.6086, -0.9827] n_targets:  2 reward:  139.49\n",
      "28.5 action:  [0.0745, -0.999] n_targets:  1 reward:  60.85\n",
      "31.0 action:  [0.2827, -0.9873] n_targets:  2 reward:  167.5\n",
      "33.5 action:  [-0.1345, -0.985] n_targets:  1 reward:  59.53\n",
      "34.0 action:  [-0.156, -0.9931] n_targets:  1 reward:  63.94\n",
      "35.9 action:  [-0.6372, -0.9961] n_targets:  2 reward:  113.54\n",
      "36.9 action:  [-0.0945, -0.9819] n_targets:  1 reward:  70.26\n",
      "37.6 action:  [0.1904, -0.9965] n_targets:  1 reward:  59.11\n",
      "38.1 action:  [-0.6072, -0.9902] n_targets:  2 reward:  116.8\n",
      "44.5 action:  [0.5536, -0.9904] n_targets:  1 reward:  70.61\n",
      "45.7 action:  [-0.0105, -0.9962] n_targets:  1 reward:  54.23\n",
      "45.9 action:  [0.4794, -0.9817] n_targets:  1 reward:  55.2\n",
      "46.3 action:  [-0.371, -0.9874] n_targets:  1 reward:  55.87\n",
      "49.9 action:  [0.1635, -0.9984] n_targets:  2 reward:  134.6\n",
      "52.1 action:  [-0.555, -0.9836] n_targets:  3 reward:  184.11\n",
      "53.8 action:  [0.1362, -0.9856] n_targets:  1 reward:  87.58\n",
      "55.6 action:  [0.1852, -0.9977] n_targets:  3 reward:  222.49\n",
      "57.0 action:  [0.2195, -0.9987] n_targets:  2 reward:  119.64\n",
      "57.4 action:  [-0.0093, -0.9923] n_targets:  2 reward:  126.51\n",
      "59.7 action:  [-0.3736, -0.9965] n_targets:  3 reward:  165.59\n",
      "71.4 action:  [0.4681, -0.9917] n_targets:  2 reward:  105.81\n",
      "74.7 action:  [0.6035, -0.9961] n_targets:  1 reward:  52.4\n",
      "77.3 action:  [-0.1719, -0.9921] n_targets:  1 reward:  68.05\n",
      "77.7 action:  [-0.0999, -0.986] n_targets:  1 reward:  67.87\n",
      "78.8 action:  [0.4306, -0.9854] n_targets:  2 reward:  122.04\n",
      "79.6 action:  [0.4325, -0.9975] n_targets:  1 reward:  59.41\n",
      "81.0 action:  [-0.3918, -0.9903] n_targets:  1 reward:  50.49\n",
      "82.2 action:  [0.0631, -0.9862] n_targets:  1 reward:  60.03\n",
      "82.4 action:  [-0.1234, -0.9927] n_targets:  1 reward:  50.73\n",
      "84.8 action:  [-0.4352, -0.9909] n_targets:  1 reward:  50.2\n",
      "85.7 action:  [-0.3988, -0.9921] n_targets:  1 reward:  50.89\n",
      "87.8 action:  [0.1204, -0.9894] n_targets:  2 reward:  121.47\n",
      "91.0 action:  [0.555, -0.9911] n_targets:  1 reward:  82.42\n",
      "93.0 action:  [-0.1937, -0.9838] n_targets:  1 reward:  88.1\n",
      "94.7 action:  [0.0375, -0.9932] n_targets:  1 reward:  57.97\n",
      "95.2 action:  [0.2938, -0.9965] n_targets:  1 reward:  52.33\n",
      "ALPHA (entropy-related):  tensor([0.3887], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.4672  0.45771 0.44833 0.43925 0.43039 0.42165 0.41309 0.40474 0.39652\n",
      " 0.38867]\n",
      "Episode: 42, Episode Reward: 4503.815953572592\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  202\n",
      "2.3 action:  [0.2436, -0.9884] n_targets:  2 reward:  116.08\n",
      "2.9 action:  [-0.3905, -0.9883] n_targets:  1 reward:  50.61\n",
      "6.3 action:  [-0.0146, -0.9863] n_targets:  1 reward:  50.16\n",
      "12.4 action:  [-0.1656, -0.9881] n_targets:  2 reward:  133.76\n",
      "15.6 action:  [0.4573, -0.9915] n_targets:  1 reward:  59.65\n",
      "24.1 action:  [0.0654, -0.9918] n_targets:  1 reward:  56.21\n",
      "24.7 action:  [-0.3568, -0.9808] n_targets:  1 reward:  60.5\n",
      "26.7 action:  [0.1478, -0.9977] n_targets:  1 reward:  54.86\n",
      "28.3 action:  [-0.4508, -0.985] n_targets:  1 reward:  89.25\n",
      "31.5 action:  [-0.0108, -0.9862] n_targets:  1 reward:  68.77\n",
      "32.4 action:  [0.5534, -0.9896] n_targets:  2 reward:  121.28\n",
      "38.8 action:  [0.2665, -0.9949] n_targets:  1 reward:  68.68\n",
      "40.3 action:  [-0.1251, -0.9968] n_targets:  2 reward:  129.72\n",
      "41.0 action:  [0.1315, -0.9985] n_targets:  1 reward:  51.59\n",
      "42.8 action:  [-0.2606, -0.9911] n_targets:  1 reward:  55.58\n",
      "44.8 action:  [0.0135, -0.9988] n_targets:  3 reward:  240.12\n",
      "45.3 action:  [-0.1689, -0.9812] n_targets:  1 reward:  70.99\n",
      "45.9 action:  [-0.4864, -0.9976] n_targets:  1 reward:  51.09\n",
      "48.9 action:  [0.3952, -0.9837] n_targets:  2 reward:  175.2\n",
      "49.7 action:  [-0.3636, -0.9951] n_targets:  1 reward:  67.43\n",
      "51.8 action:  [0.2398, -0.9951] n_targets:  1 reward:  71.69\n",
      "53.1 action:  [-0.5428, -0.9859] n_targets:  2 reward:  162.84\n",
      "54.2 action:  [-0.0559, -0.9973] n_targets:  1 reward:  60.71\n",
      "54.4 action:  [0.0821, -0.9965] n_targets:  2 reward:  110.55\n",
      "56.0 action:  [-0.099, -0.9989] n_targets:  1 reward:  75.95\n",
      "56.2 action:  [-0.2732, -0.9809] n_targets:  1 reward:  54.18\n",
      "57.8 action:  [-0.0721, -0.984] n_targets:  1 reward:  53.23\n",
      "58.4 action:  [-0.2977, -0.9929] n_targets:  1 reward:  58.19\n",
      "59.7 action:  [-0.4412, -0.9888] n_targets:  2 reward:  121.89\n",
      "62.7 action:  [-0.4679, -0.9928] n_targets:  1 reward:  52.97\n",
      "64.1 action:  [0.631, -0.9963] n_targets:  1 reward:  53.66\n",
      "69.0 action:  [-0.2799, -0.9889] n_targets:  1 reward:  54.31\n",
      "69.7 action:  [-0.1606, -0.9925] n_targets:  1 reward:  52.62\n",
      "71.1 action:  [0.1373, -0.985] n_targets:  1 reward:  62.74\n",
      "77.4 action:  [0.2775, -0.9908] n_targets:  1 reward:  55.91\n",
      "83.6 action:  [-0.4851, -0.9983] n_targets:  1 reward:  66.99\n",
      "98.4 action:  [-0.0574, -0.9899] n_targets:  1 reward:  66.42\n",
      "100.7 action:  [0.363, -0.999] n_targets:  1 reward:  58.88\n",
      "ALPHA (entropy-related):  tensor([0.3811], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.45771 0.44833 0.43925 0.43039 0.42165 0.41309 0.40474 0.39652 0.38867\n",
      " 0.38105]\n",
      "Episode: 43, Episode Reward: 3065.2469851175942\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  203\n",
      "1.2 action:  [0.3283, -0.9954] n_targets:  1 reward:  51.09\n",
      "3.2 action:  [-0.1613, -0.9964] n_targets:  4 reward:  244.32\n",
      "7.8 action:  [-0.1384, -0.9845] n_targets:  1 reward:  67.8\n",
      "9.4 action:  [-0.4168, -0.9849] n_targets:  1 reward:  50.47\n",
      "10.1 action:  [-0.3884, -0.9914] n_targets:  1 reward:  71.11\n",
      "15.8 action:  [0.1893, -0.9895] n_targets:  1 reward:  83.59\n",
      "18.6 action:  [0.4618, -0.9845] n_targets:  1 reward:  53.99\n",
      "18.9 action:  [0.1132, -0.9896] n_targets:  1 reward:  57.7\n",
      "20.3 action:  [0.0307, -0.9955] n_targets:  2 reward:  171.57\n",
      "29.5 action:  [0.1657, -0.9894] n_targets:  1 reward:  83.64\n",
      "34.5 action:  [-0.0636, -0.9845] n_targets:  2 reward:  119.01\n",
      "38.4 action:  [0.6216, -0.997] n_targets:  1 reward:  56.54\n",
      "39.2 action:  [0.0193, -0.9958] n_targets:  1 reward:  65.54\n",
      "39.9 action:  [0.0773, -0.9888] n_targets:  1 reward:  63.55\n",
      "40.2 action:  [0.0026, -0.9876] n_targets:  1 reward:  54.27\n",
      "43.9 action:  [0.1177, -1.0] n_targets:  1 reward:  62.62\n",
      "45.4 action:  [-0.0822, -0.9865] n_targets:  1 reward:  53.09\n",
      "50.1 action:  [0.1419, -0.9849] n_targets:  1 reward:  62.76\n",
      "51.9 action:  [-0.5661, -0.9906] n_targets:  1 reward:  57.45\n",
      "53.6 action:  [0.4878, -0.9862] n_targets:  1 reward:  65.68\n",
      "56.0 action:  [-0.4159, -0.986] n_targets:  1 reward:  65.82\n",
      "63.1 action:  [0.3225, -0.994] n_targets:  2 reward:  139.24\n",
      "64.4 action:  [0.1031, -0.9856] n_targets:  2 reward:  117.48\n",
      "66.1 action:  [-0.4281, -0.991] n_targets:  2 reward:  149.19\n",
      "72.6 action:  [0.0543, -0.9929] n_targets:  2 reward:  133.43\n",
      "77.9 action:  [-0.2648, -0.989] n_targets:  1 reward:  79.57\n",
      "88.3 action:  [0.0853, -0.9855] n_targets:  1 reward:  63.67\n",
      "88.8 action:  [0.1177, -0.9985] n_targets:  1 reward:  53.13\n",
      "91.1 action:  [-0.232, -0.9891] n_targets:  1 reward:  81.05\n",
      "93.5 action:  [-0.1316, -0.9958] n_targets:  2 reward:  107.33\n",
      "94.5 action:  [-0.4766, -0.9906] n_targets:  2 reward:  127.92\n",
      "97.1 action:  [-0.2584, -0.9808] n_targets:  3 reward:  233.57\n",
      "97.4 action:  [-0.0772, -0.9997] n_targets:  1 reward:  51.92\n",
      "98.2 action:  [0.332, -0.9846] n_targets:  1 reward:  76.03\n",
      "100.6 action:  [0.0477, -0.9968] n_targets:  1 reward:  74.62\n",
      "ALPHA (entropy-related):  tensor([0.3737], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.44833 0.43925 0.43039 0.42165 0.41309 0.40474 0.39652 0.38867 0.38105\n",
      " 0.37368]\n",
      "Episode: 44, Episode Reward: 3149.7476908365884\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  204\n",
      "1.2 action:  [-0.5561, -0.9962] n_targets:  1 reward:  52.28\n",
      "5.9 action:  [0.0843, -0.9991] n_targets:  1 reward:  65.56\n",
      "9.1 action:  [0.4999, -0.9914] n_targets:  1 reward:  56.58\n",
      "12.7 action:  [0.056, -0.99] n_targets:  4 reward:  283.82\n",
      "14.5 action:  [0.4941, -0.9861] n_targets:  2 reward:  119.68\n",
      "15.3 action:  [0.3366, -0.9923] n_targets:  4 reward:  243.92\n",
      "16.2 action:  [0.312, -0.9852] n_targets:  1 reward:  86.1\n",
      "19.1 action:  [-0.0689, -0.9986] n_targets:  1 reward:  74.86\n",
      "19.6 action:  [0.5605, -0.9987] n_targets:  1 reward:  50.52\n",
      "21.3 action:  [-0.3081, -0.9936] n_targets:  2 reward:  129.35\n",
      "22.1 action:  [0.3204, -0.9986] n_targets:  1 reward:  59.99\n",
      "23.0 action:  [0.3114, -0.9896] n_targets:  1 reward:  57.65\n",
      "24.3 action:  [0.3274, -0.9879] n_targets:  1 reward:  76.22\n",
      "26.4 action:  [0.3398, -0.9845] n_targets:  2 reward:  121.15\n",
      "27.1 action:  [0.0132, -0.9884] n_targets:  1 reward:  51.0\n",
      "27.3 action:  [0.4118, -0.9944] n_targets:  1 reward:  51.96\n",
      "31.8 action:  [-0.1758, -0.989] n_targets:  1 reward:  57.17\n",
      "34.1 action:  [0.1236, -0.9916] n_targets:  1 reward:  77.53\n",
      "35.0 action:  [0.4069, -0.9983] n_targets:  2 reward:  137.23\n",
      "35.8 action:  [-0.1208, -0.9973] n_targets:  1 reward:  53.13\n",
      "37.8 action:  [0.3183, -0.9996] n_targets:  2 reward:  135.41\n",
      "44.5 action:  [-0.0999, -0.9859] n_targets:  1 reward:  83.43\n",
      "46.3 action:  [-0.029, -0.9936] n_targets:  2 reward:  112.45\n",
      "46.5 action:  [-0.3381, -0.9975] n_targets:  1 reward:  59.92\n",
      "46.7 action:  [-0.4947, -0.9906] n_targets:  1 reward:  61.15\n",
      "47.8 action:  [0.2917, -0.9936] n_targets:  1 reward:  60.41\n",
      "48.0 action:  [0.3352, -0.9907] n_targets:  1 reward:  60.15\n",
      "50.5 action:  [0.1738, -0.9994] n_targets:  2 reward:  120.71\n",
      "53.8 action:  [0.0477, -0.986] n_targets:  1 reward:  71.6\n",
      "54.6 action:  [-0.0055, -0.9938] n_targets:  1 reward:  60.35\n",
      "55.5 action:  [0.2184, -0.9915] n_targets:  1 reward:  68.01\n",
      "60.0 action:  [0.2866, -0.9906] n_targets:  2 reward:  108.97\n",
      "61.9 action:  [0.51, -0.9973] n_targets:  1 reward:  58.52\n",
      "64.0 action:  [-0.1608, -0.9895] n_targets:  1 reward:  57.69\n",
      "70.3 action:  [0.0726, -0.98] n_targets:  1 reward:  68.13\n",
      "71.9 action:  [-0.4427, -0.9992] n_targets:  1 reward:  80.36\n",
      "73.0 action:  [-0.0134, -0.9891] n_targets:  1 reward:  63.7\n",
      "74.2 action:  [-0.1524, -0.9953] n_targets:  2 reward:  159.74\n",
      "79.7 action:  [0.203, -0.9948] n_targets:  1 reward:  54.07\n",
      "82.4 action:  [0.2356, -0.9881] n_targets:  1 reward:  67.8\n",
      "83.3 action:  [-0.4056, -0.9893] n_targets:  1 reward:  62.75\n",
      "84.6 action:  [-0.3264, -0.9897] n_targets:  1 reward:  58.04\n",
      "85.0 action:  [0.0648, -0.995] n_targets:  1 reward:  65.94\n",
      "92.1 action:  [-0.0224, -0.9932] n_targets:  3 reward:  181.85\n",
      "94.5 action:  [0.0418, -0.9827] n_targets:  2 reward:  114.63\n",
      "95.0 action:  [0.0812, -0.999] n_targets:  1 reward:  52.03\n",
      "100.1 action:  [-0.5134, -0.9894] n_targets:  2 reward:  141.85\n",
      "ALPHA (entropy-related):  tensor([0.3664], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.43925 0.43039 0.42165 0.41309 0.40474 0.39652 0.38867 0.38105 0.37368\n",
      " 0.36641]\n",
      "Last 100 ALPHA: [0.97788 0.95496 0.93215 0.90999 0.88852 0.86787 0.84794 0.82852 0.80965\n",
      " 0.79129 0.7734  0.75598 0.73901 0.72248 0.70635 0.69064 0.67531 0.66036\n",
      " 0.64578 0.63155 0.61769 0.60419 0.59106 0.5783  0.5658  0.55365 0.54182\n",
      " 0.53022 0.51899 0.50808 0.49747 0.48716 0.47703 0.4672  0.45771 0.44833\n",
      " 0.43925 0.43039 0.42165 0.41309 0.40474 0.39652 0.38867 0.38105 0.37368\n",
      " 0.36641]\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  205\n",
      "0.1 action:  [0.0849, -0.9899] n_targets:  2 reward:  125.25\n",
      "4.5 action:  [-0.0311, -0.9811] n_targets:  1 reward:  55.26\n",
      "10.1 action:  [-0.0316, -0.9803] n_targets:  1 reward:  67.48\n",
      "13.7 action:  [-0.0311, -0.981] n_targets:  3 reward:  174.65\n",
      "14.3 action:  [-0.0342, -0.9809] n_targets:  1 reward:  52.82\n",
      "19.9 action:  [-0.0314, -0.9811] n_targets:  2 reward:  165.65\n",
      "34.1 action:  [-0.0365, -0.98] n_targets:  1 reward:  64.14\n",
      "35.1 action:  [0.0963, -0.9814] n_targets:  1 reward:  66.5\n",
      "47.3 action:  [0.0039, -0.9866] n_targets:  1 reward:  51.29\n",
      "68.7 action:  [-0.0327, -0.9808] n_targets:  1 reward:  51.26\n",
      "76.9 action:  [-0.0355, -0.9801] n_targets:  1 reward:  60.79\n",
      "90.5 action:  [-0.0107, -0.9801] n_targets:  1 reward:  71.17\n",
      "96.7 action:  [-0.0288, -0.9813] n_targets:  3 reward:  209.43\n",
      "99.3 action:  [-0.0394, -0.9807] n_targets:  2 reward:  175.7\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  206\n",
      "7.3 action:  [-0.0352, -0.9805] n_targets:  2 reward:  128.45\n",
      "14.3 action:  [-0.0031, -0.9831] n_targets:  3 reward:  225.91\n",
      "17.3 action:  [-0.0216, -0.9813] n_targets:  4 reward:  253.69\n",
      "23.3 action:  [-0.0302, -0.9806] n_targets:  1 reward:  64.18\n",
      "32.3 action:  [-0.0274, -0.9804] n_targets:  1 reward:  67.94\n",
      "39.3 action:  [-0.0344, -0.9815] n_targets:  1 reward:  52.77\n",
      "41.3 action:  [-0.0336, -0.9803] n_targets:  4 reward:  246.63\n",
      "45.9 action:  [-0.0372, -0.9817] n_targets:  1 reward:  82.92\n",
      "47.5 action:  [-0.0292, -0.981] n_targets:  1 reward:  82.38\n",
      "47.7 action:  [-0.0295, -0.9807] n_targets:  1 reward:  51.03\n",
      "50.3 action:  [-0.0276, -0.98] n_targets:  1 reward:  53.51\n",
      "55.5 action:  [-0.0085, -0.9802] n_targets:  1 reward:  78.17\n",
      "55.7 action:  [-0.0143, -0.981] n_targets:  1 reward:  57.9\n",
      "59.3 action:  [-0.0333, -0.9805] n_targets:  1 reward:  76.74\n",
      "66.1 action:  [-0.0431, -0.9801] n_targets:  1 reward:  82.83\n",
      "67.1 action:  [-0.0206, -0.9814] n_targets:  1 reward:  75.45\n",
      "77.2 action:  [-0.0436, -0.9804] n_targets:  3 reward:  178.36\n",
      "85.8 action:  [-0.0357, -0.9801] n_targets:  1 reward:  65.62\n",
      "96.8 action:  [-0.0337, -0.981] n_targets:  2 reward:  126.08\n",
      "Best average reward: 0.0, Current average reward: 1720.9753100077312\n",
      "Best average reward = 1720.9753100077312\n",
      "Best model saved at episode 45 to None\n",
      "Evaluation rewards: [0.0, 0.0, 1720.9753100077312]\n",
      "Episode: 45, Episode Reward: 4195.330226898193\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  207\n",
      "0.7 action:  [0.1513, -0.999] n_targets:  1 reward:  55.16\n",
      "1.6 action:  [0.2261, -0.9849] n_targets:  1 reward:  65.06\n",
      "3.2 action:  [-0.2247, -0.993] n_targets:  2 reward:  135.75\n",
      "4.0 action:  [0.4074, -0.9935] n_targets:  1 reward:  56.16\n",
      "7.0 action:  [-0.0932, -0.9926] n_targets:  1 reward:  58.96\n",
      "8.7 action:  [0.1487, -0.9932] n_targets:  1 reward:  53.89\n",
      "11.8 action:  [0.1277, -0.9957] n_targets:  2 reward:  158.63\n",
      "12.7 action:  [0.0059, -0.9862] n_targets:  1 reward:  54.3\n",
      "16.0 action:  [0.1418, -0.9921] n_targets:  1 reward:  59.66\n",
      "19.2 action:  [-0.5549, -0.9835] n_targets:  1 reward:  76.2\n",
      "19.6 action:  [0.2943, -0.9995] n_targets:  1 reward:  50.75\n",
      "19.8 action:  [-0.0652, -0.9918] n_targets:  1 reward:  53.9\n",
      "21.0 action:  [0.3445, -0.9948] n_targets:  2 reward:  115.46\n",
      "21.5 action:  [0.0628, -0.9844] n_targets:  1 reward:  52.95\n",
      "23.4 action:  [-0.3185, -0.9903] n_targets:  1 reward:  55.85\n",
      "24.8 action:  [0.433, -0.9912] n_targets:  1 reward:  53.79\n",
      "25.3 action:  [0.0138, -0.9948] n_targets:  1 reward:  59.47\n",
      "26.4 action:  [0.1522, -0.9917] n_targets:  1 reward:  51.99\n",
      "40.6 action:  [-0.1332, -0.9959] n_targets:  1 reward:  50.4\n",
      "44.8 action:  [0.4629, -0.9829] n_targets:  1 reward:  58.11\n",
      "46.4 action:  [0.0916, -0.9912] n_targets:  1 reward:  52.41\n",
      "48.6 action:  [-0.1158, -0.9919] n_targets:  1 reward:  56.67\n",
      "54.1 action:  [-0.2159, -0.9886] n_targets:  2 reward:  161.04\n",
      "54.8 action:  [-0.1135, -0.9884] n_targets:  1 reward:  56.52\n",
      "55.2 action:  [-0.0708, -0.9962] n_targets:  1 reward:  61.28\n",
      "57.6 action:  [-0.0883, -0.9828] n_targets:  1 reward:  52.78\n",
      "60.1 action:  [0.2229, -0.9913] n_targets:  1 reward:  57.93\n",
      "60.9 action:  [-0.0831, -0.9833] n_targets:  1 reward:  74.13\n",
      "62.1 action:  [0.3378, -0.9829] n_targets:  1 reward:  53.01\n",
      "64.2 action:  [0.1181, -0.9806] n_targets:  2 reward:  145.34\n",
      "66.1 action:  [-0.1586, -0.9934] n_targets:  1 reward:  62.06\n",
      "67.2 action:  [-0.0711, -0.9953] n_targets:  1 reward:  57.1\n",
      "70.1 action:  [0.2219, -0.9963] n_targets:  2 reward:  117.11\n",
      "71.1 action:  [-0.159, -0.9836] n_targets:  1 reward:  52.46\n",
      "72.4 action:  [0.0323, -0.9892] n_targets:  2 reward:  150.52\n",
      "78.9 action:  [-0.6119, -0.9942] n_targets:  1 reward:  57.9\n",
      "80.0 action:  [0.2606, -0.9918] n_targets:  1 reward:  51.05\n",
      "82.6 action:  [-0.2204, -0.9966] n_targets:  1 reward:  85.07\n",
      "84.2 action:  [-0.1012, -0.9848] n_targets:  1 reward:  53.27\n",
      "86.4 action:  [-0.2516, -0.998] n_targets:  2 reward:  150.95\n",
      "87.6 action:  [-0.4162, -0.9917] n_targets:  1 reward:  53.19\n",
      "90.7 action:  [-0.2596, -0.9879] n_targets:  2 reward:  147.11\n",
      "91.3 action:  [0.103, -0.9963] n_targets:  1 reward:  65.14\n",
      "92.7 action:  [-0.3805, -0.9979] n_targets:  1 reward:  63.88\n",
      "93.5 action:  [-0.5853, -0.9838] n_targets:  1 reward:  76.2\n",
      "96.3 action:  [-0.031, -0.994] n_targets:  1 reward:  62.43\n",
      "96.8 action:  [-0.1228, -0.9855] n_targets:  2 reward:  108.04\n",
      "97.7 action:  [-0.0396, -0.9911] n_targets:  1 reward:  54.33\n",
      "98.2 action:  [-0.0382, -0.9893] n_targets:  2 reward:  109.19\n",
      "99.6 action:  [0.3517, -0.9973] n_targets:  1 reward:  57.87\n",
      "ALPHA (entropy-related):  tensor([0.3593], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.43039 0.42165 0.41309 0.40474 0.39652 0.38867 0.38105 0.37368 0.36641\n",
      " 0.35933]\n",
      "Episode: 46, Episode Reward: 3782.402512868245\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  208\n",
      "0.9 action:  [0.4195, -0.9804] n_targets:  1 reward:  67.86\n",
      "2.5 action:  [-0.3139, -0.988] n_targets:  1 reward:  79.12\n",
      "4.3 action:  [0.0138, -0.9857] n_targets:  1 reward:  56.7\n",
      "6.5 action:  [-0.1795, -0.9828] n_targets:  1 reward:  65.3\n",
      "7.2 action:  [0.0173, -0.9815] n_targets:  1 reward:  50.48\n",
      "11.4 action:  [-0.1789, -0.9893] n_targets:  1 reward:  66.59\n",
      "13.9 action:  [-0.4339, -0.9921] n_targets:  1 reward:  55.91\n",
      "16.3 action:  [0.3752, -0.9979] n_targets:  1 reward:  63.47\n",
      "18.0 action:  [-0.1145, -0.9864] n_targets:  1 reward:  87.6\n",
      "18.7 action:  [-0.0742, -0.9852] n_targets:  1 reward:  70.8\n",
      "19.1 action:  [0.5175, -0.9979] n_targets:  1 reward:  65.56\n",
      "20.3 action:  [0.6153, -0.9939] n_targets:  1 reward:  67.66\n",
      "27.1 action:  [-0.2062, -0.9918] n_targets:  1 reward:  59.59\n",
      "28.6 action:  [-0.1296, -0.9966] n_targets:  1 reward:  56.02\n",
      "29.9 action:  [-0.523, -0.9823] n_targets:  1 reward:  51.1\n",
      "31.1 action:  [0.356, -0.9951] n_targets:  1 reward:  50.26\n",
      "32.5 action:  [-0.1762, -0.9828] n_targets:  2 reward:  136.32\n",
      "36.0 action:  [-0.1578, -0.9818] n_targets:  1 reward:  59.93\n",
      "37.1 action:  [-0.3254, -0.9869] n_targets:  1 reward:  52.65\n",
      "38.1 action:  [-0.352, -0.9919] n_targets:  1 reward:  59.89\n",
      "38.8 action:  [0.1039, -0.9867] n_targets:  1 reward:  77.12\n",
      "40.7 action:  [-0.5185, -0.9965] n_targets:  1 reward:  92.81\n",
      "41.3 action:  [0.3837, -0.9911] n_targets:  1 reward:  55.55\n",
      "43.0 action:  [-0.2052, -0.9911] n_targets:  2 reward:  119.96\n",
      "43.6 action:  [0.1112, -0.9892] n_targets:  1 reward:  53.13\n",
      "48.0 action:  [-0.2387, -0.9986] n_targets:  2 reward:  121.01\n",
      "49.2 action:  [0.1061, -0.9833] n_targets:  1 reward:  69.7\n",
      "50.4 action:  [0.1293, -0.9948] n_targets:  1 reward:  54.45\n",
      "51.9 action:  [0.0686, -0.9972] n_targets:  1 reward:  77.03\n",
      "56.6 action:  [0.1997, -0.983] n_targets:  1 reward:  66.01\n",
      "57.2 action:  [-0.3957, -0.9914] n_targets:  1 reward:  63.36\n",
      "58.0 action:  [-0.2208, -0.9836] n_targets:  1 reward:  57.58\n",
      "58.7 action:  [-0.1305, -0.996] n_targets:  2 reward:  119.22\n",
      "61.0 action:  [-0.5519, -0.9878] n_targets:  1 reward:  63.1\n",
      "61.7 action:  [0.1033, -0.9932] n_targets:  2 reward:  127.51\n",
      "63.0 action:  [-0.4992, -0.9936] n_targets:  1 reward:  51.93\n",
      "64.3 action:  [0.4966, -0.992] n_targets:  2 reward:  116.62\n",
      "65.5 action:  [-0.0954, -0.9937] n_targets:  2 reward:  127.87\n",
      "67.0 action:  [-0.5052, -0.9834] n_targets:  2 reward:  125.59\n",
      "67.8 action:  [-0.208, -0.9818] n_targets:  1 reward:  71.73\n",
      "68.8 action:  [-0.0818, -0.9926] n_targets:  2 reward:  120.83\n",
      "70.0 action:  [-0.064, -0.9873] n_targets:  1 reward:  55.4\n",
      "70.3 action:  [-0.6334, -0.9858] n_targets:  1 reward:  50.38\n",
      "71.5 action:  [0.6051, -0.9874] n_targets:  2 reward:  138.85\n",
      "72.5 action:  [-0.4226, -0.9869] n_targets:  1 reward:  71.05\n",
      "79.0 action:  [0.1571, -0.9852] n_targets:  3 reward:  177.49\n",
      "88.5 action:  [-0.6226, -0.9979] n_targets:  1 reward:  55.56\n",
      "90.0 action:  [0.4176, -0.987] n_targets:  2 reward:  142.12\n",
      "90.6 action:  [-0.4817, -0.9972] n_targets:  1 reward:  60.68\n",
      "92.6 action:  [-0.2928, -0.994] n_targets:  1 reward:  57.09\n",
      "98.0 action:  [0.3585, -0.9816] n_targets:  1 reward:  54.4\n",
      "ALPHA (entropy-related):  tensor([0.3527], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.42165 0.41309 0.40474 0.39652 0.38867 0.38105 0.37368 0.36641 0.35933\n",
      " 0.35272]\n",
      "Episode: 47, Episode Reward: 4017.9318383534746\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  209\n",
      "0.5 action:  [-0.2021, -0.9875] n_targets:  1 reward:  50.86\n",
      "1.7 action:  [0.6232, -0.9887] n_targets:  1 reward:  56.65\n",
      "3.6 action:  [0.2055, -0.9883] n_targets:  1 reward:  59.55\n",
      "4.6 action:  [-0.3293, -0.9894] n_targets:  1 reward:  54.73\n",
      "5.3 action:  [-0.5629, -0.9853] n_targets:  1 reward:  62.68\n",
      "12.1 action:  [-0.3797, -0.9955] n_targets:  1 reward:  79.11\n",
      "14.0 action:  [-0.175, -0.9984] n_targets:  1 reward:  72.57\n",
      "15.7 action:  [-0.5006, -0.9932] n_targets:  1 reward:  52.37\n",
      "16.6 action:  [-0.2707, -0.9929] n_targets:  1 reward:  57.42\n",
      "17.3 action:  [-0.2227, -0.9851] n_targets:  1 reward:  58.23\n",
      "18.2 action:  [-0.332, -0.987] n_targets:  1 reward:  70.61\n",
      "19.3 action:  [-0.5437, -0.996] n_targets:  1 reward:  71.74\n",
      "20.6 action:  [-0.1295, -0.9894] n_targets:  1 reward:  74.12\n",
      "23.9 action:  [0.1628, -0.9918] n_targets:  1 reward:  56.16\n",
      "26.7 action:  [0.3546, -0.997] n_targets:  1 reward:  56.51\n",
      "28.8 action:  [-0.1164, -0.9852] n_targets:  1 reward:  57.62\n",
      "29.8 action:  [-0.2008, -0.992] n_targets:  1 reward:  50.39\n",
      "33.7 action:  [-0.5847, -0.9814] n_targets:  1 reward:  58.19\n",
      "35.3 action:  [-0.0639, -0.9865] n_targets:  2 reward:  123.64\n",
      "35.8 action:  [0.2803, -0.9834] n_targets:  1 reward:  68.68\n",
      "39.9 action:  [0.337, -0.9947] n_targets:  1 reward:  72.18\n",
      "44.9 action:  [-0.3189, -0.9827] n_targets:  1 reward:  59.16\n",
      "45.3 action:  [0.2445, -0.9878] n_targets:  1 reward:  52.24\n",
      "45.9 action:  [-0.0195, -0.9891] n_targets:  1 reward:  56.87\n",
      "46.9 action:  [-0.2638, -0.9905] n_targets:  1 reward:  55.35\n",
      "47.9 action:  [0.5528, -0.9934] n_targets:  2 reward:  143.06\n",
      "49.7 action:  [0.3538, -0.9936] n_targets:  1 reward:  79.16\n",
      "51.8 action:  [0.0065, -0.9905] n_targets:  1 reward:  70.64\n",
      "52.4 action:  [-0.2095, -0.9962] n_targets:  1 reward:  62.72\n",
      "53.6 action:  [0.1649, -0.9977] n_targets:  1 reward:  50.42\n",
      "54.3 action:  [-0.1916, -0.9976] n_targets:  2 reward:  119.04\n",
      "56.7 action:  [-0.2229, -0.9993] n_targets:  1 reward:  57.0\n",
      "57.2 action:  [0.3238, -0.9985] n_targets:  2 reward:  114.26\n",
      "60.1 action:  [-0.2386, -0.9894] n_targets:  2 reward:  153.39\n",
      "62.5 action:  [0.4082, -0.9853] n_targets:  1 reward:  59.7\n",
      "63.8 action:  [0.0304, -0.9961] n_targets:  1 reward:  55.16\n",
      "67.2 action:  [0.4382, -0.9884] n_targets:  1 reward:  56.69\n",
      "68.7 action:  [-0.2374, -0.9876] n_targets:  1 reward:  86.95\n",
      "70.0 action:  [0.0554, -0.9843] n_targets:  3 reward:  203.3\n",
      "70.9 action:  [-0.005, -0.9869] n_targets:  1 reward:  75.34\n",
      "72.4 action:  [0.0112, -0.9918] n_targets:  1 reward:  54.44\n",
      "72.9 action:  [-0.1726, -0.983] n_targets:  1 reward:  51.83\n",
      "75.1 action:  [0.4886, -0.9821] n_targets:  1 reward:  62.3\n",
      "75.5 action:  [0.2408, -0.9949] n_targets:  1 reward:  64.45\n",
      "78.4 action:  [-0.2172, -0.9852] n_targets:  1 reward:  61.2\n",
      "79.2 action:  [-0.3681, -0.9825] n_targets:  1 reward:  88.05\n",
      "84.8 action:  [0.1336, -0.9914] n_targets:  1 reward:  54.2\n",
      "86.7 action:  [-0.3, -0.9982] n_targets:  1 reward:  70.0\n",
      "87.4 action:  [-0.1273, -0.9979] n_targets:  2 reward:  128.93\n",
      "88.2 action:  [0.3778, -0.9868] n_targets:  1 reward:  67.49\n",
      "89.3 action:  [0.0682, -0.9992] n_targets:  2 reward:  123.2\n",
      "92.1 action:  [0.018, -0.9959] n_targets:  4 reward:  274.7\n",
      "92.5 action:  [-0.4213, -0.994] n_targets:  1 reward:  60.84\n",
      "94.7 action:  [0.3094, -0.9957] n_targets:  2 reward:  182.57\n",
      "96.9 action:  [0.3526, -0.999] n_targets:  1 reward:  52.1\n",
      "97.6 action:  [-0.0845, -0.99] n_targets:  2 reward:  103.94\n",
      "98.2 action:  [0.2911, -0.9948] n_targets:  1 reward:  63.49\n",
      "98.6 action:  [-0.5386, -0.9974] n_targets:  1 reward:  59.02\n",
      "ALPHA (entropy-related):  tensor([0.3462], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.41309 0.40474 0.39652 0.38867 0.38105 0.37368 0.36641 0.35933 0.35272\n",
      " 0.34622]\n",
      "Episode: 48, Episode Reward: 4597.207439422607\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  210\n",
      "0.1 action:  [-0.3391, -0.9925] n_targets:  1 reward:  59.68\n",
      "1.0 action:  [0.5067, -0.9875] n_targets:  1 reward:  51.64\n",
      "2.3 action:  [0.2232, -0.9847] n_targets:  1 reward:  62.66\n",
      "7.0 action:  [0.0974, -0.9968] n_targets:  2 reward:  126.97\n",
      "9.6 action:  [0.3027, -0.9847] n_targets:  2 reward:  157.83\n",
      "10.1 action:  [0.231, -0.9897] n_targets:  1 reward:  54.13\n",
      "21.9 action:  [-0.2599, -0.9882] n_targets:  1 reward:  77.41\n",
      "22.8 action:  [-0.1538, -0.9943] n_targets:  1 reward:  63.67\n",
      "25.2 action:  [0.2364, -0.9871] n_targets:  1 reward:  67.84\n",
      "25.9 action:  [0.1931, -0.9953] n_targets:  1 reward:  50.97\n",
      "28.7 action:  [-0.0791, -0.996] n_targets:  2 reward:  134.73\n",
      "29.3 action:  [0.4484, -0.999] n_targets:  1 reward:  68.26\n",
      "30.8 action:  [-0.2857, -0.9953] n_targets:  1 reward:  51.58\n",
      "31.9 action:  [-0.0154, -0.9895] n_targets:  2 reward:  160.7\n",
      "33.4 action:  [-0.1151, -0.981] n_targets:  1 reward:  53.08\n",
      "33.6 action:  [-0.1039, -0.9902] n_targets:  1 reward:  59.75\n",
      "35.3 action:  [0.2019, -0.9927] n_targets:  4 reward:  280.89\n",
      "36.5 action:  [0.1523, -0.9842] n_targets:  1 reward:  67.13\n",
      "37.7 action:  [0.6158, -0.9894] n_targets:  1 reward:  87.1\n",
      "40.2 action:  [-0.1791, -0.9976] n_targets:  2 reward:  157.03\n",
      "43.5 action:  [-0.0492, -0.9961] n_targets:  1 reward:  50.56\n",
      "44.8 action:  [-0.051, -0.9942] n_targets:  2 reward:  141.45\n",
      "46.3 action:  [-0.0295, -0.9915] n_targets:  1 reward:  65.89\n",
      "47.5 action:  [0.4346, -0.9911] n_targets:  2 reward:  132.37\n",
      "50.1 action:  [0.2304, -0.9871] n_targets:  1 reward:  54.41\n",
      "50.5 action:  [-0.2136, -0.9863] n_targets:  1 reward:  51.0\n",
      "52.0 action:  [0.1513, -0.9846] n_targets:  1 reward:  56.07\n",
      "53.0 action:  [-0.5842, -0.9974] n_targets:  1 reward:  94.18\n",
      "55.3 action:  [-0.1938, -0.9934] n_targets:  1 reward:  61.57\n",
      "57.1 action:  [-0.2208, -0.9916] n_targets:  1 reward:  52.21\n",
      "59.8 action:  [0.2394, -0.9887] n_targets:  1 reward:  53.52\n",
      "60.6 action:  [0.1452, -0.9915] n_targets:  2 reward:  146.49\n",
      "61.6 action:  [-0.1563, -0.9974] n_targets:  1 reward:  81.99\n",
      "62.2 action:  [-0.1953, -0.9823] n_targets:  2 reward:  132.12\n",
      "62.8 action:  [-0.309, -0.9816] n_targets:  1 reward:  73.48\n",
      "63.9 action:  [0.593, -0.9902] n_targets:  1 reward:  62.61\n",
      "64.7 action:  [0.0741, -0.9852] n_targets:  1 reward:  58.06\n",
      "69.9 action:  [-0.4387, -0.9852] n_targets:  2 reward:  132.6\n",
      "71.0 action:  [0.5624, -0.9914] n_targets:  1 reward:  50.36\n",
      "71.8 action:  [-0.3633, -0.9948] n_targets:  1 reward:  54.17\n",
      "74.6 action:  [-0.0999, -0.9831] n_targets:  2 reward:  144.11\n",
      "79.4 action:  [-0.3072, -0.9865] n_targets:  1 reward:  81.04\n",
      "80.8 action:  [-0.4257, -0.9852] n_targets:  1 reward:  59.78\n",
      "81.2 action:  [0.3407, -0.9826] n_targets:  1 reward:  57.62\n",
      "85.2 action:  [-0.1885, -0.9854] n_targets:  2 reward:  126.02\n",
      "87.2 action:  [0.4276, -0.9833] n_targets:  1 reward:  74.95\n",
      "92.5 action:  [-0.222, -0.9879] n_targets:  1 reward:  56.51\n",
      "92.7 action:  [-0.014, -0.9809] n_targets:  1 reward:  59.33\n",
      "94.1 action:  [-0.118, -0.9886] n_targets:  1 reward:  61.92\n",
      "94.5 action:  [0.0943, -0.9912] n_targets:  1 reward:  58.92\n",
      "96.9 action:  [-0.079, -0.9859] n_targets:  1 reward:  62.34\n",
      "ALPHA (entropy-related):  tensor([0.3399], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.40474 0.39652 0.38867 0.38105 0.37368 0.36641 0.35933 0.35272 0.34622\n",
      " 0.33993]\n",
      "Episode: 49, Episode Reward: 4340.697491327921\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  211\n",
      "0.1 action:  [0.1255, -0.9979] n_targets:  1 reward:  50.51\n",
      "0.5 action:  [-0.304, -0.9936] n_targets:  1 reward:  61.08\n",
      "1.9 action:  [0.3437, -0.9866] n_targets:  1 reward:  51.33\n",
      "3.5 action:  [-0.0459, -0.9996] n_targets:  1 reward:  53.33\n",
      "4.2 action:  [-0.2699, -0.9865] n_targets:  1 reward:  56.56\n",
      "5.2 action:  [0.2792, -0.9809] n_targets:  1 reward:  51.57\n",
      "6.6 action:  [0.1419, -0.9803] n_targets:  1 reward:  53.59\n",
      "8.2 action:  [-0.0919, -0.9832] n_targets:  2 reward:  155.54\n",
      "9.0 action:  [0.4628, -0.9984] n_targets:  1 reward:  61.6\n",
      "9.6 action:  [0.16, -0.9861] n_targets:  1 reward:  52.8\n",
      "10.0 action:  [-0.0451, -0.9892] n_targets:  1 reward:  50.36\n",
      "11.2 action:  [-0.4567, -0.992] n_targets:  2 reward:  139.49\n",
      "11.4 action:  [-0.2877, -0.9893] n_targets:  1 reward:  57.84\n",
      "15.0 action:  [-0.2669, -0.9824] n_targets:  2 reward:  111.19\n",
      "15.7 action:  [0.1906, -0.987] n_targets:  2 reward:  118.02\n",
      "17.1 action:  [0.4035, -0.9925] n_targets:  1 reward:  70.66\n",
      "18.8 action:  [0.2944, -0.9999] n_targets:  1 reward:  52.35\n",
      "19.7 action:  [0.0172, -0.9898] n_targets:  1 reward:  54.23\n",
      "20.3 action:  [0.1907, -0.9958] n_targets:  1 reward:  62.85\n",
      "22.0 action:  [-0.2727, -0.9831] n_targets:  2 reward:  126.37\n",
      "23.8 action:  [-0.172, -0.9955] n_targets:  1 reward:  58.29\n",
      "26.2 action:  [0.1236, -0.9915] n_targets:  1 reward:  67.24\n",
      "28.0 action:  [-0.2512, -0.9972] n_targets:  2 reward:  144.22\n",
      "28.8 action:  [0.2083, -0.9886] n_targets:  1 reward:  63.43\n",
      "29.8 action:  [0.1036, -0.9894] n_targets:  1 reward:  50.33\n",
      "30.0 action:  [-0.1384, -0.9905] n_targets:  2 reward:  112.64\n",
      "30.6 action:  [0.0863, -0.9858] n_targets:  1 reward:  57.4\n",
      "31.0 action:  [0.133, -0.9898] n_targets:  1 reward:  51.66\n",
      "32.0 action:  [0.3735, -0.9829] n_targets:  1 reward:  60.96\n",
      "32.8 action:  [-0.0455, -0.9833] n_targets:  1 reward:  79.73\n",
      "37.0 action:  [-0.1981, -0.9914] n_targets:  1 reward:  80.74\n",
      "39.9 action:  [0.4567, -0.9899] n_targets:  1 reward:  55.29\n",
      "41.5 action:  [-0.6191, -0.999] n_targets:  2 reward:  131.0\n",
      "44.0 action:  [-0.5233, -0.9879] n_targets:  3 reward:  164.07\n",
      "46.4 action:  [0.1912, -0.9806] n_targets:  1 reward:  55.69\n",
      "47.7 action:  [-0.6351, -0.9929] n_targets:  3 reward:  193.52\n",
      "48.8 action:  [0.4509, -0.9872] n_targets:  1 reward:  60.43\n",
      "55.1 action:  [0.2254, -0.9957] n_targets:  1 reward:  77.76\n",
      "56.9 action:  [0.0295, -0.9809] n_targets:  1 reward:  54.72\n",
      "57.1 action:  [0.2919, -0.9931] n_targets:  1 reward:  54.71\n",
      "62.8 action:  [-0.5341, -0.995] n_targets:  3 reward:  203.28\n",
      "64.4 action:  [0.0435, -0.9846] n_targets:  1 reward:  85.42\n",
      "66.1 action:  [-0.1892, -0.9835] n_targets:  2 reward:  178.99\n",
      "66.3 action:  [-0.2035, -0.9802] n_targets:  2 reward:  109.55\n",
      "67.3 action:  [-0.0243, -0.9809] n_targets:  1 reward:  51.35\n",
      "68.1 action:  [0.3363, -0.9888] n_targets:  1 reward:  88.97\n",
      "68.9 action:  [-0.1346, -0.9989] n_targets:  2 reward:  134.6\n",
      "70.3 action:  [-0.1265, -0.993] n_targets:  1 reward:  74.51\n",
      "71.8 action:  [-0.486, -0.99] n_targets:  1 reward:  59.98\n",
      "74.7 action:  [-0.1346, -0.9831] n_targets:  1 reward:  50.38\n",
      "77.8 action:  [-0.5379, -0.9879] n_targets:  2 reward:  142.49\n",
      "79.3 action:  [-0.3287, -0.9884] n_targets:  3 reward:  194.7\n",
      "80.9 action:  [-0.3146, -0.9832] n_targets:  2 reward:  112.47\n",
      "82.0 action:  [0.1951, -0.9962] n_targets:  1 reward:  53.47\n",
      "84.6 action:  [-0.0935, -0.9849] n_targets:  4 reward:  345.96\n",
      "87.0 action:  [-0.2638, -0.9806] n_targets:  2 reward:  105.86\n",
      "88.0 action:  [0.015, -0.9857] n_targets:  1 reward:  75.94\n",
      "89.0 action:  [0.3107, -0.9984] n_targets:  1 reward:  60.03\n",
      "94.7 action:  [0.2907, -0.997] n_targets:  1 reward:  82.27\n",
      "95.9 action:  [-0.4544, -0.9813] n_targets:  1 reward:  82.1\n",
      "98.1 action:  [-0.4118, -0.988] n_targets:  3 reward:  210.18\n",
      "99.1 action:  [-0.2616, -0.9869] n_targets:  1 reward:  77.18\n",
      "99.9 action:  [-0.418, -0.995] n_targets:  1 reward:  51.87\n",
      "102.0 action:  [0.4176, -0.9921] n_targets:  2 reward:  107.83\n",
      "ALPHA (entropy-related):  tensor([0.3340], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.39652 0.38867 0.38105 0.37368 0.36641 0.35933 0.35272 0.34622 0.33993\n",
      " 0.33402]\n",
      "Episode: 50, Episode Reward: 5904.498582204183\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  212\n",
      "0.7 action:  [-0.2469, -0.9882] n_targets:  1 reward:  79.68\n",
      "2.8 action:  [-0.58, -0.9854] n_targets:  1 reward:  53.87\n",
      "4.0 action:  [0.1772, -0.9812] n_targets:  3 reward:  221.26\n",
      "5.8 action:  [-0.171, -0.9847] n_targets:  1 reward:  73.59\n",
      "9.6 action:  [0.358, -0.9907] n_targets:  2 reward:  112.36\n",
      "13.1 action:  [-0.0131, -0.9907] n_targets:  1 reward:  57.82\n",
      "16.2 action:  [0.2181, -0.9895] n_targets:  3 reward:  214.03\n",
      "16.8 action:  [-0.0715, -0.9962] n_targets:  1 reward:  53.95\n",
      "19.4 action:  [0.2819, -0.9994] n_targets:  1 reward:  52.37\n",
      "20.2 action:  [0.0764, -0.987] n_targets:  4 reward:  283.49\n",
      "21.4 action:  [0.1189, -0.9935] n_targets:  1 reward:  69.25\n",
      "22.3 action:  [-0.0705, -0.9804] n_targets:  2 reward:  125.81\n",
      "22.5 action:  [-0.4979, -0.994] n_targets:  1 reward:  55.02\n",
      "23.1 action:  [-0.2284, -0.9935] n_targets:  1 reward:  50.39\n",
      "24.4 action:  [-0.4987, -0.9936] n_targets:  2 reward:  131.73\n",
      "25.8 action:  [-0.0541, -0.9952] n_targets:  1 reward:  77.69\n",
      "26.2 action:  [0.4723, -0.9866] n_targets:  1 reward:  52.21\n",
      "28.4 action:  [0.2378, -0.9976] n_targets:  2 reward:  125.84\n",
      "29.2 action:  [-0.0994, -0.9921] n_targets:  1 reward:  76.09\n",
      "31.5 action:  [0.1439, -0.997] n_targets:  1 reward:  51.12\n",
      "33.1 action:  [-0.1187, -0.9828] n_targets:  1 reward:  52.28\n",
      "34.3 action:  [-0.2971, -0.9808] n_targets:  1 reward:  51.47\n",
      "35.2 action:  [-0.2003, -0.9989] n_targets:  1 reward:  64.05\n",
      "35.8 action:  [-0.3259, -0.9946] n_targets:  1 reward:  60.31\n",
      "36.2 action:  [0.0104, -0.9864] n_targets:  1 reward:  54.43\n",
      "37.6 action:  [-0.2286, -0.9834] n_targets:  3 reward:  169.6\n",
      "38.0 action:  [0.4203, -0.9918] n_targets:  1 reward:  52.78\n",
      "39.0 action:  [0.3805, -0.9822] n_targets:  1 reward:  57.82\n",
      "40.0 action:  [0.4356, -0.9966] n_targets:  1 reward:  58.65\n",
      "42.8 action:  [-0.4221, -0.9955] n_targets:  1 reward:  61.34\n",
      "44.2 action:  [-0.4014, -0.9936] n_targets:  1 reward:  85.16\n",
      "44.8 action:  [-0.1335, -0.998] n_targets:  1 reward:  50.74\n",
      "45.0 action:  [0.3627, -0.9954] n_targets:  1 reward:  54.49\n",
      "45.4 action:  [-0.5604, -0.9896] n_targets:  1 reward:  65.87\n",
      "51.9 action:  [0.0293, -0.9866] n_targets:  1 reward:  76.98\n",
      "52.1 action:  [-0.0168, -0.9875] n_targets:  1 reward:  51.22\n",
      "52.5 action:  [-0.6088, -0.9837] n_targets:  1 reward:  54.01\n",
      "52.7 action:  [-0.2162, -0.9894] n_targets:  1 reward:  53.23\n",
      "57.0 action:  [-0.2011, -0.9814] n_targets:  1 reward:  60.09\n",
      "62.5 action:  [0.1717, -0.9881] n_targets:  1 reward:  58.7\n",
      "63.7 action:  [-0.2057, -0.9829] n_targets:  1 reward:  79.41\n",
      "65.7 action:  [0.2817, -0.9992] n_targets:  1 reward:  57.85\n",
      "66.5 action:  [0.0297, -0.9815] n_targets:  1 reward:  62.22\n",
      "67.3 action:  [0.3087, -0.9949] n_targets:  1 reward:  50.98\n",
      "70.1 action:  [-0.1518, -0.9962] n_targets:  1 reward:  62.65\n",
      "71.5 action:  [0.178, -0.9915] n_targets:  2 reward:  142.7\n",
      "71.7 action:  [0.256, -0.9839] n_targets:  1 reward:  54.39\n",
      "74.4 action:  [0.1957, -0.9954] n_targets:  1 reward:  51.81\n",
      "74.9 action:  [-0.1991, -0.9944] n_targets:  1 reward:  57.73\n",
      "76.3 action:  [0.3353, -0.9835] n_targets:  1 reward:  59.46\n",
      "79.8 action:  [0.6322, -0.9946] n_targets:  3 reward:  184.79\n",
      "80.8 action:  [-0.0937, -0.9956] n_targets:  3 reward:  186.81\n",
      "81.6 action:  [0.0383, -0.9945] n_targets:  1 reward:  63.72\n",
      "82.0 action:  [-0.439, -0.9906] n_targets:  1 reward:  52.97\n",
      "82.6 action:  [0.0331, -0.9833] n_targets:  1 reward:  53.6\n",
      "83.0 action:  [-0.2774, -0.9982] n_targets:  1 reward:  56.51\n",
      "85.0 action:  [-0.3566, -0.9874] n_targets:  1 reward:  50.71\n",
      "85.2 action:  [-0.5624, -0.985] n_targets:  1 reward:  56.08\n",
      "85.8 action:  [0.0134, -0.9852] n_targets:  1 reward:  50.41\n",
      "86.8 action:  [-0.3288, -0.9835] n_targets:  1 reward:  65.21\n",
      "95.2 action:  [-0.1317, -0.9939] n_targets:  1 reward:  56.98\n",
      "95.8 action:  [-0.3281, -0.9834] n_targets:  1 reward:  61.63\n",
      "98.7 action:  [-0.307, -0.998] n_targets:  1 reward:  57.39\n",
      "99.1 action:  [0.3604, -0.9955] n_targets:  1 reward:  55.43\n",
      "101.8 action:  [0.1093, -0.9928] n_targets:  1 reward:  70.87\n",
      "ALPHA (entropy-related):  tensor([0.3285], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.38867 0.38105 0.37368 0.36641 0.35933 0.35272 0.34622 0.33993 0.33402\n",
      " 0.32853]\n",
      "Episode: 51, Episode Reward: 5113.110787709553\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  213\n",
      "0.3 action:  [0.232, -0.9949] n_targets:  3 reward:  173.1\n",
      "1.5 action:  [-0.0745, -0.9904] n_targets:  1 reward:  73.95\n",
      "4.6 action:  [-0.1073, -0.9883] n_targets:  1 reward:  60.37\n",
      "6.2 action:  [-0.215, -0.9825] n_targets:  1 reward:  61.49\n",
      "7.6 action:  [-0.2503, -0.9915] n_targets:  1 reward:  54.64\n",
      "10.4 action:  [-0.3277, -0.993] n_targets:  1 reward:  60.45\n",
      "11.2 action:  [-0.0525, -0.9815] n_targets:  2 reward:  113.14\n",
      "12.8 action:  [0.2301, -0.9851] n_targets:  2 reward:  113.98\n",
      "13.4 action:  [-0.3731, -0.9992] n_targets:  1 reward:  54.73\n",
      "14.0 action:  [-0.0991, -0.9913] n_targets:  1 reward:  63.19\n",
      "16.8 action:  [0.1635, -0.9942] n_targets:  1 reward:  52.49\n",
      "20.1 action:  [-0.5421, -0.9849] n_targets:  1 reward:  65.07\n",
      "22.3 action:  [0.3904, -0.9843] n_targets:  1 reward:  64.99\n",
      "28.7 action:  [0.2569, -0.9979] n_targets:  1 reward:  54.74\n",
      "30.7 action:  [0.0845, -0.9968] n_targets:  1 reward:  57.51\n",
      "36.9 action:  [-0.1419, -0.9918] n_targets:  1 reward:  52.53\n",
      "37.3 action:  [-0.1126, -0.9911] n_targets:  2 reward:  111.8\n",
      "41.4 action:  [-0.0652, -0.9966] n_targets:  3 reward:  192.3\n",
      "42.8 action:  [-0.4876, -0.9898] n_targets:  2 reward:  112.07\n",
      "47.0 action:  [0.4771, -0.9928] n_targets:  1 reward:  51.56\n",
      "51.7 action:  [-0.303, -0.9849] n_targets:  1 reward:  52.41\n",
      "55.5 action:  [-0.3029, -0.9848] n_targets:  2 reward:  123.6\n",
      "57.1 action:  [-0.046, -0.982] n_targets:  1 reward:  63.57\n",
      "62.4 action:  [-0.5066, -0.9835] n_targets:  2 reward:  105.02\n",
      "62.6 action:  [-0.1948, -0.9974] n_targets:  1 reward:  53.68\n",
      "64.0 action:  [0.305, -0.9846] n_targets:  2 reward:  137.4\n",
      "67.3 action:  [0.5202, -0.9832] n_targets:  2 reward:  151.03\n",
      "69.3 action:  [0.1293, -0.9806] n_targets:  1 reward:  86.5\n",
      "71.0 action:  [0.0823, -0.9918] n_targets:  1 reward:  72.68\n",
      "72.0 action:  [-0.3309, -0.9985] n_targets:  1 reward:  57.94\n",
      "72.4 action:  [0.272, -0.9857] n_targets:  1 reward:  60.03\n",
      "73.0 action:  [0.3367, -0.9944] n_targets:  1 reward:  51.32\n",
      "73.4 action:  [-0.52, -0.9925] n_targets:  1 reward:  58.91\n",
      "73.6 action:  [-0.2903, -0.9906] n_targets:  1 reward:  57.08\n",
      "73.8 action:  [0.2319, -0.9919] n_targets:  1 reward:  55.66\n",
      "74.2 action:  [-0.6069, -0.9856] n_targets:  1 reward:  69.52\n",
      "74.6 action:  [-0.2153, -0.9964] n_targets:  1 reward:  57.1\n",
      "74.8 action:  [0.2269, -0.9851] n_targets:  2 reward:  102.05\n",
      "75.6 action:  [0.2422, -0.9981] n_targets:  2 reward:  126.66\n",
      "77.2 action:  [-0.1275, -0.994] n_targets:  1 reward:  68.61\n",
      "80.0 action:  [-0.6288, -0.996] n_targets:  1 reward:  65.15\n",
      "80.2 action:  [-0.44, -0.9828] n_targets:  1 reward:  60.49\n",
      "81.2 action:  [-0.2128, -0.9824] n_targets:  1 reward:  68.79\n",
      "82.0 action:  [-0.1798, -0.9868] n_targets:  2 reward:  162.99\n",
      "82.6 action:  [-0.2477, -0.9859] n_targets:  1 reward:  53.9\n",
      "83.0 action:  [-0.2743, -0.989] n_targets:  1 reward:  70.68\n",
      "87.6 action:  [0.2092, -0.9889] n_targets:  1 reward:  65.88\n",
      "88.2 action:  [-0.231, -0.994] n_targets:  2 reward:  123.1\n",
      "89.1 action:  [0.2681, -0.9868] n_targets:  1 reward:  66.46\n",
      "91.0 action:  [-0.1566, -0.9954] n_targets:  1 reward:  64.0\n",
      "91.6 action:  [0.2205, -0.982] n_targets:  1 reward:  56.99\n",
      "92.2 action:  [0.1122, -0.9945] n_targets:  1 reward:  54.69\n",
      "94.1 action:  [0.0792, -0.9855] n_targets:  1 reward:  67.61\n",
      "95.0 action:  [0.1459, -0.9934] n_targets:  1 reward:  57.73\n",
      "96.4 action:  [0.1701, -0.989] n_targets:  2 reward:  138.17\n",
      "98.8 action:  [-0.1656, -0.9927] n_targets:  1 reward:  57.59\n",
      "99.2 action:  [0.2888, -0.9811] n_targets:  1 reward:  53.44\n",
      "100.2 action:  [-0.4071, -0.9957] n_targets:  1 reward:  58.6\n",
      "102.2 action:  [-0.4131, -0.9969] n_targets:  1 reward:  82.58\n",
      "ALPHA (entropy-related):  tensor([0.3232], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.38105 0.37368 0.36641 0.35933 0.35272 0.34622 0.33993 0.33402 0.32853\n",
      " 0.32323]\n",
      "Episode: 52, Episode Reward: 4683.669396718343\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  214\n",
      "0.1 action:  [0.2128, -0.9879] n_targets:  1 reward:  88.02\n",
      "6.4 action:  [0.2568, -0.9979] n_targets:  2 reward:  136.77\n",
      "7.0 action:  [-0.185, -0.9986] n_targets:  1 reward:  51.54\n",
      "8.2 action:  [0.1314, -0.9968] n_targets:  1 reward:  50.57\n",
      "12.2 action:  [-0.0525, -0.984] n_targets:  1 reward:  52.17\n",
      "12.6 action:  [0.5873, -0.9896] n_targets:  1 reward:  55.36\n",
      "13.2 action:  [-0.0464, -0.9936] n_targets:  1 reward:  54.83\n",
      "14.4 action:  [0.172, -0.9844] n_targets:  1 reward:  62.5\n",
      "16.2 action:  [0.3749, -0.998] n_targets:  1 reward:  53.79\n",
      "16.4 action:  [0.4724, -0.994] n_targets:  1 reward:  59.77\n",
      "19.7 action:  [0.0335, -0.9981] n_targets:  1 reward:  53.89\n",
      "23.7 action:  [-0.1792, -0.9811] n_targets:  1 reward:  60.97\n",
      "26.6 action:  [-0.0271, -0.9834] n_targets:  1 reward:  61.63\n",
      "27.7 action:  [-0.3298, -0.9919] n_targets:  1 reward:  68.1\n",
      "28.7 action:  [-0.1422, -0.9937] n_targets:  1 reward:  52.08\n",
      "29.1 action:  [-0.2531, -0.9835] n_targets:  1 reward:  59.65\n",
      "30.5 action:  [-0.2269, -0.9938] n_targets:  2 reward:  131.44\n",
      "32.9 action:  [-0.5236, -0.9985] n_targets:  1 reward:  51.18\n",
      "33.7 action:  [0.1454, -0.9924] n_targets:  1 reward:  63.07\n",
      "35.6 action:  [0.1129, -0.9822] n_targets:  3 reward:  215.98\n",
      "36.7 action:  [0.1921, -0.9878] n_targets:  2 reward:  152.23\n",
      "37.9 action:  [0.5593, -0.9844] n_targets:  1 reward:  76.42\n",
      "39.0 action:  [-0.2263, -0.9953] n_targets:  1 reward:  72.79\n",
      "39.6 action:  [-0.3184, -0.9949] n_targets:  1 reward:  58.19\n",
      "40.3 action:  [0.2918, -0.9881] n_targets:  1 reward:  63.42\n",
      "41.2 action:  [-0.5003, -0.9857] n_targets:  1 reward:  53.51\n",
      "41.6 action:  [-0.2498, -0.9837] n_targets:  1 reward:  52.55\n",
      "42.8 action:  [-0.2497, -0.9887] n_targets:  1 reward:  64.32\n",
      "46.1 action:  [0.224, -0.9814] n_targets:  1 reward:  57.19\n",
      "46.7 action:  [0.1031, -0.9893] n_targets:  1 reward:  58.67\n",
      "48.7 action:  [0.2192, -0.9928] n_targets:  2 reward:  118.16\n",
      "49.3 action:  [-0.4449, -0.993] n_targets:  1 reward:  68.01\n",
      "49.9 action:  [0.0365, -0.9909] n_targets:  1 reward:  57.04\n",
      "50.1 action:  [0.3791, -0.9928] n_targets:  1 reward:  55.81\n",
      "51.1 action:  [0.0694, -0.9825] n_targets:  1 reward:  56.22\n",
      "52.6 action:  [0.6081, -0.9909] n_targets:  2 reward:  132.0\n",
      "53.0 action:  [0.2925, -0.9922] n_targets:  1 reward:  54.77\n",
      "54.8 action:  [-0.5252, -0.9983] n_targets:  1 reward:  53.59\n",
      "57.5 action:  [-0.1321, -0.9899] n_targets:  1 reward:  64.52\n",
      "63.3 action:  [-0.2322, -1.0] n_targets:  1 reward:  73.91\n",
      "65.3 action:  [-0.462, -0.9963] n_targets:  5 reward:  298.91\n",
      "67.5 action:  [0.5831, -0.9914] n_targets:  1 reward:  59.65\n",
      "67.7 action:  [0.3803, -0.9957] n_targets:  1 reward:  51.44\n",
      "71.8 action:  [0.1205, -0.9822] n_targets:  3 reward:  175.3\n",
      "73.0 action:  [-0.2086, -0.9914] n_targets:  1 reward:  53.53\n",
      "73.8 action:  [0.3002, -0.9944] n_targets:  1 reward:  87.79\n",
      "74.4 action:  [0.5232, -0.9995] n_targets:  1 reward:  51.64\n",
      "75.7 action:  [-0.2964, -0.9978] n_targets:  1 reward:  75.69\n",
      "76.1 action:  [-0.3116, -0.9968] n_targets:  1 reward:  51.57\n",
      "77.0 action:  [-0.4562, -0.9867] n_targets:  1 reward:  63.64\n",
      "77.6 action:  [0.2811, -0.9882] n_targets:  1 reward:  74.82\n",
      "78.8 action:  [-0.1165, -0.9879] n_targets:  2 reward:  139.95\n",
      "79.9 action:  [-0.0275, -0.9838] n_targets:  1 reward:  52.32\n",
      "80.9 action:  [-0.1797, -0.9812] n_targets:  1 reward:  51.67\n",
      "82.3 action:  [0.3707, -0.9876] n_targets:  1 reward:  60.6\n",
      "83.7 action:  [-0.094, -0.986] n_targets:  1 reward:  60.02\n",
      "83.9 action:  [0.102, -0.9924] n_targets:  1 reward:  53.62\n",
      "85.6 action:  [0.3081, -0.9847] n_targets:  1 reward:  51.08\n",
      "86.1 action:  [0.3087, -0.9973] n_targets:  1 reward:  55.42\n",
      "88.0 action:  [0.0412, -0.9924] n_targets:  1 reward:  54.41\n",
      "90.2 action:  [-0.133, -0.9959] n_targets:  2 reward:  108.45\n",
      "91.3 action:  [-0.2611, -0.9817] n_targets:  1 reward:  57.66\n",
      "93.9 action:  [0.0526, -0.9981] n_targets:  1 reward:  53.96\n",
      "96.7 action:  [-0.3101, -0.9922] n_targets:  4 reward:  214.16\n",
      "98.5 action:  [-0.0368, -0.9824] n_targets:  1 reward:  52.92\n",
      "99.9 action:  [-0.0245, -0.9968] n_targets:  1 reward:  58.83\n",
      "101.9 action:  [0.0547, -0.99] n_targets:  1 reward:  78.95\n",
      "102.3 action:  [-0.0891, -0.9833] n_targets:  1 reward:  54.11\n",
      "ALPHA (entropy-related):  tensor([0.3181], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.37368 0.36641 0.35933 0.35272 0.34622 0.33993 0.33402 0.32853 0.32323\n",
      " 0.3181 ]\n",
      "Episode: 53, Episode Reward: 5228.735404968262\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  215\n",
      "1.2 action:  [-0.3773, -0.9856] n_targets:  1 reward:  76.42\n",
      "2.8 action:  [-0.0714, -0.9834] n_targets:  1 reward:  65.66\n",
      "4.8 action:  [0.2437, -0.99] n_targets:  1 reward:  73.11\n",
      "9.6 action:  [-0.2212, -0.9929] n_targets:  1 reward:  65.72\n",
      "11.9 action:  [-0.1118, -0.9801] n_targets:  1 reward:  71.41\n",
      "17.6 action:  [0.2817, -0.9953] n_targets:  1 reward:  94.91\n",
      "18.4 action:  [0.1264, -0.9824] n_targets:  2 reward:  103.63\n",
      "18.8 action:  [0.0583, -0.9949] n_targets:  1 reward:  54.73\n",
      "21.0 action:  [0.1445, -0.9855] n_targets:  1 reward:  63.31\n",
      "23.8 action:  [-0.1062, -0.993] n_targets:  1 reward:  82.35\n",
      "24.0 action:  [0.0398, -0.9899] n_targets:  1 reward:  57.35\n",
      "24.4 action:  [0.352, -0.9994] n_targets:  2 reward:  125.56\n",
      "26.6 action:  [0.5784, -0.9943] n_targets:  1 reward:  52.01\n",
      "27.4 action:  [-0.2553, -0.9932] n_targets:  2 reward:  138.88\n",
      "27.8 action:  [0.3374, -0.9926] n_targets:  1 reward:  58.44\n",
      "31.1 action:  [0.6218, -0.9956] n_targets:  1 reward:  67.97\n",
      "31.3 action:  [-0.5903, -0.9872] n_targets:  1 reward:  76.37\n",
      "31.9 action:  [-0.0497, -0.9815] n_targets:  2 reward:  119.55\n",
      "34.7 action:  [-0.1547, -0.981] n_targets:  1 reward:  50.75\n",
      "35.5 action:  [0.1125, -0.9925] n_targets:  1 reward:  50.36\n",
      "39.7 action:  [0.2873, -0.9854] n_targets:  1 reward:  50.78\n",
      "41.0 action:  [0.1011, -0.9889] n_targets:  1 reward:  53.45\n",
      "42.2 action:  [0.0076, -0.9909] n_targets:  1 reward:  82.96\n",
      "42.8 action:  [0.3463, -0.9935] n_targets:  1 reward:  67.64\n",
      "44.5 action:  [0.112, -0.9847] n_targets:  1 reward:  78.4\n",
      "45.3 action:  [-0.0324, -0.9905] n_targets:  1 reward:  73.35\n",
      "45.7 action:  [-0.2071, -0.9908] n_targets:  1 reward:  54.67\n",
      "46.1 action:  [-0.1438, -0.9848] n_targets:  2 reward:  118.56\n",
      "46.7 action:  [0.3403, -0.9968] n_targets:  1 reward:  53.78\n",
      "47.5 action:  [-0.2442, -0.9875] n_targets:  1 reward:  60.3\n",
      "48.5 action:  [-0.0012, -0.9914] n_targets:  2 reward:  110.1\n",
      "49.5 action:  [0.233, -0.9864] n_targets:  1 reward:  57.23\n",
      "49.7 action:  [0.1723, -0.9881] n_targets:  1 reward:  56.52\n",
      "51.1 action:  [0.0829, -0.9999] n_targets:  1 reward:  57.18\n",
      "55.6 action:  [-0.3319, -0.9913] n_targets:  2 reward:  121.41\n",
      "58.4 action:  [0.2147, -0.9896] n_targets:  1 reward:  53.88\n",
      "60.8 action:  [0.3939, -0.9993] n_targets:  5 reward:  366.31\n",
      "63.8 action:  [-0.5372, -0.9949] n_targets:  1 reward:  54.46\n",
      "69.7 action:  [-0.2821, -0.9845] n_targets:  1 reward:  59.75\n",
      "71.3 action:  [-0.2824, -0.9859] n_targets:  1 reward:  53.75\n",
      "71.7 action:  [0.4531, -0.9948] n_targets:  1 reward:  65.43\n",
      "73.1 action:  [-0.6198, -0.994] n_targets:  1 reward:  59.77\n",
      "74.0 action:  [-0.2918, -0.9963] n_targets:  2 reward:  134.92\n",
      "76.7 action:  [-0.0652, -0.9812] n_targets:  3 reward:  233.95\n",
      "78.1 action:  [-0.2365, -0.9974] n_targets:  1 reward:  52.91\n",
      "81.4 action:  [0.1818, -0.995] n_targets:  1 reward:  73.1\n",
      "82.2 action:  [-0.1041, -0.9934] n_targets:  2 reward:  104.69\n",
      "85.4 action:  [-0.0027, -0.9936] n_targets:  1 reward:  60.36\n",
      "86.6 action:  [-0.0517, -0.9858] n_targets:  1 reward:  53.33\n",
      "88.5 action:  [0.3825, -0.981] n_targets:  1 reward:  52.36\n",
      "88.9 action:  [0.1878, -0.9963] n_targets:  1 reward:  53.77\n",
      "90.3 action:  [-0.4856, -0.9961] n_targets:  1 reward:  77.44\n",
      "91.3 action:  [-0.1991, -0.9819] n_targets:  1 reward:  60.31\n",
      "91.5 action:  [0.1027, -0.9822] n_targets:  1 reward:  58.07\n",
      "94.5 action:  [0.2199, -0.9849] n_targets:  1 reward:  69.54\n",
      "95.1 action:  [-0.1466, -0.9917] n_targets:  1 reward:  54.98\n",
      "95.3 action:  [0.1739, -0.997] n_targets:  1 reward:  50.65\n",
      "96.5 action:  [0.3488, -0.998] n_targets:  1 reward:  69.99\n",
      "99.6 action:  [0.0977, -0.9955] n_targets:  1 reward:  55.55\n",
      "100.2 action:  [0.0241, -0.9914] n_targets:  2 reward:  137.29\n",
      "100.4 action:  [-0.2787, -0.9985] n_targets:  1 reward:  59.94\n",
      "101.8 action:  [-0.1766, -0.9833] n_targets:  1 reward:  71.98\n",
      "ALPHA (entropy-related):  tensor([0.3132], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.36641 0.35933 0.35272 0.34622 0.33993 0.33402 0.32853 0.32323 0.3181\n",
      " 0.3132 ]\n",
      "Episode: 54, Episode Reward: 4943.342278798422\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  216\n",
      "0.3 action:  [0.128, -0.9883] n_targets:  2 reward:  126.47\n",
      "3.3 action:  [-0.1232, -0.9987] n_targets:  1 reward:  59.65\n",
      "4.1 action:  [-0.1304, -0.9923] n_targets:  1 reward:  55.44\n",
      "5.0 action:  [-0.0922, -0.9971] n_targets:  1 reward:  67.25\n",
      "5.6 action:  [0.3626, -0.9929] n_targets:  1 reward:  52.75\n",
      "6.0 action:  [-0.1786, -0.9905] n_targets:  1 reward:  55.11\n",
      "7.0 action:  [0.0648, -0.9904] n_targets:  1 reward:  67.84\n",
      "7.9 action:  [-0.3476, -0.9938] n_targets:  1 reward:  64.84\n",
      "9.3 action:  [0.2178, -0.9937] n_targets:  1 reward:  70.73\n",
      "9.9 action:  [0.0989, -0.9966] n_targets:  2 reward:  108.09\n",
      "12.2 action:  [0.2338, -0.9817] n_targets:  1 reward:  80.08\n",
      "14.0 action:  [-0.3108, -0.9953] n_targets:  1 reward:  64.79\n",
      "15.0 action:  [0.1679, -0.9969] n_targets:  1 reward:  50.24\n",
      "16.1 action:  [0.1821, -0.9872] n_targets:  1 reward:  55.15\n",
      "16.7 action:  [0.0232, -0.996] n_targets:  1 reward:  51.47\n",
      "17.3 action:  [-0.1125, -0.991] n_targets:  1 reward:  52.63\n",
      "18.7 action:  [-0.162, -0.9903] n_targets:  1 reward:  61.84\n",
      "19.2 action:  [0.0801, -0.9933] n_targets:  1 reward:  52.09\n",
      "20.0 action:  [0.1266, -0.9814] n_targets:  1 reward:  60.88\n",
      "21.0 action:  [-0.2777, -0.993] n_targets:  1 reward:  58.43\n",
      "22.0 action:  [0.5919, -0.9805] n_targets:  1 reward:  56.6\n",
      "23.4 action:  [-0.4945, -0.9841] n_targets:  1 reward:  53.57\n",
      "24.2 action:  [0.1985, -0.9866] n_targets:  1 reward:  74.3\n",
      "25.5 action:  [-0.0672, -0.9913] n_targets:  1 reward:  84.88\n",
      "26.5 action:  [-0.1403, -0.9818] n_targets:  1 reward:  53.78\n",
      "31.0 action:  [-0.3322, -0.9903] n_targets:  2 reward:  128.85\n",
      "31.5 action:  [-0.0689, -0.9967] n_targets:  1 reward:  64.94\n",
      "34.1 action:  [-0.0903, -0.9952] n_targets:  1 reward:  53.35\n",
      "34.5 action:  [-0.1056, -0.9847] n_targets:  2 reward:  109.15\n",
      "37.4 action:  [0.3327, -0.9853] n_targets:  2 reward:  114.73\n",
      "37.6 action:  [-0.0043, -0.9961] n_targets:  1 reward:  54.14\n",
      "41.1 action:  [-0.2183, -0.9802] n_targets:  2 reward:  128.86\n",
      "42.3 action:  [-0.4112, -0.9808] n_targets:  1 reward:  64.18\n",
      "42.5 action:  [0.3178, -0.9861] n_targets:  1 reward:  54.24\n",
      "42.7 action:  [0.2095, -0.9848] n_targets:  1 reward:  55.86\n",
      "43.5 action:  [-0.3845, -0.9907] n_targets:  2 reward:  114.8\n",
      "45.8 action:  [0.4191, -0.9855] n_targets:  1 reward:  76.44\n",
      "48.8 action:  [-0.0011, -0.9924] n_targets:  1 reward:  51.18\n",
      "53.2 action:  [0.1488, -0.9827] n_targets:  1 reward:  69.85\n",
      "56.5 action:  [-0.1772, -0.9923] n_targets:  1 reward:  56.49\n",
      "57.9 action:  [-0.3192, -0.9839] n_targets:  1 reward:  53.43\n",
      "60.6 action:  [0.0621, -0.9835] n_targets:  1 reward:  59.0\n",
      "62.6 action:  [0.3885, -0.9986] n_targets:  2 reward:  132.38\n",
      "63.0 action:  [-0.1402, -0.9911] n_targets:  1 reward:  62.24\n",
      "64.0 action:  [0.22, -0.9931] n_targets:  2 reward:  106.58\n",
      "65.2 action:  [-0.3955, -0.989] n_targets:  2 reward:  105.15\n",
      "66.7 action:  [-0.0317, -0.996] n_targets:  2 reward:  137.04\n",
      "67.5 action:  [0.3181, -0.9899] n_targets:  1 reward:  59.38\n",
      "67.9 action:  [-0.229, -0.9909] n_targets:  1 reward:  64.01\n",
      "69.7 action:  [-0.0808, -0.9851] n_targets:  1 reward:  62.6\n",
      "70.5 action:  [0.0392, -0.9815] n_targets:  1 reward:  56.7\n",
      "74.3 action:  [0.0712, -0.9961] n_targets:  2 reward:  133.13\n",
      "75.5 action:  [-0.1946, -0.9811] n_targets:  1 reward:  66.36\n",
      "75.7 action:  [0.077, -0.9944] n_targets:  1 reward:  52.52\n",
      "78.8 action:  [-0.1973, -0.9822] n_targets:  2 reward:  174.34\n",
      "82.8 action:  [-0.2096, -0.9826] n_targets:  1 reward:  52.38\n",
      "83.2 action:  [0.1627, -0.9909] n_targets:  1 reward:  54.12\n",
      "86.5 action:  [0.4357, -0.9892] n_targets:  2 reward:  119.66\n",
      "86.9 action:  [0.0112, -0.9937] n_targets:  1 reward:  59.88\n",
      "89.5 action:  [-0.2152, -0.9841] n_targets:  4 reward:  234.83\n",
      "90.7 action:  [0.3327, -0.9852] n_targets:  1 reward:  50.73\n",
      "93.5 action:  [0.1852, -0.992] n_targets:  1 reward:  53.45\n",
      "94.1 action:  [0.0972, -0.9951] n_targets:  2 reward:  120.07\n",
      "94.3 action:  [-0.0164, -0.9961] n_targets:  1 reward:  53.48\n",
      "ALPHA (entropy-related):  tensor([0.3083], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.35933 0.35272 0.34622 0.33993 0.33402 0.32853 0.32323 0.3181  0.3132\n",
      " 0.30834]\n",
      "Episode: 55, Episode Reward: 4959.440367380777\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  217\n",
      "0.3 action:  [-0.1353, -0.985] n_targets:  1 reward:  59.98\n",
      "2.0 action:  [0.1651, -0.988] n_targets:  1 reward:  63.32\n",
      "4.8 action:  [-0.2718, -0.9823] n_targets:  1 reward:  58.8\n",
      "5.6 action:  [-0.3868, -0.9911] n_targets:  2 reward:  139.65\n",
      "6.4 action:  [0.1966, -0.9972] n_targets:  1 reward:  51.65\n",
      "9.6 action:  [0.1342, -0.988] n_targets:  1 reward:  63.55\n",
      "15.5 action:  [-0.4184, -0.9989] n_targets:  2 reward:  148.89\n",
      "15.9 action:  [-0.4717, -0.9919] n_targets:  1 reward:  53.01\n",
      "16.5 action:  [0.4838, -0.9973] n_targets:  1 reward:  51.25\n",
      "17.3 action:  [0.1656, -0.9985] n_targets:  2 reward:  128.51\n",
      "18.3 action:  [-0.0459, -0.9934] n_targets:  2 reward:  137.69\n",
      "22.3 action:  [0.3011, -0.98] n_targets:  3 reward:  228.52\n",
      "23.1 action:  [0.2936, -0.9895] n_targets:  1 reward:  79.98\n",
      "25.5 action:  [-0.2718, -0.9926] n_targets:  1 reward:  63.54\n",
      "25.9 action:  [-0.4623, -0.9804] n_targets:  1 reward:  62.93\n",
      "27.2 action:  [-0.5649, -0.9947] n_targets:  1 reward:  55.27\n",
      "28.4 action:  [-0.3509, -0.9825] n_targets:  3 reward:  193.07\n",
      "31.0 action:  [0.1979, -0.9906] n_targets:  1 reward:  52.35\n",
      "32.8 action:  [0.0049, -0.9935] n_targets:  1 reward:  83.07\n",
      "35.8 action:  [-0.0566, -0.9823] n_targets:  2 reward:  150.38\n",
      "38.2 action:  [0.0498, -0.9928] n_targets:  1 reward:  78.61\n",
      "39.0 action:  [0.14, -0.9956] n_targets:  2 reward:  106.56\n",
      "40.3 action:  [-0.3234, -0.9834] n_targets:  1 reward:  54.46\n",
      "41.3 action:  [0.0165, -0.9846] n_targets:  1 reward:  62.31\n",
      "41.7 action:  [0.1223, -0.9961] n_targets:  1 reward:  62.9\n",
      "45.5 action:  [-0.0611, -0.9858] n_targets:  1 reward:  52.27\n",
      "47.8 action:  [-0.2741, -0.9924] n_targets:  1 reward:  53.05\n",
      "48.8 action:  [0.2402, -0.998] n_targets:  1 reward:  80.3\n",
      "50.3 action:  [-0.0159, -0.9843] n_targets:  2 reward:  108.96\n",
      "50.5 action:  [0.0856, -0.9822] n_targets:  1 reward:  50.92\n",
      "53.7 action:  [-0.0391, -0.9902] n_targets:  1 reward:  56.26\n",
      "55.3 action:  [0.2829, -0.9952] n_targets:  3 reward:  175.93\n",
      "57.0 action:  [-0.0547, -0.9827] n_targets:  1 reward:  54.02\n",
      "57.6 action:  [-0.0427, -0.9819] n_targets:  1 reward:  54.03\n",
      "58.0 action:  [0.0414, -0.9851] n_targets:  2 reward:  116.78\n",
      "58.6 action:  [-0.3032, -0.9806] n_targets:  2 reward:  106.59\n",
      "59.2 action:  [-0.3791, -0.9834] n_targets:  2 reward:  119.91\n",
      "59.6 action:  [0.1568, -0.986] n_targets:  1 reward:  58.74\n",
      "60.8 action:  [0.0134, -0.9942] n_targets:  1 reward:  85.28\n",
      "61.2 action:  [-0.327, -0.9973] n_targets:  1 reward:  50.85\n",
      "63.5 action:  [0.0567, -0.9974] n_targets:  1 reward:  58.18\n",
      "64.7 action:  [0.3043, -0.9882] n_targets:  1 reward:  67.27\n",
      "66.9 action:  [-0.0855, -0.9802] n_targets:  1 reward:  61.8\n",
      "68.3 action:  [-0.2763, -0.9859] n_targets:  1 reward:  59.02\n",
      "69.9 action:  [0.3668, -0.9873] n_targets:  1 reward:  57.34\n",
      "70.1 action:  [0.5724, -0.9989] n_targets:  1 reward:  56.63\n",
      "72.7 action:  [-0.2871, -0.9995] n_targets:  2 reward:  124.77\n",
      "75.0 action:  [0.4467, -0.9836] n_targets:  2 reward:  122.97\n",
      "75.2 action:  [-0.3403, -0.9901] n_targets:  1 reward:  57.14\n",
      "82.7 action:  [0.0309, -0.9935] n_targets:  1 reward:  56.27\n",
      "84.3 action:  [-0.1456, -0.9931] n_targets:  1 reward:  79.03\n",
      "84.9 action:  [0.0002, -0.9843] n_targets:  1 reward:  53.12\n",
      "85.7 action:  [-0.4188, -0.9944] n_targets:  1 reward:  63.71\n",
      "86.1 action:  [-0.0087, -0.9951] n_targets:  1 reward:  64.3\n",
      "86.3 action:  [-0.5625, -0.9804] n_targets:  1 reward:  61.21\n",
      "87.7 action:  [0.3888, -0.994] n_targets:  1 reward:  71.45\n",
      "90.7 action:  [-0.5484, -0.9813] n_targets:  2 reward:  150.55\n",
      "94.5 action:  [-0.2913, -0.9892] n_targets:  1 reward:  58.28\n",
      "95.1 action:  [-0.2035, -0.9948] n_targets:  1 reward:  53.28\n",
      "97.5 action:  [0.2895, -0.9827] n_targets:  1 reward:  68.66\n",
      "ALPHA (entropy-related):  tensor([0.3037], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.35272 0.34622 0.33993 0.33402 0.32853 0.32323 0.3181  0.3132  0.30834\n",
      " 0.30365]\n",
      "Episode: 56, Episode Reward: 4959.125629425048\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  218\n",
      "1.8 action:  [0.3554, -0.9923] n_targets:  1 reward:  60.27\n",
      "2.8 action:  [-0.3133, -0.9866] n_targets:  1 reward:  55.08\n",
      "5.6 action:  [-0.0533, -0.9889] n_targets:  1 reward:  62.92\n",
      "6.2 action:  [0.1651, -0.9882] n_targets:  1 reward:  68.27\n",
      "8.0 action:  [0.0575, -0.9806] n_targets:  1 reward:  73.65\n",
      "13.3 action:  [0.0332, -0.9821] n_targets:  1 reward:  53.18\n",
      "14.3 action:  [-0.2858, -0.9825] n_targets:  1 reward:  54.08\n",
      "17.6 action:  [-0.0711, -0.9828] n_targets:  1 reward:  78.5\n",
      "23.3 action:  [-0.1888, -0.9902] n_targets:  1 reward:  62.81\n",
      "23.9 action:  [-0.2852, -0.9931] n_targets:  2 reward:  123.72\n",
      "24.9 action:  [-0.1664, -0.9882] n_targets:  1 reward:  83.85\n",
      "26.6 action:  [-0.458, -0.983] n_targets:  1 reward:  50.98\n",
      "27.7 action:  [0.193, -0.9942] n_targets:  1 reward:  64.02\n",
      "32.9 action:  [0.5518, -0.9802] n_targets:  1 reward:  53.64\n",
      "34.1 action:  [-0.1105, -0.9926] n_targets:  1 reward:  57.1\n",
      "35.5 action:  [-0.4497, -0.9866] n_targets:  2 reward:  133.28\n",
      "36.1 action:  [0.2309, -0.9802] n_targets:  1 reward:  59.13\n",
      "39.1 action:  [0.2664, -0.9926] n_targets:  1 reward:  50.5\n",
      "42.0 action:  [-0.0437, -0.9831] n_targets:  1 reward:  65.33\n",
      "43.4 action:  [-0.2557, -0.9968] n_targets:  1 reward:  55.37\n",
      "44.4 action:  [-0.1013, -0.9805] n_targets:  1 reward:  56.53\n",
      "45.5 action:  [-0.0803, -0.9959] n_targets:  1 reward:  62.45\n",
      "48.0 action:  [0.1241, -0.9868] n_targets:  2 reward:  117.12\n",
      "48.5 action:  [-0.1567, -0.9862] n_targets:  1 reward:  71.7\n",
      "52.4 action:  [0.142, -0.9849] n_targets:  1 reward:  51.92\n",
      "56.9 action:  [0.2287, -0.9882] n_targets:  1 reward:  73.19\n",
      "58.3 action:  [0.1139, -0.9861] n_targets:  1 reward:  83.27\n",
      "60.7 action:  [0.2616, -0.9901] n_targets:  1 reward:  80.26\n",
      "61.1 action:  [0.072, -0.9893] n_targets:  1 reward:  50.08\n",
      "64.1 action:  [-0.3269, -0.9953] n_targets:  1 reward:  60.74\n",
      "68.7 action:  [0.188, -0.9912] n_targets:  1 reward:  56.4\n",
      "69.9 action:  [-0.132, -0.9854] n_targets:  1 reward:  55.62\n",
      "71.9 action:  [-0.0581, -0.9928] n_targets:  2 reward:  143.37\n",
      "74.3 action:  [0.4981, -0.9922] n_targets:  1 reward:  51.4\n",
      "75.3 action:  [-0.2408, -0.985] n_targets:  2 reward:  105.62\n",
      "75.9 action:  [-0.0501, -0.9945] n_targets:  1 reward:  60.0\n",
      "76.9 action:  [0.0919, -0.9903] n_targets:  3 reward:  207.44\n",
      "77.1 action:  [-0.2234, -0.9917] n_targets:  1 reward:  51.49\n",
      "80.0 action:  [-0.1458, -0.9826] n_targets:  1 reward:  58.39\n",
      "82.5 action:  [-0.0367, -0.9827] n_targets:  1 reward:  50.67\n",
      "83.8 action:  [-0.0228, -0.9862] n_targets:  1 reward:  53.64\n",
      "88.9 action:  [0.5416, -0.9917] n_targets:  1 reward:  51.55\n",
      "90.1 action:  [0.4373, -0.9909] n_targets:  1 reward:  58.68\n",
      "91.0 action:  [0.1804, -0.999] n_targets:  1 reward:  55.65\n",
      "91.4 action:  [-0.3947, -0.9939] n_targets:  1 reward:  53.64\n",
      "97.9 action:  [0.3544, -0.9986] n_targets:  1 reward:  52.54\n",
      "101.0 action:  [-0.3028, -0.9906] n_targets:  1 reward:  56.3\n",
      "102.2 action:  [-0.1669, -0.9929] n_targets:  2 reward:  140.0\n",
      "ALPHA (entropy-related):  tensor([0.2992], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.34622 0.33993 0.33402 0.32853 0.32323 0.3181  0.3132  0.30834 0.30365\n",
      " 0.29916]\n",
      "Episode: 57, Episode Reward: 3435.2958246866856\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  219\n",
      "0.9 action:  [0.4321, -0.9943] n_targets:  2 reward:  147.64\n",
      "2.1 action:  [-0.4843, -0.9867] n_targets:  1 reward:  68.3\n",
      "2.5 action:  [-0.089, -0.9921] n_targets:  1 reward:  50.49\n",
      "5.7 action:  [0.1129, -0.9825] n_targets:  1 reward:  54.76\n",
      "7.3 action:  [-0.0281, -0.9982] n_targets:  2 reward:  125.87\n",
      "10.7 action:  [-0.3726, -0.9897] n_targets:  1 reward:  63.32\n",
      "11.6 action:  [0.2841, -0.9809] n_targets:  1 reward:  63.68\n",
      "13.0 action:  [0.0749, -0.9908] n_targets:  2 reward:  119.52\n",
      "14.3 action:  [0.4668, -0.9866] n_targets:  2 reward:  117.5\n",
      "14.9 action:  [-0.0315, -0.9814] n_targets:  1 reward:  54.45\n",
      "16.2 action:  [-0.108, -0.9949] n_targets:  1 reward:  51.4\n",
      "18.3 action:  [-0.1121, -0.9986] n_targets:  2 reward:  105.09\n",
      "21.6 action:  [-0.4475, -0.9897] n_targets:  2 reward:  131.52\n",
      "23.0 action:  [-0.1574, -0.993] n_targets:  1 reward:  56.96\n",
      "23.2 action:  [-0.1394, -0.9977] n_targets:  1 reward:  50.25\n",
      "23.4 action:  [0.2556, -0.9806] n_targets:  1 reward:  50.43\n",
      "24.8 action:  [0.3703, -0.9919] n_targets:  2 reward:  137.25\n",
      "26.0 action:  [-0.1649, -0.9879] n_targets:  1 reward:  82.77\n",
      "27.0 action:  [0.4542, -0.9985] n_targets:  1 reward:  73.84\n",
      "27.6 action:  [0.0919, -0.9907] n_targets:  1 reward:  50.81\n",
      "28.8 action:  [-0.2559, -0.9853] n_targets:  2 reward:  160.54\n",
      "30.2 action:  [-0.0988, -0.9907] n_targets:  1 reward:  51.01\n",
      "30.8 action:  [0.1561, -0.9815] n_targets:  1 reward:  61.9\n",
      "32.2 action:  [0.1426, -0.9931] n_targets:  1 reward:  53.9\n",
      "33.4 action:  [-0.0517, -0.9962] n_targets:  2 reward:  101.17\n",
      "35.4 action:  [-0.1047, -0.9884] n_targets:  1 reward:  69.61\n",
      "37.6 action:  [0.1179, -0.9855] n_targets:  1 reward:  62.92\n",
      "39.4 action:  [0.4486, -0.9889] n_targets:  2 reward:  133.3\n",
      "41.8 action:  [0.409, -0.9815] n_targets:  3 reward:  251.45\n",
      "42.4 action:  [0.2512, -0.9972] n_targets:  2 reward:  129.39\n",
      "43.1 action:  [-0.0739, -0.9946] n_targets:  1 reward:  55.34\n",
      "44.3 action:  [0.4272, -0.9974] n_targets:  1 reward:  80.97\n",
      "46.8 action:  [-0.4306, -0.9918] n_targets:  1 reward:  54.98\n",
      "50.3 action:  [-0.2598, -0.9824] n_targets:  1 reward:  83.3\n",
      "52.6 action:  [0.2706, -0.9923] n_targets:  1 reward:  50.45\n",
      "55.0 action:  [0.0016, -0.9911] n_targets:  1 reward:  73.82\n",
      "56.2 action:  [-0.2254, -0.9959] n_targets:  2 reward:  154.56\n",
      "59.0 action:  [0.1524, -0.9916] n_targets:  2 reward:  157.08\n",
      "59.8 action:  [0.1776, -0.9807] n_targets:  1 reward:  58.07\n",
      "62.4 action:  [0.5939, -0.9953] n_targets:  2 reward:  109.76\n",
      "62.6 action:  [-0.1344, -0.9914] n_targets:  1 reward:  50.15\n",
      "63.4 action:  [0.0301, -0.9977] n_targets:  3 reward:  211.78\n",
      "65.5 action:  [-0.3395, -0.9948] n_targets:  2 reward:  123.88\n",
      "68.7 action:  [-0.372, -0.9851] n_targets:  1 reward:  55.33\n",
      "69.5 action:  [0.3376, -0.9959] n_targets:  1 reward:  55.25\n",
      "71.1 action:  [-0.1298, -0.9859] n_targets:  1 reward:  54.05\n",
      "71.3 action:  [0.3394, -0.9985] n_targets:  1 reward:  52.92\n",
      "72.1 action:  [-0.2477, -0.9927] n_targets:  1 reward:  54.88\n",
      "72.8 action:  [-0.2239, -0.9965] n_targets:  1 reward:  51.47\n",
      "73.4 action:  [-0.0442, -0.9937] n_targets:  1 reward:  59.95\n",
      "74.6 action:  [-0.4416, -0.9901] n_targets:  2 reward:  129.75\n",
      "76.0 action:  [0.0938, -0.9859] n_targets:  2 reward:  138.09\n",
      "78.0 action:  [-0.2907, -0.9808] n_targets:  1 reward:  92.66\n",
      "79.6 action:  [-0.008, -0.9956] n_targets:  1 reward:  62.11\n",
      "81.5 action:  [-0.0977, -0.9827] n_targets:  1 reward:  50.95\n",
      "83.5 action:  [0.2221, -0.9844] n_targets:  1 reward:  61.03\n",
      "84.7 action:  [0.2082, -0.9966] n_targets:  2 reward:  120.21\n",
      "85.5 action:  [-0.053, -0.9857] n_targets:  2 reward:  120.55\n",
      "86.6 action:  [-0.0726, -0.9971] n_targets:  2 reward:  153.35\n",
      "92.3 action:  [-0.112, -0.9914] n_targets:  1 reward:  61.23\n",
      "95.3 action:  [0.3186, -0.9926] n_targets:  1 reward:  57.62\n",
      "96.1 action:  [0.0342, -0.9842] n_targets:  1 reward:  62.19\n",
      "97.8 action:  [-0.1831, -0.9933] n_targets:  1 reward:  57.48\n",
      "98.6 action:  [-0.1781, -0.9905] n_targets:  1 reward:  51.92\n",
      "99.6 action:  [0.0411, -0.99] n_targets:  1 reward:  50.18\n",
      "100.2 action:  [0.2707, -0.989] n_targets:  1 reward:  58.36\n",
      "ALPHA (entropy-related):  tensor([0.2949], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.33993 0.33402 0.32853 0.32323 0.3181  0.3132  0.30834 0.30365 0.29916\n",
      " 0.2949 ]\n",
      "Episode: 58, Episode Reward: 5706.707494099934\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  220\n",
      "0.5 action:  [0.081, -0.9948] n_targets:  2 reward:  153.5\n",
      "0.9 action:  [0.0101, -0.9959] n_targets:  2 reward:  100.83\n",
      "2.1 action:  [-0.2693, -0.995] n_targets:  1 reward:  61.75\n",
      "2.5 action:  [0.0059, -0.9949] n_targets:  1 reward:  57.37\n",
      "6.3 action:  [0.3444, -0.9889] n_targets:  1 reward:  58.49\n",
      "8.9 action:  [-0.0417, -0.9994] n_targets:  1 reward:  57.11\n",
      "9.4 action:  [0.0996, -0.9926] n_targets:  1 reward:  53.49\n",
      "12.7 action:  [0.0635, -0.9844] n_targets:  1 reward:  61.15\n",
      "14.9 action:  [-0.2926, -0.9862] n_targets:  1 reward:  50.76\n",
      "15.5 action:  [0.4245, -0.9908] n_targets:  1 reward:  64.38\n",
      "18.4 action:  [0.027, -0.9957] n_targets:  1 reward:  77.37\n",
      "19.2 action:  [-0.255, -0.9898] n_targets:  1 reward:  50.9\n",
      "19.8 action:  [-0.3705, -0.9906] n_targets:  1 reward:  53.43\n",
      "20.4 action:  [-0.0701, -0.9909] n_targets:  1 reward:  76.43\n",
      "21.0 action:  [0.1892, -0.9993] n_targets:  1 reward:  56.75\n",
      "21.2 action:  [-0.1178, -0.9836] n_targets:  1 reward:  59.86\n",
      "23.4 action:  [0.2678, -0.9943] n_targets:  1 reward:  74.25\n",
      "23.6 action:  [-0.1371, -0.9847] n_targets:  2 reward:  107.44\n",
      "24.8 action:  [0.4574, -0.9851] n_targets:  1 reward:  55.94\n",
      "25.4 action:  [0.1234, -0.9959] n_targets:  1 reward:  53.0\n",
      "25.8 action:  [0.0257, -0.9862] n_targets:  1 reward:  59.31\n",
      "26.6 action:  [-0.2673, -0.9979] n_targets:  1 reward:  50.58\n",
      "28.0 action:  [-0.1105, -0.9864] n_targets:  1 reward:  56.24\n",
      "28.8 action:  [-0.233, -0.9857] n_targets:  1 reward:  50.51\n",
      "32.4 action:  [0.0555, -0.9894] n_targets:  1 reward:  58.0\n",
      "32.8 action:  [-0.3288, -0.9943] n_targets:  1 reward:  50.21\n",
      "33.2 action:  [0.2671, -0.9855] n_targets:  1 reward:  56.09\n",
      "33.6 action:  [0.0535, -0.9932] n_targets:  2 reward:  105.91\n",
      "34.4 action:  [-0.1175, -0.9867] n_targets:  3 reward:  205.86\n",
      "35.8 action:  [0.1771, -0.9986] n_targets:  1 reward:  90.06\n",
      "36.6 action:  [-0.2871, -0.9986] n_targets:  1 reward:  50.02\n",
      "39.0 action:  [-0.1056, -0.9994] n_targets:  1 reward:  75.73\n",
      "39.2 action:  [-0.1007, -0.99] n_targets:  1 reward:  52.26\n",
      "43.4 action:  [-0.1773, -0.9802] n_targets:  1 reward:  51.3\n",
      "44.0 action:  [0.0627, -0.9812] n_targets:  1 reward:  51.53\n",
      "44.6 action:  [0.3262, -0.9903] n_targets:  1 reward:  58.22\n",
      "45.6 action:  [-0.2962, -0.995] n_targets:  2 reward:  107.59\n",
      "47.0 action:  [0.0696, -0.9969] n_targets:  1 reward:  59.62\n",
      "49.6 action:  [-0.2147, -0.9946] n_targets:  1 reward:  57.21\n",
      "50.8 action:  [-0.1945, -0.9911] n_targets:  1 reward:  53.17\n",
      "53.2 action:  [0.3827, -0.9934] n_targets:  1 reward:  58.99\n",
      "53.4 action:  [-0.2095, -0.9818] n_targets:  1 reward:  52.25\n",
      "53.8 action:  [-0.2363, -0.996] n_targets:  1 reward:  54.76\n",
      "54.6 action:  [0.0148, -0.9983] n_targets:  1 reward:  76.89\n",
      "56.1 action:  [-0.0147, -0.9947] n_targets:  1 reward:  57.06\n",
      "57.3 action:  [-0.044, -0.998] n_targets:  2 reward:  132.6\n",
      "57.9 action:  [-0.0464, -0.9911] n_targets:  1 reward:  53.49\n",
      "58.7 action:  [-0.0218, -0.9926] n_targets:  1 reward:  53.66\n",
      "59.9 action:  [0.1607, -0.9936] n_targets:  1 reward:  55.16\n",
      "61.1 action:  [-0.1096, -0.9848] n_targets:  2 reward:  150.2\n",
      "64.3 action:  [0.042, -0.9921] n_targets:  1 reward:  55.08\n",
      "65.9 action:  [-0.3986, -0.986] n_targets:  1 reward:  54.88\n",
      "69.8 action:  [-0.3679, -0.9908] n_targets:  1 reward:  87.28\n",
      "70.2 action:  [-0.3625, -0.9974] n_targets:  1 reward:  63.96\n",
      "71.6 action:  [0.1631, -0.9942] n_targets:  2 reward:  109.54\n",
      "72.4 action:  [-0.0893, -0.9943] n_targets:  1 reward:  86.24\n",
      "73.3 action:  [-0.0725, -0.9829] n_targets:  1 reward:  60.0\n",
      "74.2 action:  [0.1545, -0.9942] n_targets:  1 reward:  56.57\n",
      "76.4 action:  [-0.1052, -0.9923] n_targets:  1 reward:  59.68\n",
      "77.6 action:  [-0.5262, -0.9862] n_targets:  1 reward:  55.61\n",
      "79.2 action:  [-0.0049, -0.9957] n_targets:  2 reward:  128.37\n",
      "81.2 action:  [0.2651, -0.9899] n_targets:  1 reward:  58.27\n",
      "82.6 action:  [-0.3061, -0.9899] n_targets:  1 reward:  74.6\n",
      "85.2 action:  [-0.276, -0.997] n_targets:  1 reward:  63.34\n",
      "87.6 action:  [0.2223, -0.998] n_targets:  1 reward:  54.12\n",
      "87.8 action:  [-0.3913, -0.9884] n_targets:  1 reward:  52.47\n",
      "88.0 action:  [0.216, -0.9944] n_targets:  1 reward:  56.19\n",
      "88.6 action:  [0.0408, -0.9907] n_targets:  2 reward:  119.25\n",
      "90.8 action:  [0.1579, -0.9871] n_targets:  1 reward:  58.98\n",
      "92.2 action:  [0.2601, -0.994] n_targets:  1 reward:  70.54\n",
      "92.6 action:  [0.1574, -0.9956] n_targets:  1 reward:  62.76\n",
      "93.4 action:  [0.3283, -0.9973] n_targets:  1 reward:  70.58\n",
      "93.6 action:  [-0.0145, -0.9903] n_targets:  1 reward:  61.21\n",
      "96.5 action:  [-0.1222, -0.991] n_targets:  1 reward:  69.01\n",
      "99.8 action:  [0.1071, -0.9918] n_targets:  1 reward:  50.0\n",
      "100.4 action:  [0.0002, -0.9878] n_targets:  1 reward:  56.86\n",
      "ALPHA (entropy-related):  tensor([0.2908], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.33402 0.32853 0.32323 0.3181  0.3132  0.30834 0.30365 0.29916 0.2949\n",
      " 0.29077]\n",
      "Episode: 59, Episode Reward: 5324.055096944173\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  221\n",
      "0.3 action:  [0.0314, -0.9864] n_targets:  2 reward:  138.92\n",
      "0.7 action:  [-0.6231, -0.9949] n_targets:  2 reward:  108.2\n",
      "1.9 action:  [-0.0734, -0.9898] n_targets:  1 reward:  55.1\n",
      "3.7 action:  [-0.3382, -0.9873] n_targets:  1 reward:  52.86\n",
      "5.8 action:  [0.4782, -0.9833] n_targets:  1 reward:  69.5\n",
      "6.8 action:  [-0.1218, -0.9872] n_targets:  1 reward:  60.71\n",
      "7.4 action:  [-0.063, -0.9813] n_targets:  2 reward:  107.04\n",
      "10.0 action:  [0.2902, -0.9874] n_targets:  2 reward:  164.41\n",
      "10.4 action:  [0.1701, -0.9829] n_targets:  1 reward:  62.86\n",
      "12.5 action:  [0.2799, -0.9931] n_targets:  1 reward:  71.06\n",
      "13.5 action:  [-0.2697, -0.9837] n_targets:  1 reward:  61.86\n",
      "16.9 action:  [-0.1038, -0.9954] n_targets:  3 reward:  151.2\n",
      "21.1 action:  [-0.0604, -0.9855] n_targets:  1 reward:  63.64\n",
      "22.7 action:  [0.1044, -0.9898] n_targets:  1 reward:  55.01\n",
      "24.3 action:  [-0.0169, -0.9812] n_targets:  3 reward:  201.13\n",
      "29.1 action:  [0.2209, -0.9924] n_targets:  1 reward:  66.43\n",
      "30.2 action:  [-0.0383, -0.9891] n_targets:  1 reward:  74.59\n",
      "31.4 action:  [0.1596, -0.9958] n_targets:  1 reward:  74.3\n",
      "32.7 action:  [-0.2837, -0.9822] n_targets:  4 reward:  238.72\n",
      "34.1 action:  [0.162, -0.9949] n_targets:  2 reward:  113.92\n",
      "36.9 action:  [-0.1733, -0.9984] n_targets:  1 reward:  74.99\n",
      "38.3 action:  [0.0452, -0.9927] n_targets:  2 reward:  117.33\n",
      "40.5 action:  [0.1226, -0.9974] n_targets:  1 reward:  54.36\n",
      "41.3 action:  [-0.0914, -0.989] n_targets:  1 reward:  54.48\n",
      "43.3 action:  [0.2515, -0.9929] n_targets:  1 reward:  72.41\n",
      "44.0 action:  [0.1783, -0.9965] n_targets:  1 reward:  57.71\n",
      "48.7 action:  [-0.1837, -0.9902] n_targets:  1 reward:  62.05\n",
      "51.4 action:  [0.2529, -0.9871] n_targets:  2 reward:  148.32\n",
      "53.2 action:  [0.1624, -0.9964] n_targets:  1 reward:  74.71\n",
      "55.6 action:  [0.2252, -0.9959] n_targets:  1 reward:  60.65\n",
      "56.4 action:  [-0.235, -0.9867] n_targets:  2 reward:  145.65\n",
      "57.0 action:  [-0.1178, -0.9887] n_targets:  1 reward:  52.97\n",
      "58.0 action:  [0.3207, -0.9856] n_targets:  1 reward:  62.44\n",
      "58.6 action:  [-0.2295, -0.988] n_targets:  1 reward:  53.12\n",
      "60.2 action:  [-0.1555, -0.9842] n_targets:  1 reward:  52.22\n",
      "60.6 action:  [-0.3958, -0.9871] n_targets:  1 reward:  51.32\n",
      "62.0 action:  [0.1411, -0.9843] n_targets:  2 reward:  116.75\n",
      "63.0 action:  [0.1972, -0.9805] n_targets:  1 reward:  51.57\n",
      "63.6 action:  [0.2535, -0.987] n_targets:  1 reward:  77.63\n",
      "64.2 action:  [-0.112, -0.984] n_targets:  1 reward:  50.79\n",
      "66.6 action:  [-0.0099, -0.9902] n_targets:  1 reward:  61.85\n",
      "67.0 action:  [0.3611, -0.999] n_targets:  1 reward:  57.24\n",
      "69.5 action:  [0.1901, -0.9937] n_targets:  1 reward:  56.71\n",
      "71.8 action:  [-0.3421, -0.9886] n_targets:  2 reward:  103.11\n",
      "74.2 action:  [0.5891, -0.9981] n_targets:  2 reward:  139.29\n",
      "77.2 action:  [0.2482, -0.9912] n_targets:  3 reward:  207.52\n",
      "85.2 action:  [-0.3603, -0.9874] n_targets:  1 reward:  55.62\n",
      "86.8 action:  [-0.4015, -0.9903] n_targets:  1 reward:  52.77\n",
      "88.8 action:  [-0.027, -0.9835] n_targets:  1 reward:  78.96\n",
      "89.6 action:  [0.0096, -0.9932] n_targets:  1 reward:  63.37\n",
      "91.8 action:  [-0.2512, -0.9929] n_targets:  2 reward:  134.73\n",
      "93.0 action:  [0.2691, -0.9901] n_targets:  2 reward:  113.64\n",
      "93.6 action:  [-0.1975, -0.994] n_targets:  2 reward:  105.61\n",
      "96.8 action:  [-0.1382, -0.9911] n_targets:  1 reward:  70.04\n",
      "98.2 action:  [-0.0702, -0.9863] n_targets:  1 reward:  83.38\n",
      "99.4 action:  [-0.0581, -0.991] n_targets:  1 reward:  70.07\n",
      "ALPHA (entropy-related):  tensor([0.2867], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.32853 0.32323 0.3181  0.3132  0.30834 0.30365 0.29916 0.2949  0.29077\n",
      " 0.28672]\n",
      "Last 100 ALPHA: [0.97788 0.95496 0.93215 0.90999 0.88852 0.86787 0.84794 0.82852 0.80965\n",
      " 0.79129 0.7734  0.75598 0.73901 0.72248 0.70635 0.69064 0.67531 0.66036\n",
      " 0.64578 0.63155 0.61769 0.60419 0.59106 0.5783  0.5658  0.55365 0.54182\n",
      " 0.53022 0.51899 0.50808 0.49747 0.48716 0.47703 0.4672  0.45771 0.44833\n",
      " 0.43925 0.43039 0.42165 0.41309 0.40474 0.39652 0.38867 0.38105 0.37368\n",
      " 0.36641 0.35933 0.35272 0.34622 0.33993 0.33402 0.32853 0.32323 0.3181\n",
      " 0.3132  0.30834 0.30365 0.29916 0.2949  0.29077 0.28672]\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  222\n",
      "2.1 action:  [-0.0787, -0.9826] n_targets:  2 reward:  118.19\n",
      "5.5 action:  [-0.0994, -0.9826] n_targets:  2 reward:  183.77\n",
      "57.7 action:  [-0.0784, -0.9831] n_targets:  1 reward:  95.02\n",
      "73.1 action:  [-0.0928, -0.9826] n_targets:  1 reward:  66.05\n",
      "74.5 action:  [-0.0439, -0.9832] n_targets:  1 reward:  54.38\n",
      "79.9 action:  [-0.0929, -0.9822] n_targets:  1 reward:  54.67\n",
      "81.1 action:  [-0.1006, -0.9823] n_targets:  4 reward:  249.76\n",
      "84.9 action:  [-0.0996, -0.9812] n_targets:  3 reward:  215.95\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  223\n",
      "0.1 action:  [0.0155, -0.9809] n_targets:  2 reward:  136.54\n",
      "1.3 action:  [-0.0739, -0.9802] n_targets:  2 reward:  124.63\n",
      "6.7 action:  [-0.0801, -0.9816] n_targets:  2 reward:  137.23\n",
      "7.3 action:  [-0.086, -0.981] n_targets:  1 reward:  50.83\n",
      "20.7 action:  [-0.0663, -0.9833] n_targets:  1 reward:  63.97\n",
      "39.7 action:  [-0.0923, -0.9831] n_targets:  1 reward:  96.13\n",
      "53.4 action:  [-0.0064, -0.9849] n_targets:  3 reward:  173.13\n",
      "55.4 action:  [0.0095, -0.9862] n_targets:  1 reward:  52.69\n",
      "63.2 action:  [-0.1007, -0.9815] n_targets:  1 reward:  66.05\n",
      "77.4 action:  [-0.0189, -0.9801] n_targets:  1 reward:  69.57\n",
      "81.2 action:  [-0.0761, -0.9817] n_targets:  1 reward:  85.51\n",
      "96.0 action:  [-0.0488, -0.9803] n_targets:  1 reward:  77.56\n",
      "Best average reward: 1720.9753100077312, Current average reward: 1085.8211957613628\n",
      "Evaluation rewards: [0.0, 0.0, 1720.9753100077312, 1085.8211957613628]\n",
      "Episode: 60, Episode Reward: 4936.864229838053\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  224\n",
      "0.3 action:  [-0.5764, -0.9951] n_targets:  1 reward:  58.32\n",
      "2.0 action:  [0.2367, -0.9926] n_targets:  1 reward:  67.18\n",
      "2.4 action:  [-0.1945, -0.9837] n_targets:  1 reward:  51.05\n",
      "6.2 action:  [-0.056, -0.9898] n_targets:  1 reward:  57.43\n",
      "6.8 action:  [-0.0954, -0.9918] n_targets:  1 reward:  58.33\n",
      "8.4 action:  [-0.0098, -0.9824] n_targets:  1 reward:  96.18\n",
      "8.8 action:  [0.0655, -0.9897] n_targets:  1 reward:  63.83\n",
      "10.2 action:  [-0.0893, -0.9873] n_targets:  1 reward:  54.39\n",
      "11.2 action:  [-0.2532, -0.9915] n_targets:  1 reward:  50.18\n",
      "14.4 action:  [0.3234, -0.9953] n_targets:  1 reward:  50.56\n",
      "15.4 action:  [0.1996, -0.997] n_targets:  1 reward:  61.39\n",
      "16.2 action:  [-0.4748, -0.9801] n_targets:  1 reward:  88.99\n",
      "16.8 action:  [-0.22, -0.9959] n_targets:  1 reward:  66.25\n",
      "18.4 action:  [0.4995, -0.9815] n_targets:  2 reward:  130.46\n",
      "19.2 action:  [0.2487, -0.9819] n_targets:  1 reward:  55.87\n",
      "19.8 action:  [-0.2146, -0.9864] n_targets:  1 reward:  58.22\n",
      "21.0 action:  [-0.3342, -0.9852] n_targets:  1 reward:  51.19\n",
      "22.1 action:  [0.2309, -0.9847] n_targets:  1 reward:  82.65\n",
      "23.3 action:  [-0.381, -0.9851] n_targets:  1 reward:  63.6\n",
      "24.9 action:  [-0.2059, -0.9978] n_targets:  2 reward:  113.41\n",
      "25.1 action:  [0.4411, -0.9921] n_targets:  1 reward:  51.42\n",
      "25.5 action:  [0.0889, -0.9982] n_targets:  2 reward:  116.98\n",
      "25.9 action:  [-0.2245, -0.9821] n_targets:  1 reward:  52.75\n",
      "29.2 action:  [-0.5835, -0.9972] n_targets:  1 reward:  54.95\n",
      "32.0 action:  [-0.5862, -0.9912] n_targets:  1 reward:  57.28\n",
      "33.6 action:  [0.4258, -0.9968] n_targets:  3 reward:  229.91\n",
      "34.0 action:  [0.0834, -0.9926] n_targets:  1 reward:  57.45\n",
      "36.4 action:  [-0.383, -0.9821] n_targets:  4 reward:  273.72\n",
      "39.6 action:  [-0.3616, -0.9964] n_targets:  1 reward:  57.91\n",
      "41.0 action:  [-0.1901, -0.9854] n_targets:  1 reward:  64.97\n",
      "42.2 action:  [0.2959, -0.9861] n_targets:  1 reward:  70.76\n",
      "45.4 action:  [-0.4023, -0.9877] n_targets:  2 reward:  164.17\n",
      "46.0 action:  [0.2729, -0.9942] n_targets:  1 reward:  51.84\n",
      "46.4 action:  [0.1962, -0.9919] n_targets:  1 reward:  54.03\n",
      "54.1 action:  [-0.0904, -0.9944] n_targets:  1 reward:  54.6\n",
      "54.7 action:  [-0.2668, -0.9882] n_targets:  1 reward:  62.58\n",
      "56.5 action:  [-0.0808, -0.984] n_targets:  1 reward:  51.45\n",
      "58.7 action:  [-0.3044, -0.9882] n_targets:  1 reward:  74.36\n",
      "59.5 action:  [0.2998, -0.9845] n_targets:  1 reward:  64.65\n",
      "60.8 action:  [-0.505, -0.9982] n_targets:  1 reward:  57.8\n",
      "64.0 action:  [0.2408, -0.9834] n_targets:  1 reward:  55.78\n",
      "65.3 action:  [-0.1143, -0.9804] n_targets:  2 reward:  108.46\n",
      "66.5 action:  [-0.0829, -0.9859] n_targets:  1 reward:  59.62\n",
      "68.8 action:  [-0.1467, -0.9926] n_targets:  2 reward:  107.4\n",
      "70.3 action:  [0.0107, -0.9904] n_targets:  2 reward:  107.82\n",
      "71.3 action:  [-0.0979, -0.9925] n_targets:  1 reward:  53.66\n",
      "71.5 action:  [0.0232, -0.9979] n_targets:  1 reward:  53.92\n",
      "76.1 action:  [-0.1998, -0.9998] n_targets:  1 reward:  52.4\n",
      "76.3 action:  [-0.2471, -0.9964] n_targets:  1 reward:  53.1\n",
      "77.3 action:  [0.1323, -0.9869] n_targets:  2 reward:  115.72\n",
      "78.1 action:  [-0.0178, -0.9887] n_targets:  1 reward:  57.73\n",
      "78.7 action:  [-0.3317, -0.98] n_targets:  2 reward:  122.58\n",
      "79.7 action:  [-0.4221, -0.9963] n_targets:  2 reward:  113.37\n",
      "80.4 action:  [0.1581, -0.9948] n_targets:  1 reward:  80.84\n",
      "87.6 action:  [-0.066, -0.9861] n_targets:  3 reward:  179.98\n",
      "89.8 action:  [-0.0563, -0.9889] n_targets:  2 reward:  110.57\n",
      "91.2 action:  [-0.212, -0.9913] n_targets:  1 reward:  61.93\n",
      "91.6 action:  [0.1731, -0.9827] n_targets:  1 reward:  56.65\n",
      "93.4 action:  [-0.2892, -0.9857] n_targets:  1 reward:  78.53\n",
      "94.2 action:  [-0.0275, -0.9887] n_targets:  1 reward:  54.82\n",
      "94.8 action:  [-0.4272, -0.997] n_targets:  1 reward:  50.09\n",
      "96.0 action:  [0.2773, -0.9885] n_targets:  2 reward:  110.78\n",
      "96.8 action:  [-0.0394, -0.9825] n_targets:  2 reward:  112.14\n",
      "97.0 action:  [-0.5083, -0.9835] n_targets:  1 reward:  60.12\n",
      "97.2 action:  [0.0782, -0.9906] n_targets:  1 reward:  51.45\n",
      "97.4 action:  [-0.2848, -0.9892] n_targets:  2 reward:  105.8\n",
      "99.6 action:  [0.1462, -0.9917] n_targets:  2 reward:  162.58\n",
      "102.0 action:  [0.2331, -0.9801] n_targets:  1 reward:  54.22\n",
      "ALPHA (entropy-related):  tensor([0.2828], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.32323 0.3181  0.3132  0.30834 0.30365 0.29916 0.2949  0.29077 0.28672\n",
      " 0.28285]\n",
      "Episode: 61, Episode Reward: 5495.139940897623\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  225\n",
      "0.1 action:  [0.126, -0.9965] n_targets:  2 reward:  120.37\n",
      "1.9 action:  [-0.0936, -0.9909] n_targets:  1 reward:  52.41\n",
      "3.2 action:  [-0.2603, -0.9801] n_targets:  1 reward:  74.48\n",
      "4.2 action:  [0.1585, -0.9944] n_targets:  1 reward:  60.98\n",
      "6.3 action:  [-0.1104, -0.9887] n_targets:  2 reward:  101.31\n",
      "10.0 action:  [-0.0607, -0.9858] n_targets:  1 reward:  50.24\n",
      "10.9 action:  [-0.1645, -0.9861] n_targets:  1 reward:  50.29\n",
      "13.3 action:  [0.4891, -0.9865] n_targets:  1 reward:  70.97\n",
      "13.9 action:  [-0.2925, -0.9947] n_targets:  1 reward:  53.09\n",
      "15.0 action:  [0.0955, -0.9804] n_targets:  1 reward:  59.14\n",
      "16.5 action:  [0.3216, -0.9902] n_targets:  1 reward:  66.29\n",
      "16.9 action:  [-0.1607, -0.9946] n_targets:  1 reward:  60.88\n",
      "17.5 action:  [-0.0615, -0.985] n_targets:  1 reward:  51.64\n",
      "18.7 action:  [0.0495, -0.9851] n_targets:  1 reward:  54.26\n",
      "20.9 action:  [-0.1519, -0.9947] n_targets:  2 reward:  121.48\n",
      "21.9 action:  [0.2458, -0.991] n_targets:  1 reward:  71.65\n",
      "22.1 action:  [-0.0679, -0.9912] n_targets:  3 reward:  157.29\n",
      "24.1 action:  [0.2017, -0.9821] n_targets:  1 reward:  57.53\n",
      "25.1 action:  [0.3413, -0.9909] n_targets:  1 reward:  53.25\n",
      "25.9 action:  [-0.1305, -0.9974] n_targets:  2 reward:  143.6\n",
      "26.9 action:  [0.0196, -0.981] n_targets:  1 reward:  57.1\n",
      "27.1 action:  [-0.1498, -0.9908] n_targets:  1 reward:  51.75\n",
      "28.6 action:  [-0.014, -0.9852] n_targets:  2 reward:  109.08\n",
      "30.2 action:  [-0.0172, -0.9963] n_targets:  1 reward:  60.4\n",
      "30.8 action:  [0.2867, -0.9805] n_targets:  1 reward:  56.94\n",
      "31.5 action:  [-0.1595, -0.9969] n_targets:  1 reward:  51.4\n",
      "31.7 action:  [-0.0686, -0.9891] n_targets:  1 reward:  51.93\n",
      "33.3 action:  [-0.0279, -0.988] n_targets:  2 reward:  108.27\n",
      "33.7 action:  [0.4387, -0.9802] n_targets:  1 reward:  68.03\n",
      "38.3 action:  [0.2457, -0.9911] n_targets:  1 reward:  57.5\n",
      "38.7 action:  [0.0877, -0.9922] n_targets:  2 reward:  115.49\n",
      "40.3 action:  [0.3838, -0.9916] n_targets:  2 reward:  108.43\n",
      "40.7 action:  [-0.256, -0.9994] n_targets:  1 reward:  65.72\n",
      "42.1 action:  [0.4029, -0.9848] n_targets:  2 reward:  114.95\n",
      "45.5 action:  [-0.1959, -0.9824] n_targets:  1 reward:  50.03\n",
      "45.9 action:  [0.0897, -0.9878] n_targets:  2 reward:  110.45\n",
      "46.8 action:  [0.0076, -0.9933] n_targets:  2 reward:  107.17\n",
      "48.1 action:  [0.0487, -0.9986] n_targets:  1 reward:  51.1\n",
      "53.7 action:  [0.0082, -0.9838] n_targets:  2 reward:  138.77\n",
      "57.1 action:  [-0.1929, -0.9827] n_targets:  1 reward:  55.44\n",
      "60.6 action:  [0.1658, -0.9925] n_targets:  1 reward:  57.68\n",
      "62.5 action:  [-0.0384, -0.9949] n_targets:  3 reward:  172.67\n",
      "62.7 action:  [-0.1232, -0.9856] n_targets:  1 reward:  51.92\n",
      "63.5 action:  [-0.0834, -0.9943] n_targets:  1 reward:  52.63\n",
      "64.5 action:  [0.0909, -0.9983] n_targets:  1 reward:  66.05\n",
      "65.3 action:  [-0.1804, -0.9883] n_targets:  1 reward:  55.16\n",
      "68.1 action:  [-0.4165, -0.9944] n_targets:  1 reward:  57.1\n",
      "68.7 action:  [-0.4007, -0.9878] n_targets:  1 reward:  69.74\n",
      "70.4 action:  [0.3713, -0.9867] n_targets:  2 reward:  143.48\n",
      "74.2 action:  [0.4034, -0.9865] n_targets:  1 reward:  50.23\n",
      "76.4 action:  [-0.0693, -0.998] n_targets:  2 reward:  110.36\n",
      "78.4 action:  [0.4279, -0.986] n_targets:  1 reward:  58.2\n",
      "78.6 action:  [0.0651, -0.9864] n_targets:  1 reward:  50.59\n",
      "80.0 action:  [-0.3515, -0.9806] n_targets:  1 reward:  51.65\n",
      "80.2 action:  [0.255, -0.9977] n_targets:  1 reward:  61.57\n",
      "82.2 action:  [-0.1017, -0.9988] n_targets:  1 reward:  60.17\n",
      "85.2 action:  [-0.2583, -0.9952] n_targets:  1 reward:  60.27\n",
      "86.6 action:  [-0.2036, -0.9939] n_targets:  1 reward:  76.25\n",
      "87.4 action:  [-0.2401, -0.9847] n_targets:  1 reward:  52.92\n",
      "88.8 action:  [-0.0574, -0.9931] n_targets:  1 reward:  59.94\n",
      "90.0 action:  [0.0467, -0.9802] n_targets:  2 reward:  129.0\n",
      "99.7 action:  [-0.0093, -0.9905] n_targets:  1 reward:  54.97\n",
      "99.9 action:  [0.0384, -0.9889] n_targets:  1 reward:  50.74\n",
      "100.5 action:  [-0.2679, -0.986] n_targets:  1 reward:  75.04\n",
      "101.5 action:  [-0.0621, -0.995] n_targets:  1 reward:  50.47\n",
      "101.9 action:  [0.2191, -0.988] n_targets:  1 reward:  59.76\n",
      "ALPHA (entropy-related):  tensor([0.2789], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.3181  0.3132  0.30834 0.30365 0.29916 0.2949  0.29077 0.28672 0.28285\n",
      " 0.27889]\n",
      "Episode: 62, Episode Reward: 4959.697924296061\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  226\n",
      "0.3 action:  [-0.3656, -0.9946] n_targets:  3 reward:  223.28\n",
      "1.9 action:  [-0.3085, -0.9837] n_targets:  1 reward:  53.29\n",
      "4.5 action:  [-0.327, -0.9885] n_targets:  1 reward:  53.56\n",
      "9.0 action:  [0.249, -0.9894] n_targets:  1 reward:  57.61\n",
      "10.8 action:  [-0.1697, -0.993] n_targets:  1 reward:  57.94\n",
      "11.4 action:  [-0.2911, -0.9907] n_targets:  1 reward:  69.87\n",
      "12.8 action:  [0.0453, -0.9933] n_targets:  2 reward:  122.2\n",
      "13.4 action:  [-0.311, -0.9882] n_targets:  1 reward:  60.1\n",
      "15.2 action:  [0.2627, -0.986] n_targets:  2 reward:  128.96\n",
      "16.0 action:  [0.0826, -0.9869] n_targets:  2 reward:  109.15\n",
      "16.8 action:  [0.1191, -0.988] n_targets:  1 reward:  52.54\n",
      "20.8 action:  [0.3114, -0.9879] n_targets:  2 reward:  142.46\n",
      "22.8 action:  [-0.0731, -0.9872] n_targets:  4 reward:  291.0\n",
      "26.0 action:  [-0.0043, -0.985] n_targets:  1 reward:  62.27\n",
      "27.8 action:  [0.1208, -0.9855] n_targets:  1 reward:  60.42\n",
      "31.6 action:  [0.0328, -0.9956] n_targets:  1 reward:  80.99\n",
      "32.2 action:  [0.1151, -0.9817] n_targets:  1 reward:  53.29\n",
      "33.3 action:  [-0.3826, -0.9979] n_targets:  1 reward:  51.18\n",
      "36.1 action:  [0.0896, -0.9908] n_targets:  2 reward:  111.95\n",
      "36.5 action:  [-0.293, -0.9904] n_targets:  1 reward:  59.23\n",
      "38.1 action:  [-0.0153, -0.9807] n_targets:  2 reward:  116.18\n",
      "38.5 action:  [0.0538, -0.9907] n_targets:  1 reward:  62.43\n",
      "40.5 action:  [0.3538, -0.9902] n_targets:  2 reward:  106.43\n",
      "41.9 action:  [0.3544, -0.9803] n_targets:  1 reward:  52.62\n",
      "43.5 action:  [0.068, -0.9914] n_targets:  1 reward:  52.99\n",
      "44.3 action:  [-0.3678, -0.9936] n_targets:  1 reward:  56.67\n",
      "45.5 action:  [0.395, -0.9906] n_targets:  2 reward:  121.36\n",
      "46.3 action:  [-0.3383, -0.9909] n_targets:  1 reward:  65.33\n",
      "53.3 action:  [-0.4036, -0.9895] n_targets:  1 reward:  54.55\n",
      "54.3 action:  [0.2921, -0.9838] n_targets:  1 reward:  53.23\n",
      "55.5 action:  [-0.0501, -0.9975] n_targets:  1 reward:  67.15\n",
      "56.5 action:  [0.0174, -0.9924] n_targets:  1 reward:  68.8\n",
      "58.9 action:  [0.0347, -0.9802] n_targets:  2 reward:  105.56\n",
      "61.1 action:  [0.1502, -0.9973] n_targets:  2 reward:  137.76\n",
      "62.1 action:  [0.0366, -0.9977] n_targets:  1 reward:  60.46\n",
      "62.3 action:  [-0.2227, -0.9825] n_targets:  1 reward:  56.09\n",
      "63.1 action:  [0.5332, -0.9903] n_targets:  1 reward:  52.54\n",
      "63.3 action:  [-0.0186, -0.9944] n_targets:  1 reward:  56.45\n",
      "64.3 action:  [-0.2359, -0.9805] n_targets:  3 reward:  210.9\n",
      "66.1 action:  [0.5813, -0.9835] n_targets:  1 reward:  51.79\n",
      "66.7 action:  [0.0521, -0.9861] n_targets:  1 reward:  55.24\n",
      "67.1 action:  [0.2476, -0.9964] n_targets:  1 reward:  53.39\n",
      "67.3 action:  [0.3863, -0.9874] n_targets:  1 reward:  51.51\n",
      "69.9 action:  [0.2924, -0.997] n_targets:  1 reward:  50.69\n",
      "72.7 action:  [0.4787, -0.9903] n_targets:  1 reward:  50.21\n",
      "79.7 action:  [-0.3777, -0.9879] n_targets:  1 reward:  54.62\n",
      "80.5 action:  [0.0949, -0.9858] n_targets:  2 reward:  140.52\n",
      "80.9 action:  [-0.2537, -0.9972] n_targets:  1 reward:  51.37\n",
      "82.7 action:  [0.2728, -0.9886] n_targets:  2 reward:  143.95\n",
      "84.6 action:  [0.0767, -0.9882] n_targets:  1 reward:  90.81\n",
      "84.8 action:  [-0.4357, -0.9926] n_targets:  1 reward:  52.35\n",
      "86.7 action:  [0.1668, -0.9813] n_targets:  1 reward:  63.34\n",
      "89.4 action:  [0.0866, -0.9897] n_targets:  2 reward:  125.98\n",
      "90.2 action:  [0.2641, -0.9851] n_targets:  1 reward:  50.44\n",
      "91.6 action:  [0.1345, -0.9872] n_targets:  2 reward:  138.81\n",
      "93.2 action:  [-0.1785, -0.9881] n_targets:  1 reward:  52.4\n",
      "94.4 action:  [-0.199, -0.9974] n_targets:  1 reward:  57.39\n",
      "ALPHA (entropy-related):  tensor([0.2752], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.3132  0.30834 0.30365 0.29916 0.2949  0.29077 0.28672 0.28285 0.27889\n",
      " 0.27518]\n",
      "Episode: 63, Episode Reward: 4793.606875101725\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  227\n",
      "0.1 action:  [0.6332, -0.9976] n_targets:  3 reward:  191.69\n",
      "4.4 action:  [-0.2115, -0.9953] n_targets:  1 reward:  51.0\n",
      "6.2 action:  [-0.1964, -0.9889] n_targets:  1 reward:  51.63\n",
      "10.8 action:  [-0.4108, -0.9911] n_targets:  1 reward:  54.14\n",
      "14.0 action:  [0.1081, -0.9905] n_targets:  1 reward:  68.54\n",
      "15.0 action:  [0.1502, -0.9882] n_targets:  1 reward:  57.24\n",
      "17.4 action:  [-0.1657, -0.9929] n_targets:  2 reward:  136.76\n",
      "17.8 action:  [0.4741, -0.9962] n_targets:  1 reward:  92.64\n",
      "18.4 action:  [0.3758, -0.9914] n_targets:  2 reward:  111.47\n",
      "18.8 action:  [0.0591, -0.9911] n_targets:  1 reward:  53.95\n",
      "20.0 action:  [0.0154, -0.9826] n_targets:  1 reward:  55.71\n",
      "22.6 action:  [0.4385, -0.9849] n_targets:  1 reward:  97.91\n",
      "24.8 action:  [-0.2629, -0.9897] n_targets:  1 reward:  63.33\n",
      "26.4 action:  [-0.3757, -0.9841] n_targets:  1 reward:  70.71\n",
      "27.2 action:  [0.2996, -0.9908] n_targets:  1 reward:  60.43\n",
      "28.4 action:  [0.1308, -0.9933] n_targets:  1 reward:  55.89\n",
      "34.7 action:  [-0.2571, -0.9871] n_targets:  1 reward:  78.78\n",
      "35.5 action:  [0.0624, -0.9913] n_targets:  1 reward:  54.11\n",
      "36.9 action:  [0.2758, -0.9849] n_targets:  1 reward:  51.49\n",
      "38.3 action:  [0.0959, -0.9876] n_targets:  1 reward:  61.83\n",
      "38.9 action:  [-0.0117, -0.9858] n_targets:  1 reward:  83.41\n",
      "39.3 action:  [-0.157, -0.9918] n_targets:  1 reward:  52.68\n",
      "40.3 action:  [-0.2604, -0.9955] n_targets:  1 reward:  60.45\n",
      "42.9 action:  [-0.059, -0.987] n_targets:  1 reward:  59.41\n",
      "47.9 action:  [0.048, -0.9963] n_targets:  1 reward:  61.79\n",
      "51.5 action:  [-0.4684, -0.9936] n_targets:  2 reward:  105.2\n",
      "53.1 action:  [-0.0945, -0.9846] n_targets:  1 reward:  66.0\n",
      "55.5 action:  [0.0106, -0.9893] n_targets:  1 reward:  56.37\n",
      "55.7 action:  [-0.1338, -0.9847] n_targets:  1 reward:  50.14\n",
      "57.3 action:  [0.0294, -0.9877] n_targets:  3 reward:  213.25\n",
      "61.1 action:  [-0.0826, -0.9933] n_targets:  1 reward:  69.35\n",
      "63.8 action:  [-0.4259, -0.985] n_targets:  2 reward:  141.23\n",
      "65.2 action:  [-0.4654, -0.9884] n_targets:  1 reward:  54.47\n",
      "65.7 action:  [-0.1596, -0.9814] n_targets:  1 reward:  55.6\n",
      "66.5 action:  [0.266, -0.9923] n_targets:  2 reward:  103.41\n",
      "67.9 action:  [-0.1224, -0.9867] n_targets:  1 reward:  74.13\n",
      "68.1 action:  [-0.2672, -0.9901] n_targets:  1 reward:  86.95\n",
      "71.3 action:  [-0.0776, -0.9904] n_targets:  1 reward:  53.33\n",
      "72.7 action:  [0.2561, -0.9938] n_targets:  1 reward:  51.31\n",
      "72.9 action:  [-0.0357, -0.9918] n_targets:  1 reward:  51.73\n",
      "74.7 action:  [-0.3132, -0.9961] n_targets:  1 reward:  65.65\n",
      "75.3 action:  [0.2806, -0.9851] n_targets:  1 reward:  51.89\n",
      "76.3 action:  [-0.1011, -0.9949] n_targets:  1 reward:  52.07\n",
      "76.5 action:  [-0.1189, -0.9967] n_targets:  1 reward:  54.09\n",
      "77.5 action:  [-0.2653, -0.9874] n_targets:  1 reward:  76.9\n",
      "78.5 action:  [-0.0629, -0.986] n_targets:  1 reward:  69.44\n",
      "79.1 action:  [-0.0453, -0.9877] n_targets:  1 reward:  62.67\n",
      "80.7 action:  [-0.1704, -0.9994] n_targets:  2 reward:  128.35\n",
      "82.3 action:  [-0.1848, -0.9971] n_targets:  1 reward:  51.73\n",
      "83.2 action:  [0.0537, -0.9934] n_targets:  1 reward:  55.8\n",
      "85.0 action:  [-0.2427, -0.9859] n_targets:  1 reward:  55.78\n",
      "89.6 action:  [-0.0352, -0.9955] n_targets:  1 reward:  51.52\n",
      "90.0 action:  [0.3098, -0.9846] n_targets:  2 reward:  140.28\n",
      "90.2 action:  [-0.4864, -0.9826] n_targets:  1 reward:  50.88\n",
      "91.1 action:  [0.1307, -0.9851] n_targets:  1 reward:  97.24\n",
      "93.6 action:  [0.015, -0.9961] n_targets:  1 reward:  53.96\n",
      "94.8 action:  [0.2049, -0.9973] n_targets:  3 reward:  210.28\n",
      "95.4 action:  [-0.3966, -0.9877] n_targets:  2 reward:  122.16\n",
      "96.0 action:  [-0.1985, -0.9899] n_targets:  1 reward:  53.55\n",
      "98.0 action:  [-0.3039, -0.9882] n_targets:  3 reward:  203.8\n",
      "98.4 action:  [0.19, -0.9808] n_targets:  1 reward:  53.45\n",
      "101.7 action:  [-0.0643, -0.99] n_targets:  4 reward:  264.93\n",
      "ALPHA (entropy-related):  tensor([0.2715], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.30834 0.30365 0.29916 0.2949  0.29077 0.28672 0.28285 0.27889 0.27518\n",
      " 0.27152]\n",
      "Episode: 64, Episode Reward: 5095.872485160828\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  228\n",
      "0.1 action:  [0.1215, -0.9861] n_targets:  1 reward:  57.84\n",
      "1.3 action:  [0.044, -0.9901] n_targets:  1 reward:  51.64\n",
      "1.7 action:  [0.389, -0.9856] n_targets:  1 reward:  52.98\n",
      "6.5 action:  [-0.0389, -0.9847] n_targets:  1 reward:  53.76\n",
      "8.0 action:  [-0.2427, -0.986] n_targets:  1 reward:  52.05\n",
      "9.6 action:  [0.043, -0.9944] n_targets:  3 reward:  179.69\n",
      "13.0 action:  [-0.1165, -0.9805] n_targets:  1 reward:  65.14\n",
      "13.2 action:  [-0.0861, -0.9956] n_targets:  1 reward:  53.63\n",
      "14.5 action:  [-0.1562, -0.9964] n_targets:  1 reward:  69.79\n",
      "23.0 action:  [-0.0987, -0.9961] n_targets:  1 reward:  57.11\n",
      "24.2 action:  [-0.3971, -0.9854] n_targets:  1 reward:  66.16\n",
      "24.4 action:  [0.4283, -0.9951] n_targets:  1 reward:  53.07\n",
      "25.0 action:  [-0.5856, -0.9863] n_targets:  3 reward:  163.77\n",
      "26.2 action:  [0.39, -0.9991] n_targets:  1 reward:  65.56\n",
      "28.2 action:  [0.0978, -0.9954] n_targets:  1 reward:  50.65\n",
      "28.4 action:  [-0.088, -0.9841] n_targets:  1 reward:  54.13\n",
      "29.0 action:  [0.1898, -0.9894] n_targets:  1 reward:  53.31\n",
      "29.8 action:  [0.0782, -0.9895] n_targets:  2 reward:  129.78\n",
      "31.0 action:  [0.2044, -0.9978] n_targets:  1 reward:  56.79\n",
      "31.4 action:  [0.2508, -0.9923] n_targets:  1 reward:  58.92\n",
      "38.7 action:  [0.0015, -0.9948] n_targets:  1 reward:  52.71\n",
      "41.9 action:  [-0.0175, -0.9903] n_targets:  1 reward:  54.33\n",
      "42.4 action:  [0.443, -0.9979] n_targets:  1 reward:  59.89\n",
      "43.1 action:  [0.0083, -0.9982] n_targets:  1 reward:  50.06\n",
      "44.5 action:  [0.0984, -0.9824] n_targets:  1 reward:  63.51\n",
      "45.1 action:  [-0.0231, -0.9844] n_targets:  1 reward:  51.56\n",
      "45.3 action:  [0.1904, -0.9982] n_targets:  1 reward:  52.05\n",
      "47.1 action:  [0.1469, -0.9902] n_targets:  1 reward:  55.04\n",
      "49.3 action:  [-0.0746, -0.9983] n_targets:  1 reward:  56.56\n",
      "50.7 action:  [-0.2051, -0.9878] n_targets:  1 reward:  64.87\n",
      "52.9 action:  [0.0217, -0.9809] n_targets:  2 reward:  104.78\n",
      "53.7 action:  [0.08, -0.9812] n_targets:  1 reward:  56.64\n",
      "55.7 action:  [0.1898, -0.9913] n_targets:  2 reward:  137.49\n",
      "57.7 action:  [0.2207, -0.9947] n_targets:  1 reward:  52.27\n",
      "58.8 action:  [0.1033, -0.9823] n_targets:  1 reward:  62.54\n",
      "59.5 action:  [0.0358, -0.9887] n_targets:  4 reward:  270.2\n",
      "60.5 action:  [0.1316, -0.9908] n_targets:  1 reward:  59.27\n",
      "60.9 action:  [0.1266, -0.9849] n_targets:  1 reward:  54.41\n",
      "61.3 action:  [0.1415, -0.9911] n_targets:  1 reward:  58.56\n",
      "62.3 action:  [0.0066, -0.9817] n_targets:  2 reward:  116.99\n",
      "62.9 action:  [0.0365, -0.9968] n_targets:  1 reward:  58.58\n",
      "63.1 action:  [0.2276, -0.9926] n_targets:  1 reward:  61.25\n",
      "63.3 action:  [0.2771, -0.9936] n_targets:  1 reward:  58.69\n",
      "63.9 action:  [-0.491, -0.9842] n_targets:  1 reward:  54.67\n",
      "65.1 action:  [0.1518, -0.9881] n_targets:  1 reward:  51.62\n",
      "68.9 action:  [0.2886, -0.9943] n_targets:  2 reward:  105.28\n",
      "69.9 action:  [-0.2506, -0.9942] n_targets:  1 reward:  55.72\n",
      "70.3 action:  [-0.3137, -0.9824] n_targets:  2 reward:  114.2\n",
      "71.3 action:  [-0.3436, -0.9802] n_targets:  1 reward:  57.63\n",
      "72.1 action:  [-0.275, -0.9924] n_targets:  1 reward:  52.51\n",
      "72.7 action:  [0.1327, -0.9903] n_targets:  1 reward:  60.08\n",
      "75.4 action:  [0.336, -0.984] n_targets:  1 reward:  70.14\n",
      "76.4 action:  [-0.0598, -0.9896] n_targets:  1 reward:  50.61\n",
      "78.6 action:  [0.0754, -0.9979] n_targets:  1 reward:  55.53\n",
      "78.8 action:  [0.0554, -0.9809] n_targets:  1 reward:  53.21\n",
      "80.9 action:  [0.4578, -0.9823] n_targets:  1 reward:  61.6\n",
      "83.2 action:  [0.4754, -0.9963] n_targets:  1 reward:  62.01\n",
      "85.9 action:  [0.051, -0.9866] n_targets:  1 reward:  52.42\n",
      "87.1 action:  [-0.1794, -0.9963] n_targets:  1 reward:  72.07\n",
      "88.7 action:  [-0.0002, -0.9939] n_targets:  1 reward:  52.61\n",
      "89.5 action:  [-0.01, -0.9964] n_targets:  1 reward:  59.44\n",
      "89.7 action:  [0.0075, -0.997] n_targets:  2 reward:  115.98\n",
      "90.9 action:  [-0.286, -0.9809] n_targets:  2 reward:  119.05\n",
      "91.9 action:  [0.3224, -0.9892] n_targets:  1 reward:  53.88\n",
      "94.9 action:  [-0.0767, -0.9911] n_targets:  1 reward:  64.28\n",
      "95.1 action:  [-0.3332, -0.997] n_targets:  1 reward:  50.18\n",
      "96.9 action:  [0.1196, -0.9936] n_targets:  1 reward:  52.41\n",
      "99.3 action:  [-0.1553, -0.9971] n_targets:  1 reward:  53.44\n",
      "99.5 action:  [0.0831, -0.9879] n_targets:  1 reward:  51.3\n",
      "99.9 action:  [-0.1864, -0.9959] n_targets:  3 reward:  193.18\n",
      "100.5 action:  [0.2442, -0.996] n_targets:  2 reward:  130.05\n",
      "ALPHA (entropy-related):  tensor([0.2678], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.30365 0.29916 0.2949  0.29077 0.28672 0.28285 0.27889 0.27518 0.27152\n",
      " 0.26775]\n",
      "Episode: 65, Episode Reward: 5183.076324462891\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  229\n",
      "1.1 action:  [-0.012, -0.9878] n_targets:  2 reward:  107.71\n",
      "2.3 action:  [-0.0262, -0.9965] n_targets:  2 reward:  102.66\n",
      "3.9 action:  [-0.2014, -0.9963] n_targets:  1 reward:  51.39\n",
      "4.5 action:  [-0.0642, -0.9959] n_targets:  1 reward:  53.65\n",
      "7.5 action:  [-0.3619, -0.9828] n_targets:  1 reward:  51.19\n",
      "8.8 action:  [0.1497, -0.983] n_targets:  1 reward:  83.48\n",
      "10.8 action:  [-0.2722, -0.9969] n_targets:  1 reward:  61.06\n",
      "12.2 action:  [-0.0871, -0.9837] n_targets:  2 reward:  132.93\n",
      "13.2 action:  [-0.5224, -0.98] n_targets:  1 reward:  88.47\n",
      "13.6 action:  [0.2862, -0.9862] n_targets:  1 reward:  67.81\n",
      "14.0 action:  [-0.2204, -0.9929] n_targets:  1 reward:  54.33\n",
      "17.0 action:  [-0.4904, -0.9972] n_targets:  1 reward:  77.85\n",
      "18.0 action:  [-0.4512, -0.9877] n_targets:  1 reward:  56.14\n",
      "21.4 action:  [0.1833, -0.9915] n_targets:  1 reward:  51.56\n",
      "22.5 action:  [0.0479, -0.9924] n_targets:  4 reward:  297.94\n",
      "26.1 action:  [0.0188, -0.9984] n_targets:  1 reward:  51.54\n",
      "27.9 action:  [0.2074, -0.9893] n_targets:  1 reward:  55.58\n",
      "29.7 action:  [0.0228, -0.9984] n_targets:  1 reward:  51.8\n",
      "31.5 action:  [-0.257, -0.99] n_targets:  1 reward:  71.06\n",
      "31.7 action:  [-0.0154, -0.982] n_targets:  1 reward:  51.5\n",
      "33.5 action:  [-0.1319, -0.9836] n_targets:  1 reward:  76.33\n",
      "35.1 action:  [0.0215, -0.988] n_targets:  1 reward:  92.45\n",
      "35.5 action:  [-0.338, -0.9815] n_targets:  2 reward:  116.49\n",
      "36.3 action:  [-0.1494, -0.9848] n_targets:  1 reward:  99.65\n",
      "37.5 action:  [0.2426, -0.9968] n_targets:  1 reward:  54.74\n",
      "39.3 action:  [0.1605, -0.9908] n_targets:  1 reward:  61.09\n",
      "39.9 action:  [0.0847, -0.983] n_targets:  1 reward:  50.25\n",
      "41.1 action:  [-0.0607, -0.9965] n_targets:  1 reward:  58.02\n",
      "45.1 action:  [0.1224, -0.9915] n_targets:  1 reward:  51.49\n",
      "46.1 action:  [-0.0682, -0.9864] n_targets:  2 reward:  135.92\n",
      "46.3 action:  [0.1994, -0.989] n_targets:  1 reward:  52.01\n",
      "50.2 action:  [0.0081, -0.9883] n_targets:  1 reward:  54.71\n",
      "52.2 action:  [0.2345, -0.9967] n_targets:  1 reward:  60.13\n",
      "52.6 action:  [-0.1394, -0.9883] n_targets:  1 reward:  60.22\n",
      "53.0 action:  [0.1351, -0.9843] n_targets:  1 reward:  66.16\n",
      "54.8 action:  [0.3157, -0.9849] n_targets:  2 reward:  107.7\n",
      "57.0 action:  [-0.242, -0.987] n_targets:  1 reward:  67.68\n",
      "58.0 action:  [-0.1474, -0.9853] n_targets:  1 reward:  56.41\n",
      "62.0 action:  [0.0437, -0.9975] n_targets:  1 reward:  51.03\n",
      "62.6 action:  [0.4747, -0.9926] n_targets:  1 reward:  58.21\n",
      "64.4 action:  [-0.0864, -0.9928] n_targets:  3 reward:  196.47\n",
      "65.2 action:  [0.0197, -0.9848] n_targets:  1 reward:  66.97\n",
      "65.4 action:  [-0.0824, -0.9959] n_targets:  1 reward:  51.47\n",
      "66.6 action:  [0.2813, -0.9963] n_targets:  3 reward:  186.75\n",
      "68.6 action:  [0.1304, -0.9953] n_targets:  1 reward:  57.15\n",
      "71.2 action:  [-0.13, -0.9912] n_targets:  1 reward:  50.73\n",
      "74.6 action:  [-0.6332, -0.9954] n_targets:  3 reward:  183.88\n",
      "76.4 action:  [0.1775, -0.9886] n_targets:  1 reward:  52.1\n",
      "76.8 action:  [-0.1344, -0.9934] n_targets:  1 reward:  50.15\n",
      "79.8 action:  [0.0948, -0.9962] n_targets:  2 reward:  137.08\n",
      "80.2 action:  [-0.2626, -0.9881] n_targets:  1 reward:  52.54\n",
      "82.8 action:  [0.0798, -0.9897] n_targets:  1 reward:  53.8\n",
      "83.4 action:  [0.2977, -0.9854] n_targets:  1 reward:  57.98\n",
      "84.4 action:  [-0.1763, -0.9862] n_targets:  1 reward:  64.09\n",
      "85.4 action:  [0.4669, -0.9829] n_targets:  1 reward:  71.67\n",
      "86.6 action:  [-0.2633, -0.991] n_targets:  1 reward:  52.1\n",
      "87.2 action:  [0.0245, -0.9839] n_targets:  3 reward:  170.14\n",
      "88.6 action:  [-0.1053, -0.9909] n_targets:  1 reward:  51.63\n",
      "95.4 action:  [-0.47, -0.9824] n_targets:  2 reward:  128.12\n",
      "98.0 action:  [0.2129, -0.9984] n_targets:  2 reward:  111.16\n",
      "99.2 action:  [-0.136, -0.9967] n_targets:  1 reward:  63.3\n",
      "100.2 action:  [0.3218, -0.9965] n_targets:  2 reward:  153.98\n",
      "100.4 action:  [-0.1327, -0.9836] n_targets:  1 reward:  51.78\n",
      "101.0 action:  [-0.1319, -0.995] n_targets:  1 reward:  57.73\n",
      "ALPHA (entropy-related):  tensor([0.2644], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.29916 0.2949  0.29077 0.28672 0.28285 0.27889 0.27518 0.27152 0.26775\n",
      " 0.26443]\n",
      "Episode: 66, Episode Reward: 5223.1160803238545\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  230\n",
      "0.2 action:  [-0.2019, -0.9892] n_targets:  3 reward:  203.41\n",
      "0.4 action:  [-0.1349, -0.9843] n_targets:  1 reward:  53.0\n",
      "1.6 action:  [-0.3681, -0.9857] n_targets:  1 reward:  60.75\n",
      "2.6 action:  [-0.0004, -0.9925] n_targets:  1 reward:  62.24\n",
      "4.8 action:  [-0.0845, -0.9974] n_targets:  1 reward:  63.58\n",
      "6.2 action:  [-0.1045, -0.9952] n_targets:  1 reward:  60.33\n",
      "8.0 action:  [-0.0106, -0.9916] n_targets:  1 reward:  70.39\n",
      "8.8 action:  [0.0644, -0.9886] n_targets:  1 reward:  81.8\n",
      "9.2 action:  [-0.0729, -0.9961] n_targets:  1 reward:  54.16\n",
      "10.0 action:  [0.2401, -0.9969] n_targets:  1 reward:  56.15\n",
      "11.0 action:  [-0.0132, -0.9945] n_targets:  1 reward:  52.37\n",
      "12.0 action:  [0.0089, -0.9972] n_targets:  2 reward:  137.84\n",
      "14.8 action:  [0.0699, -0.9891] n_targets:  1 reward:  52.21\n",
      "15.4 action:  [0.3405, -0.9938] n_targets:  1 reward:  60.43\n",
      "16.2 action:  [-0.0359, -0.9922] n_targets:  2 reward:  110.43\n",
      "17.4 action:  [-0.3256, -0.9887] n_targets:  2 reward:  128.73\n",
      "17.8 action:  [-0.0034, -0.9895] n_targets:  1 reward:  70.01\n",
      "18.2 action:  [-0.2149, -0.9876] n_targets:  1 reward:  51.75\n",
      "19.8 action:  [0.2158, -0.9916] n_targets:  1 reward:  58.25\n",
      "23.2 action:  [0.2687, -0.9829] n_targets:  1 reward:  69.18\n",
      "24.0 action:  [0.2982, -0.9854] n_targets:  1 reward:  55.51\n",
      "26.2 action:  [0.0994, -0.9962] n_targets:  1 reward:  64.85\n",
      "27.8 action:  [-0.0825, -0.9982] n_targets:  2 reward:  131.05\n",
      "28.0 action:  [0.0363, -0.9893] n_targets:  1 reward:  55.87\n",
      "29.6 action:  [0.257, -0.9894] n_targets:  1 reward:  56.07\n",
      "30.2 action:  [0.0294, -0.9852] n_targets:  1 reward:  79.32\n",
      "31.2 action:  [0.1024, -0.9988] n_targets:  1 reward:  64.71\n",
      "34.6 action:  [0.2243, -0.9955] n_targets:  1 reward:  50.44\n",
      "35.2 action:  [-0.5435, -0.9927] n_targets:  1 reward:  53.77\n",
      "36.1 action:  [-0.1634, -0.9853] n_targets:  1 reward:  82.85\n",
      "37.1 action:  [-0.1112, -0.9992] n_targets:  1 reward:  73.15\n",
      "37.3 action:  [0.343, -0.9933] n_targets:  1 reward:  52.58\n",
      "37.7 action:  [-0.4235, -0.9918] n_targets:  1 reward:  56.07\n",
      "38.1 action:  [0.1579, -0.9882] n_targets:  1 reward:  57.68\n",
      "40.7 action:  [-0.3013, -0.9863] n_targets:  1 reward:  55.47\n",
      "41.1 action:  [0.1807, -0.9935] n_targets:  1 reward:  51.48\n",
      "41.8 action:  [-0.0482, -0.9937] n_targets:  1 reward:  62.54\n",
      "42.4 action:  [0.1252, -0.9952] n_targets:  1 reward:  60.48\n",
      "42.6 action:  [0.0236, -0.9973] n_targets:  1 reward:  55.89\n",
      "44.6 action:  [0.0347, -0.9954] n_targets:  1 reward:  56.72\n",
      "48.0 action:  [0.091, -0.9966] n_targets:  1 reward:  57.08\n",
      "48.6 action:  [-0.0544, -0.9944] n_targets:  1 reward:  67.92\n",
      "49.2 action:  [-0.128, -0.9924] n_targets:  3 reward:  188.38\n",
      "50.0 action:  [-0.1003, -0.9829] n_targets:  2 reward:  123.05\n",
      "50.6 action:  [-0.4533, -0.9977] n_targets:  1 reward:  57.46\n",
      "53.0 action:  [-0.3596, -0.9936] n_targets:  2 reward:  157.77\n",
      "54.0 action:  [0.1081, -0.9912] n_targets:  1 reward:  53.11\n",
      "54.4 action:  [0.3568, -0.9992] n_targets:  1 reward:  63.2\n",
      "56.2 action:  [-0.0164, -0.9911] n_targets:  1 reward:  86.62\n",
      "59.7 action:  [-0.3748, -0.9815] n_targets:  1 reward:  50.26\n",
      "60.3 action:  [-0.4127, -0.9988] n_targets:  1 reward:  55.32\n",
      "62.3 action:  [-0.0192, -0.9988] n_targets:  1 reward:  53.08\n",
      "63.3 action:  [-0.1674, -0.9941] n_targets:  1 reward:  54.4\n",
      "67.1 action:  [-0.1978, -0.9906] n_targets:  1 reward:  58.59\n",
      "68.3 action:  [-0.2153, -0.9952] n_targets:  1 reward:  62.3\n",
      "70.7 action:  [0.1217, -0.993] n_targets:  1 reward:  59.9\n",
      "71.5 action:  [-0.237, -0.9914] n_targets:  3 reward:  158.69\n",
      "72.5 action:  [-0.101, -0.9893] n_targets:  1 reward:  52.91\n",
      "73.7 action:  [0.0549, -0.9829] n_targets:  1 reward:  72.59\n",
      "74.5 action:  [-0.6119, -0.991] n_targets:  2 reward:  139.67\n",
      "79.1 action:  [-0.0633, -0.9839] n_targets:  1 reward:  62.17\n",
      "79.3 action:  [-0.2409, -0.9888] n_targets:  1 reward:  53.33\n",
      "80.5 action:  [-0.4512, -0.9911] n_targets:  1 reward:  53.49\n",
      "82.7 action:  [0.0526, -0.9968] n_targets:  1 reward:  52.8\n",
      "83.1 action:  [-0.0296, -0.9947] n_targets:  1 reward:  50.4\n",
      "83.3 action:  [0.2176, -0.9853] n_targets:  1 reward:  61.13\n",
      "83.5 action:  [0.117, -0.9906] n_targets:  1 reward:  56.96\n",
      "87.5 action:  [-0.1455, -0.981] n_targets:  1 reward:  54.34\n",
      "89.3 action:  [-0.2529, -0.9951] n_targets:  1 reward:  67.54\n",
      "91.7 action:  [-0.1149, -0.9861] n_targets:  1 reward:  55.8\n",
      "93.1 action:  [-0.0417, -0.9907] n_targets:  1 reward:  58.48\n",
      "94.1 action:  [-0.0576, -0.9909] n_targets:  1 reward:  56.73\n",
      "94.3 action:  [0.412, -0.9951] n_targets:  1 reward:  50.2\n",
      "99.3 action:  [0.1976, -0.9978] n_targets:  1 reward:  54.84\n",
      "ALPHA (entropy-related):  tensor([0.2613], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.2949  0.29077 0.28672 0.28285 0.27889 0.27518 0.27152 0.26775 0.26443\n",
      " 0.26134]\n",
      "Episode: 67, Episode Reward: 5294.032573699951\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  231\n",
      "0.1 action:  [0.1933, -0.9935] n_targets:  2 reward:  144.33\n",
      "0.3 action:  [-0.3852, -0.9948] n_targets:  1 reward:  55.69\n",
      "0.9 action:  [0.0348, -0.9991] n_targets:  1 reward:  60.79\n",
      "2.1 action:  [-0.1604, -0.9947] n_targets:  1 reward:  74.23\n",
      "2.3 action:  [0.2425, -0.9951] n_targets:  1 reward:  52.64\n",
      "2.7 action:  [0.0215, -0.9973] n_targets:  1 reward:  50.97\n",
      "3.5 action:  [-0.4295, -0.9943] n_targets:  2 reward:  112.92\n",
      "5.5 action:  [-0.285, -0.984] n_targets:  1 reward:  57.96\n",
      "9.4 action:  [-0.1189, -0.9969] n_targets:  1 reward:  51.31\n",
      "12.0 action:  [-0.1502, -0.9869] n_targets:  1 reward:  72.43\n",
      "12.6 action:  [-0.4055, -0.9809] n_targets:  1 reward:  75.94\n",
      "14.6 action:  [0.2842, -0.9963] n_targets:  1 reward:  50.77\n",
      "15.0 action:  [0.0975, -0.9964] n_targets:  1 reward:  68.37\n",
      "16.8 action:  [-0.5227, -0.9897] n_targets:  1 reward:  50.16\n",
      "17.8 action:  [-0.3112, -0.9962] n_targets:  1 reward:  52.0\n",
      "18.9 action:  [0.1196, -0.9935] n_targets:  2 reward:  106.8\n",
      "19.5 action:  [-0.1952, -0.9843] n_targets:  2 reward:  139.51\n",
      "20.7 action:  [-0.1569, -0.995] n_targets:  1 reward:  60.32\n",
      "21.7 action:  [0.3433, -0.9917] n_targets:  1 reward:  67.35\n",
      "22.7 action:  [-0.0372, -0.9955] n_targets:  2 reward:  126.97\n",
      "29.8 action:  [-0.0499, -0.987] n_targets:  1 reward:  50.04\n",
      "30.0 action:  [-0.1021, -0.993] n_targets:  1 reward:  51.75\n",
      "30.2 action:  [0.435, -0.9882] n_targets:  1 reward:  57.96\n",
      "31.2 action:  [-0.274, -0.9866] n_targets:  1 reward:  51.48\n",
      "35.2 action:  [0.102, -0.9987] n_targets:  1 reward:  56.44\n",
      "35.4 action:  [-0.1311, -0.9905] n_targets:  1 reward:  51.88\n",
      "36.2 action:  [0.4313, -0.9932] n_targets:  1 reward:  55.14\n",
      "37.0 action:  [0.0028, -0.9952] n_targets:  1 reward:  55.0\n",
      "38.0 action:  [-0.2067, -0.9857] n_targets:  1 reward:  74.95\n",
      "39.0 action:  [0.2109, -0.9865] n_targets:  1 reward:  88.06\n",
      "40.4 action:  [-0.0502, -0.9872] n_targets:  1 reward:  55.35\n",
      "41.4 action:  [0.0071, -0.9889] n_targets:  1 reward:  56.8\n",
      "42.0 action:  [-0.0828, -0.9832] n_targets:  1 reward:  53.2\n",
      "43.8 action:  [0.3159, -0.9933] n_targets:  1 reward:  56.67\n",
      "44.2 action:  [-0.0213, -0.9964] n_targets:  1 reward:  61.65\n",
      "44.8 action:  [0.514, -0.9898] n_targets:  1 reward:  58.62\n",
      "47.2 action:  [0.1695, -0.9882] n_targets:  2 reward:  125.63\n",
      "48.4 action:  [0.1018, -0.9994] n_targets:  2 reward:  119.62\n",
      "59.4 action:  [-0.1117, -0.9973] n_targets:  1 reward:  51.09\n",
      "60.6 action:  [-0.2989, -0.9933] n_targets:  1 reward:  53.68\n",
      "62.6 action:  [-0.1257, -0.9907] n_targets:  1 reward:  54.43\n",
      "64.0 action:  [0.0616, -0.996] n_targets:  1 reward:  56.68\n",
      "64.2 action:  [-0.115, -0.9927] n_targets:  1 reward:  50.29\n",
      "65.4 action:  [-0.0189, -0.9923] n_targets:  2 reward:  112.77\n",
      "67.2 action:  [0.2554, -0.986] n_targets:  1 reward:  53.84\n",
      "68.4 action:  [-0.2161, -0.992] n_targets:  2 reward:  153.03\n",
      "71.3 action:  [-0.2541, -0.9926] n_targets:  1 reward:  50.55\n",
      "71.6 action:  [0.1896, -0.9918] n_targets:  1 reward:  50.7\n",
      "72.2 action:  [-0.3539, -0.992] n_targets:  1 reward:  52.19\n",
      "72.6 action:  [-0.0682, -0.9922] n_targets:  1 reward:  66.85\n",
      "73.2 action:  [0.1438, -0.9981] n_targets:  1 reward:  57.25\n",
      "74.2 action:  [-0.2949, -0.9911] n_targets:  1 reward:  57.26\n",
      "75.0 action:  [0.0129, -0.9992] n_targets:  1 reward:  52.65\n",
      "77.4 action:  [0.2896, -0.9884] n_targets:  1 reward:  50.31\n",
      "78.9 action:  [0.0026, -0.999] n_targets:  1 reward:  65.79\n",
      "80.5 action:  [0.0694, -0.9823] n_targets:  1 reward:  57.76\n",
      "83.2 action:  [0.0552, -0.9805] n_targets:  1 reward:  60.61\n",
      "85.4 action:  [0.0131, -0.9985] n_targets:  1 reward:  64.16\n",
      "86.0 action:  [0.1478, -0.9965] n_targets:  1 reward:  55.22\n",
      "88.2 action:  [-0.3955, -0.9937] n_targets:  1 reward:  57.05\n",
      "88.6 action:  [-0.5193, -0.9909] n_targets:  1 reward:  56.6\n",
      "90.2 action:  [-0.239, -0.99] n_targets:  1 reward:  59.1\n",
      "91.2 action:  [0.2173, -0.9908] n_targets:  1 reward:  50.72\n",
      "92.2 action:  [-0.0431, -0.9969] n_targets:  1 reward:  82.15\n",
      "94.1 action:  [-0.2061, -0.9985] n_targets:  3 reward:  197.1\n",
      "94.9 action:  [0.0158, -0.9936] n_targets:  2 reward:  109.16\n",
      "96.5 action:  [-0.1195, -0.9972] n_targets:  2 reward:  122.93\n",
      "99.9 action:  [0.2485, -0.9923] n_targets:  2 reward:  135.9\n",
      "101.7 action:  [-0.1671, -0.9886] n_targets:  1 reward:  56.65\n",
      "ALPHA (entropy-related):  tensor([0.2583], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.29077 0.28672 0.28285 0.27889 0.27518 0.27152 0.26775 0.26443 0.26134\n",
      " 0.2583 ]\n",
      "Episode: 68, Episode Reward: 4966.164040883382\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  232\n",
      "0.3 action:  [-0.3071, -0.9896] n_targets:  1 reward:  67.17\n",
      "1.5 action:  [-0.2234, -0.9877] n_targets:  1 reward:  58.18\n",
      "3.5 action:  [-0.1272, -0.9974] n_targets:  1 reward:  61.17\n",
      "3.9 action:  [-0.4573, -0.9964] n_targets:  1 reward:  56.42\n",
      "4.9 action:  [0.3026, -0.984] n_targets:  1 reward:  54.08\n",
      "5.5 action:  [-0.2943, -0.9974] n_targets:  2 reward:  132.47\n",
      "7.2 action:  [-0.081, -0.9948] n_targets:  1 reward:  66.95\n",
      "8.0 action:  [-0.164, -0.9981] n_targets:  1 reward:  60.34\n",
      "13.0 action:  [0.5627, -0.9984] n_targets:  1 reward:  60.18\n",
      "13.2 action:  [-0.0383, -0.9891] n_targets:  1 reward:  55.73\n",
      "14.2 action:  [-0.0196, -0.9874] n_targets:  1 reward:  64.96\n",
      "14.6 action:  [0.04, -0.991] n_targets:  1 reward:  51.29\n",
      "15.2 action:  [0.0467, -0.9853] n_targets:  1 reward:  52.4\n",
      "16.4 action:  [-0.4944, -0.9966] n_targets:  2 reward:  125.84\n",
      "17.0 action:  [-0.101, -0.9888] n_targets:  1 reward:  60.16\n",
      "17.2 action:  [0.1063, -0.9821] n_targets:  1 reward:  57.83\n",
      "19.8 action:  [0.2788, -0.9932] n_targets:  1 reward:  55.1\n",
      "21.0 action:  [0.458, -0.995] n_targets:  1 reward:  50.72\n",
      "22.8 action:  [0.0593, -0.9828] n_targets:  2 reward:  129.2\n",
      "23.2 action:  [0.0235, -0.9852] n_targets:  1 reward:  54.73\n",
      "26.7 action:  [-0.0917, -0.9967] n_targets:  1 reward:  66.38\n",
      "27.7 action:  [-0.0066, -0.9965] n_targets:  1 reward:  50.16\n",
      "28.3 action:  [0.2703, -0.991] n_targets:  3 reward:  173.67\n",
      "29.6 action:  [0.0467, -0.9877] n_targets:  2 reward:  116.01\n",
      "30.0 action:  [0.0098, -0.9844] n_targets:  1 reward:  58.56\n",
      "30.6 action:  [0.0284, -0.9825] n_targets:  1 reward:  55.89\n",
      "32.0 action:  [-0.1349, -0.99] n_targets:  1 reward:  62.72\n",
      "33.7 action:  [0.4045, -0.9931] n_targets:  1 reward:  55.28\n",
      "35.3 action:  [0.265, -0.9873] n_targets:  1 reward:  53.71\n",
      "37.7 action:  [-0.4313, -0.9894] n_targets:  1 reward:  56.07\n",
      "38.3 action:  [-0.1006, -0.9955] n_targets:  1 reward:  52.71\n",
      "38.7 action:  [-0.041, -0.9908] n_targets:  1 reward:  56.41\n",
      "39.5 action:  [0.5493, -0.9968] n_targets:  1 reward:  52.24\n",
      "40.8 action:  [0.2797, -0.9805] n_targets:  1 reward:  55.71\n",
      "41.5 action:  [-0.3698, -0.9974] n_targets:  1 reward:  52.96\n",
      "42.1 action:  [-0.0181, -0.9941] n_targets:  1 reward:  60.54\n",
      "46.0 action:  [0.3264, -0.9937] n_targets:  1 reward:  68.08\n",
      "48.6 action:  [-0.0195, -0.9807] n_targets:  1 reward:  74.85\n",
      "48.8 action:  [-0.2422, -0.9815] n_targets:  1 reward:  58.44\n",
      "50.8 action:  [-0.1069, -0.9936] n_targets:  1 reward:  50.81\n",
      "51.2 action:  [0.0182, -0.9969] n_targets:  1 reward:  58.88\n",
      "52.0 action:  [-0.0836, -0.9848] n_targets:  1 reward:  56.53\n",
      "53.6 action:  [0.2193, -0.9801] n_targets:  1 reward:  54.56\n",
      "54.3 action:  [-0.1746, -0.991] n_targets:  1 reward:  61.23\n",
      "54.5 action:  [0.0322, -0.9842] n_targets:  1 reward:  55.91\n",
      "55.3 action:  [-0.1446, -0.9828] n_targets:  1 reward:  57.68\n",
      "60.4 action:  [0.1463, -0.9892] n_targets:  1 reward:  55.62\n",
      "61.8 action:  [-0.2154, -0.995] n_targets:  1 reward:  60.2\n",
      "62.6 action:  [0.1137, -0.9896] n_targets:  1 reward:  63.44\n",
      "63.4 action:  [0.1632, -0.9812] n_targets:  1 reward:  51.47\n",
      "66.2 action:  [0.2489, -0.9951] n_targets:  1 reward:  51.38\n",
      "71.4 action:  [0.0604, -0.9884] n_targets:  2 reward:  114.53\n",
      "75.0 action:  [0.0864, -0.9958] n_targets:  1 reward:  57.11\n",
      "77.0 action:  [-0.2955, -0.9909] n_targets:  3 reward:  201.82\n",
      "78.1 action:  [-0.2388, -0.9901] n_targets:  1 reward:  62.59\n",
      "78.9 action:  [-0.1478, -0.986] n_targets:  1 reward:  52.21\n",
      "79.9 action:  [-0.1847, -0.9833] n_targets:  1 reward:  66.01\n",
      "81.7 action:  [-0.0459, -0.9942] n_targets:  3 reward:  237.87\n",
      "82.3 action:  [-0.3926, -0.9936] n_targets:  1 reward:  54.89\n",
      "82.9 action:  [-0.2047, -0.9994] n_targets:  2 reward:  115.29\n",
      "83.3 action:  [0.1448, -0.9904] n_targets:  2 reward:  129.12\n",
      "84.4 action:  [0.1612, -0.9901] n_targets:  1 reward:  64.96\n",
      "85.6 action:  [0.0186, -0.9987] n_targets:  1 reward:  50.47\n",
      "86.0 action:  [-0.1241, -0.9913] n_targets:  1 reward:  56.92\n",
      "87.4 action:  [0.0961, -0.9946] n_targets:  1 reward:  52.61\n",
      "88.0 action:  [-0.121, -0.9955] n_targets:  1 reward:  67.39\n",
      "88.8 action:  [-0.1874, -0.9916] n_targets:  2 reward:  103.64\n",
      "89.8 action:  [0.3176, -0.9915] n_targets:  1 reward:  58.34\n",
      "90.2 action:  [0.3022, -0.9977] n_targets:  1 reward:  71.35\n",
      "95.3 action:  [-0.2073, -0.9839] n_targets:  1 reward:  72.07\n",
      "100.2 action:  [0.1564, -0.9832] n_targets:  1 reward:  65.0\n",
      "102.4 action:  [0.1582, -0.9882] n_targets:  1 reward:  52.73\n",
      "ALPHA (entropy-related):  tensor([0.2552], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.28672 0.28285 0.27889 0.27518 0.27152 0.26775 0.26443 0.26134 0.2583\n",
      " 0.25516]\n",
      "Episode: 69, Episode Reward: 5139.948501586914\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  233\n",
      "0.1 action:  [-0.349, -0.9943] n_targets:  1 reward:  64.85\n",
      "2.5 action:  [0.2968, -0.984] n_targets:  1 reward:  65.45\n",
      "4.5 action:  [-0.0084, -0.993] n_targets:  2 reward:  126.51\n",
      "5.6 action:  [-0.2177, -0.9866] n_targets:  1 reward:  54.48\n",
      "6.4 action:  [-0.0865, -0.9965] n_targets:  1 reward:  54.89\n",
      "7.4 action:  [0.3321, -0.9825] n_targets:  1 reward:  52.58\n",
      "9.2 action:  [0.0601, -0.9802] n_targets:  1 reward:  51.81\n",
      "9.4 action:  [-0.2165, -0.9831] n_targets:  1 reward:  51.12\n",
      "13.0 action:  [-0.3741, -0.9918] n_targets:  1 reward:  79.35\n",
      "13.4 action:  [0.2043, -0.9983] n_targets:  1 reward:  50.85\n",
      "15.0 action:  [-0.2056, -0.9878] n_targets:  3 reward:  209.3\n",
      "16.4 action:  [0.1526, -0.9933] n_targets:  1 reward:  53.23\n",
      "17.2 action:  [-0.0213, -0.984] n_targets:  1 reward:  55.79\n",
      "20.3 action:  [-0.0448, -0.9837] n_targets:  2 reward:  101.43\n",
      "20.8 action:  [-0.4303, -0.9892] n_targets:  4 reward:  238.8\n",
      "21.4 action:  [0.019, -0.9958] n_targets:  1 reward:  50.02\n",
      "23.6 action:  [0.1612, -0.9924] n_targets:  1 reward:  59.71\n",
      "24.6 action:  [-0.0068, -0.9845] n_targets:  2 reward:  106.06\n",
      "25.8 action:  [0.0853, -0.9822] n_targets:  1 reward:  60.61\n",
      "27.0 action:  [0.0976, -0.9924] n_targets:  1 reward:  59.12\n",
      "28.4 action:  [0.0675, -0.985] n_targets:  1 reward:  50.57\n",
      "29.6 action:  [-0.262, -0.9956] n_targets:  1 reward:  51.9\n",
      "32.0 action:  [-0.0204, -0.9944] n_targets:  1 reward:  55.51\n",
      "32.2 action:  [-0.0932, -0.9892] n_targets:  1 reward:  50.26\n",
      "34.0 action:  [0.1145, -0.999] n_targets:  1 reward:  70.77\n",
      "36.0 action:  [-0.1401, -0.9867] n_targets:  1 reward:  55.57\n",
      "36.6 action:  [-0.1598, -0.9923] n_targets:  1 reward:  52.88\n",
      "42.5 action:  [-0.1198, -0.9925] n_targets:  1 reward:  94.99\n",
      "43.1 action:  [0.0548, -0.9961] n_targets:  1 reward:  65.34\n",
      "47.9 action:  [-0.2691, -0.9963] n_targets:  1 reward:  50.53\n",
      "49.1 action:  [-0.2374, -0.9904] n_targets:  1 reward:  56.97\n",
      "49.5 action:  [0.0864, -0.991] n_targets:  1 reward:  54.01\n",
      "51.5 action:  [0.1574, -0.9858] n_targets:  1 reward:  54.87\n",
      "53.7 action:  [-0.2159, -0.9997] n_targets:  1 reward:  53.68\n",
      "55.1 action:  [-0.1304, -0.995] n_targets:  2 reward:  123.89\n",
      "55.3 action:  [0.0655, -0.9904] n_targets:  1 reward:  51.39\n",
      "56.5 action:  [-0.0405, -0.9822] n_targets:  1 reward:  54.19\n",
      "57.7 action:  [0.2113, -0.9834] n_targets:  1 reward:  61.41\n",
      "62.6 action:  [-0.0048, -0.9827] n_targets:  1 reward:  59.94\n",
      "62.8 action:  [-0.2558, -0.9983] n_targets:  1 reward:  50.95\n",
      "64.0 action:  [-0.0144, -0.9908] n_targets:  1 reward:  50.53\n",
      "65.1 action:  [0.1828, -0.9957] n_targets:  1 reward:  60.48\n",
      "65.9 action:  [0.1, -0.9913] n_targets:  1 reward:  59.7\n",
      "66.9 action:  [-0.2857, -0.9871] n_targets:  1 reward:  56.01\n",
      "68.7 action:  [0.0481, -0.9934] n_targets:  2 reward:  137.88\n",
      "71.6 action:  [-0.1048, -0.9911] n_targets:  1 reward:  51.25\n",
      "71.8 action:  [0.4596, -0.996] n_targets:  1 reward:  50.5\n",
      "72.0 action:  [0.3106, -0.9892] n_targets:  1 reward:  53.23\n",
      "73.4 action:  [0.2581, -0.9987] n_targets:  2 reward:  105.44\n",
      "75.0 action:  [-0.0867, -0.9847] n_targets:  1 reward:  54.1\n",
      "75.6 action:  [0.3121, -0.9969] n_targets:  2 reward:  109.42\n",
      "76.0 action:  [-0.0408, -0.9839] n_targets:  1 reward:  57.81\n",
      "76.3 action:  [0.0263, -0.9936] n_targets:  2 reward:  130.58\n",
      "77.3 action:  [-0.1554, -0.9907] n_targets:  1 reward:  57.15\n",
      "78.4 action:  [0.0122, -0.9882] n_targets:  1 reward:  63.1\n",
      "78.8 action:  [-0.0836, -0.9803] n_targets:  1 reward:  54.51\n",
      "80.0 action:  [0.0051, -0.982] n_targets:  1 reward:  59.63\n",
      "81.7 action:  [-0.4322, -0.9944] n_targets:  1 reward:  61.02\n",
      "82.1 action:  [0.3635, -0.9868] n_targets:  1 reward:  53.23\n",
      "86.2 action:  [-0.0137, -0.9922] n_targets:  1 reward:  55.75\n",
      "86.4 action:  [-0.1309, -0.9918] n_targets:  2 reward:  104.91\n",
      "86.8 action:  [0.0901, -0.9858] n_targets:  1 reward:  52.47\n",
      "88.6 action:  [0.0737, -0.994] n_targets:  2 reward:  120.11\n",
      "90.2 action:  [0.0367, -0.9866] n_targets:  1 reward:  57.49\n",
      "90.6 action:  [-0.1434, -0.9972] n_targets:  1 reward:  50.18\n",
      "92.3 action:  [0.1445, -0.9907] n_targets:  1 reward:  54.22\n",
      "96.8 action:  [-0.2349, -0.9877] n_targets:  1 reward:  57.42\n",
      "98.1 action:  [-0.3165, -0.9964] n_targets:  1 reward:  53.37\n",
      "98.7 action:  [0.0517, -0.9955] n_targets:  1 reward:  58.76\n",
      "99.1 action:  [0.2659, -0.9896] n_targets:  1 reward:  52.51\n",
      "99.9 action:  [-0.507, -0.9878] n_targets:  1 reward:  53.28\n",
      "101.3 action:  [0.1084, -0.9981] n_targets:  1 reward:  54.26\n",
      "ALPHA (entropy-related):  tensor([0.2523], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.28285 0.27889 0.27518 0.27152 0.26775 0.26443 0.26134 0.2583  0.25516\n",
      " 0.25228]\n",
      "Episode: 70, Episode Reward: 5015.887158075968\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  234\n",
      "0.1 action:  [0.2587, -0.9928] n_targets:  3 reward:  209.8\n",
      "0.7 action:  [0.2678, -0.9989] n_targets:  2 reward:  113.02\n",
      "1.7 action:  [0.0045, -0.9842] n_targets:  1 reward:  59.48\n",
      "2.9 action:  [0.1274, -0.9907] n_targets:  1 reward:  50.4\n",
      "7.4 action:  [-0.032, -0.9996] n_targets:  1 reward:  58.21\n",
      "9.0 action:  [-0.0212, -0.9909] n_targets:  1 reward:  54.42\n",
      "9.6 action:  [0.1391, -0.9876] n_targets:  1 reward:  54.88\n",
      "10.9 action:  [0.3589, -0.989] n_targets:  2 reward:  119.57\n",
      "11.1 action:  [0.3094, -0.9837] n_targets:  1 reward:  52.37\n",
      "11.3 action:  [-0.3629, -0.9855] n_targets:  1 reward:  57.3\n",
      "13.1 action:  [0.0519, -0.9958] n_targets:  1 reward:  66.05\n",
      "14.3 action:  [-0.1893, -0.9911] n_targets:  1 reward:  52.84\n",
      "14.7 action:  [-0.2379, -0.9803] n_targets:  1 reward:  57.76\n",
      "15.7 action:  [0.1508, -0.9842] n_targets:  3 reward:  162.09\n",
      "19.5 action:  [0.4612, -0.9905] n_targets:  1 reward:  62.33\n",
      "20.9 action:  [-0.0161, -0.995] n_targets:  1 reward:  50.66\n",
      "21.3 action:  [0.0335, -0.9944] n_targets:  1 reward:  55.6\n",
      "21.9 action:  [-0.181, -0.9898] n_targets:  1 reward:  50.74\n",
      "22.7 action:  [0.2065, -0.9809] n_targets:  1 reward:  58.6\n",
      "28.4 action:  [-0.0318, -0.9893] n_targets:  1 reward:  55.15\n",
      "33.8 action:  [-0.3423, -0.982] n_targets:  1 reward:  61.42\n",
      "34.6 action:  [-0.1936, -0.991] n_targets:  1 reward:  50.62\n",
      "34.8 action:  [0.3936, -0.9863] n_targets:  1 reward:  53.11\n",
      "36.0 action:  [0.1418, -0.9839] n_targets:  1 reward:  50.83\n",
      "38.6 action:  [-0.4732, -0.9901] n_targets:  2 reward:  118.32\n",
      "38.8 action:  [-0.0541, -0.9886] n_targets:  1 reward:  56.41\n",
      "40.8 action:  [0.4206, -0.9942] n_targets:  1 reward:  57.61\n",
      "41.4 action:  [0.1097, -0.9907] n_targets:  2 reward:  111.46\n",
      "41.6 action:  [0.1172, -0.9962] n_targets:  1 reward:  55.25\n",
      "43.4 action:  [0.2763, -0.9819] n_targets:  1 reward:  51.3\n",
      "44.0 action:  [0.1487, -0.9816] n_targets:  1 reward:  56.32\n",
      "45.2 action:  [-0.4537, -0.9832] n_targets:  1 reward:  59.58\n",
      "45.6 action:  [0.1867, -0.994] n_targets:  1 reward:  50.48\n",
      "46.4 action:  [-0.1683, -0.9956] n_targets:  1 reward:  60.94\n",
      "47.4 action:  [0.1925, -0.9868] n_targets:  3 reward:  213.47\n",
      "48.2 action:  [0.1677, -0.9972] n_targets:  2 reward:  144.64\n",
      "49.0 action:  [0.1989, -0.9878] n_targets:  1 reward:  59.41\n",
      "49.6 action:  [0.1594, -0.994] n_targets:  1 reward:  53.71\n",
      "50.0 action:  [0.1438, -0.9942] n_targets:  1 reward:  54.74\n",
      "50.2 action:  [0.0003, -0.9959] n_targets:  1 reward:  57.63\n",
      "50.8 action:  [-0.1022, -0.9945] n_targets:  2 reward:  111.84\n",
      "53.0 action:  [0.2837, -0.9961] n_targets:  1 reward:  53.66\n",
      "54.0 action:  [0.0973, -0.9852] n_targets:  1 reward:  73.28\n",
      "54.6 action:  [0.0035, -0.9979] n_targets:  1 reward:  65.1\n",
      "56.2 action:  [0.1021, -0.9963] n_targets:  3 reward:  189.76\n",
      "57.2 action:  [0.082, -0.9885] n_targets:  1 reward:  58.68\n",
      "58.2 action:  [0.1656, -0.9922] n_targets:  2 reward:  127.1\n",
      "60.2 action:  [-0.0311, -0.9832] n_targets:  1 reward:  50.41\n",
      "61.4 action:  [-0.227, -0.9861] n_targets:  1 reward:  76.28\n",
      "66.9 action:  [-0.2187, -0.9864] n_targets:  1 reward:  62.27\n",
      "68.1 action:  [-0.4028, -0.9917] n_targets:  2 reward:  156.51\n",
      "68.5 action:  [-0.0524, -0.9856] n_targets:  2 reward:  120.79\n",
      "69.1 action:  [0.0802, -0.9966] n_targets:  2 reward:  126.52\n",
      "72.3 action:  [-0.4329, -0.9914] n_targets:  3 reward:  194.6\n",
      "72.5 action:  [0.3144, -0.9895] n_targets:  1 reward:  51.37\n",
      "73.5 action:  [0.1139, -0.9878] n_targets:  1 reward:  52.78\n",
      "73.7 action:  [0.4123, -0.9916] n_targets:  1 reward:  57.78\n",
      "75.7 action:  [-0.2382, -0.9868] n_targets:  1 reward:  50.16\n",
      "76.5 action:  [0.0409, -0.9926] n_targets:  1 reward:  53.63\n",
      "77.3 action:  [0.2695, -0.9974] n_targets:  1 reward:  58.13\n",
      "79.7 action:  [0.105, -0.9864] n_targets:  1 reward:  65.33\n",
      "80.7 action:  [0.0245, -0.9935] n_targets:  1 reward:  52.61\n",
      "84.7 action:  [-0.0572, -0.9971] n_targets:  1 reward:  64.15\n",
      "85.5 action:  [0.1516, -0.9801] n_targets:  2 reward:  105.52\n",
      "86.7 action:  [-0.3192, -0.9895] n_targets:  2 reward:  131.59\n",
      "86.9 action:  [-0.2204, -0.9871] n_targets:  1 reward:  51.22\n",
      "87.7 action:  [-0.4474, -0.9979] n_targets:  1 reward:  70.28\n",
      "90.3 action:  [-0.2788, -0.9919] n_targets:  1 reward:  52.49\n",
      "92.1 action:  [-0.0293, -0.9908] n_targets:  1 reward:  56.84\n",
      "92.5 action:  [0.0263, -0.9838] n_targets:  2 reward:  111.38\n",
      "94.1 action:  [-0.4296, -0.9995] n_targets:  1 reward:  52.47\n",
      "96.5 action:  [-0.1949, -0.9983] n_targets:  1 reward:  53.14\n",
      "99.9 action:  [0.1414, -0.9892] n_targets:  1 reward:  67.52\n",
      "100.9 action:  [-0.1733, -0.9892] n_targets:  1 reward:  57.62\n",
      "101.3 action:  [-0.3463, -0.9983] n_targets:  1 reward:  53.7\n",
      "ALPHA (entropy-related):  tensor([0.2495], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.27889 0.27518 0.27152 0.26775 0.26443 0.26134 0.2583  0.25516 0.25228\n",
      " 0.24955]\n",
      "Episode: 71, Episode Reward: 5815.010770161946\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  235\n",
      "0.1 action:  [0.0402, -0.9913] n_targets:  3 reward:  201.67\n",
      "1.3 action:  [0.0042, -0.9879] n_targets:  2 reward:  130.44\n",
      "2.7 action:  [0.413, -0.9825] n_targets:  1 reward:  65.36\n",
      "3.5 action:  [-0.0047, -0.9976] n_targets:  1 reward:  64.91\n",
      "4.5 action:  [-0.0398, -0.9982] n_targets:  1 reward:  50.66\n",
      "5.3 action:  [-0.3268, -0.9961] n_targets:  1 reward:  50.69\n",
      "5.9 action:  [0.0368, -0.9883] n_targets:  1 reward:  51.82\n",
      "6.3 action:  [-0.0777, -0.9878] n_targets:  1 reward:  50.82\n",
      "8.5 action:  [0.0563, -0.9899] n_targets:  2 reward:  130.55\n",
      "9.9 action:  [0.2598, -0.987] n_targets:  1 reward:  58.75\n",
      "10.5 action:  [-0.2135, -0.9977] n_targets:  1 reward:  62.47\n",
      "11.1 action:  [-0.0092, -0.9905] n_targets:  1 reward:  56.19\n",
      "11.9 action:  [-0.1432, -0.9962] n_targets:  1 reward:  67.82\n",
      "14.5 action:  [-0.4749, -0.9809] n_targets:  1 reward:  52.1\n",
      "14.9 action:  [0.2073, -0.996] n_targets:  1 reward:  57.52\n",
      "15.5 action:  [-0.3262, -0.9875] n_targets:  1 reward:  53.16\n",
      "16.1 action:  [0.3742, -0.9914] n_targets:  1 reward:  62.11\n",
      "16.9 action:  [-0.1944, -0.9896] n_targets:  2 reward:  128.74\n",
      "17.3 action:  [0.0708, -0.9931] n_targets:  1 reward:  51.32\n",
      "17.5 action:  [0.1335, -0.9981] n_targets:  1 reward:  50.26\n",
      "18.9 action:  [0.1454, -0.9884] n_targets:  1 reward:  53.83\n",
      "20.1 action:  [-0.206, -0.9864] n_targets:  1 reward:  58.05\n",
      "20.5 action:  [0.0916, -0.9903] n_targets:  1 reward:  61.36\n",
      "21.5 action:  [0.1518, -0.9946] n_targets:  2 reward:  123.67\n",
      "22.3 action:  [0.0247, -0.9891] n_targets:  1 reward:  54.54\n",
      "23.5 action:  [-0.0741, -0.997] n_targets:  2 reward:  123.82\n",
      "23.7 action:  [-0.2334, -0.9866] n_targets:  1 reward:  53.55\n",
      "24.5 action:  [-0.0173, -0.9934] n_targets:  1 reward:  53.25\n",
      "24.9 action:  [-0.0106, -0.9912] n_targets:  1 reward:  50.25\n",
      "25.1 action:  [0.0355, -0.9972] n_targets:  1 reward:  51.75\n",
      "25.5 action:  [-0.2325, -0.9985] n_targets:  1 reward:  54.42\n",
      "26.1 action:  [0.1144, -0.9898] n_targets:  1 reward:  55.15\n",
      "27.3 action:  [-0.2602, -0.9835] n_targets:  1 reward:  64.83\n",
      "29.7 action:  [0.2826, -0.981] n_targets:  1 reward:  61.12\n",
      "30.3 action:  [-0.0494, -0.9937] n_targets:  1 reward:  58.89\n",
      "30.5 action:  [0.0311, -0.9923] n_targets:  1 reward:  52.36\n",
      "31.7 action:  [-0.0496, -0.9946] n_targets:  1 reward:  71.29\n",
      "32.7 action:  [-0.2135, -0.9922] n_targets:  1 reward:  57.82\n",
      "36.3 action:  [-0.129, -0.991] n_targets:  1 reward:  77.24\n",
      "36.9 action:  [0.119, -0.9898] n_targets:  3 reward:  180.18\n",
      "37.5 action:  [0.1068, -0.991] n_targets:  1 reward:  67.82\n",
      "42.0 action:  [-0.4316, -0.9934] n_targets:  2 reward:  115.79\n",
      "42.4 action:  [-0.44, -0.9963] n_targets:  1 reward:  51.06\n",
      "43.6 action:  [0.0633, -0.9924] n_targets:  1 reward:  51.07\n",
      "43.8 action:  [0.284, -0.9991] n_targets:  2 reward:  105.72\n",
      "44.8 action:  [-0.1063, -0.9836] n_targets:  1 reward:  52.66\n",
      "46.4 action:  [-0.1258, -0.9863] n_targets:  1 reward:  53.06\n",
      "49.2 action:  [-0.1768, -0.9916] n_targets:  2 reward:  125.22\n",
      "52.0 action:  [0.2625, -0.994] n_targets:  1 reward:  57.83\n",
      "53.6 action:  [0.0132, -0.9919] n_targets:  3 reward:  193.88\n",
      "55.8 action:  [0.213, -0.9966] n_targets:  1 reward:  58.1\n",
      "56.6 action:  [0.5153, -0.9966] n_targets:  1 reward:  55.39\n",
      "57.4 action:  [-0.1428, -0.9944] n_targets:  1 reward:  53.86\n",
      "57.6 action:  [0.2072, -0.9889] n_targets:  2 reward:  107.78\n",
      "58.2 action:  [-0.0065, -0.9993] n_targets:  1 reward:  56.7\n",
      "59.8 action:  [-0.3615, -0.9967] n_targets:  1 reward:  67.28\n",
      "60.4 action:  [0.0526, -0.9812] n_targets:  1 reward:  55.73\n",
      "61.0 action:  [-0.1847, -0.995] n_targets:  1 reward:  50.92\n",
      "62.0 action:  [0.0317, -0.9974] n_targets:  1 reward:  51.9\n",
      "63.4 action:  [-0.0507, -0.9953] n_targets:  1 reward:  60.38\n",
      "64.4 action:  [-0.1361, -0.9943] n_targets:  1 reward:  50.71\n",
      "71.3 action:  [0.2103, -0.992] n_targets:  2 reward:  134.47\n",
      "72.1 action:  [0.1713, -0.9888] n_targets:  1 reward:  52.33\n",
      "73.8 action:  [0.3803, -0.9877] n_targets:  1 reward:  55.41\n",
      "75.6 action:  [-0.3208, -0.9857] n_targets:  1 reward:  80.44\n",
      "76.8 action:  [0.0188, -0.992] n_targets:  1 reward:  58.52\n",
      "77.8 action:  [0.1654, -0.9908] n_targets:  1 reward:  59.53\n",
      "80.8 action:  [-0.1288, -0.9975] n_targets:  1 reward:  51.34\n",
      "83.2 action:  [0.1318, -0.9899] n_targets:  1 reward:  82.26\n",
      "83.8 action:  [-0.1457, -0.9849] n_targets:  1 reward:  55.68\n",
      "84.2 action:  [-0.6021, -0.9874] n_targets:  2 reward:  111.85\n",
      "84.6 action:  [-0.1329, -0.9877] n_targets:  1 reward:  54.62\n",
      "86.2 action:  [0.11, -0.9844] n_targets:  1 reward:  57.47\n",
      "86.6 action:  [-0.5447, -0.9849] n_targets:  3 reward:  166.22\n",
      "89.7 action:  [-0.0244, -0.9932] n_targets:  1 reward:  64.94\n",
      "91.5 action:  [0.0301, -0.9884] n_targets:  1 reward:  60.65\n",
      "93.1 action:  [-0.0213, -0.9875] n_targets:  1 reward:  52.05\n",
      "94.9 action:  [-0.0844, -0.9926] n_targets:  1 reward:  51.61\n",
      "95.5 action:  [-0.2416, -0.9912] n_targets:  1 reward:  62.04\n",
      "95.7 action:  [0.0619, -0.996] n_targets:  1 reward:  58.89\n",
      "95.9 action:  [-0.2177, -0.9846] n_targets:  1 reward:  51.71\n",
      "96.1 action:  [-0.1007, -0.9923] n_targets:  1 reward:  56.39\n",
      "100.5 action:  [-0.095, -0.9954] n_targets:  1 reward:  51.06\n",
      "101.7 action:  [0.0694, -0.9943] n_targets:  1 reward:  64.9\n",
      "ALPHA (entropy-related):  tensor([0.2471], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.27518 0.27152 0.26775 0.26443 0.26134 0.2583  0.25516 0.25228 0.24955\n",
      " 0.24708]\n",
      "Episode: 72, Episode Reward: 6052.014096577961\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  236\n",
      "0.3 action:  [-0.0574, -0.9885] n_targets:  3 reward:  198.6\n",
      "1.9 action:  [-0.1098, -0.9918] n_targets:  1 reward:  63.95\n",
      "5.4 action:  [-0.1503, -0.9859] n_targets:  1 reward:  79.09\n",
      "6.4 action:  [-0.0799, -0.9861] n_targets:  1 reward:  54.78\n",
      "6.8 action:  [0.0319, -0.9838] n_targets:  1 reward:  55.06\n",
      "7.2 action:  [-0.1007, -0.9916] n_targets:  1 reward:  69.44\n",
      "7.8 action:  [-0.1465, -0.9921] n_targets:  1 reward:  58.79\n",
      "8.5 action:  [-0.1973, -0.9966] n_targets:  1 reward:  76.01\n",
      "9.7 action:  [0.2041, -0.9926] n_targets:  1 reward:  53.27\n",
      "10.5 action:  [0.1682, -0.9849] n_targets:  1 reward:  53.99\n",
      "15.1 action:  [0.024, -0.9901] n_targets:  2 reward:  112.42\n",
      "15.7 action:  [-0.1295, -0.9933] n_targets:  1 reward:  63.63\n",
      "17.1 action:  [0.0509, -0.9925] n_targets:  1 reward:  53.43\n",
      "17.5 action:  [-0.0964, -0.9844] n_targets:  1 reward:  57.04\n",
      "18.3 action:  [0.1364, -0.9817] n_targets:  1 reward:  58.96\n",
      "18.7 action:  [-0.041, -0.9811] n_targets:  1 reward:  50.09\n",
      "20.5 action:  [-0.1179, -0.9936] n_targets:  1 reward:  75.05\n",
      "21.5 action:  [0.007, -0.9832] n_targets:  1 reward:  56.11\n",
      "21.9 action:  [0.0449, -0.9891] n_targets:  1 reward:  59.9\n",
      "26.5 action:  [-0.1127, -0.9874] n_targets:  3 reward:  185.07\n",
      "27.5 action:  [-0.27, -0.9862] n_targets:  1 reward:  50.06\n",
      "29.9 action:  [0.4321, -0.9842] n_targets:  1 reward:  50.99\n",
      "30.1 action:  [0.0273, -0.9915] n_targets:  2 reward:  107.84\n",
      "30.5 action:  [-0.1544, -0.9819] n_targets:  2 reward:  108.68\n",
      "30.9 action:  [-0.3126, -0.9859] n_targets:  2 reward:  132.84\n",
      "31.3 action:  [-0.0453, -0.9821] n_targets:  1 reward:  64.5\n",
      "33.7 action:  [0.1635, -0.9822] n_targets:  3 reward:  170.03\n",
      "36.3 action:  [-0.0179, -0.9834] n_targets:  2 reward:  144.8\n",
      "37.1 action:  [0.1167, -0.9926] n_targets:  1 reward:  52.67\n",
      "37.3 action:  [-0.0836, -0.9877] n_targets:  1 reward:  55.56\n",
      "37.9 action:  [0.1771, -0.9844] n_targets:  2 reward:  107.6\n",
      "38.3 action:  [-0.0347, -0.9921] n_targets:  1 reward:  55.57\n",
      "39.7 action:  [0.1291, -0.9845] n_targets:  1 reward:  79.15\n",
      "40.5 action:  [-0.0869, -0.9862] n_targets:  2 reward:  153.35\n",
      "42.2 action:  [-0.2345, -0.9922] n_targets:  2 reward:  112.97\n",
      "43.8 action:  [-0.4127, -0.9894] n_targets:  1 reward:  54.42\n",
      "44.4 action:  [0.2012, -0.9917] n_targets:  1 reward:  71.04\n",
      "46.6 action:  [0.1066, -0.993] n_targets:  3 reward:  202.12\n",
      "47.4 action:  [-0.1208, -0.9905] n_targets:  2 reward:  130.17\n",
      "49.6 action:  [-0.0081, -0.9968] n_targets:  1 reward:  62.22\n",
      "51.8 action:  [-0.1373, -0.9962] n_targets:  2 reward:  106.82\n",
      "52.8 action:  [0.0819, -0.9969] n_targets:  1 reward:  60.16\n",
      "53.4 action:  [-0.3566, -0.9867] n_targets:  1 reward:  55.33\n",
      "53.6 action:  [-0.4221, -0.9881] n_targets:  1 reward:  52.3\n",
      "55.6 action:  [0.0764, -0.9955] n_targets:  1 reward:  69.81\n",
      "56.6 action:  [0.079, -0.9871] n_targets:  1 reward:  58.07\n",
      "57.4 action:  [0.3246, -0.9803] n_targets:  2 reward:  145.57\n",
      "58.0 action:  [0.0539, -0.9818] n_targets:  1 reward:  52.08\n",
      "58.2 action:  [0.0436, -0.9874] n_targets:  1 reward:  53.84\n",
      "60.2 action:  [-0.0933, -0.9925] n_targets:  1 reward:  50.81\n",
      "64.1 action:  [-0.0717, -0.9929] n_targets:  1 reward:  51.78\n",
      "64.5 action:  [0.0933, -0.9951] n_targets:  2 reward:  121.23\n",
      "66.2 action:  [-0.0808, -0.9878] n_targets:  3 reward:  193.72\n",
      "66.4 action:  [-0.3836, -0.9959] n_targets:  1 reward:  59.82\n",
      "67.2 action:  [0.0362, -0.9917] n_targets:  1 reward:  59.04\n",
      "68.0 action:  [0.1381, -0.9806] n_targets:  1 reward:  63.95\n",
      "69.2 action:  [-0.2262, -0.9955] n_targets:  1 reward:  51.53\n",
      "70.2 action:  [-0.0862, -0.9954] n_targets:  1 reward:  55.21\n",
      "71.4 action:  [0.0356, -0.9882] n_targets:  1 reward:  56.84\n",
      "72.6 action:  [-0.2832, -0.9981] n_targets:  1 reward:  50.46\n",
      "74.7 action:  [0.1415, -0.9902] n_targets:  1 reward:  93.36\n",
      "75.3 action:  [-0.1853, -0.9896] n_targets:  1 reward:  51.29\n",
      "75.7 action:  [-0.5128, -0.9954] n_targets:  1 reward:  79.17\n",
      "77.2 action:  [-0.5483, -0.9969] n_targets:  1 reward:  82.68\n",
      "83.4 action:  [-0.0951, -0.9839] n_targets:  1 reward:  51.98\n",
      "89.8 action:  [-0.3151, -0.9911] n_targets:  1 reward:  50.27\n",
      "90.2 action:  [-0.0532, -0.9861] n_targets:  1 reward:  61.83\n",
      "91.8 action:  [-0.3662, -0.991] n_targets:  1 reward:  50.17\n",
      "92.8 action:  [-0.1033, -0.9932] n_targets:  1 reward:  64.25\n",
      "94.0 action:  [-0.0644, -0.9824] n_targets:  1 reward:  59.48\n",
      "94.8 action:  [-0.0995, -0.9802] n_targets:  1 reward:  58.12\n",
      "95.8 action:  [-0.3216, -0.9911] n_targets:  1 reward:  61.88\n",
      "97.2 action:  [-0.0959, -0.9906] n_targets:  2 reward:  134.99\n",
      "97.6 action:  [-0.1269, -0.9912] n_targets:  1 reward:  58.87\n",
      "99.4 action:  [-0.143, -0.982] n_targets:  3 reward:  222.76\n",
      "100.6 action:  [0.1312, -0.9942] n_targets:  1 reward:  95.15\n",
      "101.4 action:  [-0.3914, -0.9927] n_targets:  1 reward:  65.92\n",
      "101.8 action:  [-0.5017, -0.9897] n_targets:  1 reward:  55.85\n",
      "ALPHA (entropy-related):  tensor([0.2447], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.27152 0.26775 0.26443 0.26134 0.2583  0.25516 0.25228 0.24955 0.24708\n",
      " 0.24469]\n",
      "Episode: 73, Episode Reward: 6367.616861025492\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  237\n",
      "0.8 action:  [-0.1325, -0.994] n_targets:  1 reward:  52.1\n",
      "1.0 action:  [-0.3984, -0.9972] n_targets:  1 reward:  53.78\n",
      "2.0 action:  [-0.1034, -0.9865] n_targets:  1 reward:  80.39\n",
      "2.6 action:  [0.0629, -0.992] n_targets:  1 reward:  55.53\n",
      "3.4 action:  [0.24, -0.989] n_targets:  1 reward:  54.63\n",
      "4.8 action:  [-0.0859, -0.993] n_targets:  1 reward:  54.32\n",
      "5.2 action:  [0.3706, -0.9847] n_targets:  1 reward:  51.41\n",
      "6.8 action:  [-0.1018, -0.999] n_targets:  2 reward:  114.92\n",
      "7.2 action:  [-0.4883, -0.9872] n_targets:  1 reward:  57.06\n",
      "9.2 action:  [-0.0135, -0.9821] n_targets:  1 reward:  69.18\n",
      "10.3 action:  [-0.1689, -0.9843] n_targets:  1 reward:  89.85\n",
      "11.1 action:  [0.2106, -0.9858] n_targets:  2 reward:  130.33\n",
      "12.7 action:  [0.2001, -0.9823] n_targets:  1 reward:  53.72\n",
      "14.3 action:  [0.3003, -0.9937] n_targets:  1 reward:  56.48\n",
      "17.1 action:  [-0.282, -0.9968] n_targets:  1 reward:  52.05\n",
      "18.1 action:  [-0.0553, -0.9809] n_targets:  1 reward:  56.49\n",
      "19.6 action:  [-0.272, -0.9981] n_targets:  2 reward:  126.27\n",
      "22.3 action:  [0.0305, -0.9963] n_targets:  1 reward:  69.99\n",
      "22.7 action:  [-0.0102, -0.9851] n_targets:  1 reward:  51.4\n",
      "23.3 action:  [-0.0194, -0.9912] n_targets:  1 reward:  60.47\n",
      "26.1 action:  [-0.1247, -0.9987] n_targets:  1 reward:  62.45\n",
      "29.7 action:  [0.0176, -0.9917] n_targets:  1 reward:  54.82\n",
      "30.9 action:  [-0.1434, -0.9825] n_targets:  2 reward:  116.85\n",
      "31.3 action:  [0.5166, -0.9933] n_targets:  2 reward:  109.67\n",
      "37.7 action:  [-0.195, -0.9859] n_targets:  1 reward:  91.44\n",
      "38.3 action:  [-0.028, -0.9826] n_targets:  1 reward:  54.03\n",
      "38.5 action:  [-0.1369, -0.992] n_targets:  1 reward:  52.35\n",
      "38.9 action:  [-0.38, -0.9962] n_targets:  1 reward:  53.82\n",
      "42.9 action:  [0.2133, -0.9821] n_targets:  1 reward:  50.57\n",
      "44.1 action:  [-0.4483, -0.9942] n_targets:  1 reward:  51.06\n",
      "44.5 action:  [0.1632, -0.9873] n_targets:  2 reward:  107.25\n",
      "44.7 action:  [-0.1565, -0.9982] n_targets:  1 reward:  58.34\n",
      "45.6 action:  [0.323, -0.9949] n_targets:  1 reward:  57.43\n",
      "46.4 action:  [0.025, -0.9988] n_targets:  1 reward:  65.59\n",
      "46.8 action:  [-0.022, -0.9878] n_targets:  1 reward:  51.99\n",
      "49.2 action:  [0.221, -0.9979] n_targets:  1 reward:  89.73\n",
      "49.8 action:  [0.0909, -0.9827] n_targets:  1 reward:  54.25\n",
      "50.4 action:  [-0.3084, -0.9963] n_targets:  1 reward:  51.24\n",
      "53.0 action:  [-0.4139, -0.9931] n_targets:  1 reward:  54.19\n",
      "54.4 action:  [0.3927, -0.9909] n_targets:  1 reward:  52.31\n",
      "55.0 action:  [-0.1942, -0.9897] n_targets:  1 reward:  51.02\n",
      "55.8 action:  [-0.0566, -0.9979] n_targets:  1 reward:  52.67\n",
      "56.8 action:  [-0.0027, -0.9837] n_targets:  3 reward:  171.92\n",
      "57.2 action:  [-0.3561, -0.9976] n_targets:  1 reward:  64.79\n",
      "57.8 action:  [0.0998, -0.9959] n_targets:  1 reward:  60.9\n",
      "58.4 action:  [-0.1655, -0.9987] n_targets:  1 reward:  53.0\n",
      "60.2 action:  [0.171, -0.9885] n_targets:  1 reward:  77.57\n",
      "60.8 action:  [0.0685, -0.994] n_targets:  2 reward:  116.1\n",
      "62.2 action:  [-0.1216, -0.9907] n_targets:  1 reward:  56.67\n",
      "66.2 action:  [-0.1474, -0.9813] n_targets:  1 reward:  59.44\n",
      "68.2 action:  [-0.3131, -0.9871] n_targets:  1 reward:  90.07\n",
      "70.6 action:  [-0.188, -0.9907] n_targets:  1 reward:  58.24\n",
      "70.8 action:  [0.037, -0.9987] n_targets:  1 reward:  52.82\n",
      "71.0 action:  [-0.4347, -0.9948] n_targets:  1 reward:  51.01\n",
      "71.2 action:  [0.0111, -0.9917] n_targets:  1 reward:  59.59\n",
      "73.6 action:  [0.1892, -0.9898] n_targets:  2 reward:  124.87\n",
      "77.4 action:  [-0.3353, -0.9862] n_targets:  1 reward:  72.56\n",
      "78.0 action:  [-0.5053, -0.9813] n_targets:  1 reward:  60.79\n",
      "80.0 action:  [0.4273, -0.9814] n_targets:  1 reward:  56.71\n",
      "80.4 action:  [-0.4622, -0.9948] n_targets:  1 reward:  57.69\n",
      "81.2 action:  [0.0919, -0.99] n_targets:  1 reward:  54.4\n",
      "82.6 action:  [-0.2, -0.9945] n_targets:  1 reward:  62.67\n",
      "83.8 action:  [-0.1411, -0.9896] n_targets:  1 reward:  56.44\n",
      "84.0 action:  [-0.2281, -0.9968] n_targets:  1 reward:  51.19\n",
      "84.2 action:  [0.0998, -0.9872] n_targets:  2 reward:  104.38\n",
      "84.8 action:  [-0.001, -0.9965] n_targets:  1 reward:  55.59\n",
      "85.8 action:  [-0.058, -0.9853] n_targets:  1 reward:  54.61\n",
      "86.2 action:  [0.0634, -0.9989] n_targets:  1 reward:  54.44\n",
      "86.8 action:  [-0.2153, -0.9849] n_targets:  1 reward:  57.09\n",
      "87.0 action:  [-0.1376, -0.9928] n_targets:  1 reward:  60.82\n",
      "87.2 action:  [-0.2189, -0.9848] n_targets:  1 reward:  54.02\n",
      "87.4 action:  [-0.4531, -0.989] n_targets:  2 reward:  102.86\n",
      "90.2 action:  [-0.068, -0.995] n_targets:  1 reward:  72.14\n",
      "90.6 action:  [0.3809, -0.9828] n_targets:  1 reward:  57.04\n",
      "91.6 action:  [-0.0291, -0.9926] n_targets:  1 reward:  69.7\n",
      "92.2 action:  [0.2819, -0.9843] n_targets:  1 reward:  72.09\n",
      "96.2 action:  [0.0132, -0.9812] n_targets:  1 reward:  53.26\n",
      "97.3 action:  [-0.1392, -0.9959] n_targets:  3 reward:  166.4\n",
      "97.9 action:  [0.0078, -0.9944] n_targets:  2 reward:  119.5\n",
      "98.7 action:  [-0.0019, -0.9953] n_targets:  1 reward:  50.04\n",
      "99.5 action:  [-0.0982, -0.98] n_targets:  1 reward:  50.96\n",
      "99.7 action:  [-0.2499, -0.9975] n_targets:  1 reward:  53.64\n",
      "ALPHA (entropy-related):  tensor([0.2423], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.26775 0.26443 0.26134 0.2583  0.25516 0.25228 0.24955 0.24708 0.24469\n",
      " 0.24231]\n",
      "Episode: 74, Episode Reward: 5711.508815129598\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  238\n",
      "1.2 action:  [-0.2243, -0.9946] n_targets:  2 reward:  122.23\n",
      "1.6 action:  [0.0024, -0.9951] n_targets:  1 reward:  63.35\n",
      "1.8 action:  [0.0771, -0.9965] n_targets:  1 reward:  55.68\n",
      "2.0 action:  [-0.1522, -0.9827] n_targets:  2 reward:  109.35\n",
      "2.6 action:  [-0.2174, -0.9927] n_targets:  1 reward:  57.4\n",
      "2.8 action:  [-0.2238, -0.9909] n_targets:  1 reward:  50.87\n",
      "5.0 action:  [-0.2327, -0.991] n_targets:  3 reward:  181.77\n",
      "5.6 action:  [0.2755, -0.9922] n_targets:  1 reward:  50.32\n",
      "7.8 action:  [0.0081, -0.9939] n_targets:  2 reward:  106.54\n",
      "8.2 action:  [0.158, -0.988] n_targets:  1 reward:  60.12\n",
      "8.4 action:  [-0.0798, -0.9861] n_targets:  1 reward:  61.64\n",
      "12.6 action:  [-0.1436, -0.9908] n_targets:  1 reward:  65.63\n",
      "13.6 action:  [-0.1132, -0.9896] n_targets:  1 reward:  53.56\n",
      "14.1 action:  [0.0766, -0.9823] n_targets:  1 reward:  64.68\n",
      "19.1 action:  [-0.0659, -0.9901] n_targets:  1 reward:  50.85\n",
      "19.9 action:  [0.171, -0.993] n_targets:  1 reward:  50.14\n",
      "21.3 action:  [0.0205, -0.9918] n_targets:  1 reward:  57.54\n",
      "23.7 action:  [0.036, -0.9888] n_targets:  1 reward:  78.65\n",
      "24.1 action:  [-0.0198, -0.9873] n_targets:  1 reward:  50.74\n",
      "26.3 action:  [0.1273, -0.9955] n_targets:  1 reward:  62.02\n",
      "29.5 action:  [-0.0121, -0.9862] n_targets:  2 reward:  126.27\n",
      "30.1 action:  [-0.0972, -0.9826] n_targets:  1 reward:  56.56\n",
      "30.3 action:  [0.0997, -0.986] n_targets:  2 reward:  100.99\n",
      "34.3 action:  [0.1565, -0.9916] n_targets:  1 reward:  52.97\n",
      "37.3 action:  [-0.0618, -0.995] n_targets:  1 reward:  51.62\n",
      "40.6 action:  [0.0592, -0.9978] n_targets:  1 reward:  58.53\n",
      "41.9 action:  [-0.0909, -0.9862] n_targets:  1 reward:  50.73\n",
      "42.1 action:  [-0.262, -0.9993] n_targets:  1 reward:  50.75\n",
      "42.3 action:  [-0.1999, -0.985] n_targets:  1 reward:  52.63\n",
      "47.4 action:  [0.1307, -0.9903] n_targets:  2 reward:  110.09\n",
      "48.6 action:  [0.1862, -0.9965] n_targets:  1 reward:  55.47\n",
      "50.2 action:  [-0.007, -0.9815] n_targets:  1 reward:  53.8\n",
      "52.6 action:  [0.0499, -0.9926] n_targets:  1 reward:  50.28\n",
      "54.0 action:  [-0.0563, -0.9897] n_targets:  2 reward:  109.01\n",
      "54.4 action:  [-0.2393, -0.9883] n_targets:  1 reward:  75.02\n",
      "55.8 action:  [0.1489, -0.9827] n_targets:  1 reward:  93.95\n",
      "56.4 action:  [-0.1556, -0.9839] n_targets:  1 reward:  57.72\n",
      "56.7 action:  [-0.0503, -0.9927] n_targets:  1 reward:  50.96\n",
      "58.4 action:  [0.3706, -0.9926] n_targets:  1 reward:  53.45\n",
      "59.4 action:  [-0.0548, -0.9957] n_targets:  1 reward:  53.08\n",
      "60.0 action:  [0.2004, -0.9916] n_targets:  1 reward:  63.74\n",
      "60.8 action:  [-0.0149, -0.9854] n_targets:  1 reward:  59.9\n",
      "62.2 action:  [0.1223, -0.9956] n_targets:  1 reward:  50.66\n",
      "63.8 action:  [-0.1097, -0.9822] n_targets:  3 reward:  188.24\n",
      "64.6 action:  [-0.1137, -0.9937] n_targets:  1 reward:  63.12\n",
      "68.2 action:  [-0.0905, -0.9973] n_targets:  1 reward:  71.33\n",
      "69.0 action:  [0.0709, -0.9933] n_targets:  2 reward:  116.0\n",
      "69.6 action:  [-0.2138, -0.9888] n_targets:  1 reward:  72.05\n",
      "71.2 action:  [-0.4684, -0.9932] n_targets:  1 reward:  53.72\n",
      "73.8 action:  [-0.1761, -0.9892] n_targets:  1 reward:  65.69\n",
      "74.2 action:  [-0.1212, -0.984] n_targets:  1 reward:  52.57\n",
      "76.5 action:  [-0.3212, -0.9912] n_targets:  1 reward:  57.67\n",
      "77.1 action:  [-0.0449, -0.9991] n_targets:  1 reward:  59.82\n",
      "77.9 action:  [-0.1046, -0.9969] n_targets:  1 reward:  57.28\n",
      "80.1 action:  [-0.0568, -0.985] n_targets:  1 reward:  72.81\n",
      "81.0 action:  [0.0717, -0.9878] n_targets:  1 reward:  72.97\n",
      "82.0 action:  [0.2613, -0.987] n_targets:  1 reward:  76.95\n",
      "82.4 action:  [-0.1855, -0.9844] n_targets:  2 reward:  115.88\n",
      "82.8 action:  [0.0331, -0.9882] n_targets:  1 reward:  55.08\n",
      "83.8 action:  [0.0902, -0.9841] n_targets:  1 reward:  60.86\n",
      "85.4 action:  [0.1543, -0.9968] n_targets:  1 reward:  53.31\n",
      "85.6 action:  [-0.3092, -0.9812] n_targets:  1 reward:  50.0\n",
      "86.2 action:  [-0.1587, -0.9834] n_targets:  2 reward:  117.29\n",
      "90.8 action:  [0.0068, -0.9915] n_targets:  1 reward:  53.1\n",
      "93.7 action:  [0.1867, -0.9826] n_targets:  1 reward:  58.21\n",
      "94.3 action:  [-0.4226, -0.985] n_targets:  1 reward:  53.72\n",
      "94.7 action:  [0.0967, -0.9846] n_targets:  1 reward:  59.71\n",
      "95.5 action:  [0.0777, -0.9825] n_targets:  1 reward:  68.4\n",
      "97.2 action:  [-0.0552, -0.9993] n_targets:  1 reward:  51.81\n",
      "97.4 action:  [-0.3047, -0.9927] n_targets:  1 reward:  61.49\n",
      "98.2 action:  [0.0052, -0.9934] n_targets:  2 reward:  143.0\n",
      "101.0 action:  [0.1534, -0.9803] n_targets:  1 reward:  62.63\n",
      "101.6 action:  [-0.0398, -0.9803] n_targets:  1 reward:  62.34\n",
      "ALPHA (entropy-related):  tensor([0.2398], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.26443 0.26134 0.2583  0.25516 0.25228 0.24955 0.24708 0.24469 0.24231\n",
      " 0.23982]\n",
      "Last 100 ALPHA: [0.97788 0.95496 0.93215 0.90999 0.88852 0.86787 0.84794 0.82852 0.80965\n",
      " 0.79129 0.7734  0.75598 0.73901 0.72248 0.70635 0.69064 0.67531 0.66036\n",
      " 0.64578 0.63155 0.61769 0.60419 0.59106 0.5783  0.5658  0.55365 0.54182\n",
      " 0.53022 0.51899 0.50808 0.49747 0.48716 0.47703 0.4672  0.45771 0.44833\n",
      " 0.43925 0.43039 0.42165 0.41309 0.40474 0.39652 0.38867 0.38105 0.37368\n",
      " 0.36641 0.35933 0.35272 0.34622 0.33993 0.33402 0.32853 0.32323 0.3181\n",
      " 0.3132  0.30834 0.30365 0.29916 0.2949  0.29077 0.28672 0.28285 0.27889\n",
      " 0.27518 0.27152 0.26775 0.26443 0.26134 0.2583  0.25516 0.25228 0.24955\n",
      " 0.24708 0.24469 0.24231 0.23982]\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  239\n",
      "0.1 action:  [0.0339, -0.9907] n_targets:  1 reward:  81.11\n",
      "0.5 action:  [0.0188, -0.9845] n_targets:  1 reward:  52.25\n",
      "0.7 action:  [0.0239, -0.9866] n_targets:  1 reward:  52.87\n",
      "1.7 action:  [0.019, -0.9866] n_targets:  1 reward:  51.94\n",
      "2.1 action:  [0.02, -0.9888] n_targets:  1 reward:  57.46\n",
      "3.3 action:  [0.0182, -0.9899] n_targets:  1 reward:  58.83\n",
      "4.5 action:  [0.0021, -0.9872] n_targets:  1 reward:  52.29\n",
      "4.9 action:  [-0.0098, -0.9858] n_targets:  1 reward:  52.43\n",
      "5.5 action:  [-0.0108, -0.9858] n_targets:  1 reward:  55.98\n",
      "6.3 action:  [-0.0182, -0.9859] n_targets:  1 reward:  51.89\n",
      "7.1 action:  [0.017, -0.9839] n_targets:  1 reward:  51.58\n",
      "8.3 action:  [0.0376, -0.9851] n_targets:  1 reward:  52.89\n",
      "9.7 action:  [0.0502, -0.9835] n_targets:  1 reward:  54.31\n",
      "10.7 action:  [0.0513, -0.9841] n_targets:  1 reward:  61.19\n",
      "11.9 action:  [0.0243, -0.9871] n_targets:  1 reward:  54.37\n",
      "12.5 action:  [0.0245, -0.9891] n_targets:  2 reward:  106.8\n",
      "16.3 action:  [0.0116, -0.9887] n_targets:  1 reward:  55.26\n",
      "18.3 action:  [0.0216, -0.9884] n_targets:  1 reward:  57.12\n",
      "19.5 action:  [-0.0053, -0.9878] n_targets:  1 reward:  50.34\n",
      "19.7 action:  [0.0198, -0.9914] n_targets:  2 reward:  108.1\n",
      "23.3 action:  [0.0108, -0.9862] n_targets:  1 reward:  52.46\n",
      "24.1 action:  [0.0295, -0.988] n_targets:  1 reward:  52.27\n",
      "25.1 action:  [0.0199, -0.9865] n_targets:  1 reward:  53.04\n",
      "25.3 action:  [0.0213, -0.9863] n_targets:  1 reward:  51.58\n",
      "25.5 action:  [0.011, -0.986] n_targets:  1 reward:  57.16\n",
      "25.9 action:  [0.0133, -0.986] n_targets:  1 reward:  58.96\n",
      "27.5 action:  [0.0157, -0.9868] n_targets:  1 reward:  59.68\n",
      "28.5 action:  [0.0101, -0.9879] n_targets:  1 reward:  56.89\n",
      "29.1 action:  [-0.0002, -0.9855] n_targets:  1 reward:  50.65\n",
      "29.5 action:  [-0.0206, -0.9841] n_targets:  1 reward:  56.9\n",
      "29.9 action:  [-0.0082, -0.984] n_targets:  1 reward:  56.58\n",
      "31.1 action:  [0.0039, -0.9841] n_targets:  1 reward:  52.39\n",
      "31.3 action:  [0.0141, -0.9857] n_targets:  1 reward:  50.15\n",
      "31.7 action:  [0.0342, -0.9855] n_targets:  1 reward:  58.16\n",
      "33.9 action:  [0.0276, -0.9882] n_targets:  1 reward:  56.42\n",
      "34.5 action:  [-0.0043, -0.9872] n_targets:  1 reward:  50.67\n",
      "34.7 action:  [0.0035, -0.9875] n_targets:  1 reward:  52.5\n",
      "35.1 action:  [-0.0074, -0.9867] n_targets:  2 reward:  108.48\n",
      "36.1 action:  [0.0053, -0.99] n_targets:  2 reward:  110.71\n",
      "38.3 action:  [0.0217, -0.9895] n_targets:  1 reward:  54.26\n",
      "40.5 action:  [0.0003, -0.9836] n_targets:  1 reward:  54.98\n",
      "44.9 action:  [0.0198, -0.9858] n_targets:  1 reward:  52.1\n",
      "46.3 action:  [0.0203, -0.9861] n_targets:  1 reward:  55.18\n",
      "47.1 action:  [0.03, -0.9862] n_targets:  1 reward:  56.45\n",
      "47.7 action:  [-0.0085, -0.9837] n_targets:  1 reward:  53.91\n",
      "47.9 action:  [-0.005, -0.9843] n_targets:  1 reward:  56.95\n",
      "48.5 action:  [0.0144, -0.9893] n_targets:  1 reward:  53.26\n",
      "51.5 action:  [0.0649, -0.9886] n_targets:  1 reward:  54.45\n",
      "52.1 action:  [0.0554, -0.9841] n_targets:  1 reward:  57.96\n",
      "53.7 action:  [0.0134, -0.9839] n_targets:  2 reward:  156.83\n",
      "54.7 action:  [0.0078, -0.9849] n_targets:  2 reward:  102.78\n",
      "55.5 action:  [-0.0104, -0.9866] n_targets:  1 reward:  50.73\n",
      "55.7 action:  [-0.0101, -0.9867] n_targets:  1 reward:  54.08\n",
      "61.3 action:  [0.0199, -0.9863] n_targets:  1 reward:  59.59\n",
      "62.1 action:  [0.113, -0.9847] n_targets:  1 reward:  59.58\n",
      "69.1 action:  [-0.0068, -0.9843] n_targets:  1 reward:  51.88\n",
      "69.5 action:  [0.0038, -0.9848] n_targets:  2 reward:  109.67\n",
      "69.7 action:  [-0.0021, -0.9874] n_targets:  1 reward:  51.65\n",
      "70.3 action:  [0.0015, -0.9851] n_targets:  1 reward:  52.53\n",
      "70.7 action:  [-0.0058, -0.9836] n_targets:  1 reward:  54.85\n",
      "71.1 action:  [0.015, -0.9865] n_targets:  1 reward:  55.97\n",
      "72.5 action:  [0.1035, -0.9856] n_targets:  1 reward:  50.27\n",
      "73.1 action:  [0.0399, -0.9845] n_targets:  1 reward:  51.53\n",
      "73.5 action:  [0.0408, -0.9852] n_targets:  2 reward:  108.9\n",
      "73.7 action:  [0.039, -0.9853] n_targets:  1 reward:  59.69\n",
      "76.9 action:  [-0.0078, -0.9882] n_targets:  1 reward:  60.16\n",
      "79.1 action:  [0.0106, -0.9854] n_targets:  1 reward:  51.69\n",
      "80.1 action:  [0.0131, -0.9885] n_targets:  1 reward:  57.9\n",
      "80.7 action:  [0.0052, -0.9881] n_targets:  1 reward:  56.55\n",
      "81.3 action:  [0.0238, -0.9874] n_targets:  1 reward:  56.13\n",
      "82.1 action:  [0.0274, -0.9907] n_targets:  1 reward:  52.23\n",
      "87.9 action:  [0.019, -0.9854] n_targets:  1 reward:  51.78\n",
      "89.1 action:  [0.0207, -0.9874] n_targets:  1 reward:  50.21\n",
      "89.3 action:  [0.0247, -0.985] n_targets:  1 reward:  56.94\n",
      "90.1 action:  [-0.0135, -0.9832] n_targets:  1 reward:  50.9\n",
      "90.9 action:  [0.0182, -0.9851] n_targets:  1 reward:  53.94\n",
      "92.9 action:  [0.0385, -0.985] n_targets:  1 reward:  55.85\n",
      "93.3 action:  [0.0506, -0.9838] n_targets:  1 reward:  58.19\n",
      "95.9 action:  [0.0503, -0.9835] n_targets:  1 reward:  51.51\n",
      "97.5 action:  [0.0304, -0.9862] n_targets:  2 reward:  106.43\n",
      "97.9 action:  [0.0341, -0.9864] n_targets:  1 reward:  56.67\n",
      "98.9 action:  [-0.0271, -0.9848] n_targets:  1 reward:  52.42\n",
      "100.7 action:  [0.0206, -0.9889] n_targets:  1 reward:  50.37\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  240\n",
      "0.4 action:  [0.0047, -0.987] n_targets:  1 reward:  57.95\n",
      "2.2 action:  [0.0093, -0.9861] n_targets:  1 reward:  57.81\n",
      "3.0 action:  [0.0186, -0.9867] n_targets:  1 reward:  50.56\n",
      "7.0 action:  [0.023, -0.9889] n_targets:  1 reward:  50.09\n",
      "7.8 action:  [0.0011, -0.9849] n_targets:  1 reward:  53.56\n",
      "8.2 action:  [0.0362, -0.9852] n_targets:  1 reward:  53.34\n",
      "9.4 action:  [0.0363, -0.985] n_targets:  1 reward:  55.26\n",
      "10.2 action:  [0.0401, -0.9854] n_targets:  1 reward:  53.84\n",
      "11.6 action:  [0.0238, -0.989] n_targets:  1 reward:  52.32\n",
      "11.8 action:  [0.0215, -0.9889] n_targets:  1 reward:  73.77\n",
      "12.0 action:  [0.0235, -0.9893] n_targets:  1 reward:  50.76\n",
      "14.0 action:  [0.0011, -0.9883] n_targets:  1 reward:  52.05\n",
      "15.4 action:  [0.026, -0.9884] n_targets:  1 reward:  52.3\n",
      "18.2 action:  [0.0104, -0.9866] n_targets:  1 reward:  55.09\n",
      "21.0 action:  [-0.0048, -0.9851] n_targets:  1 reward:  53.69\n",
      "22.4 action:  [0.011, -0.9893] n_targets:  1 reward:  51.57\n",
      "22.6 action:  [0.0148, -0.9897] n_targets:  1 reward:  60.27\n",
      "23.4 action:  [0.0037, -0.9861] n_targets:  1 reward:  55.98\n",
      "26.8 action:  [0.0132, -0.9886] n_targets:  1 reward:  57.32\n",
      "27.0 action:  [0.0001, -0.9855] n_targets:  1 reward:  58.25\n",
      "27.6 action:  [0.0111, -0.9886] n_targets:  2 reward:  118.07\n",
      "27.8 action:  [0.0055, -0.9885] n_targets:  1 reward:  50.82\n",
      "30.2 action:  [0.0135, -0.9853] n_targets:  1 reward:  58.95\n",
      "31.2 action:  [0.0315, -0.9826] n_targets:  1 reward:  52.53\n",
      "33.4 action:  [0.0216, -0.988] n_targets:  2 reward:  151.84\n",
      "34.6 action:  [0.017, -0.9888] n_targets:  2 reward:  105.34\n",
      "35.4 action:  [0.0074, -0.9865] n_targets:  1 reward:  60.19\n",
      "36.2 action:  [-0.0016, -0.9863] n_targets:  2 reward:  108.75\n",
      "40.0 action:  [0.0135, -0.9876] n_targets:  1 reward:  56.9\n",
      "44.0 action:  [-0.023, -0.9841] n_targets:  1 reward:  51.97\n",
      "46.4 action:  [-0.0139, -0.9839] n_targets:  1 reward:  53.44\n",
      "48.2 action:  [0.0133, -0.9895] n_targets:  1 reward:  54.97\n",
      "48.6 action:  [0.0213, -0.9909] n_targets:  1 reward:  50.77\n",
      "49.8 action:  [0.0073, -0.9875] n_targets:  1 reward:  50.66\n",
      "51.4 action:  [0.0074, -0.9896] n_targets:  1 reward:  52.97\n",
      "52.0 action:  [0.0079, -0.9887] n_targets:  1 reward:  60.7\n",
      "52.6 action:  [0.0292, -0.9865] n_targets:  1 reward:  52.13\n",
      "53.0 action:  [0.0584, -0.9853] n_targets:  1 reward:  53.27\n",
      "53.8 action:  [0.0195, -0.9873] n_targets:  1 reward:  51.05\n",
      "54.0 action:  [0.0178, -0.9873] n_targets:  1 reward:  58.5\n",
      "54.4 action:  [0.0369, -0.9836] n_targets:  1 reward:  55.85\n",
      "55.4 action:  [0.0224, -0.9893] n_targets:  1 reward:  61.34\n",
      "59.2 action:  [-0.0001, -0.9885] n_targets:  1 reward:  56.54\n",
      "59.6 action:  [0.0182, -0.9888] n_targets:  1 reward:  56.19\n",
      "60.6 action:  [0.0133, -0.9873] n_targets:  1 reward:  51.57\n",
      "61.6 action:  [0.0088, -0.9849] n_targets:  1 reward:  58.12\n",
      "61.8 action:  [0.0111, -0.985] n_targets:  1 reward:  50.84\n",
      "62.2 action:  [0.0277, -0.9901] n_targets:  1 reward:  52.06\n",
      "68.2 action:  [0.0053, -0.9899] n_targets:  1 reward:  53.13\n",
      "69.4 action:  [-0.0246, -0.9838] n_targets:  1 reward:  55.79\n",
      "70.4 action:  [0.0075, -0.989] n_targets:  1 reward:  50.92\n",
      "73.2 action:  [0.0102, -0.9858] n_targets:  1 reward:  53.27\n",
      "76.2 action:  [0.025, -0.9863] n_targets:  1 reward:  53.38\n",
      "76.8 action:  [0.027, -0.9858] n_targets:  1 reward:  50.84\n",
      "77.2 action:  [0.0045, -0.9843] n_targets:  1 reward:  53.64\n",
      "77.6 action:  [0.0097, -0.9861] n_targets:  1 reward:  57.61\n",
      "77.8 action:  [-0.0056, -0.9847] n_targets:  2 reward:  112.85\n",
      "78.0 action:  [-0.0204, -0.9834] n_targets:  1 reward:  58.41\n",
      "80.6 action:  [0.0317, -0.9877] n_targets:  1 reward:  54.63\n",
      "82.0 action:  [-0.0184, -0.9853] n_targets:  1 reward:  56.64\n",
      "82.6 action:  [0.0184, -0.989] n_targets:  1 reward:  57.18\n",
      "84.4 action:  [0.0044, -0.9866] n_targets:  1 reward:  56.41\n",
      "86.6 action:  [-0.0165, -0.9838] n_targets:  1 reward:  51.14\n",
      "87.2 action:  [0.0168, -0.9891] n_targets:  1 reward:  56.12\n",
      "87.6 action:  [0.0098, -0.9875] n_targets:  1 reward:  58.93\n",
      "88.6 action:  [-0.011, -0.9846] n_targets:  1 reward:  53.48\n",
      "89.6 action:  [0.0125, -0.99] n_targets:  2 reward:  109.36\n",
      "91.0 action:  [-0.0014, -0.9868] n_targets:  1 reward:  55.7\n",
      "91.8 action:  [0.0092, -0.989] n_targets:  1 reward:  51.54\n",
      "92.0 action:  [0.0103, -0.9892] n_targets:  1 reward:  52.82\n",
      "93.8 action:  [0.014, -0.9897] n_targets:  1 reward:  56.14\n",
      "94.8 action:  [-0.006, -0.9864] n_targets:  1 reward:  51.24\n",
      "95.0 action:  [-0.0008, -0.9847] n_targets:  1 reward:  53.22\n",
      "96.2 action:  [0.0368, -0.9849] n_targets:  1 reward:  50.96\n",
      "97.2 action:  [0.0144, -0.9877] n_targets:  1 reward:  78.26\n",
      "98.4 action:  [0.0171, -0.9875] n_targets:  1 reward:  53.55\n",
      "99.4 action:  [0.0122, -0.9895] n_targets:  1 reward:  56.84\n",
      "100.8 action:  [-0.02, -0.9829] n_targets:  1 reward:  55.77\n",
      "101.6 action:  [0.0071, -0.9892] n_targets:  1 reward:  56.85\n",
      "102.0 action:  [0.0029, -0.9883] n_targets:  2 reward:  107.67\n",
      "102.4 action:  [0.006, -0.9883] n_targets:  1 reward:  52.88\n",
      "Best average reward: 1720.9753100077312, Current average reward: 4981.922852198283\n",
      "Best average reward = 4981.922852198283\n",
      "Best model saved at episode 75 to None\n",
      "Evaluation rewards: [0.0, 0.0, 1720.9753100077312, 1085.8211957613628, 4981.922852198283]\n",
      "Episode: 75, Episode Reward: 5198.335927327474\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  241\n",
      "0.1 action:  [0.1985, -0.998] n_targets:  3 reward:  181.15\n",
      "0.5 action:  [-0.2014, -0.9966] n_targets:  2 reward:  113.76\n",
      "0.9 action:  [-0.0907, -0.9877] n_targets:  1 reward:  59.32\n",
      "3.0 action:  [0.1185, -0.9892] n_targets:  1 reward:  53.02\n",
      "5.7 action:  [0.2167, -0.9965] n_targets:  1 reward:  59.15\n",
      "5.9 action:  [-0.2868, -0.9933] n_targets:  1 reward:  54.81\n",
      "6.9 action:  [0.2258, -0.9908] n_targets:  1 reward:  63.59\n",
      "7.3 action:  [0.1883, -0.9897] n_targets:  1 reward:  57.42\n",
      "9.1 action:  [-0.1324, -0.9971] n_targets:  1 reward:  50.55\n",
      "13.9 action:  [-0.1726, -0.9839] n_targets:  1 reward:  59.89\n",
      "16.9 action:  [-0.2403, -0.9839] n_targets:  2 reward:  132.44\n",
      "17.5 action:  [-0.0381, -0.9984] n_targets:  1 reward:  52.04\n",
      "17.9 action:  [-0.0198, -0.9906] n_targets:  1 reward:  50.05\n",
      "19.3 action:  [0.1999, -0.991] n_targets:  1 reward:  55.88\n",
      "20.9 action:  [0.2791, -0.9922] n_targets:  1 reward:  53.16\n",
      "23.5 action:  [0.109, -0.9858] n_targets:  2 reward:  139.53\n",
      "23.9 action:  [-0.5432, -0.9802] n_targets:  1 reward:  52.52\n",
      "25.1 action:  [0.1281, -0.9985] n_targets:  1 reward:  51.08\n",
      "25.5 action:  [0.0508, -0.9926] n_targets:  1 reward:  56.82\n",
      "29.1 action:  [-0.2921, -0.9968] n_targets:  1 reward:  54.13\n",
      "30.0 action:  [0.0378, -0.9964] n_targets:  1 reward:  55.39\n",
      "30.4 action:  [-0.212, -0.9954] n_targets:  1 reward:  54.89\n",
      "31.8 action:  [0.0208, -0.9959] n_targets:  1 reward:  50.18\n",
      "32.0 action:  [-0.4018, -0.9896] n_targets:  1 reward:  54.75\n",
      "32.8 action:  [-0.1249, -0.9972] n_targets:  1 reward:  50.5\n",
      "33.0 action:  [-0.1594, -0.9829] n_targets:  1 reward:  59.88\n",
      "33.6 action:  [-0.4404, -0.9938] n_targets:  1 reward:  57.92\n",
      "34.0 action:  [-0.3768, -0.9841] n_targets:  1 reward:  54.13\n",
      "36.6 action:  [0.2957, -0.9919] n_targets:  1 reward:  72.56\n",
      "42.0 action:  [-0.13, -0.9959] n_targets:  2 reward:  111.69\n",
      "44.0 action:  [-0.1895, -0.9947] n_targets:  1 reward:  50.61\n",
      "44.6 action:  [0.1078, -0.9874] n_targets:  1 reward:  52.42\n",
      "44.8 action:  [-0.4199, -0.9903] n_targets:  1 reward:  56.55\n",
      "45.6 action:  [-0.0943, -0.9955] n_targets:  1 reward:  51.68\n",
      "46.0 action:  [-0.0479, -0.9863] n_targets:  1 reward:  57.25\n",
      "46.6 action:  [-0.1921, -0.9926] n_targets:  1 reward:  51.05\n",
      "48.2 action:  [-0.055, -0.9885] n_targets:  1 reward:  51.85\n",
      "48.6 action:  [0.2572, -0.9868] n_targets:  1 reward:  53.61\n",
      "48.8 action:  [-0.1125, -0.9959] n_targets:  1 reward:  52.45\n",
      "55.9 action:  [-0.1042, -0.9826] n_targets:  2 reward:  125.97\n",
      "56.9 action:  [-0.0781, -0.9928] n_targets:  1 reward:  59.17\n",
      "58.9 action:  [-0.1693, -0.9934] n_targets:  1 reward:  51.05\n",
      "59.1 action:  [-0.1103, -0.9938] n_targets:  1 reward:  51.88\n",
      "59.7 action:  [-0.3328, -0.9958] n_targets:  1 reward:  52.81\n",
      "59.9 action:  [-0.2232, -0.9842] n_targets:  1 reward:  55.19\n",
      "60.7 action:  [0.1305, -0.9908] n_targets:  1 reward:  55.33\n",
      "63.1 action:  [0.2317, -0.9817] n_targets:  3 reward:  194.95\n",
      "64.1 action:  [-0.2307, -0.9898] n_targets:  1 reward:  80.45\n",
      "65.5 action:  [-0.1151, -0.9855] n_targets:  2 reward:  107.47\n",
      "65.9 action:  [-0.3278, -0.9857] n_targets:  1 reward:  57.14\n",
      "67.5 action:  [0.0016, -0.9874] n_targets:  1 reward:  51.09\n",
      "68.3 action:  [0.3299, -0.9974] n_targets:  2 reward:  110.78\n",
      "68.9 action:  [0.061, -0.9819] n_targets:  1 reward:  51.71\n",
      "75.4 action:  [0.1427, -0.9845] n_targets:  1 reward:  55.62\n",
      "82.8 action:  [-0.0846, -0.996] n_targets:  2 reward:  114.38\n",
      "87.8 action:  [-0.1826, -0.9957] n_targets:  2 reward:  107.17\n",
      "89.4 action:  [0.1169, -0.9919] n_targets:  1 reward:  52.81\n",
      "92.2 action:  [-0.0865, -0.9986] n_targets:  2 reward:  105.28\n",
      "94.0 action:  [-0.0064, -0.9874] n_targets:  2 reward:  116.21\n",
      "96.4 action:  [0.3299, -0.9916] n_targets:  3 reward:  176.81\n",
      "97.2 action:  [-0.2306, -0.9966] n_targets:  1 reward:  50.07\n",
      "97.8 action:  [0.2585, -0.9853] n_targets:  1 reward:  52.89\n",
      "99.0 action:  [0.0291, -0.996] n_targets:  1 reward:  56.06\n",
      "102.3 action:  [0.1594, -0.9921] n_targets:  3 reward:  185.31\n",
      "ALPHA (entropy-related):  tensor([0.2372], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.26134 0.2583  0.25516 0.25228 0.24955 0.24708 0.24469 0.24231 0.23982\n",
      " 0.2372 ]\n",
      "Episode: 76, Episode Reward: 4727.248133341471\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  242\n",
      "0.1 action:  [0.1995, -0.9858] n_targets:  1 reward:  50.52\n",
      "0.9 action:  [0.1013, -0.9825] n_targets:  2 reward:  115.48\n",
      "1.3 action:  [-0.004, -0.9905] n_targets:  1 reward:  66.67\n",
      "3.1 action:  [0.0639, -0.9934] n_targets:  2 reward:  117.7\n",
      "6.1 action:  [-0.0973, -0.9844] n_targets:  1 reward:  50.51\n",
      "8.5 action:  [-0.146, -0.994] n_targets:  1 reward:  52.84\n",
      "12.1 action:  [-0.0304, -0.9882] n_targets:  1 reward:  56.78\n",
      "13.9 action:  [-0.1382, -0.9806] n_targets:  1 reward:  54.46\n",
      "14.5 action:  [-0.0368, -0.9915] n_targets:  1 reward:  54.77\n",
      "15.3 action:  [0.0802, -0.9961] n_targets:  1 reward:  55.7\n",
      "15.5 action:  [0.2464, -0.9945] n_targets:  1 reward:  58.52\n",
      "16.4 action:  [-0.222, -0.9954] n_targets:  1 reward:  51.84\n",
      "16.8 action:  [-0.3835, -0.9931] n_targets:  1 reward:  53.14\n",
      "20.8 action:  [-0.2796, -0.9945] n_targets:  1 reward:  65.77\n",
      "21.4 action:  [-0.0563, -0.9956] n_targets:  1 reward:  50.16\n",
      "22.9 action:  [-0.1503, -0.9957] n_targets:  1 reward:  50.04\n",
      "24.3 action:  [-0.0105, -0.9866] n_targets:  1 reward:  54.96\n",
      "24.9 action:  [-0.1193, -0.9811] n_targets:  1 reward:  53.39\n",
      "25.3 action:  [0.0731, -0.9946] n_targets:  1 reward:  53.19\n",
      "26.9 action:  [0.1031, -0.9848] n_targets:  1 reward:  50.85\n",
      "28.3 action:  [0.2269, -0.9905] n_targets:  1 reward:  54.25\n",
      "28.5 action:  [0.23, -0.9905] n_targets:  1 reward:  61.39\n",
      "30.1 action:  [-0.1368, -0.9907] n_targets:  1 reward:  60.62\n",
      "31.7 action:  [-0.0439, -0.9924] n_targets:  2 reward:  153.0\n",
      "33.7 action:  [0.1526, -0.9869] n_targets:  2 reward:  114.28\n",
      "35.3 action:  [-0.2756, -0.9817] n_targets:  2 reward:  135.21\n",
      "36.1 action:  [-0.2076, -0.9922] n_targets:  2 reward:  111.76\n",
      "36.7 action:  [-0.2716, -0.9929] n_targets:  1 reward:  59.36\n",
      "39.1 action:  [-0.1348, -0.9845] n_targets:  1 reward:  59.77\n",
      "40.1 action:  [0.2724, -0.9832] n_targets:  3 reward:  183.09\n",
      "41.5 action:  [-0.0473, -0.9948] n_targets:  1 reward:  64.43\n",
      "42.9 action:  [0.049, -0.9944] n_targets:  1 reward:  71.48\n",
      "43.1 action:  [0.1679, -0.9806] n_targets:  1 reward:  51.38\n",
      "43.7 action:  [0.1739, -0.9905] n_targets:  1 reward:  54.46\n",
      "44.2 action:  [-0.476, -0.9849] n_targets:  1 reward:  69.16\n",
      "44.6 action:  [-0.0089, -0.9922] n_targets:  1 reward:  51.85\n",
      "47.4 action:  [-0.0362, -0.9818] n_targets:  1 reward:  66.88\n",
      "51.6 action:  [0.019, -0.9943] n_targets:  1 reward:  63.98\n",
      "57.8 action:  [0.2396, -0.9938] n_targets:  1 reward:  53.69\n",
      "59.4 action:  [0.1003, -0.9941] n_targets:  1 reward:  53.03\n",
      "62.9 action:  [-0.3335, -0.9984] n_targets:  1 reward:  50.58\n",
      "63.5 action:  [-0.0275, -0.9944] n_targets:  1 reward:  55.95\n",
      "63.9 action:  [0.2495, -0.9934] n_targets:  1 reward:  60.59\n",
      "64.7 action:  [0.3367, -0.9935] n_targets:  1 reward:  71.93\n",
      "66.1 action:  [0.4377, -0.9979] n_targets:  1 reward:  62.89\n",
      "68.5 action:  [0.1708, -0.9883] n_targets:  1 reward:  63.78\n",
      "70.1 action:  [0.0449, -0.9885] n_targets:  3 reward:  178.27\n",
      "72.6 action:  [-0.0128, -0.999] n_targets:  3 reward:  198.1\n",
      "76.8 action:  [-0.0348, -0.9926] n_targets:  1 reward:  51.95\n",
      "77.6 action:  [-0.0188, -0.9879] n_targets:  1 reward:  51.61\n",
      "80.2 action:  [-0.1292, -0.9975] n_targets:  1 reward:  56.25\n",
      "81.8 action:  [0.0222, -0.9959] n_targets:  1 reward:  53.63\n",
      "83.9 action:  [-0.1116, -0.9939] n_targets:  1 reward:  51.1\n",
      "84.5 action:  [0.2115, -0.9886] n_targets:  1 reward:  56.29\n",
      "85.5 action:  [-0.1483, -0.9933] n_targets:  1 reward:  52.15\n",
      "86.1 action:  [0.0631, -0.9837] n_targets:  1 reward:  52.98\n",
      "90.1 action:  [-0.1271, -0.9927] n_targets:  2 reward:  135.51\n",
      "91.9 action:  [-0.0107, -0.9954] n_targets:  1 reward:  52.37\n",
      "92.7 action:  [0.0554, -0.9902] n_targets:  1 reward:  57.42\n",
      "93.1 action:  [-0.1219, -0.9887] n_targets:  1 reward:  58.13\n",
      "93.3 action:  [-0.321, -0.9817] n_targets:  1 reward:  52.89\n",
      "93.8 action:  [0.1156, -0.9895] n_targets:  1 reward:  50.05\n",
      "94.2 action:  [-0.0371, -0.9928] n_targets:  1 reward:  53.05\n",
      "96.5 action:  [-0.1124, -0.9969] n_targets:  2 reward:  119.85\n",
      "97.7 action:  [0.0273, -0.9902] n_targets:  1 reward:  53.95\n",
      "99.5 action:  [0.1784, -0.9942] n_targets:  1 reward:  50.15\n",
      "100.1 action:  [-0.1889, -0.997] n_targets:  1 reward:  55.59\n",
      "100.5 action:  [-0.2679, -0.9913] n_targets:  1 reward:  70.01\n",
      "101.7 action:  [-0.4167, -0.9818] n_targets:  1 reward:  50.36\n",
      "ALPHA (entropy-related):  tensor([0.2347], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.2583  0.25516 0.25228 0.24955 0.24708 0.24469 0.24231 0.23982 0.2372\n",
      " 0.23469]\n",
      "Episode: 77, Episode Reward: 4837.768254597981\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  243\n",
      "0.9 action:  [-0.1699, -0.9834] n_targets:  1 reward:  53.18\n",
      "1.7 action:  [-0.0623, -0.9814] n_targets:  1 reward:  54.38\n",
      "3.1 action:  [0.1967, -0.9913] n_targets:  1 reward:  58.81\n",
      "4.5 action:  [-0.2242, -0.9811] n_targets:  2 reward:  106.81\n",
      "5.3 action:  [-0.0744, -0.9908] n_targets:  1 reward:  54.84\n",
      "6.0 action:  [0.0899, -0.9875] n_targets:  1 reward:  67.84\n",
      "7.6 action:  [-0.0912, -0.9904] n_targets:  1 reward:  55.8\n",
      "12.2 action:  [-0.0973, -0.9821] n_targets:  2 reward:  112.96\n",
      "12.4 action:  [0.1257, -0.9919] n_targets:  1 reward:  50.78\n",
      "13.2 action:  [-0.3404, -0.9982] n_targets:  1 reward:  56.58\n",
      "13.5 action:  [0.0292, -0.9873] n_targets:  2 reward:  121.34\n",
      "16.3 action:  [0.2807, -0.9851] n_targets:  1 reward:  55.0\n",
      "17.7 action:  [0.0805, -0.988] n_targets:  1 reward:  53.05\n",
      "20.1 action:  [-0.1808, -0.9881] n_targets:  1 reward:  53.53\n",
      "20.7 action:  [-0.1807, -0.9944] n_targets:  3 reward:  157.4\n",
      "27.0 action:  [0.3112, -0.9973] n_targets:  1 reward:  51.71\n",
      "27.4 action:  [0.0343, -0.9929] n_targets:  1 reward:  64.9\n",
      "28.6 action:  [0.1926, -0.9889] n_targets:  1 reward:  51.96\n",
      "30.2 action:  [-0.1634, -0.9913] n_targets:  1 reward:  82.12\n",
      "31.4 action:  [0.1817, -0.9898] n_targets:  1 reward:  66.98\n",
      "32.0 action:  [0.4774, -0.9899] n_targets:  1 reward:  55.79\n",
      "32.8 action:  [-0.187, -0.9976] n_targets:  1 reward:  68.62\n",
      "33.2 action:  [-0.0806, -0.9947] n_targets:  1 reward:  58.71\n",
      "33.8 action:  [-0.4325, -0.9898] n_targets:  1 reward:  57.89\n",
      "34.0 action:  [0.033, -0.9919] n_targets:  1 reward:  50.83\n",
      "34.4 action:  [0.0393, -0.9897] n_targets:  1 reward:  64.13\n",
      "34.8 action:  [-0.2528, -0.9874] n_targets:  1 reward:  50.67\n",
      "36.0 action:  [-0.2495, -0.9822] n_targets:  1 reward:  65.42\n",
      "36.8 action:  [-0.2042, -0.9905] n_targets:  2 reward:  104.58\n",
      "39.0 action:  [0.047, -0.9954] n_targets:  1 reward:  54.09\n",
      "39.8 action:  [0.1962, -0.9942] n_targets:  2 reward:  112.81\n",
      "41.4 action:  [-0.0288, -0.9968] n_targets:  1 reward:  52.28\n",
      "43.8 action:  [-0.0713, -0.9928] n_targets:  1 reward:  54.38\n",
      "44.2 action:  [-0.0604, -0.9918] n_targets:  1 reward:  61.03\n",
      "44.8 action:  [-0.1379, -0.9969] n_targets:  1 reward:  50.62\n",
      "46.4 action:  [-0.0075, -0.99] n_targets:  1 reward:  79.59\n",
      "48.0 action:  [0.0433, -0.993] n_targets:  2 reward:  118.66\n",
      "48.4 action:  [0.0188, -0.9975] n_targets:  1 reward:  56.51\n",
      "50.0 action:  [-0.1647, -0.9897] n_targets:  1 reward:  72.28\n",
      "51.2 action:  [-0.2699, -0.9903] n_targets:  2 reward:  115.31\n",
      "52.8 action:  [-0.2106, -0.9929] n_targets:  1 reward:  68.33\n",
      "56.6 action:  [0.2984, -0.9885] n_targets:  1 reward:  82.17\n",
      "57.4 action:  [-0.2299, -0.9975] n_targets:  1 reward:  69.9\n",
      "57.6 action:  [-0.081, -0.9882] n_targets:  1 reward:  56.1\n",
      "58.4 action:  [0.3059, -0.9948] n_targets:  1 reward:  73.76\n",
      "59.6 action:  [-0.0479, -0.9975] n_targets:  1 reward:  52.07\n",
      "60.0 action:  [-0.1042, -0.9913] n_targets:  1 reward:  50.66\n",
      "61.4 action:  [-0.0478, -0.9805] n_targets:  1 reward:  56.23\n",
      "61.8 action:  [0.116, -0.9983] n_targets:  2 reward:  108.63\n",
      "63.6 action:  [0.203, -0.9936] n_targets:  1 reward:  65.34\n",
      "64.4 action:  [0.1598, -0.9901] n_targets:  2 reward:  130.14\n",
      "65.8 action:  [0.1234, -0.9966] n_targets:  1 reward:  53.8\n",
      "67.8 action:  [-0.2376, -0.9905] n_targets:  1 reward:  65.07\n",
      "68.4 action:  [-0.3576, -0.9987] n_targets:  2 reward:  122.43\n",
      "69.4 action:  [-0.215, -0.9956] n_targets:  1 reward:  51.28\n",
      "70.0 action:  [-0.1068, -0.9946] n_targets:  1 reward:  72.13\n",
      "71.0 action:  [-0.1026, -0.9913] n_targets:  1 reward:  60.6\n",
      "71.4 action:  [0.0981, -0.9938] n_targets:  1 reward:  57.14\n",
      "71.8 action:  [0.0542, -0.9864] n_targets:  1 reward:  58.13\n",
      "74.0 action:  [-0.043, -0.9922] n_targets:  1 reward:  64.09\n",
      "79.4 action:  [-0.0831, -0.989] n_targets:  1 reward:  50.6\n",
      "82.1 action:  [0.1935, -0.9805] n_targets:  1 reward:  54.72\n",
      "83.3 action:  [-0.1103, -0.9986] n_targets:  1 reward:  69.46\n",
      "85.9 action:  [0.0145, -0.9987] n_targets:  1 reward:  64.54\n",
      "86.5 action:  [0.1831, -0.9964] n_targets:  1 reward:  70.62\n",
      "87.7 action:  [-0.3218, -0.9899] n_targets:  1 reward:  65.17\n",
      "88.9 action:  [-0.4393, -0.9974] n_targets:  1 reward:  51.44\n",
      "90.1 action:  [0.0523, -0.9965] n_targets:  1 reward:  54.13\n",
      "90.7 action:  [-0.0418, -0.9815] n_targets:  2 reward:  148.64\n",
      "93.9 action:  [0.1045, -0.9987] n_targets:  1 reward:  65.68\n",
      "95.3 action:  [-0.012, -0.996] n_targets:  1 reward:  56.46\n",
      "97.1 action:  [-0.1699, -0.9811] n_targets:  1 reward:  54.81\n",
      "97.7 action:  [0.2521, -0.9971] n_targets:  1 reward:  61.2\n",
      "100.1 action:  [-0.001, -0.982] n_targets:  1 reward:  54.28\n",
      "100.7 action:  [-0.3694, -0.9945] n_targets:  1 reward:  60.82\n",
      "ALPHA (entropy-related):  tensor([0.2325], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.25516 0.25228 0.24955 0.24708 0.24469 0.24231 0.23982 0.2372  0.23469\n",
      " 0.23254]\n",
      "Episode: 78, Episode Reward: 5234.809726715088\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  244\n",
      "0.4 action:  [0.0183, -0.9951] n_targets:  1 reward:  53.3\n",
      "3.6 action:  [-0.3352, -0.9884] n_targets:  1 reward:  59.37\n",
      "4.4 action:  [0.4921, -0.9877] n_targets:  1 reward:  60.88\n",
      "5.2 action:  [0.2866, -0.9958] n_targets:  1 reward:  53.06\n",
      "7.2 action:  [-0.0206, -0.996] n_targets:  3 reward:  194.59\n",
      "8.4 action:  [0.1528, -0.9962] n_targets:  1 reward:  53.86\n",
      "10.3 action:  [0.2514, -0.9987] n_targets:  1 reward:  53.49\n",
      "11.3 action:  [-0.2611, -0.996] n_targets:  1 reward:  64.26\n",
      "13.7 action:  [-0.0626, -0.9893] n_targets:  1 reward:  57.34\n",
      "14.1 action:  [-0.3132, -0.9981] n_targets:  1 reward:  51.4\n",
      "14.4 action:  [-0.0531, -0.9915] n_targets:  1 reward:  51.84\n",
      "17.6 action:  [-0.0805, -0.9955] n_targets:  1 reward:  54.02\n",
      "19.8 action:  [-0.0334, -0.9958] n_targets:  1 reward:  51.59\n",
      "22.4 action:  [0.0488, -0.994] n_targets:  1 reward:  74.3\n",
      "23.2 action:  [-0.2991, -0.9943] n_targets:  1 reward:  69.81\n",
      "23.8 action:  [0.2282, -0.9939] n_targets:  1 reward:  76.23\n",
      "25.2 action:  [-0.0142, -0.9937] n_targets:  3 reward:  204.07\n",
      "25.6 action:  [-0.2041, -0.9976] n_targets:  1 reward:  56.04\n",
      "27.8 action:  [-0.1354, -0.9908] n_targets:  1 reward:  58.92\n",
      "28.4 action:  [0.0067, -0.9973] n_targets:  1 reward:  70.29\n",
      "29.2 action:  [0.2535, -0.9968] n_targets:  1 reward:  53.3\n",
      "30.6 action:  [-0.2753, -0.9887] n_targets:  1 reward:  56.02\n",
      "32.1 action:  [-0.2944, -0.999] n_targets:  1 reward:  56.94\n",
      "32.9 action:  [0.1343, -0.9969] n_targets:  1 reward:  58.6\n",
      "33.9 action:  [0.0398, -0.9837] n_targets:  1 reward:  88.7\n",
      "35.5 action:  [0.0017, -0.9837] n_targets:  1 reward:  50.57\n",
      "35.7 action:  [-0.1583, -0.9958] n_targets:  1 reward:  68.0\n",
      "36.7 action:  [0.3834, -0.9961] n_targets:  1 reward:  56.76\n",
      "39.8 action:  [-0.3089, -0.998] n_targets:  1 reward:  56.56\n",
      "40.0 action:  [0.1218, -0.9899] n_targets:  1 reward:  50.2\n",
      "40.8 action:  [0.217, -0.9857] n_targets:  1 reward:  54.5\n",
      "42.6 action:  [-0.1053, -0.9955] n_targets:  1 reward:  57.11\n",
      "44.0 action:  [0.0694, -0.9969] n_targets:  1 reward:  55.53\n",
      "46.0 action:  [-0.1504, -0.9881] n_targets:  1 reward:  51.38\n",
      "46.6 action:  [0.0003, -0.9972] n_targets:  1 reward:  71.89\n",
      "47.4 action:  [0.2838, -0.997] n_targets:  1 reward:  59.48\n",
      "53.3 action:  [-0.2777, -0.9874] n_targets:  1 reward:  57.47\n",
      "53.8 action:  [0.0137, -0.9877] n_targets:  1 reward:  50.63\n",
      "54.6 action:  [-0.0331, -0.9899] n_targets:  1 reward:  52.82\n",
      "55.4 action:  [-0.4929, -0.9836] n_targets:  2 reward:  116.03\n",
      "55.6 action:  [0.3084, -0.987] n_targets:  2 reward:  118.53\n",
      "55.8 action:  [-0.5297, -0.9941] n_targets:  1 reward:  55.67\n",
      "56.2 action:  [-0.0438, -0.996] n_targets:  1 reward:  55.91\n",
      "57.8 action:  [0.1289, -0.9937] n_targets:  1 reward:  60.1\n",
      "58.2 action:  [0.3156, -0.9854] n_targets:  1 reward:  50.91\n",
      "60.0 action:  [-0.0908, -0.9926] n_targets:  2 reward:  111.58\n",
      "60.4 action:  [0.027, -0.991] n_targets:  1 reward:  61.56\n",
      "62.6 action:  [0.1907, -0.9926] n_targets:  2 reward:  147.5\n",
      "63.6 action:  [-0.2149, -0.9927] n_targets:  1 reward:  56.61\n",
      "65.8 action:  [-0.3504, -0.9805] n_targets:  4 reward:  284.84\n",
      "67.4 action:  [-0.143, -0.9995] n_targets:  1 reward:  76.26\n",
      "67.8 action:  [-0.3768, -0.9949] n_targets:  1 reward:  55.12\n",
      "68.2 action:  [-0.2899, -0.9969] n_targets:  1 reward:  53.81\n",
      "70.4 action:  [0.3412, -0.9985] n_targets:  1 reward:  75.05\n",
      "70.6 action:  [-0.2202, -0.9939] n_targets:  1 reward:  54.59\n",
      "70.8 action:  [-0.2595, -0.9915] n_targets:  1 reward:  53.16\n",
      "72.0 action:  [-0.0714, -0.9894] n_targets:  1 reward:  50.32\n",
      "76.0 action:  [0.1, -0.9942] n_targets:  1 reward:  52.06\n",
      "77.0 action:  [-0.3765, -0.9813] n_targets:  5 reward:  323.19\n",
      "80.4 action:  [0.1598, -0.9839] n_targets:  1 reward:  68.09\n",
      "91.2 action:  [-0.1168, -0.9919] n_targets:  1 reward:  55.51\n",
      "92.4 action:  [0.0751, -0.9874] n_targets:  2 reward:  136.8\n",
      "93.3 action:  [0.0125, -0.9929] n_targets:  2 reward:  125.48\n",
      "93.5 action:  [-0.0496, -0.9813] n_targets:  1 reward:  59.72\n",
      "94.1 action:  [-0.2661, -0.9907] n_targets:  1 reward:  73.91\n",
      "96.1 action:  [0.0941, -0.9913] n_targets:  3 reward:  187.1\n",
      "96.3 action:  [-0.2679, -0.9937] n_targets:  2 reward:  107.49\n",
      "98.1 action:  [0.0131, -0.9885] n_targets:  2 reward:  136.31\n",
      "99.1 action:  [-0.539, -0.9863] n_targets:  2 reward:  130.72\n",
      "99.7 action:  [-0.0569, -0.9926] n_targets:  1 reward:  50.49\n",
      "101.7 action:  [-0.1009, -0.9945] n_targets:  1 reward:  93.76\n",
      "ALPHA (entropy-related):  tensor([0.2305], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.25228 0.24955 0.24708 0.24469 0.24231 0.23982 0.2372  0.23469 0.23254\n",
      " 0.23052]\n",
      "Episode: 79, Episode Reward: 5713.260374704997\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  245\n",
      "0.5 action:  [0.0246, -0.9897] n_targets:  2 reward:  118.21\n",
      "0.9 action:  [-0.012, -0.9947] n_targets:  1 reward:  71.89\n",
      "1.6 action:  [0.0934, -0.9977] n_targets:  1 reward:  61.77\n",
      "5.4 action:  [-0.4809, -0.9947] n_targets:  1 reward:  86.14\n",
      "6.4 action:  [0.1604, -0.9871] n_targets:  1 reward:  54.44\n",
      "9.9 action:  [-0.358, -0.99] n_targets:  3 reward:  186.71\n",
      "16.0 action:  [-0.2405, -0.9867] n_targets:  2 reward:  120.36\n",
      "17.2 action:  [-0.0662, -0.9874] n_targets:  1 reward:  51.1\n",
      "17.6 action:  [0.0273, -0.9902] n_targets:  2 reward:  106.36\n",
      "19.2 action:  [0.2253, -0.9928] n_targets:  1 reward:  65.28\n",
      "20.4 action:  [0.1155, -0.9824] n_targets:  1 reward:  52.4\n",
      "21.6 action:  [-0.1043, -0.9858] n_targets:  1 reward:  59.28\n",
      "22.2 action:  [0.0776, -0.992] n_targets:  2 reward:  102.3\n",
      "22.4 action:  [-0.3083, -0.9882] n_targets:  1 reward:  50.39\n",
      "23.2 action:  [0.1513, -0.9928] n_targets:  1 reward:  55.85\n",
      "23.6 action:  [-0.154, -0.9977] n_targets:  1 reward:  53.24\n",
      "24.2 action:  [-0.3839, -0.9964] n_targets:  1 reward:  52.12\n",
      "26.8 action:  [-0.2394, -0.9905] n_targets:  1 reward:  55.02\n",
      "28.0 action:  [-0.4082, -0.9905] n_targets:  1 reward:  65.89\n",
      "28.8 action:  [-0.0493, -0.9831] n_targets:  1 reward:  51.87\n",
      "29.2 action:  [-0.2236, -0.9825] n_targets:  1 reward:  56.7\n",
      "30.2 action:  [0.1327, -0.9808] n_targets:  2 reward:  127.85\n",
      "34.6 action:  [-0.176, -0.9931] n_targets:  1 reward:  65.21\n",
      "35.6 action:  [-0.1894, -0.988] n_targets:  1 reward:  50.27\n",
      "36.6 action:  [0.2053, -0.9884] n_targets:  1 reward:  56.98\n",
      "37.8 action:  [-0.3893, -0.9946] n_targets:  1 reward:  58.74\n",
      "38.2 action:  [-0.0992, -0.9851] n_targets:  2 reward:  105.68\n",
      "39.0 action:  [-0.2846, -0.9937] n_targets:  1 reward:  63.45\n",
      "39.2 action:  [-0.1363, -0.9834] n_targets:  1 reward:  53.06\n",
      "39.8 action:  [0.101, -0.9881] n_targets:  1 reward:  52.01\n",
      "42.2 action:  [-0.438, -0.9958] n_targets:  1 reward:  56.67\n",
      "44.4 action:  [-0.145, -0.9937] n_targets:  1 reward:  77.32\n",
      "44.6 action:  [-0.3835, -0.9905] n_targets:  1 reward:  54.67\n",
      "45.0 action:  [-0.2656, -0.9871] n_targets:  1 reward:  58.2\n",
      "45.8 action:  [-0.1104, -0.9826] n_targets:  1 reward:  51.95\n",
      "47.6 action:  [-0.1334, -0.9863] n_targets:  1 reward:  55.63\n",
      "48.4 action:  [-0.1537, -0.9847] n_targets:  1 reward:  51.67\n",
      "49.8 action:  [-0.0572, -0.9945] n_targets:  1 reward:  62.15\n",
      "53.4 action:  [-0.2126, -0.9849] n_targets:  2 reward:  101.25\n",
      "54.2 action:  [-0.1588, -0.9972] n_targets:  2 reward:  115.03\n",
      "54.8 action:  [-0.18, -0.9937] n_targets:  1 reward:  51.26\n",
      "55.8 action:  [0.0738, -0.9915] n_targets:  1 reward:  54.75\n",
      "58.2 action:  [0.1352, -0.9895] n_targets:  1 reward:  57.05\n",
      "58.6 action:  [0.2679, -0.9934] n_targets:  1 reward:  55.35\n",
      "59.0 action:  [0.2399, -0.9874] n_targets:  1 reward:  54.49\n",
      "62.2 action:  [0.0801, -0.9843] n_targets:  2 reward:  104.83\n",
      "63.0 action:  [-0.17, -0.9866] n_targets:  1 reward:  50.43\n",
      "64.6 action:  [0.0879, -0.9844] n_targets:  1 reward:  51.85\n",
      "66.2 action:  [0.3129, -0.9967] n_targets:  2 reward:  104.55\n",
      "68.6 action:  [-0.0212, -0.9923] n_targets:  1 reward:  50.82\n",
      "70.0 action:  [0.1686, -0.9983] n_targets:  3 reward:  167.35\n",
      "70.4 action:  [-0.2361, -0.9882] n_targets:  2 reward:  107.1\n",
      "72.6 action:  [0.102, -0.9862] n_targets:  3 reward:  192.17\n",
      "73.4 action:  [-0.0652, -0.9928] n_targets:  1 reward:  56.36\n",
      "73.8 action:  [0.0328, -0.9894] n_targets:  1 reward:  53.1\n",
      "74.2 action:  [0.4132, -0.9945] n_targets:  1 reward:  50.09\n",
      "75.2 action:  [-0.0781, -0.9941] n_targets:  1 reward:  56.09\n",
      "75.6 action:  [0.1552, -0.9896] n_targets:  2 reward:  121.3\n",
      "78.6 action:  [0.1605, -0.992] n_targets:  1 reward:  57.36\n",
      "80.0 action:  [-0.2186, -0.9832] n_targets:  1 reward:  51.25\n",
      "80.6 action:  [0.3562, -0.9841] n_targets:  3 reward:  199.42\n",
      "82.0 action:  [0.1132, -0.9977] n_targets:  1 reward:  52.93\n",
      "82.8 action:  [0.0274, -0.9824] n_targets:  1 reward:  56.96\n",
      "84.4 action:  [0.2508, -0.9928] n_targets:  1 reward:  60.75\n",
      "85.0 action:  [-0.0671, -0.9858] n_targets:  1 reward:  65.22\n",
      "85.8 action:  [0.0769, -0.987] n_targets:  1 reward:  79.66\n",
      "89.3 action:  [-0.2484, -0.9902] n_targets:  1 reward:  63.44\n",
      "89.5 action:  [0.2748, -0.9838] n_targets:  1 reward:  50.78\n",
      "90.1 action:  [0.1657, -0.9941] n_targets:  1 reward:  68.96\n",
      "94.5 action:  [-0.0117, -0.9894] n_targets:  1 reward:  79.32\n",
      "99.9 action:  [-0.0873, -0.9947] n_targets:  1 reward:  54.04\n",
      "100.7 action:  [-0.1987, -0.9829] n_targets:  1 reward:  59.67\n",
      "101.3 action:  [-0.2021, -0.9948] n_targets:  1 reward:  67.57\n",
      "ALPHA (entropy-related):  tensor([0.2284], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.24955 0.24708 0.24469 0.24231 0.23982 0.2372  0.23469 0.23254 0.23052\n",
      " 0.22838]\n",
      "Episode: 80, Episode Reward: 5401.357276916503\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  246\n",
      "0.6 action:  [0.2033, -0.9884] n_targets:  2 reward:  164.49\n",
      "1.6 action:  [0.0744, -0.9988] n_targets:  1 reward:  73.49\n",
      "1.8 action:  [-0.2274, -0.9939] n_targets:  1 reward:  55.43\n",
      "2.2 action:  [-0.2908, -0.9967] n_targets:  1 reward:  50.12\n",
      "6.7 action:  [-0.1748, -0.9804] n_targets:  2 reward:  108.0\n",
      "7.5 action:  [-0.0886, -0.9991] n_targets:  1 reward:  59.44\n",
      "8.9 action:  [-0.0739, -0.9971] n_targets:  1 reward:  54.07\n",
      "11.5 action:  [-0.2328, -0.9955] n_targets:  1 reward:  54.52\n",
      "15.5 action:  [-0.0607, -0.9963] n_targets:  1 reward:  51.82\n",
      "16.9 action:  [0.0191, -0.9945] n_targets:  1 reward:  59.64\n",
      "18.5 action:  [-0.084, -0.982] n_targets:  1 reward:  52.92\n",
      "19.0 action:  [-0.0564, -0.9967] n_targets:  1 reward:  61.58\n",
      "21.2 action:  [-0.3855, -0.9949] n_targets:  1 reward:  55.66\n",
      "23.2 action:  [-0.1809, -0.9907] n_targets:  1 reward:  57.32\n",
      "24.8 action:  [-0.1873, -0.9895] n_targets:  1 reward:  58.52\n",
      "26.4 action:  [-0.1593, -0.9971] n_targets:  1 reward:  56.32\n",
      "27.2 action:  [-0.0782, -0.9954] n_targets:  1 reward:  86.27\n",
      "27.6 action:  [-0.2786, -0.9923] n_targets:  1 reward:  56.6\n",
      "28.2 action:  [-0.3398, -0.9953] n_targets:  1 reward:  58.77\n",
      "28.4 action:  [0.1143, -0.9808] n_targets:  1 reward:  59.61\n",
      "30.0 action:  [0.0, -0.9937] n_targets:  2 reward:  145.36\n",
      "30.6 action:  [-0.0074, -0.9965] n_targets:  2 reward:  113.73\n",
      "32.4 action:  [-0.2343, -0.9982] n_targets:  1 reward:  52.85\n",
      "33.4 action:  [-0.2053, -0.9909] n_targets:  1 reward:  59.27\n",
      "34.4 action:  [-0.0751, -0.989] n_targets:  1 reward:  52.05\n",
      "35.0 action:  [0.1041, -0.9965] n_targets:  2 reward:  123.59\n",
      "38.6 action:  [-0.3336, -0.9892] n_targets:  1 reward:  52.81\n",
      "40.9 action:  [-0.0589, -0.9943] n_targets:  1 reward:  71.52\n",
      "41.1 action:  [-0.1071, -0.9946] n_targets:  1 reward:  53.85\n",
      "43.3 action:  [0.1193, -0.9915] n_targets:  1 reward:  79.39\n",
      "48.2 action:  [-0.1496, -0.9956] n_targets:  1 reward:  53.92\n",
      "50.6 action:  [-0.1752, -0.9905] n_targets:  1 reward:  61.94\n",
      "51.4 action:  [-0.2548, -0.9918] n_targets:  1 reward:  51.11\n",
      "51.8 action:  [0.0989, -0.9952] n_targets:  1 reward:  50.68\n",
      "53.9 action:  [-0.1841, -0.9961] n_targets:  1 reward:  52.61\n",
      "57.3 action:  [-0.3057, -0.9833] n_targets:  2 reward:  105.08\n",
      "57.5 action:  [-0.2495, -0.9839] n_targets:  1 reward:  50.38\n",
      "57.7 action:  [-0.4209, -0.9954] n_targets:  1 reward:  59.61\n",
      "58.4 action:  [-0.1473, -0.9858] n_targets:  1 reward:  60.58\n",
      "59.2 action:  [0.2901, -0.9944] n_targets:  1 reward:  54.97\n",
      "61.2 action:  [-0.2953, -0.9844] n_targets:  1 reward:  52.42\n",
      "61.8 action:  [-0.1547, -0.9955] n_targets:  1 reward:  54.45\n",
      "65.5 action:  [0.079, -0.9912] n_targets:  1 reward:  62.02\n",
      "65.9 action:  [0.0979, -0.9945] n_targets:  1 reward:  55.95\n",
      "69.4 action:  [-0.1978, -0.9929] n_targets:  2 reward:  150.48\n",
      "70.3 action:  [-0.1755, -0.9909] n_targets:  1 reward:  59.52\n",
      "71.9 action:  [0.1764, -0.9811] n_targets:  1 reward:  56.08\n",
      "72.4 action:  [0.0145, -0.9913] n_targets:  1 reward:  52.0\n",
      "75.0 action:  [0.0983, -0.9923] n_targets:  1 reward:  52.56\n",
      "77.6 action:  [0.1409, -0.9971] n_targets:  1 reward:  50.56\n",
      "78.1 action:  [-0.0184, -0.997] n_targets:  1 reward:  51.3\n",
      "80.8 action:  [0.1997, -0.9963] n_targets:  1 reward:  50.12\n",
      "87.6 action:  [-0.1319, -0.9973] n_targets:  1 reward:  53.68\n",
      "90.2 action:  [0.0612, -0.9917] n_targets:  1 reward:  54.89\n",
      "90.8 action:  [-0.0685, -0.9897] n_targets:  1 reward:  53.41\n",
      "92.4 action:  [0.0054, -0.9965] n_targets:  2 reward:  150.28\n",
      "93.4 action:  [-0.2017, -0.992] n_targets:  1 reward:  61.13\n",
      "93.8 action:  [-0.1008, -0.9958] n_targets:  1 reward:  51.73\n",
      "95.4 action:  [-0.1233, -0.9954] n_targets:  1 reward:  67.02\n",
      "96.0 action:  [-0.2365, -0.9933] n_targets:  1 reward:  50.61\n",
      "96.6 action:  [-0.2621, -0.9899] n_targets:  1 reward:  63.05\n",
      "96.8 action:  [0.1024, -0.9981] n_targets:  1 reward:  53.37\n",
      "97.2 action:  [-0.0089, -0.9822] n_targets:  1 reward:  51.46\n",
      "97.8 action:  [0.0148, -0.9958] n_targets:  1 reward:  52.51\n",
      "98.8 action:  [0.2902, -0.9889] n_targets:  2 reward:  104.39\n",
      "100.4 action:  [0.3506, -0.9957] n_targets:  1 reward:  58.11\n",
      "101.0 action:  [-0.0948, -0.9832] n_targets:  1 reward:  63.61\n",
      "ALPHA (entropy-related):  tensor([0.2263], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.24708 0.24469 0.24231 0.23982 0.2372  0.23469 0.23254 0.23052 0.22838\n",
      " 0.22627]\n",
      "Episode: 81, Episode Reward: 4476.600626627604\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  247\n",
      "0.1 action:  [0.1228, -0.9969] n_targets:  3 reward:  221.22\n",
      "0.3 action:  [0.1218, -0.9927] n_targets:  2 reward:  118.34\n",
      "1.1 action:  [0.2098, -0.9959] n_targets:  1 reward:  56.53\n",
      "1.7 action:  [0.1906, -0.9836] n_targets:  1 reward:  66.95\n",
      "3.9 action:  [0.0788, -0.9899] n_targets:  3 reward:  206.91\n",
      "4.5 action:  [0.1463, -0.9933] n_targets:  1 reward:  66.61\n",
      "6.1 action:  [0.0013, -0.9878] n_targets:  1 reward:  50.77\n",
      "7.1 action:  [-0.1989, -0.9894] n_targets:  1 reward:  67.89\n",
      "11.9 action:  [-0.1496, -0.9912] n_targets:  1 reward:  57.27\n",
      "12.1 action:  [-0.062, -0.9872] n_targets:  1 reward:  50.92\n",
      "13.5 action:  [0.016, -0.9828] n_targets:  1 reward:  64.98\n",
      "14.3 action:  [-0.1181, -0.9856] n_targets:  1 reward:  59.53\n",
      "15.9 action:  [-0.3138, -0.993] n_targets:  1 reward:  52.7\n",
      "17.1 action:  [0.3674, -0.9947] n_targets:  1 reward:  57.72\n",
      "20.1 action:  [-0.1572, -0.9836] n_targets:  1 reward:  51.24\n",
      "21.1 action:  [-0.1375, -0.9874] n_targets:  1 reward:  61.43\n",
      "22.7 action:  [0.151, -0.9912] n_targets:  1 reward:  66.71\n",
      "22.9 action:  [0.4398, -0.9848] n_targets:  1 reward:  62.38\n",
      "24.1 action:  [0.0832, -0.987] n_targets:  1 reward:  54.65\n",
      "24.9 action:  [-0.2669, -0.9907] n_targets:  1 reward:  52.25\n",
      "28.1 action:  [-0.143, -0.9942] n_targets:  1 reward:  59.66\n",
      "29.8 action:  [-0.2425, -0.9826] n_targets:  1 reward:  53.57\n",
      "30.6 action:  [-0.3346, -0.9876] n_targets:  1 reward:  60.48\n",
      "30.8 action:  [-0.1434, -0.9936] n_targets:  1 reward:  54.5\n",
      "38.1 action:  [-0.0776, -0.99] n_targets:  1 reward:  50.12\n",
      "39.9 action:  [-0.0776, -0.9927] n_targets:  1 reward:  51.33\n",
      "41.3 action:  [0.2335, -0.9841] n_targets:  1 reward:  50.27\n",
      "41.7 action:  [0.1281, -0.9938] n_targets:  1 reward:  51.05\n",
      "42.3 action:  [-0.1441, -0.9837] n_targets:  2 reward:  128.36\n",
      "43.1 action:  [0.2735, -0.9833] n_targets:  1 reward:  56.97\n",
      "44.1 action:  [-0.4115, -0.9887] n_targets:  1 reward:  58.81\n",
      "44.5 action:  [0.1445, -0.9941] n_targets:  1 reward:  51.14\n",
      "46.7 action:  [-0.2125, -0.9821] n_targets:  1 reward:  52.49\n",
      "47.1 action:  [-0.2901, -0.9901] n_targets:  1 reward:  52.93\n",
      "48.5 action:  [0.1, -0.9899] n_targets:  1 reward:  57.76\n",
      "50.9 action:  [-0.1847, -0.9924] n_targets:  1 reward:  54.09\n",
      "51.5 action:  [-0.1571, -0.9851] n_targets:  1 reward:  59.65\n",
      "52.3 action:  [0.4375, -0.9802] n_targets:  1 reward:  56.2\n",
      "52.5 action:  [-0.1112, -0.9864] n_targets:  1 reward:  53.47\n",
      "54.1 action:  [-0.0017, -0.9882] n_targets:  2 reward:  152.7\n",
      "55.3 action:  [-0.6089, -0.9903] n_targets:  1 reward:  65.08\n",
      "57.9 action:  [0.2142, -0.9915] n_targets:  1 reward:  55.25\n",
      "58.3 action:  [-0.0272, -0.9964] n_targets:  1 reward:  56.39\n",
      "59.7 action:  [0.1715, -0.9987] n_targets:  1 reward:  54.95\n",
      "61.1 action:  [-0.053, -0.9843] n_targets:  2 reward:  122.29\n",
      "61.3 action:  [-0.0173, -0.9956] n_targets:  1 reward:  54.81\n",
      "61.9 action:  [-0.0286, -0.9897] n_targets:  1 reward:  53.22\n",
      "65.3 action:  [0.4175, -0.9885] n_targets:  1 reward:  60.7\n",
      "69.3 action:  [-0.2138, -0.9984] n_targets:  1 reward:  52.69\n",
      "70.3 action:  [-0.1622, -0.9956] n_targets:  1 reward:  55.65\n",
      "73.3 action:  [-0.0041, -0.9891] n_targets:  2 reward:  140.05\n",
      "74.3 action:  [0.2116, -0.9853] n_targets:  2 reward:  104.09\n",
      "74.9 action:  [0.1016, -0.9928] n_targets:  1 reward:  55.41\n",
      "75.9 action:  [-0.1825, -0.9908] n_targets:  1 reward:  51.43\n",
      "77.3 action:  [0.1991, -0.9904] n_targets:  1 reward:  57.07\n",
      "79.1 action:  [0.3411, -0.9868] n_targets:  1 reward:  65.48\n",
      "79.3 action:  [-0.0211, -0.9933] n_targets:  1 reward:  51.38\n",
      "79.5 action:  [-0.2714, -0.9824] n_targets:  1 reward:  53.94\n",
      "79.9 action:  [0.0238, -0.9934] n_targets:  2 reward:  129.16\n",
      "80.9 action:  [0.313, -0.9956] n_targets:  2 reward:  128.37\n",
      "87.7 action:  [-0.2924, -0.9948] n_targets:  1 reward:  55.44\n",
      "89.4 action:  [-0.3018, -0.9879] n_targets:  1 reward:  57.25\n",
      "90.6 action:  [-0.0737, -0.9898] n_targets:  1 reward:  50.25\n",
      "92.2 action:  [-0.0171, -0.991] n_targets:  1 reward:  54.31\n",
      "93.6 action:  [-0.0507, -0.9936] n_targets:  2 reward:  126.27\n",
      "94.6 action:  [-0.2402, -0.9832] n_targets:  1 reward:  76.88\n",
      "96.6 action:  [-0.318, -0.9893] n_targets:  1 reward:  51.15\n",
      "97.2 action:  [-0.1844, -0.9823] n_targets:  1 reward:  51.24\n",
      "98.0 action:  [-0.0658, -0.9851] n_targets:  1 reward:  70.42\n",
      "99.0 action:  [0.0681, -0.983] n_targets:  3 reward:  183.03\n",
      "99.6 action:  [0.0941, -0.9908] n_targets:  1 reward:  52.85\n",
      "99.8 action:  [-0.0601, -0.9871] n_targets:  1 reward:  53.19\n",
      "102.4 action:  [0.1414, -0.983] n_targets:  1 reward:  54.58\n",
      "ALPHA (entropy-related):  tensor([0.2243], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.24469 0.24231 0.23982 0.2372  0.23469 0.23254 0.23052 0.22838 0.22627\n",
      " 0.22428]\n",
      "Episode: 82, Episode Reward: 5216.801513671875\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  248\n",
      "0.1 action:  [0.4456, -0.9955] n_targets:  1 reward:  83.86\n",
      "1.5 action:  [-0.0504, -0.997] n_targets:  1 reward:  62.63\n",
      "3.1 action:  [-0.2006, -0.9938] n_targets:  1 reward:  50.75\n",
      "3.3 action:  [0.0739, -0.9924] n_targets:  1 reward:  53.34\n",
      "5.5 action:  [0.6297, -0.9917] n_targets:  1 reward:  53.44\n",
      "6.9 action:  [-0.0741, -0.9847] n_targets:  1 reward:  55.51\n",
      "7.1 action:  [-0.398, -0.9942] n_targets:  1 reward:  53.57\n",
      "7.5 action:  [0.2635, -0.9988] n_targets:  1 reward:  55.77\n",
      "16.6 action:  [0.2297, -0.9902] n_targets:  1 reward:  57.76\n",
      "17.8 action:  [0.0283, -0.9917] n_targets:  2 reward:  103.35\n",
      "20.6 action:  [-0.1955, -0.9969] n_targets:  1 reward:  57.75\n",
      "22.2 action:  [0.0095, -0.9929] n_targets:  1 reward:  52.79\n",
      "25.2 action:  [0.5123, -0.9839] n_targets:  1 reward:  59.24\n",
      "26.2 action:  [0.0823, -0.9964] n_targets:  1 reward:  50.36\n",
      "27.6 action:  [0.3828, -0.9952] n_targets:  1 reward:  53.89\n",
      "28.2 action:  [-0.2525, -0.9978] n_targets:  1 reward:  65.29\n",
      "29.8 action:  [-0.3747, -0.9945] n_targets:  1 reward:  53.36\n",
      "30.0 action:  [-0.3476, -0.9974] n_targets:  1 reward:  57.41\n",
      "33.4 action:  [0.1811, -0.9861] n_targets:  1 reward:  78.21\n",
      "34.4 action:  [0.2607, -0.9804] n_targets:  1 reward:  64.59\n",
      "35.4 action:  [0.292, -0.9983] n_targets:  1 reward:  60.82\n",
      "37.6 action:  [-0.0666, -0.9802] n_targets:  1 reward:  66.78\n",
      "38.2 action:  [-0.0109, -0.9961] n_targets:  1 reward:  53.05\n",
      "39.8 action:  [0.3852, -0.996] n_targets:  1 reward:  62.09\n",
      "40.2 action:  [0.0339, -0.9891] n_targets:  1 reward:  55.97\n",
      "40.8 action:  [0.0788, -0.9976] n_targets:  1 reward:  52.13\n",
      "41.3 action:  [-0.2882, -0.9946] n_targets:  1 reward:  50.31\n",
      "42.5 action:  [-0.138, -0.9968] n_targets:  1 reward:  85.34\n",
      "42.9 action:  [-0.0826, -0.9984] n_targets:  1 reward:  70.41\n",
      "43.1 action:  [-0.0322, -0.9805] n_targets:  1 reward:  50.94\n",
      "43.5 action:  [-0.205, -0.9966] n_targets:  2 reward:  105.74\n",
      "44.7 action:  [-0.0684, -0.9807] n_targets:  1 reward:  52.39\n",
      "46.5 action:  [-0.2724, -0.9913] n_targets:  1 reward:  60.98\n",
      "49.7 action:  [-0.1802, -0.9915] n_targets:  1 reward:  69.26\n",
      "51.3 action:  [0.1815, -0.9929] n_targets:  2 reward:  107.44\n",
      "53.1 action:  [-0.1007, -0.9969] n_targets:  1 reward:  58.2\n",
      "54.7 action:  [0.2434, -0.9959] n_targets:  1 reward:  52.75\n",
      "54.9 action:  [-0.2878, -0.9886] n_targets:  1 reward:  54.65\n",
      "55.5 action:  [0.0151, -0.9882] n_targets:  1 reward:  81.09\n",
      "56.9 action:  [0.1829, -0.9968] n_targets:  1 reward:  53.28\n",
      "59.3 action:  [0.1449, -0.9958] n_targets:  1 reward:  52.6\n",
      "60.1 action:  [0.3056, -0.9903] n_targets:  1 reward:  57.43\n",
      "60.5 action:  [-0.2535, -0.99] n_targets:  1 reward:  67.17\n",
      "61.9 action:  [-0.1974, -0.9971] n_targets:  1 reward:  53.32\n",
      "62.5 action:  [-0.0678, -0.9945] n_targets:  1 reward:  53.89\n",
      "62.9 action:  [-0.1373, -0.9949] n_targets:  1 reward:  50.17\n",
      "64.1 action:  [0.1775, -0.9977] n_targets:  1 reward:  62.33\n",
      "64.7 action:  [0.0591, -0.9851] n_targets:  1 reward:  53.15\n",
      "67.5 action:  [-0.2464, -0.9983] n_targets:  2 reward:  120.85\n",
      "67.9 action:  [-0.1609, -0.9881] n_targets:  1 reward:  53.94\n",
      "68.9 action:  [0.0095, -0.9872] n_targets:  2 reward:  116.91\n",
      "69.5 action:  [0.0543, -0.9808] n_targets:  1 reward:  53.63\n",
      "70.1 action:  [0.1599, -0.9936] n_targets:  1 reward:  55.33\n",
      "71.7 action:  [0.0668, -0.991] n_targets:  1 reward:  65.06\n",
      "74.1 action:  [0.1021, -0.9939] n_targets:  1 reward:  50.07\n",
      "74.7 action:  [-0.152, -0.982] n_targets:  1 reward:  62.8\n",
      "74.9 action:  [-0.0545, -0.9894] n_targets:  1 reward:  53.62\n",
      "75.1 action:  [-0.206, -0.9975] n_targets:  1 reward:  57.64\n",
      "75.5 action:  [0.0353, -0.994] n_targets:  2 reward:  116.84\n",
      "75.7 action:  [-0.2506, -0.9949] n_targets:  1 reward:  58.53\n",
      "76.1 action:  [-0.3821, -0.9865] n_targets:  2 reward:  106.02\n",
      "76.9 action:  [0.1431, -0.9932] n_targets:  1 reward:  51.5\n",
      "77.5 action:  [0.033, -0.9938] n_targets:  1 reward:  52.15\n",
      "79.1 action:  [-0.3268, -0.9949] n_targets:  1 reward:  88.67\n",
      "80.5 action:  [0.1136, -0.9953] n_targets:  1 reward:  58.74\n",
      "82.5 action:  [-0.1824, -0.9977] n_targets:  1 reward:  71.28\n",
      "85.1 action:  [0.1321, -0.9977] n_targets:  1 reward:  56.35\n",
      "85.5 action:  [-0.3639, -0.9847] n_targets:  1 reward:  50.82\n",
      "86.7 action:  [0.0641, -0.9951] n_targets:  1 reward:  62.61\n",
      "92.1 action:  [-0.328, -0.9871] n_targets:  1 reward:  57.56\n",
      "92.9 action:  [0.026, -0.9914] n_targets:  1 reward:  53.96\n",
      "97.5 action:  [-0.1089, -0.9859] n_targets:  1 reward:  58.01\n",
      "100.3 action:  [-0.1535, -0.9883] n_targets:  1 reward:  52.69\n",
      "ALPHA (entropy-related):  tensor([0.2224], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.24231 0.23982 0.2372  0.23469 0.23254 0.23052 0.22838 0.22627 0.22428\n",
      " 0.22241]\n",
      "Episode: 83, Episode Reward: 4662.125708262125\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  249\n",
      "0.1 action:  [-0.507, -0.9959] n_targets:  1 reward:  83.92\n",
      "2.8 action:  [-0.1734, -0.9842] n_targets:  1 reward:  55.12\n",
      "3.2 action:  [0.1308, -0.9921] n_targets:  1 reward:  55.97\n",
      "4.8 action:  [-0.0438, -0.9897] n_targets:  1 reward:  66.5\n",
      "5.2 action:  [0.0123, -0.9874] n_targets:  1 reward:  53.55\n",
      "5.8 action:  [0.2195, -0.9982] n_targets:  1 reward:  58.41\n",
      "6.2 action:  [0.1593, -0.9868] n_targets:  1 reward:  51.83\n",
      "7.8 action:  [-0.1079, -0.9908] n_targets:  1 reward:  58.22\n",
      "9.2 action:  [-0.3377, -0.993] n_targets:  2 reward:  108.45\n",
      "10.4 action:  [-0.2504, -0.997] n_targets:  1 reward:  50.63\n",
      "11.0 action:  [0.1942, -0.9822] n_targets:  1 reward:  54.57\n",
      "11.5 action:  [-0.0411, -0.9915] n_targets:  1 reward:  59.74\n",
      "13.1 action:  [-0.3429, -0.9934] n_targets:  1 reward:  70.63\n",
      "14.3 action:  [-0.1595, -0.9952] n_targets:  1 reward:  50.11\n",
      "14.9 action:  [-0.1032, -0.9898] n_targets:  1 reward:  58.04\n",
      "16.5 action:  [-0.0943, -0.9954] n_targets:  1 reward:  54.62\n",
      "20.1 action:  [-0.064, -0.9992] n_targets:  1 reward:  55.59\n",
      "20.5 action:  [0.1365, -0.9811] n_targets:  2 reward:  117.16\n",
      "23.1 action:  [-0.1059, -0.9939] n_targets:  1 reward:  60.15\n",
      "23.7 action:  [-0.3799, -0.9949] n_targets:  1 reward:  77.76\n",
      "23.9 action:  [0.5197, -0.9924] n_targets:  1 reward:  50.38\n",
      "24.5 action:  [-0.0477, -0.9811] n_targets:  1 reward:  61.77\n",
      "24.9 action:  [-0.0413, -0.9893] n_targets:  1 reward:  54.93\n",
      "25.3 action:  [-0.0374, -0.9958] n_targets:  1 reward:  52.9\n",
      "28.4 action:  [0.1693, -0.9891] n_targets:  1 reward:  58.95\n",
      "29.0 action:  [0.0016, -0.9875] n_targets:  1 reward:  50.85\n",
      "31.6 action:  [-0.1236, -0.983] n_targets:  1 reward:  51.66\n",
      "32.0 action:  [-0.3044, -0.987] n_targets:  2 reward:  120.12\n",
      "32.8 action:  [-0.4472, -0.9974] n_targets:  1 reward:  54.2\n",
      "34.0 action:  [-0.0369, -0.9976] n_targets:  1 reward:  63.53\n",
      "35.4 action:  [-0.3484, -0.9934] n_targets:  1 reward:  56.93\n",
      "35.9 action:  [-0.2945, -0.9825] n_targets:  1 reward:  53.23\n",
      "36.3 action:  [-0.3058, -0.9981] n_targets:  1 reward:  55.39\n",
      "37.1 action:  [-0.4527, -0.9837] n_targets:  2 reward:  107.7\n",
      "37.7 action:  [-0.0421, -0.9961] n_targets:  2 reward:  111.74\n",
      "38.1 action:  [0.1675, -0.985] n_targets:  1 reward:  51.49\n",
      "43.1 action:  [-0.1333, -0.998] n_targets:  1 reward:  51.37\n",
      "44.8 action:  [-0.0799, -0.9814] n_targets:  1 reward:  55.7\n",
      "45.2 action:  [-0.0056, -0.9879] n_targets:  1 reward:  53.89\n",
      "46.6 action:  [-0.3864, -0.9816] n_targets:  1 reward:  50.6\n",
      "48.0 action:  [-0.1285, -0.9973] n_targets:  1 reward:  52.59\n",
      "49.2 action:  [0.3192, -0.9807] n_targets:  1 reward:  52.23\n",
      "49.7 action:  [0.1976, -0.9936] n_targets:  3 reward:  202.58\n",
      "51.1 action:  [-0.3875, -0.9937] n_targets:  1 reward:  76.58\n",
      "52.1 action:  [-0.2482, -0.9947] n_targets:  1 reward:  52.6\n",
      "54.2 action:  [0.0104, -0.9832] n_targets:  1 reward:  58.84\n",
      "54.8 action:  [-0.0498, -0.9912] n_targets:  1 reward:  50.73\n",
      "55.0 action:  [-0.0308, -0.983] n_targets:  1 reward:  54.21\n",
      "55.6 action:  [-0.2762, -0.9939] n_targets:  1 reward:  56.13\n",
      "56.4 action:  [0.1303, -0.9919] n_targets:  2 reward:  117.14\n",
      "58.2 action:  [-0.185, -0.9941] n_targets:  1 reward:  66.01\n",
      "59.5 action:  [-0.0322, -0.9946] n_targets:  2 reward:  115.43\n",
      "60.9 action:  [0.3398, -0.9986] n_targets:  1 reward:  56.03\n",
      "62.3 action:  [-0.2387, -0.9943] n_targets:  1 reward:  58.43\n",
      "63.0 action:  [0.1096, -0.9977] n_targets:  1 reward:  54.9\n",
      "64.4 action:  [0.2228, -0.993] n_targets:  1 reward:  51.79\n",
      "65.0 action:  [0.2551, -0.9914] n_targets:  1 reward:  51.25\n",
      "65.4 action:  [0.0403, -0.9975] n_targets:  1 reward:  51.02\n",
      "67.0 action:  [-0.1233, -0.9942] n_targets:  1 reward:  58.14\n",
      "68.0 action:  [0.1948, -0.9889] n_targets:  1 reward:  72.79\n",
      "68.6 action:  [-0.1811, -0.996] n_targets:  1 reward:  62.3\n",
      "68.8 action:  [-0.0223, -0.9851] n_targets:  1 reward:  53.42\n",
      "74.8 action:  [-0.3246, -0.9839] n_targets:  2 reward:  116.77\n",
      "75.4 action:  [0.0296, -0.9976] n_targets:  1 reward:  52.27\n",
      "76.2 action:  [-0.0069, -0.9818] n_targets:  1 reward:  55.33\n",
      "78.0 action:  [-0.4326, -0.9903] n_targets:  1 reward:  64.52\n",
      "80.9 action:  [0.0387, -0.9938] n_targets:  1 reward:  56.36\n",
      "81.3 action:  [-0.077, -0.9986] n_targets:  1 reward:  57.35\n",
      "82.3 action:  [-0.0141, -0.981] n_targets:  1 reward:  50.81\n",
      "83.1 action:  [-0.2448, -0.993] n_targets:  1 reward:  50.99\n",
      "83.3 action:  [-0.19, -0.9841] n_targets:  1 reward:  60.39\n",
      "83.7 action:  [-0.1747, -0.9935] n_targets:  1 reward:  63.12\n",
      "85.3 action:  [-0.2808, -0.9813] n_targets:  2 reward:  117.18\n",
      "87.1 action:  [0.3151, -0.9988] n_targets:  1 reward:  64.51\n",
      "87.9 action:  [0.0886, -0.9877] n_targets:  2 reward:  104.27\n",
      "89.3 action:  [0.3548, -0.9984] n_targets:  1 reward:  57.54\n",
      "92.2 action:  [0.1661, -0.9985] n_targets:  2 reward:  119.56\n",
      "92.4 action:  [0.169, -0.9897] n_targets:  1 reward:  85.73\n",
      "93.0 action:  [-0.1347, -0.9971] n_targets:  1 reward:  61.95\n",
      "93.4 action:  [-0.5915, -0.9919] n_targets:  1 reward:  52.51\n",
      "94.2 action:  [0.0123, -0.9821] n_targets:  1 reward:  54.51\n",
      "97.4 action:  [0.0389, -0.9841] n_targets:  2 reward:  131.59\n",
      "100.4 action:  [-0.3866, -0.9957] n_targets:  1 reward:  55.34\n",
      "101.6 action:  [-0.1976, -0.986] n_targets:  2 reward:  122.36\n",
      "102.4 action:  [-0.2441, -0.9862] n_targets:  1 reward:  58.24\n",
      "ALPHA (entropy-related):  tensor([0.2207], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.23982 0.2372  0.23469 0.23254 0.23052 0.22838 0.22627 0.22428 0.22241\n",
      " 0.22068]\n",
      "Episode: 84, Episode Reward: 5806.657419840494\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  250\n",
      "0.5 action:  [-0.23, -0.9817] n_targets:  1 reward:  62.41\n",
      "2.1 action:  [-0.1713, -0.9814] n_targets:  1 reward:  61.13\n",
      "4.7 action:  [-0.3218, -0.9976] n_targets:  1 reward:  54.06\n",
      "4.9 action:  [0.0817, -0.9818] n_targets:  2 reward:  105.67\n",
      "7.7 action:  [0.1988, -0.9937] n_targets:  1 reward:  52.29\n",
      "8.9 action:  [-0.0486, -0.9831] n_targets:  1 reward:  68.98\n",
      "9.5 action:  [-0.0922, -0.9991] n_targets:  1 reward:  60.2\n",
      "11.1 action:  [0.1534, -0.9887] n_targets:  1 reward:  51.88\n",
      "13.9 action:  [-0.1704, -0.9912] n_targets:  1 reward:  55.99\n",
      "18.5 action:  [0.0588, -0.9918] n_targets:  2 reward:  115.39\n",
      "18.9 action:  [-0.1704, -0.9895] n_targets:  1 reward:  59.78\n",
      "19.5 action:  [0.3483, -0.9832] n_targets:  1 reward:  56.79\n",
      "20.1 action:  [-0.1043, -0.9862] n_targets:  1 reward:  53.65\n",
      "21.9 action:  [0.1337, -0.9831] n_targets:  1 reward:  57.98\n",
      "22.9 action:  [-0.1107, -0.9933] n_targets:  1 reward:  56.74\n",
      "23.1 action:  [0.2033, -0.9921] n_targets:  1 reward:  52.47\n",
      "24.7 action:  [-0.3669, -0.9963] n_targets:  1 reward:  63.75\n",
      "25.7 action:  [-0.2198, -0.9854] n_targets:  1 reward:  50.08\n",
      "25.9 action:  [-0.2379, -0.9827] n_targets:  1 reward:  50.57\n",
      "27.9 action:  [-0.1949, -0.9875] n_targets:  1 reward:  57.15\n",
      "28.5 action:  [0.2295, -0.9811] n_targets:  1 reward:  50.17\n",
      "31.5 action:  [-0.0885, -0.9951] n_targets:  1 reward:  54.74\n",
      "32.5 action:  [-0.0191, -0.9978] n_targets:  1 reward:  55.93\n",
      "33.3 action:  [-0.0333, -0.9944] n_targets:  1 reward:  52.05\n",
      "33.9 action:  [0.2091, -0.996] n_targets:  1 reward:  54.32\n",
      "35.4 action:  [-0.0263, -0.9856] n_targets:  1 reward:  56.32\n",
      "37.0 action:  [-0.3509, -0.9924] n_targets:  1 reward:  59.3\n",
      "37.8 action:  [-0.0746, -0.9919] n_targets:  1 reward:  52.17\n",
      "39.4 action:  [0.1939, -0.9947] n_targets:  1 reward:  58.36\n",
      "42.2 action:  [-0.3552, -0.9845] n_targets:  1 reward:  54.65\n",
      "45.0 action:  [0.6089, -0.9835] n_targets:  1 reward:  58.24\n",
      "48.0 action:  [-0.0379, -0.9916] n_targets:  1 reward:  54.28\n",
      "48.4 action:  [-0.0463, -0.9881] n_targets:  1 reward:  60.16\n",
      "50.2 action:  [-0.0264, -0.983] n_targets:  1 reward:  51.57\n",
      "52.4 action:  [0.1264, -0.9879] n_targets:  2 reward:  118.21\n",
      "55.0 action:  [-0.0244, -0.9942] n_targets:  2 reward:  102.63\n",
      "55.2 action:  [-0.208, -0.9869] n_targets:  1 reward:  50.67\n",
      "56.2 action:  [0.1901, -0.9919] n_targets:  1 reward:  53.73\n",
      "57.4 action:  [0.184, -0.9944] n_targets:  1 reward:  52.91\n",
      "58.6 action:  [0.0869, -0.9976] n_targets:  1 reward:  51.63\n",
      "59.0 action:  [-0.1832, -0.9856] n_targets:  1 reward:  54.33\n",
      "59.2 action:  [0.0417, -0.9969] n_targets:  1 reward:  56.37\n",
      "63.6 action:  [-0.1366, -0.9925] n_targets:  1 reward:  55.73\n",
      "65.4 action:  [-0.102, -0.9909] n_targets:  1 reward:  56.4\n",
      "65.9 action:  [-0.1155, -0.9992] n_targets:  2 reward:  130.91\n",
      "66.3 action:  [-0.2635, -0.996] n_targets:  2 reward:  123.95\n",
      "66.7 action:  [-0.0071, -0.9892] n_targets:  1 reward:  51.07\n",
      "69.5 action:  [-0.2005, -0.9967] n_targets:  2 reward:  135.91\n",
      "71.1 action:  [-0.0709, -0.9943] n_targets:  1 reward:  53.58\n",
      "71.5 action:  [0.0893, -0.9975] n_targets:  1 reward:  60.53\n",
      "73.9 action:  [0.0925, -0.9878] n_targets:  1 reward:  53.01\n",
      "74.7 action:  [-0.2723, -0.9985] n_targets:  1 reward:  52.78\n",
      "75.1 action:  [-0.073, -0.9965] n_targets:  1 reward:  51.37\n",
      "79.8 action:  [-0.3552, -0.9975] n_targets:  1 reward:  52.91\n",
      "80.4 action:  [-0.0399, -0.9953] n_targets:  1 reward:  51.89\n",
      "80.6 action:  [-0.2282, -0.9989] n_targets:  1 reward:  56.88\n",
      "81.6 action:  [0.2806, -0.9985] n_targets:  3 reward:  223.37\n",
      "84.4 action:  [-0.2859, -0.9975] n_targets:  1 reward:  56.9\n",
      "85.0 action:  [0.1137, -0.998] n_targets:  1 reward:  63.79\n",
      "85.8 action:  [-0.3042, -0.9873] n_targets:  1 reward:  68.16\n",
      "88.6 action:  [-0.0894, -0.997] n_targets:  1 reward:  55.89\n",
      "89.0 action:  [0.2782, -0.9901] n_targets:  1 reward:  54.0\n",
      "90.8 action:  [0.0051, -0.9984] n_targets:  1 reward:  51.24\n",
      "95.9 action:  [0.0992, -0.9983] n_targets:  1 reward:  60.4\n",
      "97.3 action:  [-0.0975, -0.9912] n_targets:  1 reward:  58.7\n",
      "100.4 action:  [-0.0382, -0.9845] n_targets:  1 reward:  53.17\n",
      "100.8 action:  [-0.0493, -0.991] n_targets:  1 reward:  55.2\n",
      "101.0 action:  [-0.0611, -0.9851] n_targets:  2 reward:  105.96\n",
      "ALPHA (entropy-related):  tensor([0.2190], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.2372  0.23469 0.23254 0.23052 0.22838 0.22627 0.22428 0.22241 0.22068\n",
      " 0.21899]\n",
      "Episode: 85, Episode Reward: 4453.3876393636065\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  251\n",
      "0.3 action:  [-0.3039, -0.9903] n_targets:  1 reward:  98.2\n",
      "1.7 action:  [-0.2747, -0.9975] n_targets:  1 reward:  53.23\n",
      "3.7 action:  [0.1535, -0.9915] n_targets:  1 reward:  56.96\n",
      "4.5 action:  [-0.1482, -0.9987] n_targets:  1 reward:  56.01\n",
      "4.9 action:  [-0.2711, -0.9968] n_targets:  1 reward:  51.37\n",
      "6.9 action:  [0.0551, -0.9959] n_targets:  1 reward:  51.35\n",
      "8.3 action:  [0.074, -0.985] n_targets:  1 reward:  50.42\n",
      "8.7 action:  [-0.0003, -0.9987] n_targets:  1 reward:  54.41\n",
      "9.9 action:  [0.0522, -0.9932] n_targets:  1 reward:  57.97\n",
      "12.8 action:  [0.0153, -0.9926] n_targets:  2 reward:  123.42\n",
      "13.0 action:  [0.1648, -0.9985] n_targets:  1 reward:  52.33\n",
      "15.8 action:  [-0.2771, -0.9991] n_targets:  1 reward:  57.62\n",
      "16.2 action:  [0.1961, -0.9942] n_targets:  1 reward:  59.07\n",
      "16.4 action:  [0.2739, -0.9987] n_targets:  1 reward:  62.17\n",
      "17.5 action:  [0.0536, -0.9889] n_targets:  1 reward:  53.77\n",
      "17.7 action:  [-0.1027, -0.9965] n_targets:  2 reward:  102.21\n",
      "17.9 action:  [0.3461, -0.9875] n_targets:  1 reward:  56.31\n",
      "20.3 action:  [0.0867, -0.9938] n_targets:  1 reward:  56.67\n",
      "20.5 action:  [-0.0405, -0.9803] n_targets:  1 reward:  50.6\n",
      "21.1 action:  [0.1769, -0.9841] n_targets:  1 reward:  51.06\n",
      "21.5 action:  [-0.0323, -0.997] n_targets:  1 reward:  52.48\n",
      "23.7 action:  [0.1445, -0.9878] n_targets:  1 reward:  56.0\n",
      "23.9 action:  [-0.1359, -0.9967] n_targets:  1 reward:  51.97\n",
      "24.1 action:  [0.1481, -0.9801] n_targets:  1 reward:  53.37\n",
      "25.5 action:  [0.1262, -0.9969] n_targets:  1 reward:  53.57\n",
      "26.5 action:  [0.0498, -0.9955] n_targets:  1 reward:  66.35\n",
      "33.6 action:  [0.1203, -0.9907] n_targets:  1 reward:  61.15\n",
      "35.0 action:  [0.0617, -0.9897] n_targets:  1 reward:  54.22\n",
      "36.1 action:  [0.0688, -0.9819] n_targets:  1 reward:  63.33\n",
      "37.9 action:  [-0.2217, -0.9936] n_targets:  1 reward:  50.72\n",
      "42.9 action:  [-0.09, -0.9887] n_targets:  1 reward:  55.87\n",
      "46.3 action:  [0.1423, -0.9918] n_targets:  1 reward:  56.68\n",
      "46.5 action:  [-0.061, -0.9933] n_targets:  1 reward:  53.03\n",
      "46.9 action:  [0.0705, -0.9979] n_targets:  2 reward:  116.39\n",
      "47.1 action:  [0.4431, -0.9913] n_targets:  2 reward:  108.7\n",
      "48.0 action:  [0.0385, -0.981] n_targets:  1 reward:  57.94\n",
      "48.8 action:  [0.1974, -0.9907] n_targets:  1 reward:  60.39\n",
      "55.0 action:  [-0.0346, -0.9964] n_targets:  2 reward:  105.59\n",
      "56.2 action:  [0.0333, -0.9959] n_targets:  1 reward:  52.82\n",
      "56.6 action:  [-0.0885, -0.9933] n_targets:  1 reward:  54.9\n",
      "57.7 action:  [-0.1843, -0.9913] n_targets:  1 reward:  56.47\n",
      "58.5 action:  [-0.2921, -0.9978] n_targets:  2 reward:  147.95\n",
      "58.9 action:  [-0.0096, -0.9937] n_targets:  1 reward:  51.95\n",
      "61.3 action:  [-0.1835, -0.9967] n_targets:  1 reward:  51.75\n",
      "62.5 action:  [0.3129, -0.9885] n_targets:  1 reward:  53.29\n",
      "62.7 action:  [-0.156, -0.9893] n_targets:  1 reward:  56.32\n",
      "63.5 action:  [-0.0395, -0.9892] n_targets:  1 reward:  66.66\n",
      "65.9 action:  [0.2238, -0.9921] n_targets:  1 reward:  50.87\n",
      "67.7 action:  [-0.1682, -0.9919] n_targets:  1 reward:  56.06\n",
      "69.1 action:  [-0.2335, -0.9965] n_targets:  2 reward:  119.08\n",
      "71.3 action:  [0.0836, -0.9941] n_targets:  1 reward:  51.6\n",
      "73.9 action:  [-0.1273, -0.9915] n_targets:  1 reward:  57.17\n",
      "74.5 action:  [-0.2228, -0.9971] n_targets:  1 reward:  59.05\n",
      "75.7 action:  [-0.0835, -0.9893] n_targets:  1 reward:  55.34\n",
      "75.9 action:  [-0.0826, -0.9919] n_targets:  1 reward:  50.65\n",
      "77.7 action:  [-0.162, -0.9899] n_targets:  2 reward:  118.2\n",
      "83.3 action:  [-0.016, -0.9869] n_targets:  1 reward:  67.85\n",
      "83.5 action:  [-0.0629, -0.9898] n_targets:  1 reward:  55.59\n",
      "84.3 action:  [-0.1, -0.9979] n_targets:  1 reward:  50.25\n",
      "84.7 action:  [-0.0107, -0.999] n_targets:  1 reward:  50.93\n",
      "89.6 action:  [-0.2845, -0.9986] n_targets:  1 reward:  54.67\n",
      "90.6 action:  [0.1431, -0.9987] n_targets:  1 reward:  64.15\n",
      "93.0 action:  [-0.1188, -0.9949] n_targets:  1 reward:  55.11\n",
      "93.6 action:  [-0.1681, -0.9891] n_targets:  1 reward:  52.06\n",
      "95.6 action:  [-0.1158, -0.9848] n_targets:  1 reward:  63.26\n",
      "96.7 action:  [0.0602, -0.9907] n_targets:  2 reward:  111.08\n",
      "97.5 action:  [-0.3321, -0.9981] n_targets:  1 reward:  57.16\n",
      "100.7 action:  [-0.1751, -0.9901] n_targets:  1 reward:  56.8\n",
      "101.9 action:  [0.023, -0.9822] n_targets:  1 reward:  68.06\n",
      "ALPHA (entropy-related):  tensor([0.2174], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.23469 0.23254 0.23052 0.22838 0.22627 0.22428 0.22241 0.22068 0.21899\n",
      " 0.21737]\n",
      "Episode: 86, Episode Reward: 4449.974679629007\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  252\n",
      "0.1 action:  [0.1965, -0.998] n_targets:  2 reward:  145.06\n",
      "0.7 action:  [0.5296, -0.9817] n_targets:  1 reward:  55.8\n",
      "1.1 action:  [-0.1062, -0.9873] n_targets:  3 reward:  175.12\n",
      "1.9 action:  [-0.3437, -0.9947] n_targets:  2 reward:  124.11\n",
      "2.1 action:  [-0.1217, -0.9868] n_targets:  1 reward:  50.19\n",
      "2.9 action:  [-0.4238, -0.995] n_targets:  1 reward:  52.4\n",
      "3.1 action:  [0.5038, -0.9893] n_targets:  1 reward:  56.51\n",
      "3.3 action:  [-0.0288, -0.9973] n_targets:  1 reward:  52.94\n",
      "4.5 action:  [0.1582, -0.9951] n_targets:  1 reward:  51.68\n",
      "5.7 action:  [0.1988, -0.9981] n_targets:  1 reward:  50.95\n",
      "6.3 action:  [-0.0154, -0.9935] n_targets:  1 reward:  56.42\n",
      "6.9 action:  [0.2532, -0.9958] n_targets:  1 reward:  51.97\n",
      "9.5 action:  [-0.4015, -0.9817] n_targets:  1 reward:  55.37\n",
      "10.1 action:  [0.1232, -0.9975] n_targets:  1 reward:  51.99\n",
      "10.3 action:  [0.2239, -0.9979] n_targets:  1 reward:  53.51\n",
      "10.9 action:  [-0.0251, -0.9964] n_targets:  1 reward:  52.29\n",
      "11.3 action:  [-0.1172, -0.9931] n_targets:  1 reward:  50.9\n",
      "13.1 action:  [0.4884, -0.9983] n_targets:  1 reward:  68.18\n",
      "15.7 action:  [0.0629, -0.9984] n_targets:  1 reward:  55.51\n",
      "16.1 action:  [-0.0623, -0.9944] n_targets:  1 reward:  50.87\n",
      "16.3 action:  [-0.0151, -0.9927] n_targets:  1 reward:  57.39\n",
      "17.7 action:  [-0.1539, -0.9981] n_targets:  1 reward:  53.06\n",
      "18.7 action:  [-0.2255, -0.9856] n_targets:  1 reward:  54.6\n",
      "19.5 action:  [-0.1875, -0.9878] n_targets:  1 reward:  54.18\n",
      "21.1 action:  [0.1132, -0.9964] n_targets:  1 reward:  61.89\n",
      "21.5 action:  [0.0309, -0.9966] n_targets:  1 reward:  53.99\n",
      "22.5 action:  [-0.11, -0.9985] n_targets:  1 reward:  59.67\n",
      "23.3 action:  [-0.1637, -0.9907] n_targets:  1 reward:  55.07\n",
      "29.0 action:  [-0.4609, -0.9975] n_targets:  1 reward:  53.04\n",
      "32.4 action:  [-0.0092, -0.9944] n_targets:  1 reward:  50.66\n",
      "33.6 action:  [-0.1522, -0.9982] n_targets:  1 reward:  52.99\n",
      "34.2 action:  [-0.101, -0.9899] n_targets:  1 reward:  51.27\n",
      "34.4 action:  [-0.4644, -0.997] n_targets:  1 reward:  53.32\n",
      "38.0 action:  [-0.0242, -0.9953] n_targets:  1 reward:  50.7\n",
      "38.8 action:  [0.0658, -0.9922] n_targets:  1 reward:  51.4\n",
      "39.6 action:  [0.1268, -0.9813] n_targets:  1 reward:  52.14\n",
      "41.2 action:  [-0.074, -0.9942] n_targets:  1 reward:  54.4\n",
      "43.6 action:  [-0.0819, -0.9976] n_targets:  1 reward:  56.9\n",
      "44.0 action:  [-0.0136, -0.9954] n_targets:  1 reward:  50.65\n",
      "45.0 action:  [0.002, -0.9954] n_targets:  1 reward:  54.84\n",
      "49.4 action:  [0.2663, -0.993] n_targets:  1 reward:  51.88\n",
      "49.8 action:  [-0.0588, -0.9954] n_targets:  1 reward:  57.43\n",
      "50.5 action:  [0.113, -0.9978] n_targets:  1 reward:  51.27\n",
      "53.1 action:  [0.1426, -0.9811] n_targets:  1 reward:  56.38\n",
      "53.3 action:  [0.0247, -0.9937] n_targets:  1 reward:  62.0\n",
      "53.9 action:  [-0.0874, -0.9952] n_targets:  1 reward:  51.46\n",
      "54.7 action:  [0.0863, -0.9863] n_targets:  1 reward:  50.03\n",
      "54.9 action:  [0.0869, -0.9926] n_targets:  1 reward:  53.09\n",
      "55.1 action:  [-0.1572, -0.9948] n_targets:  1 reward:  58.46\n",
      "55.3 action:  [-0.0353, -0.9943] n_targets:  2 reward:  110.29\n",
      "55.9 action:  [-0.058, -0.9901] n_targets:  2 reward:  107.07\n",
      "56.1 action:  [0.0916, -0.9985] n_targets:  1 reward:  57.67\n",
      "57.3 action:  [-0.115, -0.9842] n_targets:  1 reward:  56.13\n",
      "58.1 action:  [-0.1141, -0.9956] n_targets:  1 reward:  65.7\n",
      "58.8 action:  [0.0413, -0.9949] n_targets:  1 reward:  61.07\n",
      "60.0 action:  [-0.1506, -0.9937] n_targets:  1 reward:  61.07\n",
      "62.2 action:  [-0.0607, -0.9849] n_targets:  1 reward:  54.11\n",
      "62.4 action:  [0.0396, -0.9984] n_targets:  1 reward:  52.46\n",
      "65.2 action:  [0.1913, -0.9924] n_targets:  1 reward:  60.64\n",
      "65.6 action:  [-0.0769, -0.9925] n_targets:  1 reward:  57.04\n",
      "69.2 action:  [-0.05, -0.9845] n_targets:  1 reward:  51.02\n",
      "69.6 action:  [0.15, -0.9837] n_targets:  1 reward:  50.05\n",
      "73.0 action:  [0.438, -0.9928] n_targets:  1 reward:  56.49\n",
      "75.0 action:  [0.0674, -0.992] n_targets:  1 reward:  51.47\n",
      "77.2 action:  [0.2043, -0.9847] n_targets:  1 reward:  65.74\n",
      "80.4 action:  [0.1981, -0.9976] n_targets:  1 reward:  55.35\n",
      "81.2 action:  [-0.2523, -0.9976] n_targets:  1 reward:  52.98\n",
      "82.3 action:  [0.0131, -0.987] n_targets:  1 reward:  50.03\n",
      "82.9 action:  [0.2208, -0.997] n_targets:  1 reward:  50.55\n",
      "84.5 action:  [-0.056, -0.9856] n_targets:  1 reward:  55.36\n",
      "87.0 action:  [0.1069, -0.9937] n_targets:  1 reward:  52.83\n",
      "87.6 action:  [0.4308, -0.9931] n_targets:  1 reward:  52.93\n",
      "89.4 action:  [0.476, -0.9891] n_targets:  1 reward:  60.56\n",
      "89.6 action:  [0.0298, -0.9942] n_targets:  1 reward:  52.21\n",
      "90.0 action:  [0.0883, -0.9907] n_targets:  1 reward:  51.48\n",
      "91.2 action:  [0.0569, -0.9967] n_targets:  2 reward:  123.0\n",
      "92.4 action:  [0.1611, -0.995] n_targets:  1 reward:  59.41\n",
      "94.6 action:  [-0.1829, -0.9927] n_targets:  2 reward:  116.27\n",
      "96.6 action:  [-0.0091, -0.9978] n_targets:  1 reward:  57.16\n",
      "98.2 action:  [0.1439, -0.996] n_targets:  1 reward:  51.36\n",
      "ALPHA (entropy-related):  tensor([0.2159], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.23254 0.23052 0.22838 0.22627 0.22428 0.22241 0.22068 0.21899 0.21737\n",
      " 0.21588]\n",
      "Episode: 87, Episode Reward: 4895.405807495117\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  253\n",
      "1.4 action:  [-0.176, -0.9915] n_targets:  1 reward:  54.45\n",
      "2.8 action:  [-0.07, -0.9899] n_targets:  1 reward:  66.71\n",
      "4.4 action:  [-0.0072, -0.9875] n_targets:  2 reward:  117.12\n",
      "6.6 action:  [-0.0238, -0.9946] n_targets:  1 reward:  52.82\n",
      "6.8 action:  [-0.2384, -0.9969] n_targets:  2 reward:  118.0\n",
      "7.0 action:  [-0.0995, -0.9974] n_targets:  1 reward:  51.81\n",
      "8.0 action:  [-0.0526, -0.9958] n_targets:  1 reward:  63.75\n",
      "8.6 action:  [-0.1445, -0.9964] n_targets:  1 reward:  73.14\n",
      "13.7 action:  [-0.4436, -0.9985] n_targets:  1 reward:  56.61\n",
      "13.9 action:  [-0.3062, -0.9816] n_targets:  2 reward:  111.83\n",
      "14.7 action:  [-0.2092, -0.9863] n_targets:  2 reward:  133.69\n",
      "15.3 action:  [0.0411, -0.9874] n_targets:  3 reward:  179.29\n",
      "17.7 action:  [-0.4242, -0.9884] n_targets:  1 reward:  50.01\n",
      "18.3 action:  [-0.0681, -0.9874] n_targets:  1 reward:  52.22\n",
      "18.9 action:  [-0.0997, -0.9912] n_targets:  1 reward:  54.15\n",
      "22.8 action:  [0.0334, -0.9984] n_targets:  1 reward:  56.25\n",
      "25.5 action:  [-0.0294, -0.996] n_targets:  1 reward:  60.14\n",
      "25.7 action:  [-0.1322, -0.9952] n_targets:  1 reward:  53.47\n",
      "26.5 action:  [0.1508, -0.9945] n_targets:  1 reward:  54.46\n",
      "27.1 action:  [0.0188, -0.9948] n_targets:  1 reward:  52.89\n",
      "27.7 action:  [0.4014, -0.9904] n_targets:  1 reward:  56.37\n",
      "27.9 action:  [0.0407, -0.9979] n_targets:  1 reward:  58.77\n",
      "29.7 action:  [0.1697, -0.9877] n_targets:  2 reward:  121.74\n",
      "31.1 action:  [-0.0553, -0.9839] n_targets:  1 reward:  59.14\n",
      "32.9 action:  [0.0616, -0.9918] n_targets:  1 reward:  60.15\n",
      "36.3 action:  [0.1891, -0.9958] n_targets:  1 reward:  56.78\n",
      "37.5 action:  [0.4487, -0.9977] n_targets:  2 reward:  128.95\n",
      "37.7 action:  [-0.2757, -0.9985] n_targets:  1 reward:  59.71\n",
      "37.9 action:  [0.0616, -0.9875] n_targets:  3 reward:  161.53\n",
      "41.9 action:  [0.2276, -0.9977] n_targets:  2 reward:  104.45\n",
      "42.1 action:  [0.098, -0.9888] n_targets:  1 reward:  50.81\n",
      "43.9 action:  [-0.0399, -0.9975] n_targets:  1 reward:  57.61\n",
      "44.7 action:  [0.0433, -0.9941] n_targets:  1 reward:  51.83\n",
      "45.9 action:  [0.2915, -0.9988] n_targets:  3 reward:  174.8\n",
      "52.3 action:  [0.1176, -0.9993] n_targets:  1 reward:  63.37\n",
      "52.9 action:  [0.0637, -0.9868] n_targets:  1 reward:  57.03\n",
      "53.9 action:  [-0.0253, -0.9884] n_targets:  1 reward:  51.15\n",
      "55.1 action:  [0.1954, -0.9935] n_targets:  1 reward:  52.51\n",
      "60.2 action:  [0.1798, -0.9937] n_targets:  1 reward:  50.18\n",
      "64.2 action:  [-0.1952, -0.9835] n_targets:  1 reward:  55.85\n",
      "68.2 action:  [0.3062, -0.9969] n_targets:  1 reward:  60.23\n",
      "71.2 action:  [-0.1006, -0.9879] n_targets:  1 reward:  54.1\n",
      "72.2 action:  [0.0389, -0.9883] n_targets:  1 reward:  68.29\n",
      "72.4 action:  [0.0982, -0.9897] n_targets:  1 reward:  56.6\n",
      "75.1 action:  [0.1052, -0.9864] n_targets:  1 reward:  50.4\n",
      "75.5 action:  [0.0005, -0.9964] n_targets:  1 reward:  52.6\n",
      "79.1 action:  [0.0737, -0.9949] n_targets:  1 reward:  60.52\n",
      "79.7 action:  [-0.0373, -0.9968] n_targets:  1 reward:  65.83\n",
      "80.3 action:  [0.1259, -0.9877] n_targets:  1 reward:  60.37\n",
      "84.1 action:  [-0.2684, -0.9806] n_targets:  1 reward:  58.52\n",
      "85.1 action:  [-0.2014, -0.9924] n_targets:  1 reward:  51.81\n",
      "86.7 action:  [-0.0427, -0.9855] n_targets:  1 reward:  51.15\n",
      "86.9 action:  [-0.2329, -0.9855] n_targets:  1 reward:  57.98\n",
      "87.3 action:  [-0.2001, -0.9978] n_targets:  1 reward:  60.06\n",
      "88.3 action:  [0.2203, -0.9972] n_targets:  1 reward:  54.34\n",
      "91.3 action:  [-0.1426, -0.9949] n_targets:  1 reward:  53.43\n",
      "91.5 action:  [0.125, -0.9932] n_targets:  1 reward:  57.48\n",
      "98.3 action:  [-0.2182, -0.995] n_targets:  1 reward:  67.31\n",
      "100.5 action:  [0.1099, -0.9881] n_targets:  1 reward:  53.71\n",
      "100.7 action:  [-0.2288, -0.9917] n_targets:  1 reward:  53.75\n",
      "ALPHA (entropy-related):  tensor([0.2145], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.23052 0.22838 0.22627 0.22428 0.22241 0.22068 0.21899 0.21737 0.21588\n",
      " 0.21455]\n",
      "Episode: 88, Episode Reward: 4193.997426350911\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  254\n",
      "0.1 action:  [-0.1466, -0.9882] n_targets:  1 reward:  72.93\n",
      "1.7 action:  [-0.1213, -0.997] n_targets:  1 reward:  54.12\n",
      "2.3 action:  [0.1245, -0.9908] n_targets:  1 reward:  54.28\n",
      "3.5 action:  [-0.0931, -0.9971] n_targets:  1 reward:  60.17\n",
      "4.1 action:  [-0.1981, -0.9964] n_targets:  1 reward:  51.82\n",
      "4.3 action:  [0.2336, -0.9965] n_targets:  1 reward:  56.23\n",
      "6.2 action:  [-0.1994, -0.9974] n_targets:  1 reward:  57.29\n",
      "7.6 action:  [-0.1114, -0.9936] n_targets:  1 reward:  53.06\n",
      "8.4 action:  [-0.2556, -0.9961] n_targets:  1 reward:  56.14\n",
      "10.0 action:  [-0.1075, -0.9848] n_targets:  1 reward:  50.76\n",
      "11.1 action:  [-0.005, -0.9925] n_targets:  1 reward:  53.76\n",
      "12.3 action:  [-0.1729, -0.9973] n_targets:  1 reward:  71.87\n",
      "12.9 action:  [-0.1169, -0.9973] n_targets:  1 reward:  52.14\n",
      "13.5 action:  [-0.0493, -0.9948] n_targets:  1 reward:  52.76\n",
      "13.7 action:  [-0.1371, -0.9988] n_targets:  1 reward:  59.66\n",
      "14.1 action:  [-0.3159, -0.9927] n_targets:  1 reward:  69.13\n",
      "15.1 action:  [0.1327, -0.9907] n_targets:  1 reward:  62.59\n",
      "18.5 action:  [-0.1674, -0.9841] n_targets:  1 reward:  59.95\n",
      "18.7 action:  [-0.0692, -0.995] n_targets:  1 reward:  52.41\n",
      "18.9 action:  [-0.1656, -0.9987] n_targets:  1 reward:  50.44\n",
      "19.7 action:  [-0.2642, -0.9986] n_targets:  1 reward:  52.48\n",
      "20.7 action:  [-0.2113, -0.9909] n_targets:  1 reward:  50.79\n",
      "21.3 action:  [0.0064, -0.9972] n_targets:  1 reward:  59.57\n",
      "22.1 action:  [-0.0858, -0.9927] n_targets:  1 reward:  51.28\n",
      "24.9 action:  [-0.0082, -0.9961] n_targets:  2 reward:  108.09\n",
      "25.3 action:  [0.1873, -0.988] n_targets:  1 reward:  53.81\n",
      "27.3 action:  [0.0548, -0.9972] n_targets:  2 reward:  111.28\n",
      "29.5 action:  [-0.1428, -0.9922] n_targets:  1 reward:  91.61\n",
      "30.1 action:  [-0.0289, -0.9903] n_targets:  1 reward:  57.85\n",
      "31.5 action:  [0.0515, -0.9906] n_targets:  1 reward:  66.85\n",
      "32.9 action:  [0.3891, -0.9886] n_targets:  2 reward:  139.79\n",
      "34.5 action:  [0.1801, -0.9979] n_targets:  1 reward:  59.06\n",
      "34.9 action:  [-0.1644, -0.9935] n_targets:  1 reward:  51.74\n",
      "37.7 action:  [0.0521, -0.9846] n_targets:  1 reward:  62.54\n",
      "39.8 action:  [0.2011, -0.9985] n_targets:  1 reward:  59.17\n",
      "40.2 action:  [0.1728, -0.9876] n_targets:  2 reward:  109.96\n",
      "41.4 action:  [-0.0114, -0.9957] n_targets:  1 reward:  67.2\n",
      "42.2 action:  [0.137, -0.994] n_targets:  1 reward:  50.09\n",
      "43.0 action:  [-0.4757, -0.9938] n_targets:  1 reward:  54.96\n",
      "43.4 action:  [-0.112, -0.9959] n_targets:  1 reward:  58.44\n",
      "45.0 action:  [0.1644, -0.9954] n_targets:  3 reward:  180.98\n",
      "45.2 action:  [0.2871, -0.9981] n_targets:  1 reward:  55.52\n",
      "45.6 action:  [0.0535, -0.9981] n_targets:  1 reward:  64.26\n",
      "46.2 action:  [0.007, -0.992] n_targets:  1 reward:  51.74\n",
      "46.6 action:  [-0.3687, -0.9945] n_targets:  2 reward:  108.59\n",
      "49.0 action:  [0.1074, -0.9982] n_targets:  1 reward:  62.24\n",
      "49.8 action:  [0.001, -0.9822] n_targets:  1 reward:  50.93\n",
      "50.0 action:  [0.119, -0.9931] n_targets:  1 reward:  53.76\n",
      "52.0 action:  [-0.0425, -0.9986] n_targets:  1 reward:  88.19\n",
      "52.2 action:  [-0.0676, -0.997] n_targets:  1 reward:  59.51\n",
      "52.8 action:  [-0.2735, -0.9889] n_targets:  1 reward:  53.02\n",
      "54.6 action:  [-0.4415, -0.9805] n_targets:  1 reward:  57.75\n",
      "58.2 action:  [0.0588, -0.9921] n_targets:  1 reward:  50.6\n",
      "64.4 action:  [-0.1431, -0.9872] n_targets:  1 reward:  50.86\n",
      "67.8 action:  [-0.1346, -0.9892] n_targets:  1 reward:  66.69\n",
      "68.0 action:  [0.0669, -0.9946] n_targets:  1 reward:  51.46\n",
      "68.4 action:  [0.1077, -0.9943] n_targets:  1 reward:  51.33\n",
      "69.0 action:  [0.2388, -0.9984] n_targets:  1 reward:  51.93\n",
      "69.4 action:  [-0.2289, -0.9945] n_targets:  1 reward:  55.46\n",
      "71.4 action:  [-0.0379, -0.9988] n_targets:  1 reward:  58.27\n",
      "71.6 action:  [-0.0695, -0.9899] n_targets:  1 reward:  56.95\n",
      "73.8 action:  [-0.0244, -0.9958] n_targets:  1 reward:  63.15\n",
      "75.8 action:  [0.2811, -0.9852] n_targets:  1 reward:  64.65\n",
      "77.6 action:  [0.0103, -0.995] n_targets:  1 reward:  80.98\n",
      "81.0 action:  [-0.4129, -0.988] n_targets:  1 reward:  58.84\n",
      "86.0 action:  [0.0019, -0.9981] n_targets:  1 reward:  66.41\n",
      "86.8 action:  [-0.188, -0.9868] n_targets:  1 reward:  55.09\n",
      "87.8 action:  [0.0328, -0.9879] n_targets:  1 reward:  60.45\n",
      "88.6 action:  [-0.0133, -0.9877] n_targets:  1 reward:  57.22\n",
      "89.6 action:  [0.1431, -0.9974] n_targets:  2 reward:  124.14\n",
      "89.8 action:  [-0.3664, -0.9889] n_targets:  1 reward:  53.34\n",
      "92.0 action:  [0.1112, -0.9827] n_targets:  1 reward:  62.09\n",
      "94.6 action:  [0.1728, -0.9951] n_targets:  1 reward:  60.42\n",
      "95.2 action:  [-0.1137, -0.9921] n_targets:  1 reward:  60.26\n",
      "96.6 action:  [-0.1068, -0.9977] n_targets:  1 reward:  55.6\n",
      "100.4 action:  [-0.0872, -0.9926] n_targets:  1 reward:  50.2\n",
      "ALPHA (entropy-related):  tensor([0.2132], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.22838 0.22627 0.22428 0.22241 0.22068 0.21899 0.21737 0.21588 0.21455\n",
      " 0.21316]\n",
      "Episode: 89, Episode Reward: 4920.945302327474\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  255\n",
      "0.1 action:  [-0.3022, -0.9912] n_targets:  3 reward:  207.78\n",
      "1.5 action:  [0.191, -0.993] n_targets:  2 reward:  111.76\n",
      "2.3 action:  [-0.1526, -0.9941] n_targets:  1 reward:  55.1\n",
      "3.7 action:  [0.0427, -0.9848] n_targets:  1 reward:  50.02\n",
      "3.9 action:  [-0.1551, -0.9967] n_targets:  1 reward:  50.62\n",
      "4.7 action:  [0.0352, -0.9956] n_targets:  1 reward:  70.27\n",
      "5.1 action:  [-0.1944, -0.9946] n_targets:  3 reward:  178.13\n",
      "5.3 action:  [-0.2875, -0.9986] n_targets:  1 reward:  53.82\n",
      "10.3 action:  [-0.1551, -0.9894] n_targets:  1 reward:  68.05\n",
      "12.3 action:  [0.2909, -0.9846] n_targets:  1 reward:  51.39\n",
      "16.1 action:  [0.0186, -0.9828] n_targets:  1 reward:  58.87\n",
      "17.8 action:  [-0.0365, -0.9975] n_targets:  1 reward:  52.49\n",
      "18.4 action:  [0.3421, -0.9947] n_targets:  1 reward:  52.88\n",
      "19.6 action:  [-0.08, -0.9936] n_targets:  1 reward:  55.94\n",
      "19.8 action:  [-0.0631, -0.9893] n_targets:  1 reward:  50.37\n",
      "22.5 action:  [-0.1256, -0.9985] n_targets:  1 reward:  51.68\n",
      "22.7 action:  [0.1749, -0.9977] n_targets:  1 reward:  51.38\n",
      "22.9 action:  [0.0238, -0.9936] n_targets:  1 reward:  53.86\n",
      "24.5 action:  [-0.2066, -0.992] n_targets:  1 reward:  53.37\n",
      "24.9 action:  [0.0729, -0.9977] n_targets:  1 reward:  60.36\n",
      "26.3 action:  [-0.1627, -0.9883] n_targets:  1 reward:  53.76\n",
      "27.1 action:  [0.3284, -0.9895] n_targets:  1 reward:  59.78\n",
      "27.9 action:  [-0.0447, -0.9889] n_targets:  1 reward:  60.36\n",
      "29.9 action:  [-0.1944, -0.9888] n_targets:  1 reward:  50.45\n",
      "30.7 action:  [-0.2008, -0.9835] n_targets:  1 reward:  53.09\n",
      "33.4 action:  [-0.0641, -0.9991] n_targets:  1 reward:  57.5\n",
      "35.6 action:  [-0.0526, -0.9971] n_targets:  1 reward:  54.58\n",
      "36.6 action:  [-0.0705, -0.9897] n_targets:  1 reward:  50.32\n",
      "37.2 action:  [-0.092, -0.998] n_targets:  1 reward:  55.18\n",
      "41.6 action:  [0.0095, -0.9939] n_targets:  1 reward:  51.84\n",
      "41.8 action:  [0.1654, -0.983] n_targets:  1 reward:  54.82\n",
      "42.4 action:  [-0.2237, -0.98] n_targets:  1 reward:  60.25\n",
      "42.8 action:  [0.2195, -0.9981] n_targets:  1 reward:  53.47\n",
      "44.4 action:  [0.085, -0.9816] n_targets:  1 reward:  55.05\n",
      "45.6 action:  [0.1073, -0.9929] n_targets:  2 reward:  116.69\n",
      "47.7 action:  [-0.3099, -0.9978] n_targets:  1 reward:  51.66\n",
      "48.9 action:  [0.0802, -0.9879] n_targets:  1 reward:  54.82\n",
      "49.7 action:  [-0.0653, -0.9988] n_targets:  1 reward:  56.3\n",
      "51.5 action:  [0.0312, -0.995] n_targets:  1 reward:  60.83\n",
      "52.7 action:  [0.0336, -0.9882] n_targets:  1 reward:  50.86\n",
      "57.3 action:  [-0.0971, -0.9922] n_targets:  1 reward:  51.93\n",
      "59.9 action:  [-0.0324, -0.9905] n_targets:  1 reward:  56.52\n",
      "60.8 action:  [0.135, -0.9939] n_targets:  1 reward:  51.38\n",
      "61.6 action:  [-0.3185, -0.9908] n_targets:  1 reward:  59.46\n",
      "62.2 action:  [-0.1009, -0.9835] n_targets:  1 reward:  51.93\n",
      "62.4 action:  [0.0376, -0.9969] n_targets:  1 reward:  51.31\n",
      "64.1 action:  [0.0284, -0.9876] n_targets:  1 reward:  50.91\n",
      "64.3 action:  [-0.0011, -0.9971] n_targets:  1 reward:  62.57\n",
      "64.9 action:  [-0.0545, -0.9866] n_targets:  1 reward:  52.08\n",
      "66.5 action:  [0.0187, -0.9874] n_targets:  2 reward:  120.91\n",
      "68.7 action:  [-0.1134, -0.9899] n_targets:  1 reward:  60.55\n",
      "69.5 action:  [0.3047, -0.9979] n_targets:  1 reward:  50.14\n",
      "70.9 action:  [-0.1518, -0.986] n_targets:  1 reward:  52.04\n",
      "71.7 action:  [-0.0819, -0.9987] n_targets:  1 reward:  62.7\n",
      "77.2 action:  [-0.2327, -0.986] n_targets:  1 reward:  57.77\n",
      "79.2 action:  [-0.0545, -0.9876] n_targets:  1 reward:  59.34\n",
      "81.7 action:  [-0.0767, -0.9848] n_targets:  1 reward:  50.7\n",
      "81.9 action:  [-0.3429, -0.9994] n_targets:  1 reward:  52.42\n",
      "82.9 action:  [-0.1801, -0.9855] n_targets:  1 reward:  51.29\n",
      "83.5 action:  [0.0769, -0.9976] n_targets:  1 reward:  62.01\n",
      "84.3 action:  [-0.0676, -0.9938] n_targets:  1 reward:  53.91\n",
      "88.3 action:  [0.0053, -0.9953] n_targets:  1 reward:  63.94\n",
      "89.1 action:  [-0.1178, -0.9971] n_targets:  1 reward:  58.38\n",
      "91.7 action:  [-0.3276, -0.9961] n_targets:  1 reward:  51.3\n",
      "91.9 action:  [-0.3047, -0.982] n_targets:  1 reward:  51.78\n",
      "94.3 action:  [-0.0858, -0.9954] n_targets:  1 reward:  51.86\n",
      "97.9 action:  [-0.2358, -0.997] n_targets:  1 reward:  50.67\n",
      "98.9 action:  [0.0196, -0.9939] n_targets:  2 reward:  115.49\n",
      "99.1 action:  [-0.1274, -0.9939] n_targets:  1 reward:  56.7\n",
      "99.7 action:  [0.0827, -0.9923] n_targets:  2 reward:  139.97\n",
      "100.4 action:  [-0.23, -0.9955] n_targets:  1 reward:  58.57\n",
      "100.8 action:  [-0.1738, -0.9932] n_targets:  2 reward:  110.89\n",
      "102.0 action:  [0.0112, -0.9964] n_targets:  3 reward:  221.7\n",
      "102.4 action:  [-0.3016, -0.9977] n_targets:  1 reward:  59.99\n",
      "ALPHA (entropy-related):  tensor([0.2117], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.22627 0.22428 0.22241 0.22068 0.21899 0.21737 0.21588 0.21455 0.21316\n",
      " 0.21174]\n",
      "Last 100 ALPHA: [0.97788 0.95496 0.93215 0.90999 0.88852 0.86787 0.84794 0.82852 0.80965\n",
      " 0.79129 0.7734  0.75598 0.73901 0.72248 0.70635 0.69064 0.67531 0.66036\n",
      " 0.64578 0.63155 0.61769 0.60419 0.59106 0.5783  0.5658  0.55365 0.54182\n",
      " 0.53022 0.51899 0.50808 0.49747 0.48716 0.47703 0.4672  0.45771 0.44833\n",
      " 0.43925 0.43039 0.42165 0.41309 0.40474 0.39652 0.38867 0.38105 0.37368\n",
      " 0.36641 0.35933 0.35272 0.34622 0.33993 0.33402 0.32853 0.32323 0.3181\n",
      " 0.3132  0.30834 0.30365 0.29916 0.2949  0.29077 0.28672 0.28285 0.27889\n",
      " 0.27518 0.27152 0.26775 0.26443 0.26134 0.2583  0.25516 0.25228 0.24955\n",
      " 0.24708 0.24469 0.24231 0.23982 0.2372  0.23469 0.23254 0.23052 0.22838\n",
      " 0.22627 0.22428 0.22241 0.22068 0.21899 0.21737 0.21588 0.21455 0.21316\n",
      " 0.21174]\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  256\n",
      "0.1 action:  [-0.1206, -0.993] n_targets:  1 reward:  55.85\n",
      "3.3 action:  [-0.0821, -0.9912] n_targets:  1 reward:  54.28\n",
      "4.3 action:  [-0.0553, -0.99] n_targets:  1 reward:  52.35\n",
      "5.1 action:  [-0.1009, -0.9926] n_targets:  1 reward:  53.22\n",
      "5.9 action:  [-0.0586, -0.9899] n_targets:  1 reward:  52.37\n",
      "6.1 action:  [-0.102, -0.9921] n_targets:  1 reward:  54.78\n",
      "8.9 action:  [-0.0921, -0.9936] n_targets:  1 reward:  53.04\n",
      "9.5 action:  [-0.0985, -0.9912] n_targets:  1 reward:  51.63\n",
      "10.7 action:  [-0.023, -0.9876] n_targets:  1 reward:  59.59\n",
      "11.9 action:  [-0.0672, -0.9903] n_targets:  1 reward:  64.24\n",
      "13.3 action:  [-0.0617, -0.99] n_targets:  1 reward:  59.28\n",
      "13.5 action:  [-0.0597, -0.9899] n_targets:  1 reward:  54.42\n",
      "13.7 action:  [-0.0951, -0.9916] n_targets:  1 reward:  59.41\n",
      "14.1 action:  [-0.0786, -0.9907] n_targets:  1 reward:  56.58\n",
      "16.1 action:  [-0.0932, -0.9917] n_targets:  1 reward:  51.12\n",
      "16.7 action:  [-0.1072, -0.9929] n_targets:  1 reward:  54.25\n",
      "17.3 action:  [-0.0702, -0.9907] n_targets:  1 reward:  59.31\n",
      "19.3 action:  [-0.1017, -0.9925] n_targets:  2 reward:  100.72\n",
      "20.7 action:  [-0.1033, -0.9925] n_targets:  1 reward:  56.0\n",
      "22.1 action:  [-0.1148, -0.9928] n_targets:  1 reward:  52.57\n",
      "23.9 action:  [-0.1186, -0.9932] n_targets:  1 reward:  52.03\n",
      "25.9 action:  [-0.0797, -0.9906] n_targets:  2 reward:  111.03\n",
      "30.1 action:  [-0.0566, -0.9898] n_targets:  1 reward:  54.77\n",
      "31.7 action:  [-0.0564, -0.9898] n_targets:  1 reward:  54.94\n",
      "32.5 action:  [-0.1076, -0.9928] n_targets:  1 reward:  53.57\n",
      "33.1 action:  [-0.0875, -0.991] n_targets:  1 reward:  51.91\n",
      "33.3 action:  [-0.1089, -0.9933] n_targets:  1 reward:  53.65\n",
      "34.7 action:  [-0.0568, -0.9899] n_targets:  1 reward:  51.4\n",
      "36.9 action:  [-0.0799, -0.9911] n_targets:  1 reward:  55.19\n",
      "37.1 action:  [-0.0828, -0.9913] n_targets:  1 reward:  51.98\n",
      "38.5 action:  [-0.0781, -0.9911] n_targets:  1 reward:  50.69\n",
      "39.5 action:  [-0.1058, -0.993] n_targets:  1 reward:  59.37\n",
      "39.7 action:  [-0.1103, -0.9932] n_targets:  1 reward:  58.99\n",
      "40.9 action:  [-0.0591, -0.99] n_targets:  1 reward:  52.4\n",
      "43.7 action:  [-0.0792, -0.9913] n_targets:  1 reward:  52.66\n",
      "43.9 action:  [-0.0591, -0.9899] n_targets:  1 reward:  50.85\n",
      "44.5 action:  [-0.0482, -0.9901] n_targets:  1 reward:  57.77\n",
      "44.9 action:  [-0.024, -0.9877] n_targets:  2 reward:  111.58\n",
      "45.9 action:  [-0.0744, -0.9906] n_targets:  1 reward:  50.85\n",
      "47.3 action:  [-0.0794, -0.9906] n_targets:  2 reward:  112.82\n",
      "48.1 action:  [-0.12, -0.993] n_targets:  1 reward:  60.07\n",
      "48.5 action:  [-0.0994, -0.9923] n_targets:  1 reward:  50.8\n",
      "50.1 action:  [-0.0899, -0.9912] n_targets:  1 reward:  57.29\n",
      "51.3 action:  [-0.0631, -0.9902] n_targets:  1 reward:  54.76\n",
      "51.9 action:  [-0.1177, -0.993] n_targets:  1 reward:  54.42\n",
      "53.7 action:  [-0.0596, -0.9898] n_targets:  1 reward:  57.35\n",
      "55.5 action:  [-0.0577, -0.9899] n_targets:  1 reward:  51.1\n",
      "56.5 action:  [-0.0831, -0.991] n_targets:  1 reward:  55.64\n",
      "57.7 action:  [-0.1001, -0.9921] n_targets:  1 reward:  50.81\n",
      "58.1 action:  [-0.0608, -0.9899] n_targets:  1 reward:  53.31\n",
      "58.3 action:  [-0.0886, -0.9921] n_targets:  1 reward:  56.48\n",
      "59.1 action:  [-0.1111, -0.9932] n_targets:  1 reward:  50.93\n",
      "59.7 action:  [-0.075, -0.9909] n_targets:  1 reward:  51.18\n",
      "61.1 action:  [-0.0794, -0.9902] n_targets:  2 reward:  105.91\n",
      "61.7 action:  [-0.0765, -0.9905] n_targets:  1 reward:  50.31\n",
      "63.1 action:  [-0.099, -0.9915] n_targets:  1 reward:  52.07\n",
      "63.7 action:  [-0.0753, -0.9901] n_targets:  1 reward:  59.44\n",
      "64.5 action:  [-0.1203, -0.9929] n_targets:  1 reward:  59.27\n",
      "67.9 action:  [-0.111, -0.9924] n_targets:  1 reward:  59.28\n",
      "69.9 action:  [-0.1, -0.9924] n_targets:  1 reward:  54.52\n",
      "70.3 action:  [-0.0646, -0.99] n_targets:  1 reward:  57.71\n",
      "73.7 action:  [-0.1028, -0.9934] n_targets:  1 reward:  52.49\n",
      "73.9 action:  [-0.101, -0.9936] n_targets:  2 reward:  107.08\n",
      "74.1 action:  [-0.1143, -0.9929] n_targets:  2 reward:  109.13\n",
      "75.5 action:  [-0.0874, -0.991] n_targets:  1 reward:  57.41\n",
      "76.5 action:  [-0.083, -0.9909] n_targets:  1 reward:  56.29\n",
      "76.9 action:  [-0.1151, -0.9916] n_targets:  1 reward:  52.71\n",
      "77.7 action:  [-0.0362, -0.9896] n_targets:  1 reward:  51.28\n",
      "77.9 action:  [-0.0356, -0.9896] n_targets:  1 reward:  51.73\n",
      "79.3 action:  [-0.0223, -0.9876] n_targets:  1 reward:  50.4\n",
      "84.5 action:  [-0.0574, -0.9898] n_targets:  1 reward:  50.5\n",
      "84.9 action:  [-0.0564, -0.9898] n_targets:  1 reward:  57.28\n",
      "85.5 action:  [-0.1098, -0.9919] n_targets:  1 reward:  54.51\n",
      "88.1 action:  [-0.1354, -0.9928] n_targets:  1 reward:  57.07\n",
      "91.9 action:  [-0.1097, -0.9925] n_targets:  1 reward:  60.55\n",
      "92.9 action:  [-0.0697, -0.9904] n_targets:  1 reward:  58.93\n",
      "93.7 action:  [-0.1052, -0.9925] n_targets:  1 reward:  50.31\n",
      "98.3 action:  [-0.1254, -0.993] n_targets:  1 reward:  55.02\n",
      "98.7 action:  [-0.1039, -0.9919] n_targets:  1 reward:  58.52\n",
      "99.1 action:  [-0.0813, -0.9909] n_targets:  1 reward:  57.58\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  257\n",
      "0.1 action:  [-0.0532, -0.9904] n_targets:  1 reward:  73.37\n",
      "2.7 action:  [-0.0732, -0.9909] n_targets:  1 reward:  53.03\n",
      "2.9 action:  [-0.0662, -0.9902] n_targets:  1 reward:  50.8\n",
      "3.1 action:  [-0.0826, -0.9909] n_targets:  1 reward:  51.62\n",
      "3.9 action:  [-0.1058, -0.9926] n_targets:  2 reward:  111.7\n",
      "4.3 action:  [-0.0677, -0.9903] n_targets:  1 reward:  57.14\n",
      "5.3 action:  [-0.114, -0.9932] n_targets:  1 reward:  52.07\n",
      "5.7 action:  [-0.1054, -0.9921] n_targets:  1 reward:  52.75\n",
      "6.3 action:  [-0.0839, -0.9917] n_targets:  1 reward:  60.12\n",
      "6.5 action:  [-0.0846, -0.9918] n_targets:  1 reward:  53.96\n",
      "6.7 action:  [-0.0863, -0.9918] n_targets:  1 reward:  50.2\n",
      "7.1 action:  [-0.0699, -0.9903] n_targets:  1 reward:  50.26\n",
      "8.1 action:  [-0.0453, -0.9903] n_targets:  2 reward:  107.01\n",
      "11.9 action:  [-0.0823, -0.9912] n_targets:  1 reward:  62.76\n",
      "13.1 action:  [-0.0732, -0.9905] n_targets:  1 reward:  55.73\n",
      "13.7 action:  [-0.1184, -0.993] n_targets:  1 reward:  51.83\n",
      "14.1 action:  [-0.0754, -0.9903] n_targets:  2 reward:  113.83\n",
      "14.7 action:  [-0.0899, -0.9919] n_targets:  1 reward:  58.93\n",
      "15.9 action:  [-0.0683, -0.9901] n_targets:  1 reward:  52.55\n",
      "19.7 action:  [-0.069, -0.9908] n_targets:  3 reward:  156.44\n",
      "20.1 action:  [-0.0687, -0.9901] n_targets:  1 reward:  50.91\n",
      "22.3 action:  [-0.0812, -0.9911] n_targets:  1 reward:  51.37\n",
      "25.3 action:  [-0.1095, -0.9931] n_targets:  1 reward:  51.2\n",
      "25.7 action:  [-0.0761, -0.9911] n_targets:  1 reward:  50.61\n",
      "26.9 action:  [-0.0934, -0.9909] n_targets:  1 reward:  61.49\n",
      "27.9 action:  [-0.0505, -0.9899] n_targets:  1 reward:  57.24\n",
      "28.1 action:  [-0.0514, -0.9899] n_targets:  1 reward:  55.32\n",
      "28.3 action:  [-0.0416, -0.9886] n_targets:  1 reward:  52.48\n",
      "30.1 action:  [-0.0665, -0.9904] n_targets:  1 reward:  51.26\n",
      "31.3 action:  [-0.1061, -0.9919] n_targets:  1 reward:  58.45\n",
      "31.9 action:  [-0.1218, -0.9927] n_targets:  1 reward:  55.84\n",
      "32.1 action:  [-0.0977, -0.9919] n_targets:  1 reward:  56.45\n",
      "32.9 action:  [-0.0726, -0.991] n_targets:  1 reward:  55.25\n",
      "33.3 action:  [-0.0669, -0.9905] n_targets:  2 reward:  107.62\n",
      "34.7 action:  [-0.0743, -0.9906] n_targets:  1 reward:  54.81\n",
      "36.3 action:  [-0.0602, -0.9902] n_targets:  1 reward:  57.92\n",
      "38.1 action:  [-0.1197, -0.9931] n_targets:  2 reward:  109.05\n",
      "40.3 action:  [-0.0749, -0.9908] n_targets:  2 reward:  109.42\n",
      "40.5 action:  [-0.1092, -0.9927] n_targets:  1 reward:  78.7\n",
      "41.3 action:  [-0.1056, -0.9934] n_targets:  1 reward:  59.01\n",
      "41.7 action:  [-0.1024, -0.9931] n_targets:  1 reward:  50.78\n",
      "43.3 action:  [-0.1048, -0.993] n_targets:  1 reward:  57.83\n",
      "47.9 action:  [-0.0804, -0.9909] n_targets:  1 reward:  50.44\n",
      "53.5 action:  [-0.077, -0.9904] n_targets:  1 reward:  55.89\n",
      "54.5 action:  [-0.0894, -0.9918] n_targets:  1 reward:  50.71\n",
      "55.9 action:  [-0.0563, -0.9898] n_targets:  1 reward:  51.25\n",
      "56.3 action:  [-0.1059, -0.9927] n_targets:  1 reward:  53.6\n",
      "57.9 action:  [-0.0637, -0.9902] n_targets:  1 reward:  55.59\n",
      "58.5 action:  [-0.068, -0.9902] n_targets:  1 reward:  59.71\n",
      "58.9 action:  [-0.114, -0.9925] n_targets:  1 reward:  58.95\n",
      "59.5 action:  [-0.0606, -0.991] n_targets:  1 reward:  51.1\n",
      "61.3 action:  [-0.0473, -0.9895] n_targets:  1 reward:  50.23\n",
      "61.5 action:  [-0.0448, -0.9893] n_targets:  1 reward:  50.91\n",
      "61.7 action:  [-0.0279, -0.9889] n_targets:  1 reward:  59.84\n",
      "64.3 action:  [-0.0734, -0.9905] n_targets:  1 reward:  56.92\n",
      "67.3 action:  [-0.1107, -0.9926] n_targets:  1 reward:  58.13\n",
      "69.5 action:  [-0.1161, -0.9928] n_targets:  1 reward:  57.56\n",
      "70.1 action:  [-0.0989, -0.9914] n_targets:  1 reward:  58.13\n",
      "70.3 action:  [-0.0699, -0.9902] n_targets:  2 reward:  113.29\n",
      "70.5 action:  [-0.1077, -0.9924] n_targets:  1 reward:  55.97\n",
      "72.1 action:  [-0.1165, -0.9926] n_targets:  1 reward:  58.63\n",
      "72.3 action:  [-0.063, -0.9902] n_targets:  1 reward:  50.78\n",
      "73.3 action:  [-0.1196, -0.9929] n_targets:  1 reward:  60.22\n",
      "73.5 action:  [-0.1189, -0.9929] n_targets:  1 reward:  55.47\n",
      "75.1 action:  [-0.1001, -0.9924] n_targets:  1 reward:  53.39\n",
      "77.1 action:  [-0.0796, -0.9904] n_targets:  1 reward:  50.27\n",
      "78.5 action:  [-0.054, -0.9907] n_targets:  2 reward:  106.97\n",
      "78.7 action:  [-0.0528, -0.9906] n_targets:  1 reward:  55.02\n",
      "80.3 action:  [-0.0573, -0.9898] n_targets:  1 reward:  51.51\n",
      "80.5 action:  [-0.0599, -0.9899] n_targets:  1 reward:  55.5\n",
      "85.5 action:  [-0.1107, -0.9922] n_targets:  1 reward:  59.48\n",
      "86.1 action:  [-0.0798, -0.9907] n_targets:  1 reward:  51.12\n",
      "88.9 action:  [-0.0749, -0.9908] n_targets:  1 reward:  53.68\n",
      "89.7 action:  [-0.119, -0.9929] n_targets:  1 reward:  59.58\n",
      "89.9 action:  [-0.1157, -0.9928] n_targets:  1 reward:  53.76\n",
      "91.3 action:  [-0.103, -0.9924] n_targets:  1 reward:  54.88\n",
      "91.7 action:  [-0.0765, -0.9909] n_targets:  1 reward:  56.8\n",
      "92.1 action:  [-0.1181, -0.993] n_targets:  1 reward:  53.9\n",
      "94.5 action:  [-0.08, -0.9907] n_targets:  1 reward:  50.4\n",
      "95.3 action:  [-0.0616, -0.9897] n_targets:  1 reward:  50.33\n",
      "96.3 action:  [-0.1026, -0.9924] n_targets:  1 reward:  94.41\n",
      "96.7 action:  [-0.1067, -0.9924] n_targets:  1 reward:  50.58\n",
      "97.1 action:  [-0.1106, -0.9931] n_targets:  1 reward:  58.52\n",
      "97.9 action:  [-0.0585, -0.99] n_targets:  2 reward:  109.33\n",
      "98.3 action:  [-0.0618, -0.9901] n_targets:  1 reward:  53.18\n",
      "98.5 action:  [-0.0598, -0.99] n_targets:  1 reward:  61.78\n",
      "Best average reward: 4981.922852198283, Current average reward: 5070.855417569475\n",
      "Best average reward = 5070.855417569475\n",
      "Best model saved at episode 90 to None\n",
      "Evaluation rewards: [0.0, 0.0, 1720.9753100077312, 1085.8211957613628, 4981.922852198283, 5070.855417569475]\n",
      "Episode: 90, Episode Reward: 4912.86373647054\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  258\n",
      "1.5 action:  [0.1228, -0.9959] n_targets:  1 reward:  74.75\n",
      "1.7 action:  [-0.0301, -0.9922] n_targets:  1 reward:  59.18\n",
      "2.9 action:  [-0.1923, -0.9801] n_targets:  1 reward:  55.52\n",
      "4.5 action:  [0.3941, -0.9938] n_targets:  1 reward:  51.49\n",
      "6.5 action:  [0.0498, -0.9858] n_targets:  1 reward:  67.93\n",
      "8.9 action:  [0.038, -0.9977] n_targets:  1 reward:  64.87\n",
      "9.9 action:  [-0.0376, -0.9841] n_targets:  1 reward:  53.74\n",
      "13.5 action:  [-0.2593, -0.9949] n_targets:  1 reward:  55.08\n",
      "14.3 action:  [-0.1615, -0.9984] n_targets:  1 reward:  52.22\n",
      "16.2 action:  [0.0841, -0.9865] n_targets:  1 reward:  51.63\n",
      "16.6 action:  [-0.0941, -0.9918] n_targets:  1 reward:  50.92\n",
      "17.1 action:  [0.0425, -0.9941] n_targets:  1 reward:  57.76\n",
      "18.7 action:  [0.0185, -0.9986] n_targets:  1 reward:  59.06\n",
      "19.9 action:  [-0.096, -0.983] n_targets:  1 reward:  55.64\n",
      "20.1 action:  [0.2885, -0.9945] n_targets:  1 reward:  53.34\n",
      "20.3 action:  [0.0538, -0.9969] n_targets:  1 reward:  58.1\n",
      "21.1 action:  [-0.2294, -0.9933] n_targets:  1 reward:  53.49\n",
      "21.9 action:  [0.0718, -0.996] n_targets:  1 reward:  54.66\n",
      "24.1 action:  [-0.1293, -0.9973] n_targets:  1 reward:  62.69\n",
      "25.1 action:  [-0.0818, -0.988] n_targets:  1 reward:  61.62\n",
      "31.1 action:  [0.2054, -0.9853] n_targets:  1 reward:  50.84\n",
      "32.5 action:  [-0.1513, -0.9882] n_targets:  1 reward:  61.23\n",
      "33.5 action:  [0.1318, -0.9909] n_targets:  1 reward:  55.28\n",
      "33.9 action:  [0.0488, -0.987] n_targets:  1 reward:  59.38\n",
      "34.1 action:  [-0.2245, -0.9969] n_targets:  1 reward:  55.28\n",
      "34.3 action:  [-0.2412, -0.9949] n_targets:  1 reward:  56.56\n",
      "35.1 action:  [0.0903, -0.9871] n_targets:  1 reward:  50.73\n",
      "36.3 action:  [-0.0118, -0.9817] n_targets:  1 reward:  50.28\n",
      "37.5 action:  [0.125, -0.983] n_targets:  1 reward:  69.59\n",
      "38.1 action:  [-0.225, -0.9865] n_targets:  2 reward:  164.35\n",
      "38.9 action:  [-0.0765, -0.9934] n_targets:  1 reward:  58.18\n",
      "39.9 action:  [0.1407, -0.9971] n_targets:  1 reward:  51.91\n",
      "44.7 action:  [0.0381, -0.997] n_targets:  1 reward:  58.26\n",
      "45.1 action:  [0.3052, -0.9864] n_targets:  1 reward:  53.85\n",
      "45.3 action:  [-0.0341, -0.9899] n_targets:  1 reward:  56.66\n",
      "46.9 action:  [-0.0197, -0.9924] n_targets:  1 reward:  55.11\n",
      "47.5 action:  [-0.084, -0.9802] n_targets:  1 reward:  53.36\n",
      "47.7 action:  [-0.0567, -0.9879] n_targets:  2 reward:  110.96\n",
      "52.9 action:  [0.0689, -0.9981] n_targets:  2 reward:  114.37\n",
      "54.1 action:  [0.3178, -0.999] n_targets:  1 reward:  52.05\n",
      "56.1 action:  [0.0333, -0.9995] n_targets:  3 reward:  167.54\n",
      "60.9 action:  [-0.175, -0.9974] n_targets:  1 reward:  58.97\n",
      "62.1 action:  [-0.1769, -0.9946] n_targets:  2 reward:  106.98\n",
      "62.9 action:  [0.1566, -0.9958] n_targets:  1 reward:  52.49\n",
      "63.9 action:  [-0.4138, -0.9869] n_targets:  2 reward:  124.51\n",
      "64.8 action:  [-0.1162, -0.9942] n_targets:  1 reward:  84.79\n",
      "65.2 action:  [-0.0858, -0.9903] n_targets:  1 reward:  51.48\n",
      "68.8 action:  [-0.0087, -0.997] n_targets:  1 reward:  51.34\n",
      "69.6 action:  [0.0083, -0.9877] n_targets:  1 reward:  60.48\n",
      "70.6 action:  [0.0254, -0.9881] n_targets:  1 reward:  54.39\n",
      "70.8 action:  [-0.1361, -0.9922] n_targets:  2 reward:  108.89\n",
      "74.4 action:  [0.0865, -0.9946] n_targets:  1 reward:  50.01\n",
      "81.0 action:  [-0.0347, -0.9971] n_targets:  1 reward:  56.23\n",
      "82.2 action:  [-0.0953, -0.9989] n_targets:  1 reward:  54.91\n",
      "82.6 action:  [0.1096, -0.998] n_targets:  1 reward:  50.54\n",
      "83.2 action:  [-0.0477, -0.9917] n_targets:  1 reward:  55.23\n",
      "83.8 action:  [0.1145, -0.9889] n_targets:  1 reward:  53.56\n",
      "84.4 action:  [-0.2623, -0.9963] n_targets:  2 reward:  179.88\n",
      "84.6 action:  [-0.1321, -0.9902] n_targets:  1 reward:  59.9\n",
      "85.7 action:  [-0.0864, -0.9904] n_targets:  1 reward:  56.28\n",
      "86.3 action:  [0.0105, -0.9965] n_targets:  1 reward:  53.88\n",
      "89.6 action:  [0.1659, -0.9874] n_targets:  1 reward:  59.27\n",
      "91.3 action:  [-0.1559, -0.9872] n_targets:  1 reward:  52.44\n",
      "91.7 action:  [-0.1043, -0.9928] n_targets:  1 reward:  60.39\n",
      "92.3 action:  [-0.1955, -0.9873] n_targets:  1 reward:  53.42\n",
      "93.1 action:  [-0.135, -0.9868] n_targets:  1 reward:  53.82\n",
      "94.3 action:  [0.2083, -0.9956] n_targets:  1 reward:  61.76\n",
      "95.1 action:  [0.0184, -0.9824] n_targets:  2 reward:  101.7\n",
      "98.1 action:  [-0.3907, -0.9841] n_targets:  1 reward:  58.71\n",
      "99.3 action:  [-0.0127, -0.9855] n_targets:  1 reward:  63.77\n",
      "ALPHA (entropy-related):  tensor([0.2103], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.22428 0.22241 0.22068 0.21899 0.21737 0.21588 0.21455 0.21316 0.21174\n",
      " 0.21035]\n",
      "Episode: 91, Episode Reward: 4649.509638468424\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  259\n",
      "1.2 action:  [0.2328, -0.993] n_targets:  1 reward:  50.72\n",
      "2.8 action:  [-0.0455, -0.9862] n_targets:  1 reward:  84.02\n",
      "3.7 action:  [-0.0148, -0.9975] n_targets:  2 reward:  127.91\n",
      "4.1 action:  [0.1613, -0.9952] n_targets:  1 reward:  65.53\n",
      "5.1 action:  [-0.2934, -0.9865] n_targets:  1 reward:  55.6\n",
      "5.8 action:  [0.1186, -0.9978] n_targets:  1 reward:  62.45\n",
      "7.4 action:  [-0.2835, -0.9823] n_targets:  1 reward:  57.34\n",
      "8.6 action:  [-0.2905, -0.9802] n_targets:  1 reward:  53.85\n",
      "9.2 action:  [-0.2339, -0.9973] n_targets:  1 reward:  59.88\n",
      "10.6 action:  [-0.2076, -0.996] n_targets:  1 reward:  62.36\n",
      "12.2 action:  [-0.2393, -0.9932] n_targets:  1 reward:  57.05\n",
      "12.6 action:  [-0.0726, -0.9939] n_targets:  1 reward:  55.39\n",
      "13.8 action:  [-0.0627, -0.9972] n_targets:  1 reward:  59.08\n",
      "16.0 action:  [-0.1207, -0.9939] n_targets:  1 reward:  53.78\n",
      "16.2 action:  [-0.0495, -0.9879] n_targets:  1 reward:  64.48\n",
      "17.2 action:  [-0.1866, -0.9969] n_targets:  1 reward:  50.16\n",
      "19.0 action:  [0.0465, -0.9928] n_targets:  1 reward:  59.46\n",
      "20.8 action:  [-0.2948, -0.9943] n_targets:  1 reward:  55.27\n",
      "24.8 action:  [-0.327, -0.9823] n_targets:  1 reward:  67.82\n",
      "25.0 action:  [-0.0912, -0.9872] n_targets:  1 reward:  76.06\n",
      "25.2 action:  [-0.088, -0.9912] n_targets:  1 reward:  60.09\n",
      "25.8 action:  [0.1413, -0.9888] n_targets:  1 reward:  50.43\n",
      "26.2 action:  [-0.1768, -0.998] n_targets:  1 reward:  58.24\n",
      "27.2 action:  [-0.0768, -0.9968] n_targets:  1 reward:  63.38\n",
      "30.5 action:  [0.0504, -0.9917] n_targets:  1 reward:  54.05\n",
      "30.9 action:  [0.1924, -0.982] n_targets:  1 reward:  54.48\n",
      "33.0 action:  [0.1332, -0.9991] n_targets:  3 reward:  216.01\n",
      "35.2 action:  [-0.1039, -0.9966] n_targets:  2 reward:  116.13\n",
      "37.4 action:  [-0.0845, -0.9936] n_targets:  1 reward:  58.87\n",
      "37.6 action:  [-0.1332, -0.9934] n_targets:  2 reward:  104.48\n",
      "38.9 action:  [-0.1695, -0.9938] n_targets:  1 reward:  53.97\n",
      "45.0 action:  [0.0322, -0.9864] n_targets:  1 reward:  58.44\n",
      "45.2 action:  [0.3418, -0.9855] n_targets:  1 reward:  50.78\n",
      "46.2 action:  [-0.2073, -0.9945] n_targets:  1 reward:  50.2\n",
      "47.4 action:  [-0.1081, -0.9966] n_targets:  1 reward:  57.84\n",
      "48.2 action:  [0.1459, -0.989] n_targets:  1 reward:  58.35\n",
      "49.4 action:  [-0.0269, -0.9954] n_targets:  1 reward:  50.09\n",
      "50.8 action:  [-0.1358, -0.9884] n_targets:  1 reward:  53.03\n",
      "51.0 action:  [-0.185, -0.9903] n_targets:  1 reward:  51.01\n",
      "52.0 action:  [-0.2793, -0.9831] n_targets:  1 reward:  51.59\n",
      "53.4 action:  [0.3925, -0.9933] n_targets:  1 reward:  56.91\n",
      "53.6 action:  [-0.0094, -0.9823] n_targets:  1 reward:  54.39\n",
      "53.8 action:  [0.2303, -0.9901] n_targets:  1 reward:  52.56\n",
      "54.0 action:  [-0.0374, -0.9954] n_targets:  1 reward:  61.74\n",
      "54.6 action:  [0.0882, -0.9967] n_targets:  2 reward:  138.67\n",
      "55.0 action:  [-0.1521, -0.9948] n_targets:  2 reward:  122.65\n",
      "55.8 action:  [-0.1568, -0.9968] n_targets:  1 reward:  58.11\n",
      "56.4 action:  [0.2034, -0.9812] n_targets:  1 reward:  55.16\n",
      "56.8 action:  [-0.3862, -0.9829] n_targets:  1 reward:  51.1\n",
      "58.2 action:  [0.1538, -0.9872] n_targets:  2 reward:  127.97\n",
      "59.2 action:  [0.0051, -0.9869] n_targets:  1 reward:  52.13\n",
      "60.2 action:  [-0.0924, -0.9967] n_targets:  1 reward:  81.2\n",
      "61.6 action:  [-0.1734, -0.9956] n_targets:  1 reward:  50.6\n",
      "61.8 action:  [-0.1285, -0.9915] n_targets:  1 reward:  53.86\n",
      "65.2 action:  [-0.2531, -0.9855] n_targets:  1 reward:  60.86\n",
      "67.8 action:  [0.0985, -0.9982] n_targets:  1 reward:  59.6\n",
      "69.2 action:  [0.0216, -0.9942] n_targets:  1 reward:  61.03\n",
      "70.0 action:  [-0.1014, -0.9946] n_targets:  1 reward:  52.86\n",
      "70.2 action:  [-0.1448, -0.9988] n_targets:  1 reward:  54.17\n",
      "70.8 action:  [-0.195, -0.999] n_targets:  1 reward:  51.01\n",
      "73.5 action:  [-0.077, -0.9959] n_targets:  1 reward:  57.16\n",
      "74.3 action:  [-0.1616, -0.9895] n_targets:  1 reward:  64.17\n",
      "74.9 action:  [-0.2531, -0.983] n_targets:  1 reward:  62.58\n",
      "75.1 action:  [0.1197, -0.9854] n_targets:  1 reward:  60.52\n",
      "76.1 action:  [-0.1154, -0.9968] n_targets:  1 reward:  54.01\n",
      "76.3 action:  [0.275, -0.9893] n_targets:  1 reward:  53.65\n",
      "77.7 action:  [-0.2168, -0.993] n_targets:  1 reward:  52.11\n",
      "77.9 action:  [-0.1339, -0.9956] n_targets:  1 reward:  52.3\n",
      "80.9 action:  [-0.3008, -0.9835] n_targets:  1 reward:  53.43\n",
      "82.7 action:  [-0.0825, -0.9892] n_targets:  2 reward:  108.5\n",
      "83.1 action:  [0.0628, -0.9946] n_targets:  1 reward:  57.53\n",
      "83.5 action:  [0.2599, -0.9964] n_targets:  2 reward:  102.79\n",
      "84.5 action:  [0.002, -0.9952] n_targets:  1 reward:  54.76\n",
      "86.8 action:  [-0.1453, -0.9979] n_targets:  1 reward:  58.05\n",
      "88.8 action:  [-0.048, -0.9903] n_targets:  1 reward:  57.28\n",
      "89.8 action:  [0.3135, -0.9975] n_targets:  1 reward:  80.63\n",
      "90.8 action:  [-0.1494, -0.9979] n_targets:  1 reward:  50.99\n",
      "92.2 action:  [-0.0596, -0.991] n_targets:  1 reward:  58.61\n",
      "92.6 action:  [-0.1182, -0.9983] n_targets:  2 reward:  127.75\n",
      "93.8 action:  [-0.1212, -0.9943] n_targets:  1 reward:  55.85\n",
      "94.8 action:  [-0.2548, -0.998] n_targets:  1 reward:  52.78\n",
      "95.3 action:  [-0.0387, -0.9908] n_targets:  1 reward:  54.59\n",
      "99.9 action:  [-0.158, -0.9963] n_targets:  1 reward:  57.37\n",
      "100.3 action:  [0.0477, -0.9944] n_targets:  1 reward:  56.92\n",
      "100.5 action:  [-0.1122, -0.9948] n_targets:  2 reward:  106.15\n",
      "101.1 action:  [-0.1518, -0.9941] n_targets:  1 reward:  56.49\n",
      "ALPHA (entropy-related):  tensor([0.2088], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.22241 0.22068 0.21899 0.21737 0.21588 0.21455 0.21316 0.21174 0.21035\n",
      " 0.2088 ]\n",
      "Episode: 92, Episode Reward: 5713.230514526367\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  260\n",
      "0.2 action:  [0.1926, -0.9937] n_targets:  2 reward:  141.13\n",
      "1.2 action:  [0.1806, -0.9914] n_targets:  1 reward:  52.94\n",
      "1.8 action:  [-0.1434, -0.9904] n_targets:  1 reward:  56.26\n",
      "2.4 action:  [-0.1962, -0.9807] n_targets:  1 reward:  62.72\n",
      "5.7 action:  [-0.0163, -0.9973] n_targets:  1 reward:  52.34\n",
      "6.7 action:  [0.0586, -0.9965] n_targets:  1 reward:  53.36\n",
      "7.3 action:  [0.3747, -0.9967] n_targets:  1 reward:  59.92\n",
      "8.8 action:  [0.1787, -0.9981] n_targets:  1 reward:  84.06\n",
      "9.6 action:  [0.0389, -0.9949] n_targets:  2 reward:  112.49\n",
      "10.0 action:  [-0.0072, -0.9957] n_targets:  1 reward:  58.59\n",
      "11.4 action:  [0.2486, -0.9976] n_targets:  1 reward:  59.12\n",
      "12.2 action:  [-0.2031, -0.9944] n_targets:  2 reward:  130.36\n",
      "16.2 action:  [-0.0757, -0.9842] n_targets:  1 reward:  67.71\n",
      "17.8 action:  [-0.1748, -0.9979] n_targets:  1 reward:  51.8\n",
      "19.2 action:  [0.1878, -0.9978] n_targets:  1 reward:  54.1\n",
      "20.8 action:  [-0.1527, -0.9915] n_targets:  1 reward:  51.61\n",
      "23.0 action:  [0.3267, -0.9845] n_targets:  1 reward:  53.08\n",
      "23.8 action:  [-0.1092, -0.9959] n_targets:  1 reward:  54.99\n",
      "24.4 action:  [0.1141, -0.9992] n_targets:  1 reward:  63.4\n",
      "24.8 action:  [0.0046, -0.9942] n_targets:  1 reward:  55.64\n",
      "26.0 action:  [0.3315, -0.9918] n_targets:  1 reward:  51.02\n",
      "29.8 action:  [0.0953, -0.9927] n_targets:  2 reward:  111.63\n",
      "30.2 action:  [-0.1743, -0.994] n_targets:  2 reward:  110.95\n",
      "30.4 action:  [-0.0994, -0.9941] n_targets:  1 reward:  53.97\n",
      "30.6 action:  [0.3283, -0.9964] n_targets:  1 reward:  58.69\n",
      "31.5 action:  [-0.0701, -0.9866] n_targets:  1 reward:  68.13\n",
      "33.2 action:  [0.094, -0.9938] n_targets:  1 reward:  61.58\n",
      "34.9 action:  [0.1564, -0.9888] n_targets:  1 reward:  56.17\n",
      "36.1 action:  [-0.1269, -0.9868] n_targets:  1 reward:  56.08\n",
      "36.5 action:  [-0.1008, -0.9872] n_targets:  1 reward:  56.8\n",
      "37.5 action:  [-0.0059, -0.9935] n_targets:  1 reward:  63.51\n",
      "38.4 action:  [0.0662, -0.9981] n_targets:  1 reward:  56.65\n",
      "39.0 action:  [-0.2874, -0.9961] n_targets:  1 reward:  53.38\n",
      "41.4 action:  [0.1034, -0.9953] n_targets:  1 reward:  73.04\n",
      "41.8 action:  [-0.0513, -0.998] n_targets:  1 reward:  58.1\n",
      "44.9 action:  [0.1308, -0.9944] n_targets:  1 reward:  63.15\n",
      "45.7 action:  [-0.2125, -0.9934] n_targets:  1 reward:  50.78\n",
      "47.1 action:  [0.0408, -0.9848] n_targets:  1 reward:  52.8\n",
      "48.1 action:  [0.0804, -0.9893] n_targets:  1 reward:  59.65\n",
      "49.2 action:  [0.1347, -0.9979] n_targets:  1 reward:  59.9\n",
      "50.4 action:  [0.2255, -0.9983] n_targets:  1 reward:  52.94\n",
      "52.1 action:  [-0.0351, -0.9957] n_targets:  1 reward:  50.51\n",
      "53.3 action:  [0.166, -0.9849] n_targets:  1 reward:  57.8\n",
      "53.5 action:  [0.2053, -0.9958] n_targets:  1 reward:  54.03\n",
      "54.3 action:  [0.1586, -0.9939] n_targets:  1 reward:  53.37\n",
      "54.9 action:  [-0.0071, -0.9946] n_targets:  2 reward:  113.58\n",
      "56.4 action:  [0.2542, -0.9965] n_targets:  1 reward:  56.04\n",
      "57.2 action:  [0.0333, -0.9866] n_targets:  1 reward:  61.63\n",
      "58.6 action:  [-0.1459, -0.9996] n_targets:  1 reward:  50.86\n",
      "61.0 action:  [-0.1604, -0.9963] n_targets:  1 reward:  56.68\n",
      "61.6 action:  [-0.2329, -0.994] n_targets:  1 reward:  55.78\n",
      "62.8 action:  [-0.369, -0.9977] n_targets:  1 reward:  72.22\n",
      "64.0 action:  [-0.1209, -0.9887] n_targets:  1 reward:  91.36\n",
      "64.2 action:  [-0.2287, -0.9861] n_targets:  1 reward:  51.96\n",
      "65.1 action:  [0.0538, -0.9939] n_targets:  1 reward:  53.54\n",
      "66.1 action:  [-0.1757, -0.9941] n_targets:  1 reward:  61.23\n",
      "67.3 action:  [0.3133, -0.9933] n_targets:  1 reward:  59.85\n",
      "68.1 action:  [-0.0019, -0.9872] n_targets:  1 reward:  54.16\n",
      "68.7 action:  [-0.0885, -0.9806] n_targets:  1 reward:  64.37\n",
      "70.6 action:  [-0.1507, -0.9974] n_targets:  1 reward:  50.15\n",
      "71.0 action:  [0.2023, -0.9934] n_targets:  1 reward:  51.79\n",
      "71.2 action:  [-0.2334, -0.9983] n_targets:  1 reward:  50.62\n",
      "73.6 action:  [0.0081, -0.9957] n_targets:  1 reward:  57.86\n",
      "74.3 action:  [-0.0879, -0.9968] n_targets:  1 reward:  62.1\n",
      "75.9 action:  [0.2744, -0.9972] n_targets:  2 reward:  105.71\n",
      "80.7 action:  [-0.3396, -0.9947] n_targets:  1 reward:  53.33\n",
      "81.5 action:  [0.1697, -0.9837] n_targets:  2 reward:  118.96\n",
      "82.5 action:  [0.0932, -0.9986] n_targets:  1 reward:  55.67\n",
      "83.5 action:  [-0.168, -0.9934] n_targets:  1 reward:  53.14\n",
      "84.1 action:  [-0.1343, -0.9982] n_targets:  1 reward:  51.9\n",
      "88.5 action:  [0.0598, -0.9986] n_targets:  1 reward:  50.06\n",
      "88.7 action:  [0.0689, -0.9934] n_targets:  2 reward:  102.02\n",
      "88.9 action:  [0.2924, -0.9939] n_targets:  1 reward:  53.77\n",
      "90.8 action:  [-0.1528, -0.9959] n_targets:  1 reward:  69.79\n",
      "91.6 action:  [-0.0379, -0.9958] n_targets:  1 reward:  50.66\n",
      "93.2 action:  [-0.1241, -0.9953] n_targets:  1 reward:  51.18\n",
      "97.0 action:  [0.1373, -0.9949] n_targets:  1 reward:  61.47\n",
      "97.6 action:  [-0.121, -0.9853] n_targets:  1 reward:  62.3\n",
      "98.0 action:  [0.2425, -0.9803] n_targets:  1 reward:  68.91\n",
      "98.6 action:  [0.1878, -0.9854] n_targets:  1 reward:  56.1\n",
      "101.4 action:  [-0.3173, -0.9859] n_targets:  1 reward:  51.78\n",
      "ALPHA (entropy-related):  tensor([0.2070], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.22068 0.21899 0.21737 0.21588 0.21455 0.21316 0.21174 0.21035 0.2088\n",
      " 0.20702]\n",
      "Episode: 93, Episode Reward: 5212.789682388305\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  261\n",
      "0.4 action:  [0.0688, -0.9976] n_targets:  2 reward:  163.98\n",
      "3.2 action:  [0.1907, -0.9815] n_targets:  1 reward:  51.28\n",
      "8.5 action:  [0.0721, -0.9982] n_targets:  2 reward:  109.66\n",
      "9.3 action:  [-0.1579, -0.9988] n_targets:  1 reward:  61.3\n",
      "10.1 action:  [-0.2072, -0.9935] n_targets:  1 reward:  77.34\n",
      "11.5 action:  [-0.334, -0.9962] n_targets:  1 reward:  77.18\n",
      "12.9 action:  [-0.2653, -0.9849] n_targets:  1 reward:  55.92\n",
      "14.7 action:  [-0.0237, -0.9848] n_targets:  1 reward:  50.37\n",
      "15.5 action:  [0.1652, -0.9925] n_targets:  2 reward:  119.92\n",
      "15.7 action:  [0.1545, -0.9948] n_targets:  1 reward:  52.71\n",
      "16.9 action:  [-0.0951, -0.9949] n_targets:  1 reward:  58.58\n",
      "19.7 action:  [-0.0093, -0.9938] n_targets:  1 reward:  52.57\n",
      "22.9 action:  [-0.1672, -0.9987] n_targets:  1 reward:  50.66\n",
      "23.3 action:  [0.0648, -0.9889] n_targets:  2 reward:  102.15\n",
      "24.1 action:  [-0.2433, -0.9935] n_targets:  1 reward:  53.32\n",
      "25.7 action:  [0.0317, -0.9932] n_targets:  2 reward:  110.4\n",
      "26.1 action:  [0.0733, -0.9972] n_targets:  1 reward:  54.88\n",
      "26.9 action:  [-0.0, -0.9929] n_targets:  1 reward:  63.37\n",
      "27.1 action:  [-0.2796, -0.9918] n_targets:  1 reward:  59.54\n",
      "31.6 action:  [-0.0159, -0.9936] n_targets:  2 reward:  136.0\n",
      "34.4 action:  [-0.108, -0.9956] n_targets:  1 reward:  51.24\n",
      "36.5 action:  [0.1392, -0.9949] n_targets:  1 reward:  63.84\n",
      "37.5 action:  [0.1184, -0.9828] n_targets:  1 reward:  53.23\n",
      "37.9 action:  [0.0621, -0.9966] n_targets:  1 reward:  50.69\n",
      "39.1 action:  [-0.2404, -0.9934] n_targets:  1 reward:  56.55\n",
      "39.5 action:  [-0.4002, -0.9884] n_targets:  1 reward:  52.39\n",
      "42.1 action:  [-0.0904, -0.9913] n_targets:  1 reward:  72.73\n",
      "42.5 action:  [0.0127, -0.9952] n_targets:  1 reward:  52.71\n",
      "43.5 action:  [0.0437, -0.9979] n_targets:  1 reward:  53.32\n",
      "44.9 action:  [-0.25, -0.9915] n_targets:  1 reward:  52.55\n",
      "45.5 action:  [-0.096, -0.999] n_targets:  1 reward:  50.74\n",
      "46.9 action:  [-0.0708, -0.9935] n_targets:  2 reward:  113.2\n",
      "47.3 action:  [-0.0561, -0.9944] n_targets:  1 reward:  56.14\n",
      "51.1 action:  [0.1228, -0.9916] n_targets:  1 reward:  55.3\n",
      "51.5 action:  [0.1906, -0.9932] n_targets:  1 reward:  50.14\n",
      "52.3 action:  [-0.1685, -0.9991] n_targets:  1 reward:  55.03\n",
      "53.9 action:  [0.0295, -0.9946] n_targets:  1 reward:  57.26\n",
      "56.7 action:  [0.0173, -0.9952] n_targets:  2 reward:  114.31\n",
      "56.9 action:  [-0.1151, -0.9956] n_targets:  1 reward:  61.95\n",
      "57.1 action:  [-0.2934, -0.9861] n_targets:  1 reward:  53.36\n",
      "57.3 action:  [0.2191, -0.9909] n_targets:  1 reward:  56.15\n",
      "58.1 action:  [-0.0409, -0.9923] n_targets:  1 reward:  61.1\n",
      "61.7 action:  [0.2668, -0.9937] n_targets:  1 reward:  62.46\n",
      "61.9 action:  [-0.0994, -0.9918] n_targets:  1 reward:  54.88\n",
      "63.1 action:  [-0.1536, -0.9966] n_targets:  2 reward:  103.08\n",
      "63.9 action:  [0.0819, -0.9921] n_targets:  1 reward:  57.49\n",
      "64.9 action:  [-0.1636, -0.9974] n_targets:  1 reward:  53.64\n",
      "67.9 action:  [-0.1182, -0.9925] n_targets:  1 reward:  63.19\n",
      "68.9 action:  [0.3038, -0.9872] n_targets:  1 reward:  66.29\n",
      "69.7 action:  [0.1373, -0.9977] n_targets:  2 reward:  121.21\n",
      "70.9 action:  [-0.1203, -0.9907] n_targets:  1 reward:  66.16\n",
      "71.5 action:  [-0.1245, -0.9891] n_targets:  1 reward:  51.57\n",
      "72.1 action:  [0.2279, -0.9991] n_targets:  1 reward:  50.26\n",
      "72.7 action:  [0.1816, -0.9875] n_targets:  1 reward:  63.77\n",
      "73.5 action:  [-0.0144, -0.9939] n_targets:  1 reward:  55.9\n",
      "74.5 action:  [0.0839, -0.989] n_targets:  1 reward:  71.19\n",
      "76.5 action:  [0.2712, -0.9992] n_targets:  1 reward:  55.88\n",
      "76.9 action:  [0.0479, -0.9951] n_targets:  1 reward:  53.52\n",
      "77.7 action:  [0.2962, -0.9899] n_targets:  2 reward:  132.77\n",
      "78.3 action:  [0.0974, -0.9938] n_targets:  1 reward:  60.91\n",
      "78.9 action:  [-0.1395, -0.9982] n_targets:  1 reward:  51.04\n",
      "79.3 action:  [-0.0557, -0.9967] n_targets:  1 reward:  51.05\n",
      "79.7 action:  [0.1201, -0.9974] n_targets:  1 reward:  56.82\n",
      "80.1 action:  [-0.0888, -0.9981] n_targets:  1 reward:  70.72\n",
      "80.5 action:  [-0.0161, -0.986] n_targets:  1 reward:  61.39\n",
      "81.5 action:  [-0.0681, -0.9969] n_targets:  2 reward:  112.5\n",
      "85.3 action:  [-0.0907, -0.9985] n_targets:  1 reward:  50.93\n",
      "85.9 action:  [0.1495, -0.998] n_targets:  1 reward:  57.6\n",
      "86.5 action:  [0.2038, -0.9955] n_targets:  1 reward:  52.14\n",
      "86.7 action:  [0.1987, -0.9938] n_targets:  1 reward:  51.15\n",
      "86.9 action:  [-0.18, -0.9975] n_targets:  1 reward:  59.29\n",
      "88.5 action:  [-0.2842, -0.9961] n_targets:  1 reward:  50.89\n",
      "93.9 action:  [-0.0388, -0.9969] n_targets:  1 reward:  59.0\n",
      "96.1 action:  [-0.0756, -0.9955] n_targets:  1 reward:  51.37\n",
      "96.5 action:  [0.0545, -0.995] n_targets:  2 reward:  109.13\n",
      "97.9 action:  [-0.0197, -0.9947] n_targets:  1 reward:  54.17\n",
      "98.3 action:  [-0.0168, -0.99] n_targets:  1 reward:  54.99\n",
      "100.1 action:  [-0.0799, -0.9971] n_targets:  1 reward:  52.11\n",
      "ALPHA (entropy-related):  tensor([0.2055], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.21899 0.21737 0.21588 0.21455 0.21316 0.21174 0.21035 0.2088  0.20702\n",
      " 0.20546]\n",
      "Episode: 94, Episode Reward: 5255.534263610839\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  262\n",
      "0.3 action:  [-0.1948, -0.9839] n_targets:  1 reward:  61.92\n",
      "3.9 action:  [-0.172, -0.9933] n_targets:  1 reward:  52.36\n",
      "4.1 action:  [-0.1773, -0.9985] n_targets:  1 reward:  57.19\n",
      "5.3 action:  [0.05, -0.9984] n_targets:  1 reward:  53.47\n",
      "8.1 action:  [0.1554, -0.9834] n_targets:  2 reward:  109.2\n",
      "8.9 action:  [-0.0224, -0.9918] n_targets:  1 reward:  57.65\n",
      "10.5 action:  [-0.2492, -0.9819] n_targets:  1 reward:  51.28\n",
      "11.5 action:  [-0.1085, -0.9986] n_targets:  2 reward:  155.88\n",
      "11.7 action:  [0.1496, -0.998] n_targets:  1 reward:  60.76\n",
      "12.9 action:  [-0.082, -0.9976] n_targets:  1 reward:  64.04\n",
      "15.5 action:  [-0.0236, -0.9983] n_targets:  2 reward:  106.68\n",
      "16.5 action:  [-0.0757, -0.9983] n_targets:  1 reward:  70.33\n",
      "20.9 action:  [-0.1477, -0.9981] n_targets:  1 reward:  50.44\n",
      "21.9 action:  [-0.1634, -0.9811] n_targets:  1 reward:  51.43\n",
      "23.5 action:  [-0.3638, -0.9943] n_targets:  1 reward:  50.4\n",
      "24.7 action:  [0.0229, -0.994] n_targets:  2 reward:  108.5\n",
      "25.3 action:  [-0.0229, -0.99] n_targets:  1 reward:  62.2\n",
      "26.7 action:  [-0.3786, -0.9963] n_targets:  1 reward:  56.11\n",
      "26.9 action:  [0.0952, -0.9813] n_targets:  1 reward:  53.67\n",
      "28.5 action:  [-0.117, -0.9983] n_targets:  1 reward:  50.07\n",
      "31.7 action:  [-0.364, -0.9903] n_targets:  1 reward:  56.6\n",
      "34.7 action:  [-0.1706, -0.9976] n_targets:  1 reward:  54.92\n",
      "36.7 action:  [-0.1085, -0.9983] n_targets:  1 reward:  51.45\n",
      "37.3 action:  [0.1742, -0.9974] n_targets:  1 reward:  55.87\n",
      "39.5 action:  [-0.4209, -0.9853] n_targets:  1 reward:  51.32\n",
      "39.9 action:  [-0.2485, -0.9947] n_targets:  1 reward:  66.51\n",
      "40.3 action:  [0.4256, -0.9928] n_targets:  1 reward:  61.16\n",
      "41.3 action:  [0.1233, -0.9966] n_targets:  2 reward:  113.01\n",
      "42.1 action:  [0.1843, -0.9964] n_targets:  2 reward:  121.92\n",
      "43.3 action:  [-0.0898, -0.9963] n_targets:  1 reward:  52.63\n",
      "43.7 action:  [-0.394, -0.9874] n_targets:  1 reward:  61.75\n",
      "44.1 action:  [-0.2467, -0.9993] n_targets:  1 reward:  54.19\n",
      "44.3 action:  [0.0975, -0.9978] n_targets:  1 reward:  50.42\n",
      "51.5 action:  [-0.3558, -0.997] n_targets:  1 reward:  54.63\n",
      "55.1 action:  [-0.1567, -0.9922] n_targets:  1 reward:  50.74\n",
      "55.5 action:  [0.1064, -0.997] n_targets:  1 reward:  53.55\n",
      "56.3 action:  [-0.3541, -0.989] n_targets:  1 reward:  50.73\n",
      "57.1 action:  [0.2102, -0.9981] n_targets:  1 reward:  69.36\n",
      "58.1 action:  [-0.277, -0.9988] n_targets:  1 reward:  53.01\n",
      "58.5 action:  [-0.2404, -0.9953] n_targets:  1 reward:  57.8\n",
      "59.1 action:  [0.2389, -0.9851] n_targets:  1 reward:  54.21\n",
      "60.5 action:  [0.1167, -0.9988] n_targets:  1 reward:  57.41\n",
      "60.9 action:  [-0.0246, -0.9951] n_targets:  1 reward:  55.42\n",
      "61.1 action:  [0.0132, -0.9991] n_targets:  1 reward:  57.47\n",
      "62.3 action:  [-0.212, -0.9984] n_targets:  1 reward:  74.71\n",
      "62.9 action:  [-0.3322, -0.9909] n_targets:  1 reward:  55.65\n",
      "63.1 action:  [0.1306, -0.9825] n_targets:  1 reward:  51.93\n",
      "63.9 action:  [-0.0483, -0.9975] n_targets:  1 reward:  52.41\n",
      "64.5 action:  [-0.0294, -0.989] n_targets:  2 reward:  114.0\n",
      "65.1 action:  [0.135, -0.9969] n_targets:  1 reward:  53.66\n",
      "66.1 action:  [-0.1275, -0.99] n_targets:  1 reward:  54.93\n",
      "66.5 action:  [-0.0188, -0.9918] n_targets:  1 reward:  50.76\n",
      "67.3 action:  [-0.0514, -0.988] n_targets:  1 reward:  63.94\n",
      "67.7 action:  [-0.1807, -0.9905] n_targets:  1 reward:  53.51\n",
      "68.7 action:  [-0.2475, -0.9959] n_targets:  1 reward:  50.9\n",
      "72.1 action:  [-0.3299, -0.998] n_targets:  1 reward:  59.58\n",
      "72.9 action:  [0.0387, -0.9944] n_targets:  1 reward:  55.69\n",
      "75.2 action:  [-0.323, -0.9982] n_targets:  2 reward:  120.25\n",
      "77.4 action:  [0.031, -0.9884] n_targets:  1 reward:  56.15\n",
      "78.6 action:  [-0.1901, -0.9973] n_targets:  1 reward:  54.01\n",
      "80.0 action:  [-0.3158, -0.9852] n_targets:  1 reward:  57.86\n",
      "80.2 action:  [0.1672, -0.9952] n_targets:  1 reward:  50.85\n",
      "80.8 action:  [-0.2695, -0.9817] n_targets:  2 reward:  107.47\n",
      "82.8 action:  [0.0938, -0.9981] n_targets:  1 reward:  53.83\n",
      "83.8 action:  [0.0781, -0.9952] n_targets:  1 reward:  78.23\n",
      "84.2 action:  [-0.0211, -0.9925] n_targets:  1 reward:  52.26\n",
      "84.6 action:  [0.2632, -0.9961] n_targets:  1 reward:  58.42\n",
      "85.1 action:  [-0.2345, -0.9894] n_targets:  1 reward:  56.15\n",
      "86.3 action:  [-0.1123, -0.9882] n_targets:  1 reward:  50.69\n",
      "89.9 action:  [-0.271, -0.9863] n_targets:  1 reward:  51.12\n",
      "90.7 action:  [0.4554, -0.9946] n_targets:  1 reward:  54.13\n",
      "92.7 action:  [0.2066, -0.9979] n_targets:  1 reward:  55.23\n",
      "96.9 action:  [-0.0989, -0.9821] n_targets:  1 reward:  52.28\n",
      "100.4 action:  [-0.0525, -0.9845] n_targets:  1 reward:  54.65\n",
      "ALPHA (entropy-related):  tensor([0.2041], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.21737 0.21588 0.21455 0.21316 0.21174 0.21035 0.2088  0.20702 0.20546\n",
      " 0.20408]\n",
      "Episode: 95, Episode Reward: 4704.947611490885\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  263\n",
      "0.2 action:  [0.1991, -0.9979] n_targets:  2 reward:  106.75\n",
      "1.4 action:  [-0.2072, -0.9989] n_targets:  1 reward:  52.58\n",
      "4.2 action:  [0.2624, -0.9909] n_targets:  1 reward:  51.19\n",
      "5.0 action:  [0.2914, -0.9964] n_targets:  1 reward:  64.73\n",
      "7.0 action:  [0.4076, -0.9876] n_targets:  2 reward:  107.72\n",
      "8.0 action:  [0.1164, -0.9831] n_targets:  1 reward:  52.18\n",
      "8.4 action:  [-0.1227, -0.9914] n_targets:  1 reward:  61.92\n",
      "8.6 action:  [0.4735, -0.9863] n_targets:  1 reward:  53.7\n",
      "10.2 action:  [0.1461, -0.9887] n_targets:  1 reward:  59.5\n",
      "10.6 action:  [0.1793, -0.9957] n_targets:  1 reward:  59.08\n",
      "11.2 action:  [0.0219, -0.9819] n_targets:  2 reward:  101.2\n",
      "15.8 action:  [-0.0271, -0.9849] n_targets:  1 reward:  50.23\n",
      "21.8 action:  [0.0631, -0.9992] n_targets:  1 reward:  55.3\n",
      "22.6 action:  [0.1004, -0.9975] n_targets:  3 reward:  188.96\n",
      "23.4 action:  [0.1253, -0.9892] n_targets:  1 reward:  53.09\n",
      "25.2 action:  [0.0151, -0.9934] n_targets:  1 reward:  50.43\n",
      "25.8 action:  [-0.215, -0.9984] n_targets:  1 reward:  55.97\n",
      "26.2 action:  [0.0082, -0.9896] n_targets:  1 reward:  59.7\n",
      "26.4 action:  [0.3474, -0.9914] n_targets:  1 reward:  57.26\n",
      "26.6 action:  [0.1519, -0.9907] n_targets:  2 reward:  113.09\n",
      "27.6 action:  [-0.0665, -0.9908] n_targets:  1 reward:  58.34\n",
      "28.8 action:  [-0.3728, -0.9925] n_targets:  1 reward:  51.95\n",
      "29.2 action:  [-0.0526, -0.9985] n_targets:  1 reward:  53.13\n",
      "29.4 action:  [-0.1167, -0.9978] n_targets:  1 reward:  51.98\n",
      "29.6 action:  [0.0683, -0.9972] n_targets:  1 reward:  53.68\n",
      "30.8 action:  [-0.0304, -0.999] n_targets:  1 reward:  52.28\n",
      "32.8 action:  [-0.0321, -0.9967] n_targets:  1 reward:  61.12\n",
      "33.0 action:  [0.0375, -0.9966] n_targets:  1 reward:  55.76\n",
      "33.2 action:  [0.0297, -0.9954] n_targets:  1 reward:  55.82\n",
      "33.4 action:  [0.2183, -0.997] n_targets:  1 reward:  55.22\n",
      "33.8 action:  [-0.0569, -0.9881] n_targets:  2 reward:  109.68\n",
      "34.4 action:  [0.1113, -0.9918] n_targets:  1 reward:  67.78\n",
      "34.6 action:  [0.2657, -0.9979] n_targets:  2 reward:  113.02\n",
      "37.4 action:  [-0.1429, -0.9902] n_targets:  1 reward:  50.83\n",
      "37.6 action:  [-0.0731, -0.9863] n_targets:  1 reward:  57.08\n",
      "38.6 action:  [-0.1088, -0.9954] n_targets:  1 reward:  57.66\n",
      "39.4 action:  [0.0276, -0.9873] n_targets:  1 reward:  58.21\n",
      "39.8 action:  [0.0915, -0.9904] n_targets:  1 reward:  50.82\n",
      "40.0 action:  [-0.0844, -0.9955] n_targets:  1 reward:  50.79\n",
      "40.6 action:  [0.1364, -0.9959] n_targets:  2 reward:  105.82\n",
      "43.6 action:  [-0.2836, -0.995] n_targets:  1 reward:  62.39\n",
      "44.2 action:  [0.3765, -0.9801] n_targets:  1 reward:  53.27\n",
      "46.4 action:  [0.1281, -0.982] n_targets:  1 reward:  54.08\n",
      "46.6 action:  [0.0889, -0.9977] n_targets:  1 reward:  59.87\n",
      "46.8 action:  [0.0334, -0.9893] n_targets:  1 reward:  72.31\n",
      "47.0 action:  [0.0319, -0.9802] n_targets:  1 reward:  50.31\n",
      "47.8 action:  [-0.17, -0.98] n_targets:  1 reward:  56.04\n",
      "48.2 action:  [0.107, -0.9918] n_targets:  1 reward:  58.6\n",
      "51.2 action:  [0.3007, -0.9848] n_targets:  1 reward:  51.3\n",
      "55.4 action:  [-0.2717, -0.9969] n_targets:  1 reward:  58.42\n",
      "56.0 action:  [-0.0433, -0.9861] n_targets:  1 reward:  52.95\n",
      "63.2 action:  [-0.0364, -0.9903] n_targets:  1 reward:  50.6\n",
      "63.4 action:  [0.0857, -0.9939] n_targets:  1 reward:  54.35\n",
      "63.8 action:  [0.0828, -0.993] n_targets:  1 reward:  50.33\n",
      "65.2 action:  [0.0795, -0.9928] n_targets:  1 reward:  53.38\n",
      "68.2 action:  [-0.1315, -0.9955] n_targets:  1 reward:  54.33\n",
      "70.2 action:  [0.129, -0.9975] n_targets:  1 reward:  64.02\n",
      "70.4 action:  [-0.1152, -0.9947] n_targets:  1 reward:  50.52\n",
      "71.4 action:  [-0.0601, -0.9864] n_targets:  2 reward:  117.03\n",
      "73.6 action:  [0.2215, -0.9962] n_targets:  1 reward:  52.09\n",
      "74.0 action:  [-0.1252, -0.9878] n_targets:  1 reward:  56.46\n",
      "74.2 action:  [-0.0691, -0.9933] n_targets:  1 reward:  56.19\n",
      "74.8 action:  [0.0762, -0.9929] n_targets:  1 reward:  57.02\n",
      "75.0 action:  [0.0279, -0.9891] n_targets:  1 reward:  59.07\n",
      "77.4 action:  [0.1949, -0.9812] n_targets:  1 reward:  60.22\n",
      "77.6 action:  [0.1194, -0.9816] n_targets:  1 reward:  51.65\n",
      "78.8 action:  [-0.0915, -0.9806] n_targets:  1 reward:  63.56\n",
      "79.8 action:  [-0.0802, -0.995] n_targets:  1 reward:  66.76\n",
      "80.6 action:  [0.0543, -0.9987] n_targets:  1 reward:  55.99\n",
      "81.2 action:  [-0.0856, -0.9852] n_targets:  1 reward:  50.68\n",
      "81.8 action:  [0.0338, -0.9977] n_targets:  1 reward:  50.26\n",
      "83.4 action:  [0.1221, -0.9922] n_targets:  1 reward:  56.08\n",
      "84.2 action:  [0.0703, -0.9902] n_targets:  1 reward:  61.12\n",
      "84.7 action:  [0.2443, -0.9955] n_targets:  1 reward:  52.97\n",
      "85.1 action:  [-0.2869, -0.9925] n_targets:  1 reward:  51.87\n",
      "85.5 action:  [-0.4079, -0.9906] n_targets:  1 reward:  59.14\n",
      "86.7 action:  [0.0077, -0.986] n_targets:  1 reward:  52.06\n",
      "86.9 action:  [-0.0355, -0.9955] n_targets:  1 reward:  52.67\n",
      "87.3 action:  [0.257, -0.9835] n_targets:  1 reward:  64.69\n",
      "88.7 action:  [0.0375, -0.9848] n_targets:  1 reward:  57.92\n",
      "88.9 action:  [0.1029, -0.9982] n_targets:  1 reward:  50.34\n",
      "90.7 action:  [-0.058, -0.9859] n_targets:  1 reward:  51.17\n",
      "91.5 action:  [-0.078, -0.9874] n_targets:  1 reward:  51.22\n",
      "92.5 action:  [0.0546, -0.997] n_targets:  1 reward:  65.35\n",
      "93.5 action:  [0.1291, -0.9921] n_targets:  1 reward:  53.55\n",
      "95.1 action:  [-0.0688, -0.9932] n_targets:  1 reward:  59.0\n",
      "96.5 action:  [0.2672, -0.9963] n_targets:  1 reward:  51.67\n",
      "96.7 action:  [-0.0355, -0.9846] n_targets:  1 reward:  61.64\n",
      "96.9 action:  [-0.1003, -0.998] n_targets:  1 reward:  51.27\n",
      "98.1 action:  [-0.3455, -0.9968] n_targets:  1 reward:  52.14\n",
      "99.3 action:  [0.0368, -0.9897] n_targets:  3 reward:  179.26\n",
      "99.7 action:  [-0.3154, -0.9869] n_targets:  1 reward:  62.47\n",
      "101.3 action:  [-0.007, -0.9822] n_targets:  1 reward:  50.19\n",
      "101.5 action:  [0.039, -0.9955] n_targets:  1 reward:  57.18\n",
      "102.3 action:  [0.1568, -0.9851] n_targets:  1 reward:  55.93\n",
      "ALPHA (entropy-related):  tensor([0.2027], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.21588 0.21455 0.21316 0.21174 0.21035 0.2088  0.20702 0.20546 0.20408\n",
      " 0.20271]\n",
      "Episode: 96, Episode Reward: 5990.513249715169\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  264\n",
      "0.1 action:  [-0.2013, -0.9904] n_targets:  1 reward:  88.08\n",
      "1.1 action:  [0.0578, -0.9974] n_targets:  1 reward:  56.21\n",
      "1.7 action:  [0.0144, -0.9946] n_targets:  1 reward:  51.18\n",
      "2.3 action:  [0.0095, -0.9986] n_targets:  1 reward:  54.27\n",
      "2.9 action:  [-0.1821, -0.993] n_targets:  1 reward:  59.04\n",
      "3.3 action:  [0.0545, -0.9887] n_targets:  1 reward:  52.97\n",
      "7.3 action:  [-0.205, -0.9982] n_targets:  1 reward:  59.87\n",
      "7.5 action:  [-0.0361, -0.9979] n_targets:  1 reward:  50.45\n",
      "9.7 action:  [-0.0706, -0.9922] n_targets:  1 reward:  54.25\n",
      "10.3 action:  [0.0083, -0.9965] n_targets:  1 reward:  53.95\n",
      "10.5 action:  [-0.2574, -0.9906] n_targets:  1 reward:  58.48\n",
      "11.7 action:  [0.0882, -0.9915] n_targets:  2 reward:  109.31\n",
      "12.3 action:  [-0.1685, -0.9942] n_targets:  1 reward:  52.97\n",
      "13.5 action:  [0.0096, -0.9968] n_targets:  2 reward:  125.85\n",
      "13.9 action:  [-0.1043, -0.984] n_targets:  1 reward:  50.76\n",
      "14.3 action:  [-0.0208, -0.9971] n_targets:  1 reward:  71.27\n",
      "15.1 action:  [0.1475, -0.9927] n_targets:  3 reward:  168.43\n",
      "16.3 action:  [0.1208, -0.9944] n_targets:  1 reward:  58.89\n",
      "19.3 action:  [0.0518, -0.997] n_targets:  1 reward:  56.5\n",
      "20.3 action:  [-0.0778, -0.9902] n_targets:  1 reward:  51.54\n",
      "20.9 action:  [0.1807, -0.9911] n_targets:  1 reward:  72.66\n",
      "22.2 action:  [-0.0063, -0.9981] n_targets:  1 reward:  56.32\n",
      "24.5 action:  [-0.1864, -0.989] n_targets:  1 reward:  50.17\n",
      "25.9 action:  [-0.0187, -0.9864] n_targets:  1 reward:  63.98\n",
      "26.3 action:  [0.1623, -0.9962] n_targets:  2 reward:  122.61\n",
      "26.5 action:  [-0.1438, -0.9927] n_targets:  1 reward:  59.36\n",
      "29.1 action:  [-0.0644, -0.9926] n_targets:  1 reward:  52.46\n",
      "33.1 action:  [-0.0225, -0.9959] n_targets:  2 reward:  108.06\n",
      "33.5 action:  [0.1187, -0.9921] n_targets:  1 reward:  60.59\n",
      "35.2 action:  [0.1611, -0.9935] n_targets:  1 reward:  62.59\n",
      "35.6 action:  [-0.0069, -0.9884] n_targets:  1 reward:  81.95\n",
      "38.8 action:  [0.0925, -0.9942] n_targets:  1 reward:  52.92\n",
      "39.8 action:  [0.1956, -0.9978] n_targets:  1 reward:  59.46\n",
      "40.4 action:  [-0.2469, -0.992] n_targets:  1 reward:  56.78\n",
      "42.2 action:  [0.2048, -0.9863] n_targets:  1 reward:  52.06\n",
      "42.6 action:  [0.0415, -0.9815] n_targets:  1 reward:  55.67\n",
      "42.8 action:  [0.2894, -0.9905] n_targets:  1 reward:  58.3\n",
      "44.0 action:  [-0.2431, -0.9937] n_targets:  1 reward:  53.12\n",
      "46.6 action:  [0.1718, -0.9987] n_targets:  1 reward:  54.21\n",
      "47.0 action:  [0.1004, -0.996] n_targets:  1 reward:  53.47\n",
      "47.6 action:  [0.2733, -0.9971] n_targets:  1 reward:  56.09\n",
      "48.4 action:  [-0.0236, -0.9906] n_targets:  1 reward:  52.16\n",
      "49.8 action:  [0.1992, -0.9837] n_targets:  1 reward:  55.26\n",
      "50.6 action:  [0.1564, -0.9965] n_targets:  1 reward:  64.02\n",
      "51.4 action:  [-0.0397, -0.9976] n_targets:  2 reward:  102.68\n",
      "52.2 action:  [0.0996, -0.9882] n_targets:  1 reward:  53.21\n",
      "52.4 action:  [-0.0898, -0.9989] n_targets:  1 reward:  53.34\n",
      "54.9 action:  [-0.0867, -0.9957] n_targets:  1 reward:  51.82\n",
      "55.5 action:  [0.0255, -0.9803] n_targets:  1 reward:  54.73\n",
      "55.9 action:  [-0.0995, -0.9869] n_targets:  1 reward:  53.56\n",
      "58.2 action:  [-0.0081, -0.9875] n_targets:  1 reward:  50.18\n",
      "58.7 action:  [-0.2344, -0.9893] n_targets:  1 reward:  50.33\n",
      "59.7 action:  [0.132, -0.9958] n_targets:  1 reward:  50.74\n",
      "60.3 action:  [0.1964, -0.9847] n_targets:  1 reward:  50.85\n",
      "60.5 action:  [0.0293, -0.9954] n_targets:  1 reward:  53.82\n",
      "68.0 action:  [-0.1248, -0.984] n_targets:  3 reward:  156.02\n",
      "68.2 action:  [-0.3617, -0.9964] n_targets:  1 reward:  61.9\n",
      "70.4 action:  [0.1501, -0.9956] n_targets:  1 reward:  59.76\n",
      "71.8 action:  [-0.3136, -0.9851] n_targets:  1 reward:  51.71\n",
      "72.0 action:  [-0.0953, -0.9924] n_targets:  1 reward:  55.11\n",
      "72.4 action:  [-0.074, -0.9954] n_targets:  1 reward:  51.81\n",
      "73.0 action:  [-0.245, -0.9807] n_targets:  1 reward:  51.01\n",
      "73.4 action:  [-0.1397, -0.9966] n_targets:  1 reward:  61.95\n",
      "73.6 action:  [0.1464, -0.9962] n_targets:  1 reward:  58.9\n",
      "75.6 action:  [0.2062, -0.9879] n_targets:  1 reward:  58.61\n",
      "76.4 action:  [0.1327, -0.9962] n_targets:  1 reward:  53.48\n",
      "77.2 action:  [0.1468, -0.9919] n_targets:  1 reward:  63.06\n",
      "77.4 action:  [-0.1584, -0.9977] n_targets:  1 reward:  59.09\n",
      "78.0 action:  [-0.3319, -0.9942] n_targets:  1 reward:  60.4\n",
      "78.4 action:  [0.2217, -0.9956] n_targets:  2 reward:  106.54\n",
      "78.6 action:  [0.3754, -0.9983] n_targets:  1 reward:  56.46\n",
      "79.6 action:  [0.3252, -0.9932] n_targets:  2 reward:  114.62\n",
      "80.2 action:  [0.2141, -0.995] n_targets:  2 reward:  116.87\n",
      "81.2 action:  [-0.2639, -0.9928] n_targets:  1 reward:  57.3\n",
      "82.2 action:  [-0.209, -0.985] n_targets:  1 reward:  66.74\n",
      "86.4 action:  [0.1031, -0.9856] n_targets:  1 reward:  58.6\n",
      "87.8 action:  [0.1825, -0.9928] n_targets:  1 reward:  57.46\n",
      "88.2 action:  [0.2588, -0.9935] n_targets:  1 reward:  55.4\n",
      "89.4 action:  [-0.067, -0.997] n_targets:  1 reward:  53.71\n",
      "93.3 action:  [-0.186, -0.9992] n_targets:  1 reward:  55.28\n",
      "96.1 action:  [-0.1893, -0.9934] n_targets:  1 reward:  54.02\n",
      "97.5 action:  [-0.2207, -0.9985] n_targets:  1 reward:  50.85\n",
      "ALPHA (entropy-related):  tensor([0.2014], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.21455 0.21316 0.21174 0.21035 0.2088  0.20702 0.20546 0.20408 0.20271\n",
      " 0.20141]\n",
      "Episode: 97, Episode Reward: 5330.443640391031\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  265\n",
      "0.1 action:  [0.1875, -0.9932] n_targets:  1 reward:  66.22\n",
      "0.9 action:  [-0.2879, -0.9968] n_targets:  1 reward:  56.73\n",
      "1.5 action:  [0.1624, -0.9968] n_targets:  1 reward:  52.44\n",
      "4.3 action:  [0.0147, -0.9993] n_targets:  1 reward:  53.24\n",
      "4.7 action:  [0.0075, -0.9921] n_targets:  1 reward:  52.6\n",
      "5.1 action:  [0.1423, -0.9957] n_targets:  1 reward:  51.7\n",
      "5.9 action:  [0.0977, -0.9975] n_targets:  1 reward:  52.0\n",
      "10.7 action:  [-0.0939, -0.9956] n_targets:  2 reward:  103.6\n",
      "12.7 action:  [0.247, -0.996] n_targets:  1 reward:  58.85\n",
      "13.5 action:  [0.1742, -0.9982] n_targets:  1 reward:  58.56\n",
      "13.9 action:  [0.2294, -0.9958] n_targets:  1 reward:  61.69\n",
      "14.1 action:  [-0.0357, -0.9948] n_targets:  1 reward:  55.2\n",
      "15.9 action:  [-0.1889, -0.9967] n_targets:  1 reward:  52.01\n",
      "16.9 action:  [-0.0556, -0.9936] n_targets:  1 reward:  55.41\n",
      "17.1 action:  [-0.2055, -0.9967] n_targets:  1 reward:  52.74\n",
      "18.1 action:  [-0.1571, -0.9855] n_targets:  1 reward:  50.13\n",
      "19.9 action:  [0.0514, -0.9895] n_targets:  1 reward:  51.9\n",
      "20.1 action:  [-0.0433, -0.9957] n_targets:  1 reward:  56.29\n",
      "29.1 action:  [0.2499, -0.9972] n_targets:  1 reward:  52.12\n",
      "30.1 action:  [-0.0598, -0.9976] n_targets:  2 reward:  116.3\n",
      "30.5 action:  [0.0979, -0.9952] n_targets:  1 reward:  61.19\n",
      "30.7 action:  [0.2545, -0.9977] n_targets:  1 reward:  55.22\n",
      "33.5 action:  [-0.1518, -0.9977] n_targets:  1 reward:  51.72\n",
      "34.7 action:  [-0.0867, -0.98] n_targets:  1 reward:  51.75\n",
      "36.0 action:  [0.0741, -0.9877] n_targets:  1 reward:  86.34\n",
      "36.4 action:  [0.1848, -0.9964] n_targets:  1 reward:  52.0\n",
      "36.6 action:  [0.1584, -0.9961] n_targets:  1 reward:  59.91\n",
      "40.4 action:  [0.3395, -0.9969] n_targets:  1 reward:  54.72\n",
      "43.2 action:  [0.0356, -0.9944] n_targets:  1 reward:  51.24\n",
      "43.6 action:  [-0.1309, -0.9913] n_targets:  1 reward:  51.47\n",
      "44.0 action:  [0.054, -0.9912] n_targets:  2 reward:  127.49\n",
      "44.4 action:  [0.0249, -0.9971] n_targets:  1 reward:  57.26\n",
      "45.0 action:  [0.0893, -0.9968] n_targets:  2 reward:  111.19\n",
      "46.8 action:  [-0.0836, -0.9941] n_targets:  1 reward:  63.69\n",
      "49.0 action:  [-0.0343, -0.9846] n_targets:  1 reward:  75.24\n",
      "49.8 action:  [0.0288, -0.9955] n_targets:  1 reward:  67.34\n",
      "50.8 action:  [0.0601, -0.9989] n_targets:  1 reward:  51.02\n",
      "51.6 action:  [0.0461, -0.9934] n_targets:  1 reward:  57.84\n",
      "52.2 action:  [0.1553, -0.9992] n_targets:  1 reward:  57.18\n",
      "52.4 action:  [-0.0443, -0.9962] n_targets:  2 reward:  114.09\n",
      "52.6 action:  [-0.026, -0.997] n_targets:  1 reward:  52.18\n",
      "53.8 action:  [-0.0095, -0.993] n_targets:  2 reward:  108.73\n",
      "55.8 action:  [0.2166, -0.9884] n_targets:  2 reward:  111.03\n",
      "57.6 action:  [-0.0776, -0.9898] n_targets:  1 reward:  50.45\n",
      "57.8 action:  [0.0112, -0.9959] n_targets:  1 reward:  52.57\n",
      "58.0 action:  [0.0621, -0.9826] n_targets:  1 reward:  52.87\n",
      "60.0 action:  [-0.017, -0.9994] n_targets:  1 reward:  55.94\n",
      "61.6 action:  [0.0518, -0.9833] n_targets:  1 reward:  54.01\n",
      "64.6 action:  [-0.1039, -0.9982] n_targets:  1 reward:  50.76\n",
      "64.8 action:  [-0.1714, -0.9915] n_targets:  1 reward:  52.62\n",
      "67.2 action:  [0.0169, -0.9934] n_targets:  1 reward:  52.03\n",
      "69.5 action:  [0.0831, -0.9816] n_targets:  1 reward:  70.43\n",
      "71.3 action:  [0.3698, -0.9925] n_targets:  1 reward:  52.07\n",
      "72.9 action:  [0.3622, -0.9901] n_targets:  1 reward:  74.81\n",
      "73.5 action:  [-0.1113, -0.9982] n_targets:  2 reward:  103.5\n",
      "73.7 action:  [0.0936, -0.9965] n_targets:  1 reward:  50.59\n",
      "74.5 action:  [0.0669, -0.9866] n_targets:  1 reward:  56.64\n",
      "74.9 action:  [0.3338, -0.9902] n_targets:  1 reward:  57.28\n",
      "76.1 action:  [0.0744, -0.9927] n_targets:  1 reward:  52.39\n",
      "77.3 action:  [0.3347, -0.9885] n_targets:  1 reward:  56.2\n",
      "79.5 action:  [0.1625, -0.9975] n_targets:  1 reward:  71.66\n",
      "81.5 action:  [0.0793, -0.9931] n_targets:  1 reward:  51.73\n",
      "87.3 action:  [0.0602, -0.9985] n_targets:  1 reward:  52.33\n",
      "88.4 action:  [0.1512, -0.9886] n_targets:  1 reward:  52.3\n",
      "88.6 action:  [-0.0959, -0.9972] n_targets:  1 reward:  51.69\n",
      "89.0 action:  [-0.1595, -0.9936] n_targets:  1 reward:  57.54\n",
      "89.4 action:  [0.0238, -0.9839] n_targets:  2 reward:  110.01\n",
      "91.0 action:  [-0.1437, -0.9888] n_targets:  1 reward:  67.04\n",
      "92.0 action:  [-0.0738, -0.9971] n_targets:  1 reward:  56.09\n",
      "92.2 action:  [-0.1476, -0.9989] n_targets:  1 reward:  50.64\n",
      "96.2 action:  [-0.0685, -0.9902] n_targets:  1 reward:  64.87\n",
      "99.1 action:  [-0.0689, -0.9971] n_targets:  1 reward:  58.28\n",
      "100.5 action:  [0.3682, -0.9911] n_targets:  1 reward:  55.35\n",
      "100.9 action:  [-0.1264, -0.9957] n_targets:  1 reward:  52.5\n",
      "101.7 action:  [-0.3443, -0.9895] n_targets:  1 reward:  50.07\n",
      "ALPHA (entropy-related):  tensor([0.2001], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.21316 0.21174 0.21035 0.2088  0.20702 0.20546 0.20408 0.20271 0.20141\n",
      " 0.20007]\n",
      "Episode: 98, Episode Reward: 4736.834185282389\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  266\n",
      "0.1 action:  [0.2867, -0.9968] n_targets:  3 reward:  179.68\n",
      "1.3 action:  [-0.1479, -0.9964] n_targets:  1 reward:  59.93\n",
      "3.9 action:  [-0.2548, -0.9971] n_targets:  1 reward:  50.25\n",
      "4.7 action:  [-0.0513, -0.9923] n_targets:  1 reward:  66.28\n",
      "5.1 action:  [0.1101, -0.9984] n_targets:  1 reward:  50.62\n",
      "6.3 action:  [0.0552, -0.9947] n_targets:  1 reward:  74.0\n",
      "8.3 action:  [-0.1688, -0.9912] n_targets:  1 reward:  56.38\n",
      "10.8 action:  [0.0868, -0.9899] n_targets:  1 reward:  95.49\n",
      "12.8 action:  [0.0978, -0.9871] n_targets:  2 reward:  103.94\n",
      "13.2 action:  [-0.4027, -0.9961] n_targets:  2 reward:  117.29\n",
      "14.4 action:  [0.077, -0.9888] n_targets:  1 reward:  59.28\n",
      "16.0 action:  [-0.1172, -0.9952] n_targets:  1 reward:  50.32\n",
      "17.8 action:  [0.1603, -0.9858] n_targets:  1 reward:  58.75\n",
      "18.0 action:  [-0.2383, -0.9978] n_targets:  1 reward:  55.11\n",
      "18.2 action:  [0.049, -0.9893] n_targets:  1 reward:  59.99\n",
      "19.4 action:  [0.0442, -0.9911] n_targets:  1 reward:  62.2\n",
      "20.2 action:  [-0.0828, -0.9959] n_targets:  2 reward:  115.81\n",
      "20.8 action:  [-0.0903, -0.9968] n_targets:  1 reward:  56.9\n",
      "21.6 action:  [-0.1263, -0.994] n_targets:  1 reward:  55.49\n",
      "23.2 action:  [0.1038, -0.9988] n_targets:  1 reward:  54.99\n",
      "23.6 action:  [-0.1263, -0.9873] n_targets:  1 reward:  55.31\n",
      "24.0 action:  [-0.089, -0.9944] n_targets:  2 reward:  104.07\n",
      "24.6 action:  [0.0284, -0.9871] n_targets:  1 reward:  50.37\n",
      "25.6 action:  [-0.1446, -0.9956] n_targets:  1 reward:  51.75\n",
      "26.6 action:  [-0.0348, -0.9995] n_targets:  2 reward:  115.76\n",
      "27.1 action:  [-0.4937, -0.9824] n_targets:  1 reward:  59.79\n",
      "28.2 action:  [-0.1578, -0.9962] n_targets:  1 reward:  56.67\n",
      "30.2 action:  [-0.0275, -0.995] n_targets:  1 reward:  53.12\n",
      "30.6 action:  [-0.0276, -0.9862] n_targets:  1 reward:  57.91\n",
      "30.8 action:  [-0.1783, -0.9947] n_targets:  1 reward:  55.48\n",
      "36.1 action:  [-0.1936, -0.9993] n_targets:  2 reward:  104.64\n",
      "36.5 action:  [0.211, -0.9956] n_targets:  1 reward:  60.5\n",
      "37.1 action:  [-0.1677, -0.9804] n_targets:  1 reward:  55.59\n",
      "39.3 action:  [-0.0773, -0.9928] n_targets:  1 reward:  58.4\n",
      "41.9 action:  [0.0794, -0.996] n_targets:  1 reward:  55.69\n",
      "44.4 action:  [0.0848, -0.9963] n_targets:  1 reward:  58.15\n",
      "44.6 action:  [0.1043, -0.9967] n_targets:  1 reward:  55.52\n",
      "48.0 action:  [0.0344, -0.9967] n_targets:  1 reward:  54.05\n",
      "48.2 action:  [-0.294, -0.9889] n_targets:  1 reward:  56.47\n",
      "49.2 action:  [-0.0405, -0.9978] n_targets:  1 reward:  50.72\n",
      "51.8 action:  [0.1532, -0.9905] n_targets:  1 reward:  52.95\n",
      "52.0 action:  [0.2878, -0.9959] n_targets:  1 reward:  52.45\n",
      "52.6 action:  [0.0284, -0.9908] n_targets:  1 reward:  63.69\n",
      "54.1 action:  [0.1423, -0.9888] n_targets:  1 reward:  54.5\n",
      "54.7 action:  [-0.019, -0.9923] n_targets:  1 reward:  62.36\n",
      "55.3 action:  [0.3615, -0.9842] n_targets:  1 reward:  61.04\n",
      "55.7 action:  [0.2276, -0.9928] n_targets:  1 reward:  55.05\n",
      "56.1 action:  [-0.0219, -0.9929] n_targets:  1 reward:  57.85\n",
      "58.5 action:  [-0.1219, -0.9907] n_targets:  1 reward:  62.53\n",
      "59.9 action:  [0.1059, -0.9964] n_targets:  2 reward:  130.44\n",
      "61.0 action:  [0.279, -0.998] n_targets:  1 reward:  54.79\n",
      "66.7 action:  [-0.1753, -0.9972] n_targets:  1 reward:  51.18\n",
      "67.5 action:  [0.0448, -0.9986] n_targets:  2 reward:  115.5\n",
      "71.2 action:  [0.0405, -0.9909] n_targets:  1 reward:  54.47\n",
      "72.0 action:  [0.0291, -0.9878] n_targets:  1 reward:  50.01\n",
      "72.4 action:  [-0.0169, -0.9938] n_targets:  2 reward:  105.95\n",
      "73.6 action:  [-0.0102, -0.9898] n_targets:  1 reward:  50.96\n",
      "73.8 action:  [-0.0662, -0.987] n_targets:  1 reward:  51.69\n",
      "75.6 action:  [0.5053, -0.9898] n_targets:  1 reward:  53.07\n",
      "78.5 action:  [-0.0193, -0.9936] n_targets:  1 reward:  56.22\n",
      "80.9 action:  [-0.2861, -0.9801] n_targets:  1 reward:  57.86\n",
      "81.9 action:  [-0.0259, -0.9963] n_targets:  1 reward:  50.78\n",
      "82.5 action:  [0.4364, -0.9962] n_targets:  1 reward:  50.15\n",
      "83.7 action:  [-0.304, -0.9914] n_targets:  1 reward:  55.3\n",
      "84.1 action:  [-0.0488, -0.9948] n_targets:  1 reward:  51.23\n",
      "84.3 action:  [-0.0092, -0.9817] n_targets:  1 reward:  53.21\n",
      "85.7 action:  [-0.1526, -0.9967] n_targets:  1 reward:  52.5\n",
      "89.1 action:  [-0.1031, -0.9946] n_targets:  1 reward:  60.66\n",
      "89.7 action:  [0.0041, -0.9949] n_targets:  3 reward:  194.99\n",
      "90.1 action:  [0.0452, -0.9906] n_targets:  1 reward:  51.23\n",
      "90.5 action:  [0.0414, -0.9949] n_targets:  2 reward:  128.3\n",
      "91.7 action:  [-0.188, -0.9984] n_targets:  1 reward:  51.14\n",
      "94.3 action:  [0.0715, -0.9938] n_targets:  3 reward:  199.4\n",
      "94.9 action:  [-0.1757, -0.9937] n_targets:  2 reward:  110.28\n",
      "95.3 action:  [-0.1094, -0.9976] n_targets:  1 reward:  52.15\n",
      "98.3 action:  [-0.0856, -0.9823] n_targets:  1 reward:  61.8\n",
      "100.3 action:  [-0.0082, -0.9984] n_targets:  1 reward:  60.96\n",
      "102.1 action:  [-0.02, -0.986] n_targets:  1 reward:  53.14\n",
      "ALPHA (entropy-related):  tensor([0.1987], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.21174 0.21035 0.2088  0.20702 0.20546 0.20408 0.20271 0.20141 0.20007\n",
      " 0.19875]\n",
      "Episode: 99, Episode Reward: 5444.450489679972\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  267\n",
      "0.2 action:  [-0.0022, -0.9885] n_targets:  2 reward:  139.89\n",
      "4.0 action:  [-0.0114, -0.9897] n_targets:  1 reward:  52.16\n",
      "4.6 action:  [-0.1094, -0.9841] n_targets:  1 reward:  50.94\n",
      "9.8 action:  [0.0061, -0.9924] n_targets:  2 reward:  118.73\n",
      "11.3 action:  [0.2108, -0.9871] n_targets:  1 reward:  55.49\n",
      "12.3 action:  [-0.4827, -0.9895] n_targets:  1 reward:  52.95\n",
      "12.5 action:  [0.1774, -0.9933] n_targets:  1 reward:  56.77\n",
      "13.8 action:  [0.0708, -0.9916] n_targets:  1 reward:  56.5\n",
      "17.0 action:  [-0.3261, -0.9822] n_targets:  1 reward:  50.43\n",
      "17.6 action:  [-0.1863, -0.9902] n_targets:  1 reward:  51.43\n",
      "18.0 action:  [-0.1034, -0.9934] n_targets:  1 reward:  51.16\n",
      "18.2 action:  [0.3116, -0.9935] n_targets:  1 reward:  56.17\n",
      "19.2 action:  [-0.362, -0.9931] n_targets:  1 reward:  52.62\n",
      "19.6 action:  [0.3079, -0.9986] n_targets:  1 reward:  56.29\n",
      "20.3 action:  [0.1413, -0.9958] n_targets:  1 reward:  70.5\n",
      "20.5 action:  [0.0874, -0.9808] n_targets:  1 reward:  50.15\n",
      "23.5 action:  [0.2944, -0.9949] n_targets:  2 reward:  112.96\n",
      "24.3 action:  [-0.3161, -0.9988] n_targets:  1 reward:  52.24\n",
      "26.1 action:  [0.1433, -0.9957] n_targets:  1 reward:  71.89\n",
      "27.5 action:  [0.0568, -0.9981] n_targets:  1 reward:  50.56\n",
      "29.5 action:  [-0.0704, -0.9936] n_targets:  1 reward:  50.45\n",
      "30.5 action:  [-0.1627, -0.9912] n_targets:  1 reward:  60.23\n",
      "32.3 action:  [-0.0021, -0.9921] n_targets:  1 reward:  57.5\n",
      "32.9 action:  [0.0092, -0.9898] n_targets:  1 reward:  67.59\n",
      "34.1 action:  [-0.2557, -0.9932] n_targets:  1 reward:  54.47\n",
      "36.1 action:  [-0.0876, -0.998] n_targets:  1 reward:  55.46\n",
      "37.5 action:  [-0.0737, -0.996] n_targets:  1 reward:  53.17\n",
      "37.7 action:  [0.0732, -0.9935] n_targets:  1 reward:  54.88\n",
      "43.3 action:  [-0.1865, -0.9858] n_targets:  1 reward:  57.11\n",
      "45.1 action:  [0.232, -0.9884] n_targets:  1 reward:  55.23\n",
      "51.1 action:  [-0.2118, -0.9928] n_targets:  1 reward:  51.74\n",
      "51.9 action:  [0.0947, -0.9898] n_targets:  1 reward:  58.06\n",
      "52.1 action:  [0.051, -0.9832] n_targets:  1 reward:  55.95\n",
      "52.5 action:  [0.0402, -0.9919] n_targets:  1 reward:  57.39\n",
      "53.3 action:  [0.1595, -0.9875] n_targets:  1 reward:  54.05\n",
      "54.3 action:  [0.0431, -0.9951] n_targets:  1 reward:  53.76\n",
      "55.1 action:  [-0.1593, -0.9986] n_targets:  1 reward:  60.26\n",
      "56.7 action:  [-0.1096, -0.9816] n_targets:  1 reward:  51.49\n",
      "60.9 action:  [-0.0211, -0.987] n_targets:  1 reward:  53.78\n",
      "61.9 action:  [0.036, -0.9921] n_targets:  2 reward:  122.89\n",
      "63.1 action:  [-0.2047, -0.9856] n_targets:  1 reward:  57.26\n",
      "63.9 action:  [0.1151, -0.9879] n_targets:  1 reward:  50.15\n",
      "64.3 action:  [-0.3587, -0.9935] n_targets:  1 reward:  52.52\n",
      "65.5 action:  [-0.1329, -0.9932] n_targets:  1 reward:  63.39\n",
      "67.9 action:  [-0.0748, -0.995] n_targets:  1 reward:  53.82\n",
      "68.7 action:  [0.022, -0.9938] n_targets:  1 reward:  57.09\n",
      "69.3 action:  [-0.2497, -0.9991] n_targets:  1 reward:  65.05\n",
      "70.1 action:  [0.0029, -0.9979] n_targets:  1 reward:  54.28\n",
      "71.9 action:  [-0.2111, -0.9921] n_targets:  1 reward:  51.71\n",
      "72.1 action:  [0.2027, -0.989] n_targets:  1 reward:  51.4\n",
      "72.3 action:  [-0.096, -0.9836] n_targets:  1 reward:  51.72\n",
      "73.4 action:  [0.0688, -0.9976] n_targets:  1 reward:  61.81\n",
      "73.8 action:  [-0.2055, -0.9946] n_targets:  1 reward:  61.31\n",
      "74.2 action:  [-0.3314, -0.9948] n_targets:  1 reward:  60.24\n",
      "74.4 action:  [-0.1827, -0.9944] n_targets:  1 reward:  56.04\n",
      "76.8 action:  [-0.0811, -0.9934] n_targets:  1 reward:  60.37\n",
      "77.4 action:  [0.0813, -0.9855] n_targets:  2 reward:  117.2\n",
      "78.2 action:  [-0.1824, -0.9962] n_targets:  2 reward:  124.59\n",
      "78.6 action:  [-0.1674, -0.9988] n_targets:  1 reward:  54.28\n",
      "80.8 action:  [-0.2154, -0.9976] n_targets:  1 reward:  56.4\n",
      "81.8 action:  [-0.0904, -0.9912] n_targets:  1 reward:  50.54\n",
      "84.4 action:  [-0.1456, -0.9869] n_targets:  1 reward:  55.36\n",
      "85.8 action:  [-0.0442, -0.9971] n_targets:  2 reward:  129.05\n",
      "86.2 action:  [-0.1118, -0.9934] n_targets:  1 reward:  61.32\n",
      "88.4 action:  [-0.1543, -0.9964] n_targets:  1 reward:  50.8\n",
      "89.0 action:  [0.1455, -0.9973] n_targets:  1 reward:  56.85\n",
      "90.2 action:  [-0.0261, -0.9982] n_targets:  1 reward:  55.54\n",
      "90.4 action:  [-0.0284, -0.9979] n_targets:  1 reward:  58.98\n",
      "91.8 action:  [0.0196, -0.9962] n_targets:  1 reward:  52.53\n",
      "93.0 action:  [-0.1526, -0.9888] n_targets:  1 reward:  69.21\n",
      "94.2 action:  [0.2802, -0.9991] n_targets:  1 reward:  59.11\n",
      "94.4 action:  [0.0324, -0.9974] n_targets:  1 reward:  54.59\n",
      "95.8 action:  [-0.2088, -0.9831] n_targets:  2 reward:  121.21\n",
      "97.4 action:  [-0.1733, -0.9988] n_targets:  1 reward:  66.46\n",
      "97.8 action:  [0.2126, -0.9984] n_targets:  1 reward:  57.99\n",
      "100.4 action:  [-0.1639, -0.9987] n_targets:  1 reward:  61.69\n",
      "ALPHA (entropy-related):  tensor([0.1977], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.21035 0.2088  0.20702 0.20546 0.20408 0.20271 0.20141 0.20007 0.19875\n",
      " 0.19767]\n",
      "Episode: 100, Episode Reward: 4817.121225992838\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  268\n",
      "1.3 action:  [-0.3523, -0.9935] n_targets:  2 reward:  121.62\n",
      "2.1 action:  [-0.0184, -0.9991] n_targets:  1 reward:  51.28\n",
      "4.3 action:  [0.1073, -0.9953] n_targets:  1 reward:  53.75\n",
      "5.1 action:  [-0.2054, -0.9937] n_targets:  1 reward:  57.27\n",
      "6.0 action:  [-0.404, -0.9939] n_targets:  1 reward:  50.13\n",
      "7.1 action:  [-0.0504, -0.9986] n_targets:  1 reward:  61.5\n",
      "8.1 action:  [-0.0114, -0.9972] n_targets:  1 reward:  57.56\n",
      "8.7 action:  [-0.1954, -0.993] n_targets:  1 reward:  61.04\n",
      "8.9 action:  [-0.1507, -0.9995] n_targets:  1 reward:  53.48\n",
      "9.3 action:  [-0.1193, -0.9851] n_targets:  3 reward:  187.61\n",
      "10.7 action:  [0.1401, -0.9976] n_targets:  1 reward:  68.69\n",
      "11.5 action:  [-0.1323, -0.9933] n_targets:  1 reward:  71.08\n",
      "14.1 action:  [-0.0833, -0.9918] n_targets:  1 reward:  56.96\n",
      "16.9 action:  [-0.138, -0.9897] n_targets:  1 reward:  50.02\n",
      "19.7 action:  [-0.1623, -0.999] n_targets:  1 reward:  57.49\n",
      "23.3 action:  [0.0271, -0.9901] n_targets:  1 reward:  52.58\n",
      "23.5 action:  [-0.1067, -0.9959] n_targets:  1 reward:  57.3\n",
      "24.6 action:  [-0.171, -0.9825] n_targets:  1 reward:  57.35\n",
      "25.0 action:  [-0.0276, -0.9972] n_targets:  2 reward:  106.5\n",
      "25.8 action:  [-0.0741, -0.9814] n_targets:  1 reward:  50.55\n",
      "26.0 action:  [0.0356, -0.9973] n_targets:  1 reward:  60.63\n",
      "26.8 action:  [-0.1689, -0.9958] n_targets:  2 reward:  110.2\n",
      "29.1 action:  [-0.0988, -0.9975] n_targets:  1 reward:  52.35\n",
      "29.3 action:  [-0.0376, -0.9959] n_targets:  1 reward:  52.18\n",
      "30.1 action:  [0.1488, -0.9925] n_targets:  1 reward:  51.34\n",
      "34.1 action:  [-0.1625, -0.9836] n_targets:  1 reward:  56.95\n",
      "35.7 action:  [0.0356, -0.9955] n_targets:  2 reward:  110.44\n",
      "35.9 action:  [-0.0611, -0.9844] n_targets:  1 reward:  52.39\n",
      "36.9 action:  [-0.0798, -0.9928] n_targets:  1 reward:  51.7\n",
      "37.5 action:  [-0.0436, -0.9808] n_targets:  1 reward:  50.89\n",
      "37.9 action:  [-0.0724, -0.9901] n_targets:  1 reward:  61.35\n",
      "39.0 action:  [-0.1235, -0.9812] n_targets:  1 reward:  54.25\n",
      "39.2 action:  [-0.1658, -0.9912] n_targets:  2 reward:  110.96\n",
      "40.2 action:  [0.0192, -0.9957] n_targets:  1 reward:  51.49\n",
      "42.4 action:  [0.2053, -0.9889] n_targets:  3 reward:  184.8\n",
      "44.5 action:  [-0.107, -0.9951] n_targets:  1 reward:  53.28\n",
      "46.7 action:  [0.1445, -0.9877] n_targets:  1 reward:  51.42\n",
      "46.9 action:  [-0.0748, -0.9948] n_targets:  1 reward:  51.29\n",
      "47.1 action:  [0.1973, -0.9944] n_targets:  1 reward:  59.97\n",
      "47.5 action:  [-0.1093, -0.9868] n_targets:  1 reward:  52.53\n",
      "47.9 action:  [-0.0598, -0.9859] n_targets:  1 reward:  61.0\n",
      "48.7 action:  [-0.1971, -0.9959] n_targets:  1 reward:  73.89\n",
      "49.1 action:  [0.1193, -0.9931] n_targets:  1 reward:  52.04\n",
      "49.5 action:  [-0.0595, -0.9929] n_targets:  1 reward:  53.44\n",
      "49.7 action:  [-0.1224, -0.989] n_targets:  1 reward:  58.09\n",
      "50.5 action:  [0.0324, -0.9979] n_targets:  1 reward:  52.56\n",
      "52.7 action:  [-0.1355, -0.9941] n_targets:  1 reward:  52.72\n",
      "58.2 action:  [-0.0233, -0.9977] n_targets:  1 reward:  52.73\n",
      "62.1 action:  [-0.415, -0.9937] n_targets:  1 reward:  57.83\n",
      "63.8 action:  [0.1705, -0.9911] n_targets:  1 reward:  52.96\n",
      "67.7 action:  [0.312, -0.9952] n_targets:  1 reward:  53.88\n",
      "68.3 action:  [0.1138, -0.9984] n_targets:  1 reward:  58.21\n",
      "70.0 action:  [0.0293, -0.9853] n_targets:  1 reward:  50.18\n",
      "70.6 action:  [-0.3591, -0.9856] n_targets:  1 reward:  55.49\n",
      "71.0 action:  [-0.162, -0.995] n_targets:  1 reward:  50.78\n",
      "72.1 action:  [-0.0628, -0.9915] n_targets:  1 reward:  50.52\n",
      "72.3 action:  [0.0907, -0.9803] n_targets:  1 reward:  51.1\n",
      "76.3 action:  [-0.0126, -0.9945] n_targets:  2 reward:  109.96\n",
      "77.3 action:  [0.0471, -0.9982] n_targets:  1 reward:  77.38\n",
      "77.9 action:  [-0.053, -0.9973] n_targets:  1 reward:  55.43\n",
      "78.3 action:  [-0.0481, -0.9896] n_targets:  2 reward:  104.47\n",
      "80.5 action:  [0.1961, -0.9921] n_targets:  1 reward:  56.47\n",
      "83.4 action:  [-0.092, -0.9992] n_targets:  1 reward:  59.45\n",
      "85.6 action:  [-0.3582, -0.9932] n_targets:  1 reward:  55.57\n",
      "91.2 action:  [-0.3249, -0.9851] n_targets:  1 reward:  59.71\n",
      "92.4 action:  [-0.3543, -0.9941] n_targets:  1 reward:  61.86\n",
      "93.6 action:  [-0.0027, -0.9898] n_targets:  1 reward:  52.16\n",
      "95.2 action:  [-0.2358, -0.9929] n_targets:  1 reward:  50.51\n",
      "96.6 action:  [-0.1204, -0.9973] n_targets:  1 reward:  50.22\n",
      "100.4 action:  [-0.0224, -0.996] n_targets:  1 reward:  55.19\n",
      "ALPHA (entropy-related):  tensor([0.1966], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.2088  0.20702 0.20546 0.20408 0.20271 0.20141 0.20007 0.19875 0.19767\n",
      " 0.19656]\n",
      "Episode: 101, Episode Reward: 4549.96541595459\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  269\n",
      "0.6 action:  [0.0155, -0.9943] n_targets:  1 reward:  69.48\n",
      "1.4 action:  [-0.1794, -0.9941] n_targets:  1 reward:  55.25\n",
      "2.8 action:  [-0.0631, -0.9977] n_targets:  1 reward:  55.43\n",
      "4.4 action:  [-0.2843, -0.9929] n_targets:  2 reward:  106.7\n",
      "4.6 action:  [0.1327, -0.9946] n_targets:  1 reward:  55.75\n",
      "4.8 action:  [-0.3114, -0.9961] n_targets:  2 reward:  123.92\n",
      "5.8 action:  [-0.1088, -0.9986] n_targets:  1 reward:  50.67\n",
      "6.8 action:  [-0.1663, -0.9957] n_targets:  1 reward:  65.59\n",
      "7.6 action:  [0.235, -0.9998] n_targets:  1 reward:  61.82\n",
      "7.8 action:  [0.1213, -0.9978] n_targets:  1 reward:  56.77\n",
      "8.6 action:  [-0.3041, -0.9824] n_targets:  1 reward:  57.06\n",
      "9.0 action:  [-0.143, -0.9969] n_targets:  1 reward:  63.73\n",
      "9.6 action:  [-0.2087, -0.9977] n_targets:  1 reward:  68.45\n",
      "10.0 action:  [0.0175, -0.9912] n_targets:  1 reward:  56.69\n",
      "10.8 action:  [-0.0736, -0.9915] n_targets:  1 reward:  59.38\n",
      "12.4 action:  [-0.25, -0.9951] n_targets:  1 reward:  54.83\n",
      "15.0 action:  [0.0525, -0.9944] n_targets:  2 reward:  111.78\n",
      "16.1 action:  [-0.1664, -0.9972] n_targets:  1 reward:  60.87\n",
      "18.3 action:  [-0.1535, -0.9923] n_targets:  1 reward:  54.66\n",
      "20.7 action:  [-0.0061, -0.9922] n_targets:  2 reward:  106.59\n",
      "22.3 action:  [0.4757, -0.9906] n_targets:  1 reward:  53.77\n",
      "22.9 action:  [-0.2263, -0.9957] n_targets:  1 reward:  50.53\n",
      "23.9 action:  [0.0249, -0.9952] n_targets:  3 reward:  170.72\n",
      "28.9 action:  [0.1635, -0.989] n_targets:  2 reward:  164.08\n",
      "29.9 action:  [-0.3121, -0.9966] n_targets:  1 reward:  60.79\n",
      "30.5 action:  [-0.3833, -0.9945] n_targets:  2 reward:  117.38\n",
      "32.9 action:  [-0.0688, -0.9987] n_targets:  3 reward:  174.06\n",
      "34.9 action:  [-0.2177, -0.9893] n_targets:  1 reward:  52.44\n",
      "35.7 action:  [-0.0475, -0.9975] n_targets:  1 reward:  57.14\n",
      "36.3 action:  [-0.1509, -0.9924] n_targets:  1 reward:  51.75\n",
      "37.9 action:  [-0.0317, -0.9861] n_targets:  1 reward:  60.76\n",
      "41.5 action:  [-0.1544, -0.9936] n_targets:  1 reward:  61.04\n",
      "41.7 action:  [0.0106, -0.9974] n_targets:  1 reward:  53.52\n",
      "45.3 action:  [0.078, -0.9933] n_targets:  1 reward:  50.22\n",
      "49.5 action:  [0.1645, -0.981] n_targets:  1 reward:  56.53\n",
      "49.7 action:  [-0.1316, -0.9981] n_targets:  1 reward:  53.76\n",
      "51.1 action:  [-0.1707, -0.9976] n_targets:  1 reward:  51.35\n",
      "52.7 action:  [-0.146, -0.9944] n_targets:  2 reward:  107.4\n",
      "53.1 action:  [-0.2041, -0.9915] n_targets:  2 reward:  110.34\n",
      "53.3 action:  [-0.3365, -0.9826] n_targets:  1 reward:  53.18\n",
      "55.9 action:  [-0.1451, -0.9958] n_targets:  1 reward:  53.25\n",
      "58.5 action:  [-0.1213, -0.9994] n_targets:  1 reward:  52.37\n",
      "58.9 action:  [-0.4185, -0.9914] n_targets:  1 reward:  50.11\n",
      "59.1 action:  [-0.0125, -0.9901] n_targets:  1 reward:  53.03\n",
      "59.3 action:  [-0.2216, -0.9818] n_targets:  1 reward:  55.21\n",
      "60.3 action:  [-0.0635, -0.9958] n_targets:  1 reward:  55.86\n",
      "63.5 action:  [-0.3538, -0.9909] n_targets:  1 reward:  53.26\n",
      "64.9 action:  [-0.1855, -0.9944] n_targets:  1 reward:  58.71\n",
      "65.5 action:  [-0.1393, -0.9936] n_targets:  1 reward:  57.6\n",
      "66.1 action:  [-0.0512, -0.9955] n_targets:  1 reward:  55.84\n",
      "67.1 action:  [-0.3249, -0.998] n_targets:  1 reward:  61.87\n",
      "69.3 action:  [0.0408, -0.9867] n_targets:  1 reward:  69.01\n",
      "71.5 action:  [-0.2485, -0.9802] n_targets:  1 reward:  50.31\n",
      "71.9 action:  [-0.0095, -0.9863] n_targets:  3 reward:  160.25\n",
      "72.9 action:  [-0.0668, -0.9978] n_targets:  1 reward:  52.05\n",
      "73.5 action:  [-0.0769, -0.9932] n_targets:  1 reward:  51.58\n",
      "73.9 action:  [-0.4148, -0.992] n_targets:  2 reward:  113.84\n",
      "75.3 action:  [-0.2232, -0.9917] n_targets:  1 reward:  55.4\n",
      "75.7 action:  [-0.0762, -0.9977] n_targets:  1 reward:  52.36\n",
      "76.2 action:  [-0.1127, -0.9943] n_targets:  2 reward:  113.94\n",
      "80.5 action:  [-0.1667, -0.9961] n_targets:  1 reward:  53.03\n",
      "81.7 action:  [-0.078, -0.9969] n_targets:  1 reward:  51.54\n",
      "82.1 action:  [-0.1859, -0.9929] n_targets:  1 reward:  58.59\n",
      "82.5 action:  [-0.1004, -0.9981] n_targets:  1 reward:  54.69\n",
      "82.7 action:  [-0.0985, -0.9929] n_targets:  1 reward:  50.31\n",
      "83.5 action:  [-0.196, -0.998] n_targets:  1 reward:  54.86\n",
      "85.7 action:  [0.2068, -0.9915] n_targets:  1 reward:  52.6\n",
      "86.1 action:  [-0.0972, -0.9984] n_targets:  2 reward:  124.7\n",
      "87.5 action:  [-0.0038, -0.9918] n_targets:  1 reward:  56.0\n",
      "87.7 action:  [-0.2261, -0.9817] n_targets:  1 reward:  52.3\n",
      "89.3 action:  [-0.0564, -0.9935] n_targets:  1 reward:  50.43\n",
      "90.5 action:  [-0.0667, -0.9869] n_targets:  1 reward:  62.61\n",
      "91.7 action:  [-0.1237, -0.988] n_targets:  1 reward:  61.29\n",
      "93.3 action:  [0.016, -0.9939] n_targets:  1 reward:  53.42\n",
      "95.5 action:  [-0.291, -0.9942] n_targets:  1 reward:  50.27\n",
      "95.9 action:  [-0.0797, -0.9873] n_targets:  1 reward:  56.33\n",
      "97.9 action:  [-0.0502, -0.9846] n_targets:  1 reward:  62.59\n",
      "ALPHA (entropy-related):  tensor([0.1955], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.20702 0.20546 0.20408 0.20271 0.20141 0.20007 0.19875 0.19767 0.19656\n",
      " 0.19547]\n",
      "Episode: 102, Episode Reward: 5337.5431569417315\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  270\n",
      "0.2 action:  [0.1567, -0.9942] n_targets:  1 reward:  57.82\n",
      "3.0 action:  [-0.3588, -0.9921] n_targets:  1 reward:  55.21\n",
      "3.4 action:  [0.074, -0.9909] n_targets:  1 reward:  60.94\n",
      "4.8 action:  [-0.1336, -0.9914] n_targets:  1 reward:  51.17\n",
      "5.2 action:  [0.0104, -0.9812] n_targets:  1 reward:  50.82\n",
      "5.4 action:  [-0.0477, -0.9974] n_targets:  1 reward:  60.74\n",
      "6.8 action:  [-0.1785, -0.9933] n_targets:  1 reward:  64.6\n",
      "7.4 action:  [-0.0372, -0.9984] n_targets:  1 reward:  55.21\n",
      "7.6 action:  [-0.1463, -0.9983] n_targets:  1 reward:  51.73\n",
      "8.1 action:  [-0.2125, -0.9971] n_targets:  1 reward:  50.66\n",
      "8.9 action:  [0.0281, -0.9965] n_targets:  1 reward:  57.73\n",
      "10.9 action:  [0.0093, -0.9955] n_targets:  1 reward:  55.85\n",
      "11.2 action:  [0.0837, -0.9933] n_targets:  1 reward:  61.12\n",
      "13.0 action:  [0.1902, -0.9994] n_targets:  1 reward:  50.83\n",
      "14.4 action:  [0.0618, -0.9858] n_targets:  1 reward:  65.89\n",
      "19.0 action:  [0.0435, -0.9928] n_targets:  1 reward:  56.86\n",
      "19.2 action:  [-0.1501, -0.9918] n_targets:  1 reward:  55.69\n",
      "21.0 action:  [0.138, -0.9924] n_targets:  1 reward:  53.27\n",
      "21.4 action:  [-0.1755, -0.9967] n_targets:  1 reward:  50.27\n",
      "23.4 action:  [0.2293, -0.9957] n_targets:  1 reward:  50.89\n",
      "24.8 action:  [0.0059, -0.9976] n_targets:  1 reward:  58.87\n",
      "25.2 action:  [0.0661, -0.9952] n_targets:  1 reward:  56.13\n",
      "25.8 action:  [0.1396, -0.9809] n_targets:  2 reward:  104.08\n",
      "27.8 action:  [-0.2477, -0.9937] n_targets:  1 reward:  54.82\n",
      "29.2 action:  [-0.2425, -0.9906] n_targets:  1 reward:  54.84\n",
      "29.4 action:  [-0.0597, -0.9923] n_targets:  1 reward:  53.03\n",
      "31.4 action:  [-0.0412, -0.9981] n_targets:  1 reward:  57.86\n",
      "31.8 action:  [0.0519, -0.9948] n_targets:  1 reward:  57.43\n",
      "34.6 action:  [-0.1751, -0.9929] n_targets:  1 reward:  55.07\n",
      "35.0 action:  [0.0827, -0.9923] n_targets:  1 reward:  60.26\n",
      "37.4 action:  [0.1129, -0.9915] n_targets:  1 reward:  54.01\n",
      "37.6 action:  [-0.054, -0.9972] n_targets:  1 reward:  52.07\n",
      "38.8 action:  [-0.1066, -0.9927] n_targets:  1 reward:  60.73\n",
      "40.0 action:  [-0.0919, -0.9947] n_targets:  2 reward:  106.05\n",
      "41.0 action:  [0.1178, -0.9942] n_targets:  1 reward:  55.65\n",
      "42.6 action:  [-0.1207, -0.9965] n_targets:  1 reward:  62.29\n",
      "44.8 action:  [-0.2593, -0.988] n_targets:  1 reward:  64.13\n",
      "46.2 action:  [0.1275, -0.998] n_targets:  3 reward:  177.1\n",
      "50.4 action:  [-0.1755, -0.9992] n_targets:  1 reward:  56.74\n",
      "51.4 action:  [-0.4913, -0.9959] n_targets:  1 reward:  55.32\n",
      "52.8 action:  [-0.3288, -0.999] n_targets:  1 reward:  55.4\n",
      "53.4 action:  [-0.1009, -0.9839] n_targets:  1 reward:  52.11\n",
      "53.9 action:  [-0.2397, -0.9834] n_targets:  1 reward:  51.38\n",
      "54.1 action:  [0.1702, -0.9893] n_targets:  1 reward:  50.31\n",
      "54.7 action:  [-0.2631, -0.9878] n_targets:  1 reward:  50.96\n",
      "55.1 action:  [0.0184, -0.9914] n_targets:  2 reward:  114.62\n",
      "55.5 action:  [0.0464, -0.9934] n_targets:  2 reward:  111.88\n",
      "56.9 action:  [-0.0541, -0.9977] n_targets:  1 reward:  54.13\n",
      "57.1 action:  [-0.3112, -0.9963] n_targets:  1 reward:  55.65\n",
      "57.3 action:  [-0.0586, -0.9976] n_targets:  1 reward:  56.98\n",
      "57.5 action:  [-0.254, -0.9917] n_targets:  1 reward:  57.44\n",
      "58.1 action:  [-0.303, -0.9982] n_targets:  2 reward:  119.68\n",
      "58.5 action:  [-0.175, -0.9921] n_targets:  1 reward:  62.02\n",
      "59.7 action:  [-0.1059, -0.9944] n_targets:  3 reward:  219.99\n",
      "65.5 action:  [-0.2713, -0.9967] n_targets:  1 reward:  60.73\n",
      "66.7 action:  [-0.0929, -0.9903] n_targets:  1 reward:  57.87\n",
      "67.5 action:  [0.0763, -0.9962] n_targets:  1 reward:  72.4\n",
      "67.7 action:  [-0.2528, -0.9894] n_targets:  1 reward:  54.83\n",
      "67.9 action:  [-0.206, -0.9864] n_targets:  1 reward:  51.36\n",
      "68.7 action:  [0.0622, -0.9962] n_targets:  3 reward:  154.57\n",
      "69.3 action:  [0.2902, -0.997] n_targets:  1 reward:  53.58\n",
      "70.1 action:  [0.1124, -0.9953] n_targets:  1 reward:  50.99\n",
      "72.7 action:  [-0.4477, -0.9953] n_targets:  1 reward:  53.09\n",
      "74.3 action:  [-0.1813, -0.9914] n_targets:  1 reward:  51.33\n",
      "74.7 action:  [-0.1527, -0.9915] n_targets:  1 reward:  50.46\n",
      "75.5 action:  [0.0345, -0.9961] n_targets:  1 reward:  65.01\n",
      "75.9 action:  [-0.1803, -0.9964] n_targets:  1 reward:  64.95\n",
      "76.3 action:  [-0.0311, -0.9958] n_targets:  1 reward:  54.37\n",
      "76.7 action:  [-0.0558, -0.9905] n_targets:  2 reward:  105.98\n",
      "78.3 action:  [-0.0354, -0.998] n_targets:  2 reward:  127.21\n",
      "80.3 action:  [-0.1905, -0.989] n_targets:  1 reward:  56.38\n",
      "81.5 action:  [0.2309, -0.9937] n_targets:  1 reward:  58.05\n",
      "81.9 action:  [0.0104, -0.9971] n_targets:  1 reward:  59.91\n",
      "82.9 action:  [-0.2033, -0.9952] n_targets:  1 reward:  55.31\n",
      "84.6 action:  [0.1965, -0.9974] n_targets:  1 reward:  52.13\n",
      "84.8 action:  [0.1467, -0.9923] n_targets:  1 reward:  56.5\n",
      "88.8 action:  [0.1312, -0.9964] n_targets:  1 reward:  51.28\n",
      "91.0 action:  [0.126, -0.9972] n_targets:  1 reward:  71.91\n",
      "92.2 action:  [-0.3794, -0.9967] n_targets:  1 reward:  54.74\n",
      "95.8 action:  [-0.5036, -0.9903] n_targets:  1 reward:  52.62\n",
      "96.6 action:  [0.4055, -0.9984] n_targets:  1 reward:  62.93\n",
      "97.0 action:  [-0.1643, -0.9908] n_targets:  1 reward:  52.49\n",
      "98.2 action:  [0.133, -0.9978] n_targets:  2 reward:  105.48\n",
      "98.8 action:  [-0.0863, -0.988] n_targets:  1 reward:  63.25\n",
      "101.4 action:  [-0.1957, -0.9978] n_targets:  1 reward:  51.27\n",
      "ALPHA (entropy-related):  tensor([0.1947], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.20546 0.20408 0.20271 0.20141 0.20007 0.19875 0.19767 0.19656 0.19547\n",
      " 0.19465]\n",
      "Episode: 103, Episode Reward: 5619.311243693033\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  271\n",
      "0.1 action:  [-0.2544, -0.9808] n_targets:  1 reward:  54.26\n",
      "1.1 action:  [-0.2498, -0.9925] n_targets:  1 reward:  55.04\n",
      "2.5 action:  [-0.0953, -0.9808] n_targets:  1 reward:  61.67\n",
      "2.9 action:  [0.0508, -0.9951] n_targets:  1 reward:  58.34\n",
      "3.1 action:  [0.0432, -0.9921] n_targets:  1 reward:  52.65\n",
      "4.4 action:  [0.0024, -0.9916] n_targets:  1 reward:  55.78\n",
      "5.0 action:  [-0.481, -0.9955] n_targets:  1 reward:  54.46\n",
      "5.6 action:  [0.0772, -0.9967] n_targets:  1 reward:  50.78\n",
      "7.2 action:  [-0.1825, -0.9948] n_targets:  2 reward:  131.33\n",
      "9.2 action:  [0.0527, -0.9977] n_targets:  3 reward:  196.77\n",
      "9.8 action:  [-0.1314, -0.9942] n_targets:  1 reward:  57.06\n",
      "10.0 action:  [0.1439, -0.9935] n_targets:  2 reward:  109.17\n",
      "11.4 action:  [-0.0153, -0.9886] n_targets:  1 reward:  52.97\n",
      "13.4 action:  [-0.1222, -0.9971] n_targets:  2 reward:  118.17\n",
      "14.0 action:  [-0.4156, -0.9982] n_targets:  1 reward:  55.72\n",
      "14.8 action:  [0.1685, -0.9964] n_targets:  2 reward:  111.4\n",
      "16.4 action:  [0.3405, -0.9941] n_targets:  1 reward:  58.87\n",
      "17.8 action:  [-0.1412, -0.9824] n_targets:  1 reward:  50.08\n",
      "20.6 action:  [-0.0298, -0.9966] n_targets:  1 reward:  55.86\n",
      "22.6 action:  [-0.0079, -0.9946] n_targets:  1 reward:  57.1\n",
      "23.6 action:  [0.0428, -0.9873] n_targets:  1 reward:  52.9\n",
      "24.4 action:  [0.2324, -0.994] n_targets:  1 reward:  50.54\n",
      "25.6 action:  [-0.1523, -0.9895] n_targets:  1 reward:  57.67\n",
      "26.4 action:  [-0.2326, -0.9916] n_targets:  1 reward:  55.62\n",
      "30.2 action:  [-0.0833, -0.9809] n_targets:  1 reward:  54.85\n",
      "33.2 action:  [-0.1837, -0.9981] n_targets:  1 reward:  50.01\n",
      "33.6 action:  [-0.3815, -0.999] n_targets:  1 reward:  56.22\n",
      "34.4 action:  [-0.076, -0.9897] n_targets:  1 reward:  50.84\n",
      "34.8 action:  [0.0322, -0.9903] n_targets:  1 reward:  66.25\n",
      "35.4 action:  [-0.0491, -0.9984] n_targets:  1 reward:  51.7\n",
      "36.0 action:  [0.1341, -0.988] n_targets:  1 reward:  50.96\n",
      "37.0 action:  [0.1, -0.9957] n_targets:  1 reward:  54.71\n",
      "37.4 action:  [-0.131, -0.9892] n_targets:  1 reward:  55.12\n",
      "38.8 action:  [-0.1578, -0.998] n_targets:  1 reward:  52.5\n",
      "39.8 action:  [-0.2218, -0.9937] n_targets:  1 reward:  52.01\n",
      "43.3 action:  [-0.0102, -0.9893] n_targets:  1 reward:  52.86\n",
      "43.9 action:  [0.2691, -0.9884] n_targets:  1 reward:  53.46\n",
      "45.5 action:  [-0.1941, -0.9948] n_targets:  1 reward:  54.16\n",
      "45.7 action:  [-0.1005, -0.99] n_targets:  1 reward:  53.1\n",
      "46.5 action:  [-0.1514, -0.9928] n_targets:  2 reward:  127.81\n",
      "47.9 action:  [0.277, -0.9939] n_targets:  1 reward:  50.26\n",
      "48.5 action:  [0.2314, -0.9951] n_targets:  1 reward:  52.58\n",
      "49.5 action:  [0.0629, -0.9889] n_targets:  2 reward:  146.3\n",
      "50.9 action:  [-0.414, -0.993] n_targets:  1 reward:  56.28\n",
      "51.3 action:  [0.084, -0.9956] n_targets:  1 reward:  55.56\n",
      "51.5 action:  [0.1421, -0.9983] n_targets:  1 reward:  54.88\n",
      "51.9 action:  [0.02, -0.9964] n_targets:  1 reward:  54.74\n",
      "52.3 action:  [0.1975, -0.9834] n_targets:  1 reward:  50.92\n",
      "52.5 action:  [0.0198, -0.9837] n_targets:  1 reward:  51.88\n",
      "56.3 action:  [-0.147, -0.9912] n_targets:  1 reward:  63.44\n",
      "56.7 action:  [0.1333, -0.9987] n_targets:  1 reward:  54.06\n",
      "56.9 action:  [0.0686, -0.9956] n_targets:  1 reward:  53.73\n",
      "57.9 action:  [-0.0927, -0.993] n_targets:  1 reward:  57.77\n",
      "62.1 action:  [0.0697, -0.9919] n_targets:  2 reward:  116.59\n",
      "63.3 action:  [-0.1164, -0.9852] n_targets:  1 reward:  57.81\n",
      "63.9 action:  [0.1564, -0.997] n_targets:  2 reward:  116.42\n",
      "65.1 action:  [-0.1361, -0.9984] n_targets:  1 reward:  52.78\n",
      "65.3 action:  [-0.1566, -0.9981] n_targets:  1 reward:  51.08\n",
      "66.1 action:  [0.0819, -0.9974] n_targets:  1 reward:  51.91\n",
      "66.5 action:  [0.0525, -0.9884] n_targets:  1 reward:  51.01\n",
      "66.9 action:  [-0.1957, -0.996] n_targets:  1 reward:  50.58\n",
      "68.6 action:  [-0.2759, -0.9906] n_targets:  1 reward:  54.09\n",
      "71.0 action:  [-0.004, -0.9948] n_targets:  2 reward:  110.58\n",
      "72.2 action:  [-0.2488, -0.9967] n_targets:  1 reward:  55.89\n",
      "72.4 action:  [0.0206, -0.9976] n_targets:  1 reward:  56.63\n",
      "73.0 action:  [-0.162, -0.9992] n_targets:  1 reward:  50.93\n",
      "74.2 action:  [0.0092, -0.9857] n_targets:  1 reward:  51.12\n",
      "74.4 action:  [-0.0659, -0.9981] n_targets:  1 reward:  53.6\n",
      "75.2 action:  [0.0288, -0.9931] n_targets:  1 reward:  52.28\n",
      "75.4 action:  [-0.2214, -0.9869] n_targets:  1 reward:  53.32\n",
      "77.6 action:  [-0.1244, -0.9934] n_targets:  1 reward:  53.27\n",
      "78.0 action:  [-0.2541, -0.989] n_targets:  1 reward:  57.67\n",
      "79.0 action:  [0.0925, -0.9943] n_targets:  1 reward:  59.03\n",
      "79.4 action:  [-0.1789, -0.9911] n_targets:  2 reward:  106.44\n",
      "80.2 action:  [-0.091, -0.9974] n_targets:  1 reward:  72.65\n",
      "80.6 action:  [0.028, -0.9983] n_targets:  1 reward:  59.99\n",
      "81.2 action:  [0.0912, -0.9809] n_targets:  1 reward:  53.7\n",
      "82.8 action:  [-0.0087, -0.982] n_targets:  1 reward:  50.01\n",
      "86.2 action:  [-0.2139, -0.9893] n_targets:  1 reward:  55.44\n",
      "86.4 action:  [0.055, -0.9961] n_targets:  1 reward:  51.23\n",
      "87.2 action:  [-0.1507, -0.989] n_targets:  1 reward:  58.43\n",
      "89.8 action:  [-0.0712, -0.9907] n_targets:  1 reward:  54.53\n",
      "90.4 action:  [0.1704, -0.9958] n_targets:  1 reward:  51.15\n",
      "93.6 action:  [0.0775, -0.9967] n_targets:  1 reward:  60.01\n",
      "98.0 action:  [0.0272, -0.9937] n_targets:  1 reward:  73.67\n",
      "99.2 action:  [-0.0149, -0.9841] n_targets:  1 reward:  57.62\n",
      "99.4 action:  [-0.1238, -0.9909] n_targets:  1 reward:  50.88\n",
      "ALPHA (entropy-related):  tensor([0.1937], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.20408 0.20271 0.20141 0.20007 0.19875 0.19767 0.19656 0.19547 0.19465\n",
      " 0.19375]\n",
      "Episode: 104, Episode Reward: 5565.537734985352\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  272\n",
      "0.1 action:  [0.0675, -0.9939] n_targets:  1 reward:  74.63\n",
      "1.7 action:  [-0.0683, -0.9959] n_targets:  1 reward:  61.34\n",
      "8.8 action:  [0.0337, -0.9802] n_targets:  1 reward:  57.17\n",
      "9.0 action:  [0.0433, -0.9996] n_targets:  1 reward:  54.18\n",
      "9.6 action:  [0.0093, -0.9948] n_targets:  1 reward:  51.17\n",
      "11.6 action:  [-0.1896, -0.9975] n_targets:  1 reward:  57.17\n",
      "12.0 action:  [-0.0422, -0.9931] n_targets:  1 reward:  52.49\n",
      "12.3 action:  [-0.0284, -0.9954] n_targets:  1 reward:  61.78\n",
      "12.7 action:  [-0.0559, -0.9974] n_targets:  1 reward:  58.72\n",
      "14.5 action:  [-0.2406, -0.9944] n_targets:  1 reward:  58.09\n",
      "14.9 action:  [0.286, -0.9925] n_targets:  1 reward:  51.1\n",
      "17.3 action:  [-0.0753, -0.9923] n_targets:  1 reward:  57.22\n",
      "17.9 action:  [-0.2121, -0.993] n_targets:  1 reward:  50.33\n",
      "18.5 action:  [-0.1583, -0.9967] n_targets:  1 reward:  52.03\n",
      "19.7 action:  [-0.1081, -0.9927] n_targets:  1 reward:  54.28\n",
      "19.9 action:  [-0.1206, -0.9917] n_targets:  1 reward:  51.31\n",
      "20.1 action:  [0.2841, -0.9854] n_targets:  1 reward:  51.18\n",
      "21.9 action:  [-0.0214, -0.9961] n_targets:  1 reward:  78.39\n",
      "22.7 action:  [0.0649, -0.9944] n_targets:  1 reward:  55.14\n",
      "24.7 action:  [0.1258, -0.9926] n_targets:  1 reward:  73.43\n",
      "25.7 action:  [-0.074, -0.9948] n_targets:  2 reward:  113.25\n",
      "27.8 action:  [-0.0922, -0.9988] n_targets:  1 reward:  57.98\n",
      "29.2 action:  [-0.0808, -0.9934] n_targets:  1 reward:  51.6\n",
      "30.0 action:  [-0.0789, -0.9966] n_targets:  1 reward:  50.55\n",
      "30.2 action:  [-0.0901, -0.9876] n_targets:  1 reward:  65.26\n",
      "30.4 action:  [0.1328, -0.9985] n_targets:  1 reward:  59.75\n",
      "33.5 action:  [-0.1834, -0.99] n_targets:  1 reward:  53.63\n",
      "34.3 action:  [-0.0425, -0.9965] n_targets:  1 reward:  51.82\n",
      "36.9 action:  [-0.1233, -0.9944] n_targets:  1 reward:  53.2\n",
      "37.5 action:  [0.0262, -0.9933] n_targets:  1 reward:  50.16\n",
      "37.7 action:  [0.2522, -0.9854] n_targets:  1 reward:  50.47\n",
      "40.1 action:  [-0.1498, -0.9968] n_targets:  1 reward:  56.95\n",
      "40.3 action:  [-0.0568, -0.9929] n_targets:  1 reward:  65.24\n",
      "40.7 action:  [0.1885, -0.9935] n_targets:  2 reward:  111.02\n",
      "42.1 action:  [-0.0536, -0.9833] n_targets:  1 reward:  50.2\n",
      "44.9 action:  [-0.1263, -0.9958] n_targets:  1 reward:  56.42\n",
      "45.5 action:  [-0.1796, -0.9978] n_targets:  1 reward:  56.23\n",
      "45.9 action:  [-0.1341, -0.9966] n_targets:  1 reward:  56.55\n",
      "47.7 action:  [-0.1508, -0.9933] n_targets:  1 reward:  53.88\n",
      "48.3 action:  [-0.1983, -0.9955] n_targets:  1 reward:  59.4\n",
      "49.5 action:  [-0.0253, -0.9932] n_targets:  1 reward:  51.87\n",
      "50.1 action:  [-0.2579, -0.995] n_targets:  1 reward:  50.21\n",
      "50.3 action:  [0.2008, -0.9932] n_targets:  1 reward:  51.31\n",
      "50.5 action:  [-0.0149, -0.9978] n_targets:  1 reward:  62.24\n",
      "54.4 action:  [0.1716, -0.9944] n_targets:  2 reward:  130.68\n",
      "55.8 action:  [-0.2941, -0.994] n_targets:  1 reward:  53.21\n",
      "57.6 action:  [-0.1331, -0.9904] n_targets:  1 reward:  56.2\n",
      "60.2 action:  [-0.1868, -0.9937] n_targets:  2 reward:  118.67\n",
      "60.6 action:  [0.0478, -0.9974] n_targets:  1 reward:  51.85\n",
      "62.6 action:  [0.0488, -0.9967] n_targets:  1 reward:  54.44\n",
      "63.6 action:  [0.1226, -0.9934] n_targets:  1 reward:  59.22\n",
      "64.0 action:  [-0.277, -0.9979] n_targets:  1 reward:  54.43\n",
      "65.0 action:  [0.1887, -0.9986] n_targets:  1 reward:  52.87\n",
      "65.2 action:  [-0.4005, -0.9964] n_targets:  1 reward:  50.74\n",
      "65.4 action:  [-0.3248, -0.9924] n_targets:  2 reward:  110.79\n",
      "66.4 action:  [0.0554, -0.9839] n_targets:  1 reward:  51.65\n",
      "68.0 action:  [0.146, -0.9942] n_targets:  1 reward:  51.52\n",
      "70.2 action:  [0.0239, -0.9977] n_targets:  1 reward:  51.28\n",
      "70.4 action:  [0.1315, -0.9833] n_targets:  1 reward:  53.34\n",
      "72.0 action:  [-0.1182, -0.996] n_targets:  1 reward:  56.5\n",
      "72.4 action:  [-0.1544, -0.9875] n_targets:  1 reward:  73.71\n",
      "72.6 action:  [-0.3053, -0.9851] n_targets:  1 reward:  52.23\n",
      "74.6 action:  [0.0864, -0.997] n_targets:  1 reward:  58.9\n",
      "74.8 action:  [0.2813, -0.996] n_targets:  1 reward:  50.69\n",
      "75.4 action:  [-0.2587, -0.9816] n_targets:  1 reward:  51.69\n",
      "78.7 action:  [-0.1999, -0.9976] n_targets:  4 reward:  260.28\n",
      "79.1 action:  [0.0939, -0.9943] n_targets:  1 reward:  60.2\n",
      "81.4 action:  [-0.0672, -0.9926] n_targets:  1 reward:  67.92\n",
      "82.0 action:  [-0.0192, -0.9948] n_targets:  1 reward:  73.84\n",
      "82.4 action:  [-0.203, -0.9992] n_targets:  1 reward:  50.23\n",
      "83.2 action:  [-0.1473, -0.9969] n_targets:  4 reward:  270.85\n",
      "83.8 action:  [-0.1244, -0.9912] n_targets:  1 reward:  61.07\n",
      "84.2 action:  [-0.089, -0.9978] n_targets:  2 reward:  112.75\n",
      "87.2 action:  [-0.2841, -0.9965] n_targets:  1 reward:  64.81\n",
      "89.8 action:  [-0.1735, -0.9915] n_targets:  1 reward:  58.63\n",
      "90.6 action:  [-0.0936, -0.9974] n_targets:  1 reward:  68.8\n",
      "91.2 action:  [0.2067, -0.9939] n_targets:  1 reward:  56.36\n",
      "91.6 action:  [0.1518, -0.9967] n_targets:  2 reward:  101.81\n",
      "92.0 action:  [-0.2954, -0.9973] n_targets:  1 reward:  53.24\n",
      "92.4 action:  [-0.1939, -0.9865] n_targets:  1 reward:  51.39\n",
      "93.2 action:  [0.1622, -0.9842] n_targets:  4 reward:  244.2\n",
      "93.4 action:  [-0.1795, -0.9928] n_targets:  3 reward:  165.62\n",
      "94.2 action:  [-0.1033, -0.9816] n_targets:  1 reward:  52.3\n",
      "96.2 action:  [-0.0581, -0.9955] n_targets:  1 reward:  59.21\n",
      "96.4 action:  [-0.0047, -0.9907] n_targets:  1 reward:  51.6\n",
      "97.8 action:  [0.0238, -0.9981] n_targets:  1 reward:  53.73\n",
      "98.8 action:  [-0.3497, -0.9922] n_targets:  3 reward:  185.55\n",
      "99.2 action:  [0.1192, -0.9947] n_targets:  1 reward:  54.55\n",
      "99.4 action:  [-0.163, -0.9828] n_targets:  1 reward:  53.86\n",
      "99.6 action:  [-0.0877, -0.9876] n_targets:  2 reward:  106.23\n",
      "100.4 action:  [0.2776, -0.9887] n_targets:  1 reward:  67.52\n",
      "102.0 action:  [-0.0863, -0.9842] n_targets:  1 reward:  60.36\n",
      "102.4 action:  [0.0762, -0.9874] n_targets:  1 reward:  71.81\n",
      "ALPHA (entropy-related):  tensor([0.1929], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.20271 0.20141 0.20007 0.19875 0.19767 0.19656 0.19547 0.19465 0.19375\n",
      " 0.19287]\n",
      "Last 100 ALPHA: [0.84794 0.82852 0.80965 0.79129 0.7734  0.75598 0.73901 0.72248 0.70635\n",
      " 0.69064 0.67531 0.66036 0.64578 0.63155 0.61769 0.60419 0.59106 0.5783\n",
      " 0.5658  0.55365 0.54182 0.53022 0.51899 0.50808 0.49747 0.48716 0.47703\n",
      " 0.4672  0.45771 0.44833 0.43925 0.43039 0.42165 0.41309 0.40474 0.39652\n",
      " 0.38867 0.38105 0.37368 0.36641 0.35933 0.35272 0.34622 0.33993 0.33402\n",
      " 0.32853 0.32323 0.3181  0.3132  0.30834 0.30365 0.29916 0.2949  0.29077\n",
      " 0.28672 0.28285 0.27889 0.27518 0.27152 0.26775 0.26443 0.26134 0.2583\n",
      " 0.25516 0.25228 0.24955 0.24708 0.24469 0.24231 0.23982 0.2372  0.23469\n",
      " 0.23254 0.23052 0.22838 0.22627 0.22428 0.22241 0.22068 0.21899 0.21737\n",
      " 0.21588 0.21455 0.21316 0.21174 0.21035 0.2088  0.20702 0.20546 0.20408\n",
      " 0.20271 0.20141 0.20007 0.19875 0.19767 0.19656 0.19547 0.19465 0.19375\n",
      " 0.19287]\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  273\n",
      "0.1 action:  [0.0047, -0.993] n_targets:  2 reward:  103.05\n",
      "0.3 action:  [-0.029, -0.993] n_targets:  1 reward:  62.29\n",
      "2.9 action:  [-0.0277, -0.9936] n_targets:  1 reward:  53.62\n",
      "3.1 action:  [-0.0204, -0.993] n_targets:  1 reward:  57.19\n",
      "3.7 action:  [-0.0309, -0.9929] n_targets:  2 reward:  104.12\n",
      "4.5 action:  [-0.014, -0.9925] n_targets:  1 reward:  51.41\n",
      "5.1 action:  [-0.0146, -0.9925] n_targets:  1 reward:  52.04\n",
      "5.5 action:  [-0.015, -0.9929] n_targets:  1 reward:  54.21\n",
      "5.7 action:  [-0.0073, -0.9926] n_targets:  2 reward:  104.0\n",
      "6.9 action:  [-0.0318, -0.9929] n_targets:  1 reward:  51.45\n",
      "7.7 action:  [-0.055, -0.9927] n_targets:  1 reward:  56.58\n",
      "8.5 action:  [-0.0493, -0.9925] n_targets:  1 reward:  58.47\n",
      "11.1 action:  [-0.054, -0.9915] n_targets:  1 reward:  51.1\n",
      "13.7 action:  [-0.0269, -0.9927] n_targets:  1 reward:  56.67\n",
      "15.1 action:  [-0.0283, -0.9931] n_targets:  1 reward:  54.86\n",
      "17.3 action:  [-0.0236, -0.9929] n_targets:  1 reward:  55.61\n",
      "18.5 action:  [-0.0218, -0.9926] n_targets:  1 reward:  52.33\n",
      "18.9 action:  [-0.0207, -0.9926] n_targets:  1 reward:  55.82\n",
      "21.5 action:  [-0.0201, -0.9924] n_targets:  1 reward:  50.42\n",
      "21.7 action:  [-0.0111, -0.992] n_targets:  1 reward:  52.76\n",
      "23.5 action:  [-0.0387, -0.993] n_targets:  1 reward:  59.44\n",
      "23.7 action:  [-0.0326, -0.9927] n_targets:  1 reward:  60.55\n",
      "24.3 action:  [-0.0048, -0.9924] n_targets:  1 reward:  50.78\n",
      "26.5 action:  [-0.0021, -0.9921] n_targets:  1 reward:  50.67\n",
      "27.3 action:  [-0.0223, -0.9922] n_targets:  2 reward:  159.31\n",
      "27.9 action:  [-0.0082, -0.9922] n_targets:  1 reward:  57.36\n",
      "28.5 action:  [-0.0153, -0.9925] n_targets:  1 reward:  55.48\n",
      "33.3 action:  [-0.0162, -0.9927] n_targets:  1 reward:  57.37\n",
      "33.7 action:  [-0.0175, -0.9928] n_targets:  1 reward:  51.08\n",
      "34.1 action:  [-0.0205, -0.9928] n_targets:  1 reward:  60.12\n",
      "34.3 action:  [-0.0205, -0.9929] n_targets:  1 reward:  50.09\n",
      "34.9 action:  [-0.0366, -0.9932] n_targets:  1 reward:  51.76\n",
      "35.5 action:  [-0.0262, -0.9926] n_targets:  1 reward:  53.15\n",
      "38.5 action:  [-0.0618, -0.9936] n_targets:  1 reward:  58.71\n",
      "39.9 action:  [-0.0399, -0.9926] n_targets:  1 reward:  54.07\n",
      "41.9 action:  [-0.0143, -0.9911] n_targets:  1 reward:  55.01\n",
      "42.9 action:  [-0.075, -0.9919] n_targets:  1 reward:  55.78\n",
      "50.9 action:  [-0.0174, -0.9924] n_targets:  1 reward:  60.58\n",
      "51.5 action:  [-0.0184, -0.9925] n_targets:  1 reward:  51.45\n",
      "52.1 action:  [-0.0322, -0.9928] n_targets:  2 reward:  115.37\n",
      "52.3 action:  [-0.0257, -0.9927] n_targets:  1 reward:  51.69\n",
      "52.5 action:  [-0.0235, -0.9927] n_targets:  1 reward:  51.19\n",
      "54.3 action:  [-0.0255, -0.9928] n_targets:  2 reward:  120.66\n",
      "55.3 action:  [-0.0245, -0.9927] n_targets:  1 reward:  52.66\n",
      "55.9 action:  [-0.0092, -0.9923] n_targets:  1 reward:  52.06\n",
      "56.1 action:  [-0.0287, -0.9925] n_targets:  1 reward:  60.13\n",
      "56.3 action:  [-0.0183, -0.9924] n_targets:  1 reward:  54.85\n",
      "57.5 action:  [-0.0738, -0.9919] n_targets:  1 reward:  53.86\n",
      "59.9 action:  [-0.0109, -0.9925] n_targets:  1 reward:  65.84\n",
      "60.1 action:  [-0.0348, -0.9933] n_targets:  1 reward:  57.63\n",
      "60.5 action:  [-0.0224, -0.993] n_targets:  1 reward:  53.92\n",
      "61.7 action:  [-0.0251, -0.9934] n_targets:  1 reward:  51.54\n",
      "63.3 action:  [-0.0163, -0.9924] n_targets:  1 reward:  54.39\n",
      "64.3 action:  [-0.018, -0.9926] n_targets:  1 reward:  56.7\n",
      "65.3 action:  [-0.026, -0.9927] n_targets:  1 reward:  50.23\n",
      "65.7 action:  [-0.018, -0.9926] n_targets:  1 reward:  50.11\n",
      "66.9 action:  [-0.01, -0.9923] n_targets:  1 reward:  57.72\n",
      "68.5 action:  [-0.0284, -0.9931] n_targets:  1 reward:  61.49\n",
      "70.7 action:  [-0.0003, -0.9924] n_targets:  1 reward:  57.29\n",
      "74.5 action:  [-0.0148, -0.9929] n_targets:  2 reward:  111.95\n",
      "74.9 action:  [-0.0815, -0.9922] n_targets:  1 reward:  52.03\n",
      "76.1 action:  [-0.0249, -0.9928] n_targets:  3 reward:  223.69\n",
      "77.3 action:  [-0.0173, -0.9925] n_targets:  1 reward:  58.13\n",
      "80.3 action:  [-0.0184, -0.9925] n_targets:  1 reward:  50.15\n",
      "80.5 action:  [-0.0153, -0.9927] n_targets:  1 reward:  52.79\n",
      "81.1 action:  [-0.0277, -0.993] n_targets:  1 reward:  53.85\n",
      "81.3 action:  [-0.0324, -0.993] n_targets:  1 reward:  53.88\n",
      "81.5 action:  [-0.0249, -0.9927] n_targets:  1 reward:  52.75\n",
      "82.1 action:  [-0.0227, -0.993] n_targets:  1 reward:  57.18\n",
      "82.5 action:  [-0.0158, -0.9928] n_targets:  1 reward:  54.94\n",
      "83.7 action:  [-0.0251, -0.9929] n_targets:  1 reward:  52.73\n",
      "83.9 action:  [-0.0135, -0.9927] n_targets:  1 reward:  54.6\n",
      "84.5 action:  [-0.0091, -0.9925] n_targets:  1 reward:  56.71\n",
      "88.3 action:  [-0.0141, -0.9924] n_targets:  1 reward:  50.97\n",
      "89.9 action:  [-0.0242, -0.9922] n_targets:  1 reward:  54.54\n",
      "91.7 action:  [-0.0759, -0.9927] n_targets:  1 reward:  53.91\n",
      "93.7 action:  [-0.0223, -0.9924] n_targets:  2 reward:  108.45\n",
      "96.7 action:  [-0.016, -0.9926] n_targets:  1 reward:  51.37\n",
      "97.5 action:  [-0.0295, -0.9928] n_targets:  2 reward:  104.53\n",
      "98.7 action:  [-0.0171, -0.9927] n_targets:  2 reward:  115.98\n",
      "98.9 action:  [-0.0241, -0.993] n_targets:  1 reward:  59.53\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  274\n",
      "0.5 action:  [-0.0165, -0.9928] n_targets:  1 reward:  59.53\n",
      "1.7 action:  [-0.0081, -0.9928] n_targets:  1 reward:  51.57\n",
      "2.5 action:  [-0.0321, -0.9927] n_targets:  1 reward:  58.72\n",
      "2.7 action:  [-0.0303, -0.9926] n_targets:  1 reward:  52.9\n",
      "4.1 action:  [-0.0218, -0.9928] n_targets:  1 reward:  58.73\n",
      "4.9 action:  [-0.0198, -0.9925] n_targets:  1 reward:  51.15\n",
      "5.1 action:  [-0.0231, -0.9927] n_targets:  1 reward:  54.85\n",
      "5.3 action:  [-0.0357, -0.9932] n_targets:  1 reward:  58.7\n",
      "6.3 action:  [-0.0232, -0.993] n_targets:  1 reward:  52.02\n",
      "6.9 action:  [-0.0191, -0.9929] n_targets:  1 reward:  50.93\n",
      "8.1 action:  [0.005, -0.9922] n_targets:  1 reward:  55.8\n",
      "9.5 action:  [0.0069, -0.9922] n_targets:  1 reward:  51.46\n",
      "9.9 action:  [0.0007, -0.9924] n_targets:  1 reward:  53.44\n",
      "13.1 action:  [-0.0199, -0.9929] n_targets:  1 reward:  50.6\n",
      "15.1 action:  [-0.0413, -0.9935] n_targets:  1 reward:  52.96\n",
      "15.7 action:  [-0.0154, -0.9924] n_targets:  1 reward:  52.62\n",
      "17.1 action:  [-0.0084, -0.9919] n_targets:  2 reward:  107.47\n",
      "17.3 action:  [-0.004, -0.9918] n_targets:  1 reward:  50.68\n",
      "18.5 action:  [-0.0481, -0.9932] n_targets:  1 reward:  51.16\n",
      "20.1 action:  [-0.0303, -0.993] n_targets:  1 reward:  55.27\n",
      "20.7 action:  [-0.0255, -0.9932] n_targets:  1 reward:  54.41\n",
      "21.1 action:  [-0.0131, -0.9927] n_targets:  1 reward:  51.15\n",
      "21.3 action:  [-0.0059, -0.9923] n_targets:  1 reward:  60.49\n",
      "21.9 action:  [-0.0042, -0.9922] n_targets:  1 reward:  55.03\n",
      "22.3 action:  [-0.0312, -0.993] n_targets:  1 reward:  55.57\n",
      "23.7 action:  [-0.022, -0.9924] n_targets:  1 reward:  50.48\n",
      "23.9 action:  [-0.0246, -0.9925] n_targets:  1 reward:  53.73\n",
      "24.3 action:  [-0.0299, -0.9931] n_targets:  1 reward:  54.43\n",
      "24.7 action:  [-0.0112, -0.9922] n_targets:  1 reward:  55.54\n",
      "26.5 action:  [-0.0459, -0.9931] n_targets:  1 reward:  61.53\n",
      "26.7 action:  [-0.0436, -0.9929] n_targets:  1 reward:  60.62\n",
      "27.1 action:  [-0.0139, -0.9924] n_targets:  2 reward:  104.29\n",
      "27.5 action:  [-0.066, -0.993] n_targets:  2 reward:  112.08\n",
      "27.7 action:  [0.0382, -0.9927] n_targets:  1 reward:  58.67\n",
      "28.5 action:  [-0.0231, -0.9925] n_targets:  1 reward:  77.85\n",
      "28.9 action:  [-0.02, -0.9929] n_targets:  1 reward:  55.88\n",
      "29.9 action:  [-0.0178, -0.9929] n_targets:  1 reward:  52.45\n",
      "30.3 action:  [-0.0027, -0.9921] n_targets:  1 reward:  51.98\n",
      "30.9 action:  [-0.0273, -0.9925] n_targets:  1 reward:  53.45\n",
      "31.1 action:  [-0.0265, -0.9929] n_targets:  1 reward:  53.74\n",
      "32.7 action:  [-0.0099, -0.9924] n_targets:  1 reward:  59.63\n",
      "33.9 action:  [-0.0014, -0.992] n_targets:  1 reward:  53.79\n",
      "35.1 action:  [-0.005, -0.9923] n_targets:  2 reward:  103.18\n",
      "37.9 action:  [-0.0063, -0.9923] n_targets:  1 reward:  54.72\n",
      "40.5 action:  [-0.029, -0.993] n_targets:  1 reward:  55.75\n",
      "44.3 action:  [-0.0183, -0.9921] n_targets:  1 reward:  73.52\n",
      "46.1 action:  [-0.0044, -0.9923] n_targets:  1 reward:  53.95\n",
      "46.3 action:  [0.0043, -0.992] n_targets:  1 reward:  50.13\n",
      "49.1 action:  [-0.0245, -0.9929] n_targets:  2 reward:  101.94\n",
      "49.5 action:  [-0.0345, -0.9928] n_targets:  1 reward:  56.76\n",
      "49.7 action:  [-0.0272, -0.9925] n_targets:  2 reward:  111.97\n",
      "54.9 action:  [-0.0309, -0.9929] n_targets:  1 reward:  51.42\n",
      "55.7 action:  [-0.0224, -0.9925] n_targets:  1 reward:  58.93\n",
      "56.1 action:  [-0.0355, -0.9933] n_targets:  1 reward:  56.44\n",
      "56.9 action:  [-0.0061, -0.9923] n_targets:  1 reward:  55.89\n",
      "59.1 action:  [-0.0122, -0.9926] n_targets:  2 reward:  114.02\n",
      "60.1 action:  [-0.022, -0.9923] n_targets:  1 reward:  54.13\n",
      "61.5 action:  [-0.0322, -0.9925] n_targets:  1 reward:  54.02\n",
      "62.3 action:  [-0.0196, -0.9926] n_targets:  1 reward:  50.88\n",
      "64.9 action:  [-0.0056, -0.9924] n_targets:  1 reward:  53.67\n",
      "68.1 action:  [-0.0078, -0.9922] n_targets:  1 reward:  56.76\n",
      "68.7 action:  [-0.024, -0.9925] n_targets:  1 reward:  55.55\n",
      "69.7 action:  [-0.0379, -0.9928] n_targets:  1 reward:  58.33\n",
      "70.3 action:  [-0.0062, -0.9924] n_targets:  1 reward:  56.79\n",
      "71.1 action:  [-0.0212, -0.9928] n_targets:  1 reward:  56.47\n",
      "72.5 action:  [-0.0071, -0.9922] n_targets:  1 reward:  51.29\n",
      "74.5 action:  [-0.0173, -0.9923] n_targets:  2 reward:  109.56\n",
      "75.3 action:  [-0.0189, -0.9923] n_targets:  1 reward:  58.55\n",
      "77.3 action:  [-0.0545, -0.9931] n_targets:  1 reward:  58.84\n",
      "78.1 action:  [-0.0216, -0.9928] n_targets:  2 reward:  179.05\n",
      "78.9 action:  [-0.0314, -0.9928] n_targets:  1 reward:  51.34\n",
      "82.3 action:  [-0.0254, -0.9925] n_targets:  1 reward:  54.55\n",
      "82.7 action:  [-0.0212, -0.9924] n_targets:  1 reward:  56.68\n",
      "84.9 action:  [-0.0379, -0.993] n_targets:  1 reward:  50.89\n",
      "85.5 action:  [0.0044, -0.992] n_targets:  2 reward:  103.57\n",
      "88.1 action:  [-0.0181, -0.9927] n_targets:  1 reward:  59.21\n",
      "88.3 action:  [-0.0235, -0.9929] n_targets:  1 reward:  54.13\n",
      "88.5 action:  [-0.0382, -0.9932] n_targets:  1 reward:  53.3\n",
      "89.1 action:  [-0.0233, -0.9927] n_targets:  1 reward:  54.28\n",
      "89.9 action:  [-0.0191, -0.9923] n_targets:  1 reward:  52.02\n",
      "90.3 action:  [-0.0243, -0.9929] n_targets:  1 reward:  57.93\n",
      "91.7 action:  [-0.0246, -0.9926] n_targets:  1 reward:  52.76\n",
      "96.3 action:  [-0.0069, -0.9923] n_targets:  1 reward:  54.91\n",
      "97.1 action:  [-0.0047, -0.9922] n_targets:  3 reward:  156.75\n",
      "97.9 action:  [-0.0191, -0.9927] n_targets:  2 reward:  108.76\n",
      "98.3 action:  [-0.0259, -0.9924] n_targets:  1 reward:  62.45\n",
      "99.1 action:  [-0.0107, -0.9925] n_targets:  1 reward:  51.34\n",
      "100.5 action:  [-0.0398, -0.9932] n_targets:  1 reward:  56.75\n",
      "101.1 action:  [-0.0297, -0.9931] n_targets:  1 reward:  58.33\n",
      "102.1 action:  [-0.0372, -0.9932] n_targets:  1 reward:  52.25\n",
      "Best average reward: 5070.855417569475, Current average reward: 5468.409524281821\n",
      "Best average reward = 5468.409524281821\n",
      "Best model saved at episode 105 to None\n",
      "Evaluation rewards: [0.0, 0.0, 1720.9753100077312, 1085.8211957613628, 4981.922852198283, 5070.855417569475, 5468.409524281821]\n",
      "Episode: 105, Episode Reward: 6583.6675542195635\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  275\n",
      "1.3 action:  [0.0317, -0.9989] n_targets:  1 reward:  62.92\n",
      "3.7 action:  [-0.3984, -0.9912] n_targets:  1 reward:  56.11\n",
      "4.5 action:  [-0.1572, -0.9951] n_targets:  1 reward:  64.99\n",
      "9.4 action:  [-0.1422, -0.9916] n_targets:  1 reward:  51.38\n",
      "9.6 action:  [-0.0615, -0.9923] n_targets:  1 reward:  50.96\n",
      "10.2 action:  [-0.2533, -0.9856] n_targets:  1 reward:  57.09\n",
      "11.8 action:  [-0.0352, -0.9978] n_targets:  1 reward:  51.31\n",
      "12.0 action:  [0.0148, -0.9959] n_targets:  1 reward:  59.43\n",
      "14.0 action:  [0.3633, -0.9935] n_targets:  1 reward:  58.77\n",
      "14.6 action:  [0.0192, -0.9969] n_targets:  1 reward:  58.03\n",
      "15.4 action:  [0.2753, -0.9977] n_targets:  1 reward:  52.44\n",
      "16.6 action:  [-0.1435, -0.9953] n_targets:  2 reward:  112.34\n",
      "17.0 action:  [-0.1133, -0.9982] n_targets:  1 reward:  55.41\n",
      "17.8 action:  [0.2335, -0.9892] n_targets:  1 reward:  51.68\n",
      "19.6 action:  [0.0611, -0.9903] n_targets:  1 reward:  65.89\n",
      "20.2 action:  [0.0945, -0.9986] n_targets:  2 reward:  109.97\n",
      "21.2 action:  [-0.3885, -0.9848] n_targets:  1 reward:  58.85\n",
      "21.6 action:  [-0.296, -0.9968] n_targets:  1 reward:  54.06\n",
      "22.2 action:  [0.1927, -0.9811] n_targets:  1 reward:  50.45\n",
      "24.4 action:  [0.1007, -0.988] n_targets:  2 reward:  111.33\n",
      "24.6 action:  [0.4167, -0.9946] n_targets:  1 reward:  57.8\n",
      "26.0 action:  [0.3328, -0.9975] n_targets:  1 reward:  50.75\n",
      "27.6 action:  [0.1486, -0.9978] n_targets:  1 reward:  52.66\n",
      "29.2 action:  [-0.2236, -0.9984] n_targets:  1 reward:  52.49\n",
      "29.4 action:  [-0.0812, -0.9901] n_targets:  1 reward:  55.56\n",
      "30.8 action:  [-0.0243, -0.9941] n_targets:  1 reward:  54.53\n",
      "31.4 action:  [-0.151, -0.9979] n_targets:  1 reward:  55.95\n",
      "33.1 action:  [-0.2062, -0.988] n_targets:  1 reward:  59.0\n",
      "34.3 action:  [-0.1112, -0.9973] n_targets:  1 reward:  51.5\n",
      "36.3 action:  [0.1894, -0.9911] n_targets:  1 reward:  52.86\n",
      "36.5 action:  [0.1153, -0.9961] n_targets:  1 reward:  50.91\n",
      "37.5 action:  [-0.1013, -0.9937] n_targets:  2 reward:  111.62\n",
      "41.3 action:  [0.0967, -0.9967] n_targets:  1 reward:  50.39\n",
      "42.7 action:  [0.1315, -0.9986] n_targets:  2 reward:  111.0\n",
      "43.1 action:  [0.1768, -0.9957] n_targets:  1 reward:  66.18\n",
      "43.7 action:  [-0.0761, -0.9944] n_targets:  1 reward:  55.76\n",
      "44.1 action:  [-0.0177, -0.9949] n_targets:  1 reward:  51.0\n",
      "46.5 action:  [-0.0031, -0.9923] n_targets:  1 reward:  56.57\n",
      "46.7 action:  [-0.1263, -0.9996] n_targets:  1 reward:  58.54\n",
      "50.1 action:  [0.0766, -0.986] n_targets:  1 reward:  57.1\n",
      "50.5 action:  [-0.1469, -0.9942] n_targets:  1 reward:  57.2\n",
      "51.5 action:  [-0.0423, -0.9828] n_targets:  1 reward:  58.7\n",
      "52.5 action:  [-0.2247, -0.996] n_targets:  1 reward:  56.29\n",
      "53.1 action:  [-0.2639, -0.9895] n_targets:  1 reward:  56.19\n",
      "54.1 action:  [-0.3041, -0.9908] n_targets:  1 reward:  72.57\n",
      "54.9 action:  [-0.1998, -0.9974] n_targets:  1 reward:  54.29\n",
      "55.7 action:  [0.3026, -0.9979] n_targets:  1 reward:  57.47\n",
      "56.5 action:  [-0.1378, -0.996] n_targets:  1 reward:  57.37\n",
      "56.9 action:  [0.0518, -0.9969] n_targets:  1 reward:  53.86\n",
      "57.9 action:  [-0.3981, -0.9966] n_targets:  1 reward:  57.64\n",
      "58.7 action:  [0.004, -0.9949] n_targets:  1 reward:  55.39\n",
      "59.5 action:  [0.1093, -0.9901] n_targets:  1 reward:  54.5\n",
      "60.3 action:  [0.1063, -0.9964] n_targets:  1 reward:  50.27\n",
      "61.7 action:  [-0.1547, -0.9932] n_targets:  1 reward:  52.91\n",
      "63.1 action:  [-0.0291, -0.9925] n_targets:  1 reward:  60.11\n",
      "63.7 action:  [0.1005, -0.9892] n_targets:  1 reward:  59.52\n",
      "64.3 action:  [0.0212, -0.9985] n_targets:  2 reward:  142.14\n",
      "64.7 action:  [-0.0186, -0.9932] n_targets:  2 reward:  113.72\n",
      "65.1 action:  [0.0196, -0.9938] n_targets:  1 reward:  53.98\n",
      "66.1 action:  [-0.171, -0.991] n_targets:  1 reward:  51.34\n",
      "66.3 action:  [0.3221, -0.9847] n_targets:  2 reward:  101.8\n",
      "68.3 action:  [-0.2329, -0.9965] n_targets:  1 reward:  54.89\n",
      "68.7 action:  [-0.1168, -0.9891] n_targets:  2 reward:  122.4\n",
      "68.9 action:  [0.0706, -0.9993] n_targets:  1 reward:  55.55\n",
      "69.1 action:  [0.25, -0.9953] n_targets:  1 reward:  50.95\n",
      "69.9 action:  [-0.0852, -0.9984] n_targets:  1 reward:  60.84\n",
      "73.1 action:  [-0.4314, -0.9902] n_targets:  2 reward:  106.19\n",
      "74.3 action:  [-0.0804, -0.9978] n_targets:  1 reward:  59.05\n",
      "76.3 action:  [-0.3216, -0.9987] n_targets:  1 reward:  71.41\n",
      "76.9 action:  [-0.1312, -0.9925] n_targets:  1 reward:  59.53\n",
      "77.1 action:  [-0.2396, -0.9951] n_targets:  1 reward:  62.65\n",
      "82.3 action:  [-0.293, -0.9918] n_targets:  2 reward:  105.53\n",
      "82.7 action:  [-0.0726, -0.9832] n_targets:  1 reward:  63.26\n",
      "84.1 action:  [0.0898, -0.9872] n_targets:  1 reward:  58.52\n",
      "85.5 action:  [-0.1505, -0.9838] n_targets:  1 reward:  57.06\n",
      "85.7 action:  [-0.1879, -0.9984] n_targets:  1 reward:  59.83\n",
      "88.9 action:  [-0.0821, -0.9982] n_targets:  3 reward:  157.5\n",
      "90.3 action:  [-0.0209, -0.9994] n_targets:  1 reward:  50.29\n",
      "90.7 action:  [-0.0911, -0.9994] n_targets:  1 reward:  55.62\n",
      "90.9 action:  [-0.2166, -0.996] n_targets:  1 reward:  53.03\n",
      "91.9 action:  [-0.0501, -0.9831] n_targets:  1 reward:  53.57\n",
      "92.5 action:  [-0.0105, -0.9886] n_targets:  1 reward:  58.12\n",
      "93.1 action:  [-0.1004, -0.998] n_targets:  1 reward:  55.2\n",
      "95.0 action:  [-0.1885, -0.9965] n_targets:  1 reward:  50.28\n",
      "97.0 action:  [-0.0981, -0.9955] n_targets:  1 reward:  82.64\n",
      "97.8 action:  [-0.2256, -0.9988] n_targets:  1 reward:  58.62\n",
      "98.2 action:  [0.4427, -0.9854] n_targets:  2 reward:  103.89\n",
      "98.4 action:  [-0.1117, -0.9926] n_targets:  1 reward:  51.71\n",
      "98.6 action:  [-0.1982, -0.9931] n_targets:  1 reward:  50.49\n",
      "99.9 action:  [-0.0994, -0.9981] n_targets:  1 reward:  61.16\n",
      "101.5 action:  [0.0117, -0.9961] n_targets:  1 reward:  52.55\n",
      "102.3 action:  [-0.191, -0.991] n_targets:  2 reward:  101.88\n",
      "ALPHA (entropy-related):  tensor([0.1919], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.20141 0.20007 0.19875 0.19767 0.19656 0.19547 0.19465 0.19375 0.19287\n",
      " 0.19195]\n",
      "Episode: 106, Episode Reward: 6027.079863230387\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  276\n",
      "0.2 action:  [-0.0358, -0.9945] n_targets:  1 reward:  53.77\n",
      "2.6 action:  [-0.3428, -0.9969] n_targets:  1 reward:  57.68\n",
      "4.0 action:  [0.0286, -0.9844] n_targets:  1 reward:  53.44\n",
      "5.4 action:  [0.2483, -0.9881] n_targets:  1 reward:  54.45\n",
      "6.9 action:  [0.1017, -0.9969] n_targets:  1 reward:  59.4\n",
      "7.3 action:  [-0.0284, -0.9981] n_targets:  1 reward:  61.18\n",
      "7.5 action:  [0.0401, -0.9942] n_targets:  1 reward:  52.46\n",
      "10.4 action:  [-0.1692, -0.9914] n_targets:  1 reward:  59.2\n",
      "11.6 action:  [-0.0403, -0.9972] n_targets:  1 reward:  56.83\n",
      "12.2 action:  [-0.1589, -0.9972] n_targets:  1 reward:  53.35\n",
      "12.4 action:  [0.2114, -0.9859] n_targets:  1 reward:  59.11\n",
      "14.0 action:  [-0.2305, -0.991] n_targets:  1 reward:  63.31\n",
      "14.6 action:  [0.248, -0.9945] n_targets:  1 reward:  56.8\n",
      "17.4 action:  [0.1711, -0.9908] n_targets:  1 reward:  50.59\n",
      "18.4 action:  [0.0072, -0.9827] n_targets:  1 reward:  54.23\n",
      "21.8 action:  [-0.0219, -0.9922] n_targets:  1 reward:  62.86\n",
      "22.8 action:  [-0.1706, -0.9991] n_targets:  1 reward:  55.88\n",
      "24.2 action:  [0.4079, -0.9845] n_targets:  1 reward:  52.28\n",
      "24.8 action:  [0.0443, -0.9999] n_targets:  1 reward:  61.37\n",
      "25.0 action:  [0.1095, -0.9952] n_targets:  1 reward:  52.12\n",
      "25.2 action:  [0.2569, -0.992] n_targets:  1 reward:  53.94\n",
      "25.4 action:  [-0.0663, -0.9939] n_targets:  1 reward:  53.19\n",
      "31.0 action:  [-0.0535, -0.9842] n_targets:  1 reward:  66.58\n",
      "32.4 action:  [0.0001, -0.9941] n_targets:  1 reward:  55.4\n",
      "41.9 action:  [0.0227, -0.9882] n_targets:  1 reward:  55.37\n",
      "44.1 action:  [-0.0031, -0.9995] n_targets:  1 reward:  52.55\n",
      "44.5 action:  [-0.012, -0.997] n_targets:  1 reward:  55.78\n",
      "47.1 action:  [0.1012, -0.9977] n_targets:  1 reward:  59.67\n",
      "47.9 action:  [-0.1803, -0.992] n_targets:  1 reward:  51.56\n",
      "48.6 action:  [-0.1286, -0.992] n_targets:  1 reward:  57.52\n",
      "48.8 action:  [-0.4769, -0.9944] n_targets:  1 reward:  52.23\n",
      "49.0 action:  [0.0439, -0.9947] n_targets:  1 reward:  51.05\n",
      "49.2 action:  [-0.1313, -0.9934] n_targets:  1 reward:  56.99\n",
      "52.4 action:  [0.2271, -0.9883] n_targets:  1 reward:  65.4\n",
      "53.6 action:  [-0.1419, -0.9924] n_targets:  1 reward:  52.85\n",
      "56.8 action:  [-0.0395, -0.9926] n_targets:  2 reward:  113.55\n",
      "57.6 action:  [-0.2134, -0.9949] n_targets:  1 reward:  53.21\n",
      "65.5 action:  [0.1488, -0.991] n_targets:  1 reward:  51.99\n",
      "70.9 action:  [-0.0285, -0.9971] n_targets:  1 reward:  57.32\n",
      "71.7 action:  [-0.1828, -0.9968] n_targets:  1 reward:  53.58\n",
      "72.1 action:  [0.1057, -0.9879] n_targets:  1 reward:  51.66\n",
      "73.6 action:  [-0.2275, -0.9945] n_targets:  1 reward:  51.57\n",
      "74.6 action:  [-0.0507, -0.9925] n_targets:  1 reward:  55.0\n",
      "76.6 action:  [-0.0285, -0.9929] n_targets:  5 reward:  334.81\n",
      "79.6 action:  [-0.1816, -0.9961] n_targets:  1 reward:  52.4\n",
      "80.8 action:  [-0.1471, -0.9977] n_targets:  1 reward:  59.88\n",
      "81.8 action:  [-0.241, -0.9963] n_targets:  1 reward:  51.73\n",
      "82.6 action:  [-0.024, -0.9865] n_targets:  1 reward:  56.78\n",
      "83.2 action:  [0.0199, -0.9926] n_targets:  1 reward:  57.89\n",
      "86.8 action:  [0.3782, -0.9964] n_targets:  1 reward:  51.36\n",
      "87.0 action:  [0.3182, -0.9992] n_targets:  2 reward:  108.14\n",
      "93.4 action:  [0.2107, -0.9903] n_targets:  1 reward:  54.08\n",
      "96.2 action:  [0.0728, -0.9982] n_targets:  1 reward:  56.52\n",
      "96.4 action:  [-0.2002, -0.9939] n_targets:  1 reward:  52.85\n",
      "96.6 action:  [-0.1009, -0.9927] n_targets:  1 reward:  54.73\n",
      "97.0 action:  [0.0023, -0.9968] n_targets:  1 reward:  50.23\n",
      "97.8 action:  [-0.1382, -0.9975] n_targets:  1 reward:  60.0\n",
      "98.2 action:  [-0.2732, -0.9924] n_targets:  1 reward:  50.23\n",
      "101.2 action:  [0.0955, -0.9962] n_targets:  1 reward:  55.73\n",
      "101.6 action:  [0.2226, -0.9956] n_targets:  1 reward:  51.13\n",
      "ALPHA (entropy-related):  tensor([0.1909], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.20007 0.19875 0.19767 0.19656 0.19547 0.19465 0.19375 0.19287 0.19195\n",
      " 0.19094]\n",
      "Episode: 107, Episode Reward: 3716.7317174275713\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  277\n",
      "0.2 action:  [0.0203, -0.9968] n_targets:  1 reward:  63.92\n",
      "0.6 action:  [-0.1253, -0.9968] n_targets:  1 reward:  53.22\n",
      "2.0 action:  [0.1073, -0.9968] n_targets:  2 reward:  107.75\n",
      "2.2 action:  [-0.0941, -0.9942] n_targets:  1 reward:  53.56\n",
      "3.6 action:  [0.0664, -0.9994] n_targets:  1 reward:  56.21\n",
      "4.0 action:  [0.2134, -0.9868] n_targets:  1 reward:  51.0\n",
      "4.4 action:  [0.082, -0.9988] n_targets:  1 reward:  70.17\n",
      "6.0 action:  [0.0725, -0.9968] n_targets:  1 reward:  51.04\n",
      "7.2 action:  [-0.093, -0.983] n_targets:  1 reward:  59.53\n",
      "7.6 action:  [-0.2625, -0.9944] n_targets:  1 reward:  57.45\n",
      "9.5 action:  [-0.0099, -0.9945] n_targets:  1 reward:  62.5\n",
      "10.7 action:  [0.1403, -0.9834] n_targets:  1 reward:  54.94\n",
      "11.1 action:  [-0.0487, -0.9961] n_targets:  1 reward:  56.72\n",
      "13.1 action:  [0.2057, -0.9835] n_targets:  1 reward:  51.69\n",
      "15.1 action:  [-0.1771, -0.998] n_targets:  1 reward:  58.0\n",
      "21.2 action:  [-0.1116, -0.9959] n_targets:  1 reward:  56.78\n",
      "21.4 action:  [-0.503, -0.9855] n_targets:  1 reward:  61.51\n",
      "22.0 action:  [-0.0642, -0.9927] n_targets:  1 reward:  50.23\n",
      "22.6 action:  [-0.2272, -0.9957] n_targets:  1 reward:  54.82\n",
      "23.0 action:  [-0.072, -0.9886] n_targets:  1 reward:  51.04\n",
      "30.8 action:  [-0.0242, -0.9847] n_targets:  1 reward:  72.43\n",
      "31.4 action:  [0.1416, -0.9928] n_targets:  1 reward:  52.74\n",
      "32.4 action:  [-0.0495, -0.998] n_targets:  2 reward:  109.42\n",
      "32.6 action:  [-0.1814, -0.9967] n_targets:  1 reward:  51.47\n",
      "35.7 action:  [-0.0084, -0.989] n_targets:  1 reward:  54.74\n",
      "36.1 action:  [0.0026, -0.9901] n_targets:  1 reward:  63.34\n",
      "37.5 action:  [0.0548, -0.997] n_targets:  1 reward:  60.18\n",
      "37.9 action:  [-0.0354, -0.9969] n_targets:  1 reward:  50.78\n",
      "39.1 action:  [0.0412, -0.9963] n_targets:  1 reward:  56.13\n",
      "40.7 action:  [-0.1718, -0.9964] n_targets:  3 reward:  157.07\n",
      "41.1 action:  [0.0034, -0.9905] n_targets:  1 reward:  58.55\n",
      "41.7 action:  [-0.1413, -0.9945] n_targets:  1 reward:  57.99\n",
      "43.5 action:  [0.0592, -0.9968] n_targets:  1 reward:  55.63\n",
      "44.1 action:  [0.0697, -0.9962] n_targets:  1 reward:  56.97\n",
      "45.3 action:  [-0.0542, -0.9938] n_targets:  1 reward:  57.3\n",
      "47.2 action:  [-0.0458, -0.9944] n_targets:  1 reward:  55.34\n",
      "47.8 action:  [0.06, -0.9907] n_targets:  1 reward:  50.82\n",
      "48.6 action:  [-0.2147, -0.9985] n_targets:  1 reward:  58.58\n",
      "49.0 action:  [0.1806, -0.9924] n_targets:  1 reward:  55.57\n",
      "50.0 action:  [0.1985, -0.9954] n_targets:  1 reward:  55.68\n",
      "50.2 action:  [-0.0947, -0.9895] n_targets:  1 reward:  52.02\n",
      "51.8 action:  [0.135, -0.9914] n_targets:  1 reward:  51.38\n",
      "53.4 action:  [-0.147, -0.9951] n_targets:  1 reward:  51.07\n",
      "54.1 action:  [-0.2253, -0.9831] n_targets:  1 reward:  60.73\n",
      "55.1 action:  [-0.1332, -0.999] n_targets:  1 reward:  56.39\n",
      "55.5 action:  [-0.1986, -0.9997] n_targets:  1 reward:  57.34\n",
      "60.1 action:  [0.1465, -0.9956] n_targets:  1 reward:  66.15\n",
      "60.9 action:  [0.1165, -0.9955] n_targets:  1 reward:  66.87\n",
      "61.7 action:  [-0.0384, -0.9963] n_targets:  1 reward:  57.2\n",
      "62.8 action:  [-0.1243, -0.9991] n_targets:  2 reward:  111.91\n",
      "65.2 action:  [-0.1286, -0.998] n_targets:  1 reward:  50.16\n",
      "68.8 action:  [0.2992, -0.9953] n_targets:  2 reward:  108.03\n",
      "69.0 action:  [0.1563, -0.9973] n_targets:  1 reward:  50.28\n",
      "71.2 action:  [0.0857, -0.9966] n_targets:  1 reward:  62.1\n",
      "73.2 action:  [0.1357, -0.9887] n_targets:  1 reward:  50.54\n",
      "73.7 action:  [0.1042, -0.9975] n_targets:  1 reward:  59.2\n",
      "74.3 action:  [0.1345, -0.9873] n_targets:  2 reward:  106.35\n",
      "74.5 action:  [0.2608, -0.9928] n_targets:  1 reward:  56.61\n",
      "74.9 action:  [0.2665, -0.9952] n_targets:  1 reward:  53.13\n",
      "76.7 action:  [-0.1954, -0.9964] n_targets:  1 reward:  52.39\n",
      "79.3 action:  [0.1944, -0.9947] n_targets:  1 reward:  81.14\n",
      "80.5 action:  [0.0073, -0.994] n_targets:  1 reward:  51.29\n",
      "80.9 action:  [0.2521, -0.985] n_targets:  2 reward:  103.51\n",
      "81.3 action:  [0.0349, -0.9866] n_targets:  1 reward:  60.17\n",
      "83.3 action:  [0.1856, -0.992] n_targets:  1 reward:  50.55\n",
      "84.2 action:  [0.0944, -0.986] n_targets:  1 reward:  51.49\n",
      "85.0 action:  [0.1902, -0.993] n_targets:  1 reward:  55.11\n",
      "85.6 action:  [-0.0161, -0.9936] n_targets:  1 reward:  51.9\n",
      "86.2 action:  [0.1219, -0.9974] n_targets:  1 reward:  50.96\n",
      "87.8 action:  [0.3874, -0.9967] n_targets:  1 reward:  52.66\n",
      "89.6 action:  [-0.0975, -0.9865] n_targets:  1 reward:  56.46\n",
      "91.6 action:  [0.0095, -0.9957] n_targets:  1 reward:  53.8\n",
      "92.4 action:  [0.1628, -0.9996] n_targets:  1 reward:  57.77\n",
      "93.2 action:  [-0.1849, -0.9946] n_targets:  1 reward:  60.43\n",
      "95.8 action:  [0.1734, -0.9975] n_targets:  3 reward:  169.28\n",
      "96.2 action:  [0.0424, -0.9963] n_targets:  2 reward:  119.94\n",
      "96.8 action:  [0.1005, -0.9983] n_targets:  1 reward:  55.0\n",
      "98.8 action:  [0.045, -0.9903] n_targets:  1 reward:  50.67\n",
      "99.6 action:  [-0.0028, -0.9938] n_targets:  1 reward:  73.74\n",
      "101.1 action:  [-0.1108, -0.9984] n_targets:  1 reward:  61.24\n",
      "101.7 action:  [0.0467, -0.9905] n_targets:  1 reward:  57.21\n",
      "ALPHA (entropy-related):  tensor([0.1898], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.19875 0.19767 0.19656 0.19547 0.19465 0.19375 0.19287 0.19195 0.19094\n",
      " 0.18984]\n",
      "Episode: 108, Episode Reward: 5176.978226979573\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  278\n",
      "0.2 action:  [0.3004, -0.9977] n_targets:  1 reward:  55.41\n",
      "0.4 action:  [-0.3387, -0.9846] n_targets:  1 reward:  70.07\n",
      "0.6 action:  [0.01, -0.9993] n_targets:  1 reward:  62.09\n",
      "0.8 action:  [-0.3263, -0.9807] n_targets:  1 reward:  54.37\n",
      "1.0 action:  [-0.5333, -0.9971] n_targets:  1 reward:  50.28\n",
      "4.8 action:  [0.1303, -0.9967] n_targets:  1 reward:  52.29\n",
      "6.4 action:  [-0.0042, -0.994] n_targets:  1 reward:  55.26\n",
      "6.8 action:  [-0.2109, -0.997] n_targets:  1 reward:  53.98\n",
      "7.6 action:  [0.0147, -0.9965] n_targets:  1 reward:  51.23\n",
      "8.2 action:  [0.0412, -0.9922] n_targets:  1 reward:  51.73\n",
      "11.6 action:  [0.3252, -0.9868] n_targets:  1 reward:  57.42\n",
      "13.0 action:  [0.169, -0.9979] n_targets:  1 reward:  61.25\n",
      "14.0 action:  [-0.3045, -0.9968] n_targets:  2 reward:  104.71\n",
      "15.0 action:  [-0.2055, -0.9876] n_targets:  1 reward:  80.57\n",
      "18.0 action:  [0.2058, -0.9988] n_targets:  1 reward:  54.36\n",
      "19.4 action:  [0.209, -0.9914] n_targets:  1 reward:  58.1\n",
      "19.6 action:  [0.008, -0.9972] n_targets:  1 reward:  59.01\n",
      "21.6 action:  [-0.2249, -0.998] n_targets:  1 reward:  50.15\n",
      "22.2 action:  [0.0399, -0.9941] n_targets:  1 reward:  59.19\n",
      "22.4 action:  [-0.3629, -0.9986] n_targets:  1 reward:  53.49\n",
      "23.2 action:  [0.1533, -0.9974] n_targets:  1 reward:  57.78\n",
      "25.4 action:  [-0.2128, -0.9947] n_targets:  1 reward:  68.39\n",
      "26.2 action:  [-0.0131, -0.9925] n_targets:  2 reward:  114.35\n",
      "27.6 action:  [-0.2799, -0.9917] n_targets:  1 reward:  50.23\n",
      "31.4 action:  [-0.1251, -0.9844] n_targets:  2 reward:  107.57\n",
      "31.6 action:  [-0.1378, -0.9929] n_targets:  1 reward:  52.8\n",
      "32.2 action:  [0.0841, -0.9948] n_targets:  1 reward:  50.46\n",
      "32.8 action:  [0.0995, -0.9976] n_targets:  1 reward:  64.1\n",
      "33.2 action:  [-0.1807, -0.9843] n_targets:  1 reward:  50.73\n",
      "36.8 action:  [0.0583, -0.9989] n_targets:  1 reward:  60.36\n",
      "37.4 action:  [-0.1524, -0.988] n_targets:  2 reward:  105.18\n",
      "37.6 action:  [0.2812, -0.9988] n_targets:  1 reward:  51.11\n",
      "38.0 action:  [0.1756, -0.996] n_targets:  1 reward:  59.17\n",
      "38.2 action:  [-0.0466, -0.9946] n_targets:  2 reward:  107.8\n",
      "41.8 action:  [-0.065, -0.9989] n_targets:  1 reward:  54.26\n",
      "46.6 action:  [0.2673, -0.995] n_targets:  1 reward:  66.1\n",
      "48.2 action:  [-0.0202, -0.9975] n_targets:  1 reward:  54.96\n",
      "49.2 action:  [0.02, -0.9923] n_targets:  1 reward:  51.19\n",
      "52.6 action:  [0.3523, -0.9964] n_targets:  1 reward:  55.39\n",
      "53.0 action:  [-0.1337, -0.9933] n_targets:  1 reward:  55.59\n",
      "54.0 action:  [0.1976, -0.9932] n_targets:  1 reward:  53.13\n",
      "54.2 action:  [0.3276, -0.998] n_targets:  1 reward:  51.65\n",
      "58.8 action:  [0.1522, -0.9833] n_targets:  1 reward:  53.14\n",
      "59.2 action:  [0.4202, -0.9917] n_targets:  1 reward:  50.27\n",
      "60.8 action:  [0.4031, -0.9895] n_targets:  1 reward:  52.5\n",
      "64.6 action:  [-0.0219, -0.9945] n_targets:  2 reward:  114.47\n",
      "65.4 action:  [0.0652, -0.9925] n_targets:  1 reward:  51.57\n",
      "67.4 action:  [-0.0119, -0.9897] n_targets:  1 reward:  50.5\n",
      "68.6 action:  [0.0598, -0.9973] n_targets:  2 reward:  115.64\n",
      "73.2 action:  [-0.2507, -0.9987] n_targets:  1 reward:  60.01\n",
      "73.8 action:  [-0.0158, -0.9971] n_targets:  1 reward:  51.56\n",
      "74.2 action:  [-0.0429, -0.9862] n_targets:  2 reward:  107.14\n",
      "75.0 action:  [0.0087, -0.9925] n_targets:  1 reward:  58.37\n",
      "75.4 action:  [-0.2038, -0.9971] n_targets:  2 reward:  123.94\n",
      "78.0 action:  [0.1658, -0.9942] n_targets:  1 reward:  54.59\n",
      "79.2 action:  [0.2881, -0.9982] n_targets:  2 reward:  132.85\n",
      "79.6 action:  [-0.1916, -0.9944] n_targets:  1 reward:  56.07\n",
      "81.8 action:  [-0.1049, -0.9921] n_targets:  1 reward:  54.65\n",
      "82.0 action:  [-0.1724, -0.9915] n_targets:  1 reward:  54.5\n",
      "84.4 action:  [-0.2515, -0.9894] n_targets:  1 reward:  56.75\n",
      "84.8 action:  [-0.0344, -0.9919] n_targets:  2 reward:  100.88\n",
      "85.4 action:  [0.3202, -0.9819] n_targets:  2 reward:  132.74\n",
      "88.8 action:  [-0.0352, -0.9801] n_targets:  1 reward:  53.76\n",
      "89.2 action:  [0.0454, -0.9852] n_targets:  1 reward:  51.78\n",
      "89.4 action:  [-0.0805, -0.9985] n_targets:  1 reward:  56.61\n",
      "90.2 action:  [0.0956, -0.9862] n_targets:  1 reward:  53.41\n",
      "90.4 action:  [-0.1518, -0.9983] n_targets:  1 reward:  53.93\n",
      "91.2 action:  [-0.3107, -0.9917] n_targets:  1 reward:  57.59\n",
      "91.8 action:  [-0.095, -0.9917] n_targets:  2 reward:  111.16\n",
      "93.8 action:  [-0.0422, -0.9938] n_targets:  1 reward:  52.03\n",
      "94.6 action:  [-0.111, -0.9962] n_targets:  1 reward:  59.4\n",
      "94.8 action:  [0.1169, -0.9975] n_targets:  1 reward:  55.76\n",
      "95.4 action:  [-0.1092, -0.9928] n_targets:  1 reward:  57.66\n",
      "96.4 action:  [0.1, -0.9853] n_targets:  1 reward:  59.84\n",
      "96.6 action:  [0.0484, -0.9847] n_targets:  1 reward:  55.77\n",
      "101.4 action:  [-0.0979, -0.9856] n_targets:  1 reward:  51.93\n",
      "ALPHA (entropy-related):  tensor([0.1889], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.19767 0.19656 0.19547 0.19465 0.19375 0.19287 0.19195 0.19094 0.18984\n",
      " 0.18886]\n",
      "Episode: 109, Episode Reward: 5000.059321085611\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  279\n",
      "0.5 action:  [-0.2029, -0.9985] n_targets:  1 reward:  50.49\n",
      "1.1 action:  [0.0436, -0.9917] n_targets:  1 reward:  60.58\n",
      "1.7 action:  [-0.3754, -0.9872] n_targets:  1 reward:  59.87\n",
      "2.1 action:  [-0.132, -0.9941] n_targets:  1 reward:  60.58\n",
      "3.1 action:  [-0.2404, -0.9915] n_targets:  1 reward:  55.45\n",
      "6.3 action:  [-0.1191, -0.9887] n_targets:  1 reward:  50.92\n",
      "9.3 action:  [-0.3598, -0.9896] n_targets:  1 reward:  59.0\n",
      "9.9 action:  [-0.3601, -0.9941] n_targets:  1 reward:  53.09\n",
      "10.3 action:  [-0.2271, -0.9905] n_targets:  1 reward:  51.19\n",
      "10.5 action:  [-0.1301, -0.9975] n_targets:  1 reward:  53.66\n",
      "18.1 action:  [-0.0122, -0.9928] n_targets:  1 reward:  52.72\n",
      "18.9 action:  [-0.2362, -0.9812] n_targets:  1 reward:  53.12\n",
      "19.3 action:  [0.0408, -0.9956] n_targets:  1 reward:  58.1\n",
      "19.7 action:  [-0.0896, -0.9961] n_targets:  1 reward:  60.72\n",
      "22.9 action:  [-0.1844, -0.998] n_targets:  1 reward:  50.74\n",
      "23.5 action:  [-0.2558, -0.9987] n_targets:  2 reward:  101.47\n",
      "24.1 action:  [-0.3437, -0.9941] n_targets:  1 reward:  53.86\n",
      "24.3 action:  [0.003, -0.9916] n_targets:  1 reward:  50.9\n",
      "24.7 action:  [-0.2547, -0.9932] n_targets:  2 reward:  113.52\n",
      "25.3 action:  [0.0411, -0.9926] n_targets:  2 reward:  109.25\n",
      "26.9 action:  [-0.1031, -0.9864] n_targets:  1 reward:  57.88\n",
      "28.7 action:  [0.2203, -0.9872] n_targets:  1 reward:  80.59\n",
      "29.3 action:  [0.0803, -0.999] n_targets:  3 reward:  168.23\n",
      "29.7 action:  [-0.2071, -0.9902] n_targets:  1 reward:  61.35\n",
      "29.9 action:  [0.0037, -0.999] n_targets:  1 reward:  56.4\n",
      "30.5 action:  [-0.1145, -0.9915] n_targets:  1 reward:  53.42\n",
      "30.7 action:  [0.0955, -0.9883] n_targets:  1 reward:  50.6\n",
      "31.7 action:  [0.0295, -0.998] n_targets:  1 reward:  57.95\n",
      "32.5 action:  [0.0787, -0.995] n_targets:  1 reward:  57.5\n",
      "33.9 action:  [0.2961, -0.983] n_targets:  1 reward:  55.95\n",
      "35.1 action:  [-0.0808, -0.9961] n_targets:  1 reward:  50.4\n",
      "36.9 action:  [-0.0853, -0.9872] n_targets:  1 reward:  51.55\n",
      "37.1 action:  [0.2979, -0.9945] n_targets:  1 reward:  51.29\n",
      "37.7 action:  [-0.0405, -0.9977] n_targets:  1 reward:  54.31\n",
      "38.5 action:  [0.1283, -0.9864] n_targets:  1 reward:  54.77\n",
      "38.7 action:  [0.2709, -0.9971] n_targets:  1 reward:  55.45\n",
      "39.7 action:  [-0.1547, -0.9949] n_targets:  1 reward:  50.25\n",
      "39.9 action:  [-0.1594, -0.9952] n_targets:  1 reward:  54.67\n",
      "40.3 action:  [-0.2345, -0.9969] n_targets:  1 reward:  69.04\n",
      "43.9 action:  [0.2127, -0.9981] n_targets:  3 reward:  168.28\n",
      "46.8 action:  [0.2827, -0.9876] n_targets:  1 reward:  54.3\n",
      "48.0 action:  [0.0961, -0.9952] n_targets:  1 reward:  60.51\n",
      "49.0 action:  [-0.1507, -0.9971] n_targets:  1 reward:  55.31\n",
      "49.8 action:  [-0.1067, -0.9936] n_targets:  1 reward:  54.1\n",
      "50.4 action:  [0.1817, -0.9975] n_targets:  1 reward:  55.56\n",
      "51.2 action:  [0.0243, -0.9941] n_targets:  1 reward:  54.09\n",
      "52.2 action:  [0.2109, -0.9904] n_targets:  1 reward:  66.21\n",
      "52.8 action:  [0.2142, -0.9976] n_targets:  1 reward:  51.64\n",
      "54.2 action:  [-0.0544, -0.9973] n_targets:  1 reward:  50.94\n",
      "58.3 action:  [0.1392, -0.9878] n_targets:  3 reward:  171.02\n",
      "58.5 action:  [-0.047, -0.9899] n_targets:  1 reward:  52.76\n",
      "61.9 action:  [-0.2587, -0.9898] n_targets:  1 reward:  54.33\n",
      "62.7 action:  [-0.1129, -0.9923] n_targets:  1 reward:  59.3\n",
      "63.5 action:  [0.1496, -0.9867] n_targets:  1 reward:  60.2\n",
      "64.1 action:  [0.1911, -0.9913] n_targets:  1 reward:  52.02\n",
      "69.5 action:  [-0.1111, -0.9953] n_targets:  1 reward:  57.73\n",
      "74.1 action:  [0.0541, -0.995] n_targets:  1 reward:  54.05\n",
      "76.7 action:  [-0.0903, -0.9923] n_targets:  2 reward:  118.45\n",
      "76.9 action:  [-0.0419, -0.9971] n_targets:  2 reward:  115.38\n",
      "77.3 action:  [-0.3515, -0.9922] n_targets:  1 reward:  52.35\n",
      "79.5 action:  [-0.0473, -0.9909] n_targets:  3 reward:  167.25\n",
      "81.9 action:  [0.0739, -0.9973] n_targets:  1 reward:  53.34\n",
      "84.9 action:  [-0.2003, -0.9975] n_targets:  1 reward:  60.95\n",
      "85.1 action:  [-0.0057, -0.9839] n_targets:  1 reward:  51.03\n",
      "85.9 action:  [-0.0942, -0.996] n_targets:  1 reward:  55.37\n",
      "87.3 action:  [0.097, -0.9926] n_targets:  1 reward:  51.99\n",
      "87.7 action:  [-0.0567, -0.9973] n_targets:  1 reward:  51.2\n",
      "88.9 action:  [0.2477, -0.9938] n_targets:  2 reward:  138.38\n",
      "89.7 action:  [0.2552, -0.9997] n_targets:  1 reward:  50.64\n",
      "90.1 action:  [-0.2514, -0.9859] n_targets:  1 reward:  69.95\n",
      "91.5 action:  [-0.5187, -0.9928] n_targets:  1 reward:  55.24\n",
      "92.5 action:  [0.1093, -0.9969] n_targets:  1 reward:  51.25\n",
      "93.3 action:  [-0.1823, -0.9958] n_targets:  1 reward:  50.54\n",
      "93.5 action:  [-0.3219, -0.9929] n_targets:  1 reward:  51.04\n",
      "93.7 action:  [0.1018, -0.995] n_targets:  1 reward:  50.04\n",
      "95.9 action:  [-0.2254, -0.9979] n_targets:  1 reward:  56.83\n",
      "98.9 action:  [-0.2935, -0.9921] n_targets:  1 reward:  52.21\n",
      "99.5 action:  [0.2045, -0.993] n_targets:  2 reward:  111.24\n",
      "100.5 action:  [-0.1472, -0.9975] n_targets:  2 reward:  144.2\n",
      "101.1 action:  [0.1936, -0.997] n_targets:  1 reward:  62.23\n",
      "101.3 action:  [-0.1269, -0.9902] n_targets:  1 reward:  59.04\n",
      "101.9 action:  [-0.0831, -0.9907] n_targets:  1 reward:  50.05\n",
      "ALPHA (entropy-related):  tensor([0.1881], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.19656 0.19547 0.19465 0.19375 0.19287 0.19195 0.19094 0.18984 0.18886\n",
      " 0.18805]\n",
      "Episode: 110, Episode Reward: 5513.397895812988\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  280\n",
      "0.3 action:  [-0.1448, -0.9895] n_targets:  2 reward:  128.42\n",
      "0.5 action:  [-0.4044, -0.9986] n_targets:  1 reward:  52.21\n",
      "1.3 action:  [-0.044, -0.9949] n_targets:  1 reward:  54.94\n",
      "2.7 action:  [0.0327, -0.9978] n_targets:  1 reward:  56.1\n",
      "3.5 action:  [0.2345, -0.9976] n_targets:  1 reward:  54.97\n",
      "3.7 action:  [-0.1188, -0.9977] n_targets:  1 reward:  62.14\n",
      "4.1 action:  [-0.2143, -0.9966] n_targets:  1 reward:  51.78\n",
      "4.9 action:  [0.048, -0.9969] n_targets:  2 reward:  111.89\n",
      "5.9 action:  [-0.0785, -0.9865] n_targets:  2 reward:  110.57\n",
      "6.3 action:  [-0.2114, -0.9932] n_targets:  1 reward:  63.54\n",
      "7.3 action:  [-0.0928, -0.9958] n_targets:  1 reward:  56.02\n",
      "10.1 action:  [-0.2278, -0.9982] n_targets:  1 reward:  58.37\n",
      "10.3 action:  [-0.2165, -0.9954] n_targets:  1 reward:  55.63\n",
      "10.9 action:  [-0.0431, -0.9867] n_targets:  1 reward:  50.79\n",
      "11.5 action:  [-0.0253, -0.9859] n_targets:  1 reward:  62.15\n",
      "12.1 action:  [0.2283, -0.9956] n_targets:  1 reward:  55.11\n",
      "12.3 action:  [0.2791, -0.9986] n_targets:  1 reward:  53.43\n",
      "12.5 action:  [-0.2427, -0.9959] n_targets:  1 reward:  52.52\n",
      "13.7 action:  [-0.1012, -0.988] n_targets:  1 reward:  57.96\n",
      "14.5 action:  [-0.166, -0.9936] n_targets:  1 reward:  51.73\n",
      "17.7 action:  [0.1328, -0.9887] n_targets:  1 reward:  61.22\n",
      "18.5 action:  [-0.3392, -0.9808] n_targets:  1 reward:  54.14\n",
      "19.1 action:  [0.0429, -0.9959] n_targets:  2 reward:  126.76\n",
      "21.5 action:  [0.0159, -0.9975] n_targets:  1 reward:  60.26\n",
      "24.1 action:  [-0.1295, -0.9954] n_targets:  1 reward:  59.33\n",
      "24.9 action:  [-0.11, -0.997] n_targets:  2 reward:  136.66\n",
      "26.1 action:  [0.0222, -0.9888] n_targets:  1 reward:  57.28\n",
      "26.9 action:  [-0.2982, -0.9926] n_targets:  1 reward:  53.25\n",
      "27.1 action:  [0.0368, -0.9885] n_targets:  1 reward:  53.31\n",
      "28.5 action:  [0.195, -0.9977] n_targets:  1 reward:  53.14\n",
      "29.3 action:  [-0.1829, -0.9901] n_targets:  1 reward:  65.48\n",
      "31.3 action:  [0.1672, -0.9816] n_targets:  1 reward:  67.26\n",
      "32.3 action:  [-0.3338, -0.9942] n_targets:  1 reward:  66.0\n",
      "35.7 action:  [-0.2874, -0.9961] n_targets:  2 reward:  135.76\n",
      "36.1 action:  [-0.0166, -0.9963] n_targets:  1 reward:  54.45\n",
      "36.5 action:  [-0.1364, -0.9939] n_targets:  1 reward:  52.65\n",
      "38.3 action:  [0.0681, -0.9833] n_targets:  1 reward:  59.5\n",
      "41.5 action:  [-0.1017, -0.988] n_targets:  1 reward:  51.53\n",
      "42.3 action:  [-0.1794, -0.9906] n_targets:  2 reward:  109.85\n",
      "43.5 action:  [-0.1005, -0.982] n_targets:  1 reward:  57.41\n",
      "46.3 action:  [0.0958, -0.99] n_targets:  2 reward:  119.57\n",
      "46.9 action:  [-0.0883, -0.9983] n_targets:  2 reward:  142.68\n",
      "47.3 action:  [-0.2079, -0.9971] n_targets:  1 reward:  60.29\n",
      "49.9 action:  [-0.2591, -0.988] n_targets:  1 reward:  74.19\n",
      "50.1 action:  [-0.2226, -0.9949] n_targets:  1 reward:  52.07\n",
      "50.7 action:  [-0.0843, -0.9948] n_targets:  1 reward:  55.42\n",
      "51.1 action:  [-0.241, -0.9978] n_targets:  1 reward:  53.95\n",
      "51.9 action:  [-0.3147, -0.9923] n_targets:  2 reward:  137.48\n",
      "53.9 action:  [-0.0834, -0.9953] n_targets:  1 reward:  61.41\n",
      "54.9 action:  [0.051, -0.9983] n_targets:  1 reward:  60.31\n",
      "56.1 action:  [-0.1373, -0.9954] n_targets:  1 reward:  61.59\n",
      "57.7 action:  [-0.0424, -0.9846] n_targets:  1 reward:  58.36\n",
      "58.3 action:  [-0.1506, -0.9897] n_targets:  2 reward:  108.47\n",
      "60.1 action:  [-0.2323, -0.997] n_targets:  1 reward:  84.27\n",
      "60.9 action:  [-0.4113, -0.9961] n_targets:  1 reward:  54.29\n",
      "62.9 action:  [-0.1342, -0.9883] n_targets:  1 reward:  51.36\n",
      "64.3 action:  [0.185, -0.9991] n_targets:  1 reward:  50.54\n",
      "64.9 action:  [-0.0756, -0.9979] n_targets:  1 reward:  57.78\n",
      "67.5 action:  [0.1974, -0.9942] n_targets:  1 reward:  52.4\n",
      "68.1 action:  [-0.0591, -0.9959] n_targets:  1 reward:  50.99\n",
      "68.3 action:  [-0.275, -0.9952] n_targets:  1 reward:  52.29\n",
      "69.1 action:  [0.0037, -0.9943] n_targets:  1 reward:  50.3\n",
      "69.3 action:  [-0.2138, -0.9939] n_targets:  1 reward:  52.16\n",
      "70.0 action:  [0.0133, -0.9956] n_targets:  1 reward:  61.97\n",
      "71.8 action:  [0.0186, -0.9859] n_targets:  1 reward:  60.02\n",
      "72.6 action:  [-0.0887, -0.9912] n_targets:  1 reward:  56.65\n",
      "73.0 action:  [-0.2302, -0.9852] n_targets:  3 reward:  160.9\n",
      "73.6 action:  [-0.2177, -0.9962] n_targets:  1 reward:  54.85\n",
      "76.4 action:  [-0.0814, -0.9981] n_targets:  1 reward:  59.34\n",
      "77.4 action:  [-0.0414, -0.9917] n_targets:  1 reward:  62.32\n",
      "77.8 action:  [-0.2656, -0.9973] n_targets:  2 reward:  107.59\n",
      "78.6 action:  [-0.0782, -0.9896] n_targets:  2 reward:  102.85\n",
      "79.0 action:  [-0.229, -0.9964] n_targets:  1 reward:  56.47\n",
      "80.2 action:  [-0.2857, -0.9803] n_targets:  1 reward:  57.96\n",
      "82.8 action:  [-0.1326, -0.9876] n_targets:  1 reward:  52.94\n",
      "85.0 action:  [-0.0675, -0.9901] n_targets:  1 reward:  52.2\n",
      "85.8 action:  [0.0519, -0.9895] n_targets:  1 reward:  66.55\n",
      "87.8 action:  [-0.1135, -0.9991] n_targets:  2 reward:  114.69\n",
      "90.0 action:  [0.0384, -0.9987] n_targets:  1 reward:  50.53\n",
      "93.1 action:  [-0.1064, -0.9887] n_targets:  1 reward:  52.05\n",
      "97.7 action:  [-0.1598, -0.9987] n_targets:  1 reward:  58.0\n",
      "98.1 action:  [0.1868, -0.9965] n_targets:  1 reward:  54.4\n",
      "ALPHA (entropy-related):  tensor([0.1872], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.19547 0.19465 0.19375 0.19287 0.19195 0.19094 0.18984 0.18886 0.18805\n",
      " 0.18719]\n",
      "Episode: 111, Episode Reward: 5679.9492925008135\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  281\n",
      "0.2 action:  [0.1932, -0.9971] n_targets:  1 reward:  51.92\n",
      "2.2 action:  [0.1282, -0.9965] n_targets:  1 reward:  65.5\n",
      "2.6 action:  [-0.1602, -0.9981] n_targets:  1 reward:  51.15\n",
      "5.4 action:  [-0.26, -0.9838] n_targets:  1 reward:  53.17\n",
      "7.4 action:  [0.0699, -0.9989] n_targets:  1 reward:  60.64\n",
      "9.6 action:  [0.0809, -0.9968] n_targets:  1 reward:  60.75\n",
      "10.0 action:  [0.0035, -0.9926] n_targets:  1 reward:  58.35\n",
      "11.2 action:  [-0.0031, -0.998] n_targets:  2 reward:  149.29\n",
      "11.8 action:  [-0.0706, -0.9931] n_targets:  1 reward:  58.23\n",
      "13.4 action:  [-0.3197, -0.9947] n_targets:  1 reward:  65.07\n",
      "13.8 action:  [-0.193, -0.9978] n_targets:  2 reward:  106.27\n",
      "14.0 action:  [-0.1879, -0.9852] n_targets:  2 reward:  109.62\n",
      "18.0 action:  [0.2619, -0.9953] n_targets:  1 reward:  57.56\n",
      "19.6 action:  [0.1532, -0.9971] n_targets:  1 reward:  51.86\n",
      "22.6 action:  [-0.0889, -0.9948] n_targets:  1 reward:  55.14\n",
      "24.0 action:  [-0.0143, -0.9939] n_targets:  1 reward:  73.62\n",
      "28.1 action:  [-0.0625, -0.9897] n_targets:  1 reward:  51.0\n",
      "28.5 action:  [-0.1767, -0.9912] n_targets:  1 reward:  57.46\n",
      "28.7 action:  [0.1583, -0.997] n_targets:  1 reward:  52.02\n",
      "31.1 action:  [-0.2618, -0.9862] n_targets:  2 reward:  103.08\n",
      "31.3 action:  [0.2033, -0.996] n_targets:  1 reward:  52.86\n",
      "31.9 action:  [0.1036, -0.9864] n_targets:  1 reward:  57.09\n",
      "32.5 action:  [-0.1416, -0.9926] n_targets:  1 reward:  57.34\n",
      "33.3 action:  [0.0828, -0.9947] n_targets:  1 reward:  53.19\n",
      "35.7 action:  [-0.1511, -0.9862] n_targets:  1 reward:  56.6\n",
      "37.3 action:  [0.0048, -0.9958] n_targets:  1 reward:  50.04\n",
      "38.7 action:  [-0.2586, -0.992] n_targets:  1 reward:  50.7\n",
      "38.9 action:  [-0.0588, -0.988] n_targets:  2 reward:  101.51\n",
      "39.1 action:  [0.0035, -0.9944] n_targets:  1 reward:  57.43\n",
      "39.7 action:  [-0.1395, -0.9919] n_targets:  2 reward:  128.41\n",
      "40.1 action:  [0.1729, -0.9865] n_targets:  1 reward:  67.32\n",
      "40.5 action:  [-0.0231, -0.9902] n_targets:  1 reward:  53.18\n",
      "43.4 action:  [-0.1009, -0.9842] n_targets:  1 reward:  52.35\n",
      "44.8 action:  [-0.0006, -0.998] n_targets:  1 reward:  58.87\n",
      "45.2 action:  [0.0266, -0.9969] n_targets:  1 reward:  51.51\n",
      "47.8 action:  [0.0083, -0.987] n_targets:  1 reward:  61.07\n",
      "48.8 action:  [0.3055, -0.9928] n_targets:  1 reward:  50.3\n",
      "49.8 action:  [-0.0728, -0.996] n_targets:  1 reward:  57.95\n",
      "50.2 action:  [0.3482, -0.9873] n_targets:  1 reward:  51.92\n",
      "55.4 action:  [0.1658, -0.9835] n_targets:  2 reward:  134.68\n",
      "56.6 action:  [-0.2829, -0.9815] n_targets:  1 reward:  53.89\n",
      "57.8 action:  [-0.0929, -0.997] n_targets:  1 reward:  69.35\n",
      "60.2 action:  [-0.1981, -0.9978] n_targets:  1 reward:  61.24\n",
      "62.4 action:  [0.0222, -0.9923] n_targets:  1 reward:  59.39\n",
      "63.3 action:  [0.0449, -0.9945] n_targets:  1 reward:  62.31\n",
      "63.7 action:  [-0.1823, -0.9965] n_targets:  1 reward:  55.31\n",
      "68.7 action:  [0.1424, -0.997] n_targets:  1 reward:  88.34\n",
      "69.3 action:  [0.0271, -0.9987] n_targets:  1 reward:  60.2\n",
      "70.1 action:  [-0.2219, -0.9973] n_targets:  1 reward:  54.96\n",
      "71.1 action:  [0.1831, -0.9987] n_targets:  1 reward:  52.93\n",
      "71.5 action:  [-0.1311, -0.9919] n_targets:  1 reward:  72.82\n",
      "75.1 action:  [0.0513, -0.9871] n_targets:  1 reward:  53.37\n",
      "75.3 action:  [-0.0049, -0.9942] n_targets:  1 reward:  52.16\n",
      "75.5 action:  [-0.0516, -0.9963] n_targets:  1 reward:  51.08\n",
      "76.1 action:  [0.065, -0.9947] n_targets:  1 reward:  54.93\n",
      "77.2 action:  [0.0269, -0.9861] n_targets:  1 reward:  60.29\n",
      "78.0 action:  [0.1154, -0.9984] n_targets:  1 reward:  57.99\n",
      "79.2 action:  [0.0672, -0.995] n_targets:  1 reward:  59.15\n",
      "80.0 action:  [-0.1464, -0.9886] n_targets:  1 reward:  54.06\n",
      "81.0 action:  [-0.0912, -0.9971] n_targets:  1 reward:  50.43\n",
      "83.2 action:  [0.1605, -0.9959] n_targets:  1 reward:  62.71\n",
      "83.4 action:  [0.0343, -0.9873] n_targets:  2 reward:  105.26\n",
      "87.2 action:  [0.2771, -0.9959] n_targets:  1 reward:  55.18\n",
      "87.8 action:  [-0.1251, -0.9984] n_targets:  1 reward:  52.61\n",
      "88.2 action:  [-0.001, -0.9984] n_targets:  1 reward:  50.61\n",
      "91.2 action:  [-0.1964, -0.995] n_targets:  1 reward:  51.27\n",
      "92.0 action:  [-0.0291, -0.9946] n_targets:  1 reward:  53.67\n",
      "93.2 action:  [-0.2426, -0.9945] n_targets:  1 reward:  73.87\n",
      "93.8 action:  [0.3725, -0.9994] n_targets:  1 reward:  54.53\n",
      "94.0 action:  [0.0772, -0.993] n_targets:  1 reward:  57.32\n",
      "95.8 action:  [0.0377, -0.9916] n_targets:  1 reward:  55.33\n",
      "96.2 action:  [0.1875, -0.9976] n_targets:  1 reward:  54.27\n",
      "96.6 action:  [-0.2414, -0.99] n_targets:  1 reward:  51.72\n",
      "98.6 action:  [-0.0231, -0.9908] n_targets:  1 reward:  51.72\n",
      "100.4 action:  [-0.0664, -0.9978] n_targets:  1 reward:  57.33\n",
      "101.8 action:  [0.1486, -0.9947] n_targets:  1 reward:  50.81\n",
      "102.0 action:  [-0.3287, -0.9946] n_targets:  1 reward:  60.45\n",
      "102.4 action:  [-0.0852, -0.9861] n_targets:  1 reward:  56.51\n",
      "ALPHA (entropy-related):  tensor([0.1862], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.19465 0.19375 0.19287 0.19195 0.19094 0.18984 0.18886 0.18805 0.18719\n",
      " 0.18618]\n",
      "Episode: 112, Episode Reward: 4935.366808573404\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  282\n",
      "0.1 action:  [-0.0474, -0.9892] n_targets:  2 reward:  127.56\n",
      "1.6 action:  [-0.0496, -0.9986] n_targets:  1 reward:  51.4\n",
      "2.2 action:  [0.0849, -0.99] n_targets:  1 reward:  60.3\n",
      "2.4 action:  [0.0457, -0.9888] n_targets:  1 reward:  51.25\n",
      "3.2 action:  [0.1454, -0.9883] n_targets:  2 reward:  106.23\n",
      "5.0 action:  [0.0969, -0.9935] n_targets:  1 reward:  67.35\n",
      "5.4 action:  [0.1138, -0.9951] n_targets:  1 reward:  72.35\n",
      "8.3 action:  [0.1744, -0.9964] n_targets:  1 reward:  53.66\n",
      "8.5 action:  [-0.1359, -0.9973] n_targets:  1 reward:  53.15\n",
      "10.7 action:  [0.2205, -0.994] n_targets:  1 reward:  53.11\n",
      "12.5 action:  [-0.2013, -0.9892] n_targets:  1 reward:  56.23\n",
      "13.7 action:  [-0.0422, -0.9958] n_targets:  1 reward:  57.17\n",
      "13.9 action:  [-0.0053, -0.9813] n_targets:  1 reward:  51.09\n",
      "19.7 action:  [0.0331, -0.9887] n_targets:  1 reward:  53.44\n",
      "21.3 action:  [-0.1174, -0.9977] n_targets:  1 reward:  66.1\n",
      "21.5 action:  [0.0847, -0.9909] n_targets:  1 reward:  52.6\n",
      "22.5 action:  [0.1035, -0.9975] n_targets:  1 reward:  52.33\n",
      "22.7 action:  [-0.0098, -0.998] n_targets:  1 reward:  56.62\n",
      "24.1 action:  [0.0836, -0.9903] n_targets:  1 reward:  53.27\n",
      "24.9 action:  [0.2226, -0.992] n_targets:  1 reward:  55.14\n",
      "28.5 action:  [-0.1316, -0.9988] n_targets:  1 reward:  55.12\n",
      "28.7 action:  [0.3633, -0.9921] n_targets:  1 reward:  54.72\n",
      "30.6 action:  [-0.1337, -0.9919] n_targets:  1 reward:  50.34\n",
      "32.2 action:  [0.1347, -0.9972] n_targets:  1 reward:  52.73\n",
      "32.6 action:  [-0.2292, -0.9994] n_targets:  1 reward:  52.85\n",
      "34.0 action:  [0.0828, -0.993] n_targets:  2 reward:  112.19\n",
      "34.2 action:  [-0.0297, -0.9979] n_targets:  1 reward:  54.56\n",
      "35.2 action:  [-0.2927, -0.9832] n_targets:  2 reward:  141.58\n",
      "35.6 action:  [0.1922, -0.9989] n_targets:  2 reward:  106.71\n",
      "37.8 action:  [0.1537, -0.9955] n_targets:  2 reward:  118.73\n",
      "38.6 action:  [0.0253, -0.9853] n_targets:  1 reward:  58.91\n",
      "38.8 action:  [0.1459, -0.9878] n_targets:  1 reward:  55.75\n",
      "39.2 action:  [0.1564, -0.9961] n_targets:  1 reward:  52.67\n",
      "39.6 action:  [-0.2459, -0.9859] n_targets:  2 reward:  135.6\n",
      "40.4 action:  [0.0549, -0.9981] n_targets:  1 reward:  54.47\n",
      "42.2 action:  [-0.0345, -0.9881] n_targets:  1 reward:  54.91\n",
      "44.0 action:  [0.034, -0.9802] n_targets:  1 reward:  51.77\n",
      "45.0 action:  [-0.0751, -0.9833] n_targets:  1 reward:  52.92\n",
      "46.5 action:  [-0.0539, -0.9889] n_targets:  1 reward:  54.3\n",
      "50.0 action:  [-0.3542, -0.988] n_targets:  1 reward:  52.51\n",
      "53.8 action:  [-0.1345, -0.9934] n_targets:  1 reward:  52.53\n",
      "56.8 action:  [0.0085, -0.9995] n_targets:  1 reward:  51.12\n",
      "58.6 action:  [0.1101, -0.9967] n_targets:  1 reward:  53.81\n",
      "61.0 action:  [0.1498, -0.9944] n_targets:  1 reward:  56.27\n",
      "64.0 action:  [-0.1805, -0.9833] n_targets:  1 reward:  56.66\n",
      "64.2 action:  [-0.0508, -0.9949] n_targets:  1 reward:  51.87\n",
      "64.4 action:  [-0.3663, -0.9945] n_targets:  1 reward:  55.58\n",
      "64.6 action:  [-0.1886, -0.9919] n_targets:  1 reward:  57.42\n",
      "65.2 action:  [0.0943, -0.9849] n_targets:  2 reward:  101.6\n",
      "65.8 action:  [0.1397, -0.9992] n_targets:  1 reward:  62.12\n",
      "68.2 action:  [0.043, -0.9901] n_targets:  1 reward:  51.24\n",
      "68.6 action:  [-0.1971, -0.9995] n_targets:  1 reward:  51.27\n",
      "68.8 action:  [0.0571, -0.9896] n_targets:  1 reward:  83.56\n",
      "73.0 action:  [-0.3248, -0.9986] n_targets:  1 reward:  55.48\n",
      "74.6 action:  [0.3648, -0.9917] n_targets:  1 reward:  56.02\n",
      "74.8 action:  [0.1275, -0.9985] n_targets:  1 reward:  52.95\n",
      "76.2 action:  [-0.1253, -0.9984] n_targets:  2 reward:  109.59\n",
      "76.4 action:  [-0.0764, -0.9973] n_targets:  1 reward:  57.41\n",
      "79.0 action:  [-0.0771, -0.999] n_targets:  1 reward:  56.04\n",
      "81.0 action:  [-0.4256, -0.9979] n_targets:  1 reward:  50.17\n",
      "85.2 action:  [0.1496, -0.9945] n_targets:  1 reward:  58.25\n",
      "85.6 action:  [-0.1376, -0.9983] n_targets:  1 reward:  53.63\n",
      "87.2 action:  [-0.1525, -0.994] n_targets:  1 reward:  54.23\n",
      "87.8 action:  [0.2857, -0.9972] n_targets:  1 reward:  60.21\n",
      "88.0 action:  [0.0767, -0.99] n_targets:  1 reward:  58.32\n",
      "89.0 action:  [-0.0749, -0.9863] n_targets:  1 reward:  58.3\n",
      "89.4 action:  [-0.2654, -0.9992] n_targets:  1 reward:  51.21\n",
      "90.8 action:  [-0.2239, -0.9901] n_targets:  2 reward:  103.16\n",
      "91.0 action:  [0.4141, -0.9828] n_targets:  1 reward:  60.27\n",
      "91.4 action:  [0.396, -0.9833] n_targets:  1 reward:  52.09\n",
      "91.6 action:  [-0.0235, -0.9978] n_targets:  1 reward:  55.62\n",
      "92.6 action:  [-0.0607, -0.993] n_targets:  1 reward:  50.4\n",
      "93.8 action:  [0.1295, -0.998] n_targets:  1 reward:  50.07\n",
      "94.4 action:  [-0.1147, -0.996] n_targets:  1 reward:  59.81\n",
      "95.0 action:  [0.0244, -0.9946] n_targets:  1 reward:  58.66\n",
      "96.6 action:  [0.271, -0.9983] n_targets:  1 reward:  53.02\n",
      "96.8 action:  [0.0382, -0.9897] n_targets:  1 reward:  50.55\n",
      "102.0 action:  [0.221, -0.9932] n_targets:  1 reward:  67.69\n",
      "ALPHA (entropy-related):  tensor([0.1852], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.19375 0.19287 0.19195 0.19094 0.18984 0.18886 0.18805 0.18719 0.18618\n",
      " 0.18521]\n",
      "Episode: 113, Episode Reward: 4951.905564626058\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  283\n",
      "0.1 action:  [0.2603, -0.9961] n_targets:  2 reward:  117.45\n",
      "2.5 action:  [-0.254, -0.9986] n_targets:  1 reward:  52.22\n",
      "3.3 action:  [-0.2423, -0.9959] n_targets:  1 reward:  60.43\n",
      "3.5 action:  [0.3327, -0.998] n_targets:  1 reward:  51.24\n",
      "5.1 action:  [0.3169, -0.9895] n_targets:  1 reward:  58.43\n",
      "5.7 action:  [-0.084, -0.9956] n_targets:  1 reward:  56.73\n",
      "6.1 action:  [0.3353, -0.988] n_targets:  1 reward:  65.6\n",
      "8.5 action:  [-0.2281, -0.9969] n_targets:  1 reward:  55.37\n",
      "9.5 action:  [0.0376, -0.9869] n_targets:  1 reward:  52.94\n",
      "13.1 action:  [0.2537, -0.9904] n_targets:  1 reward:  51.8\n",
      "14.1 action:  [0.1521, -0.9955] n_targets:  1 reward:  55.97\n",
      "14.9 action:  [0.0588, -0.9909] n_targets:  1 reward:  59.94\n",
      "15.6 action:  [-0.1289, -0.9819] n_targets:  1 reward:  63.0\n",
      "16.6 action:  [-0.0339, -0.9974] n_targets:  1 reward:  52.55\n",
      "17.8 action:  [0.006, -0.9962] n_targets:  1 reward:  69.29\n",
      "18.0 action:  [0.0015, -0.9934] n_targets:  1 reward:  51.29\n",
      "19.0 action:  [0.0718, -0.993] n_targets:  1 reward:  51.95\n",
      "19.2 action:  [-0.1933, -0.9894] n_targets:  2 reward:  110.32\n",
      "24.0 action:  [0.0458, -0.9983] n_targets:  2 reward:  106.14\n",
      "25.6 action:  [-0.1835, -0.9975] n_targets:  1 reward:  56.79\n",
      "25.8 action:  [0.0184, -0.9967] n_targets:  3 reward:  169.2\n",
      "26.0 action:  [0.0911, -0.9966] n_targets:  1 reward:  51.07\n",
      "26.4 action:  [-0.1392, -0.9818] n_targets:  1 reward:  60.93\n",
      "26.7 action:  [-0.0306, -0.9955] n_targets:  1 reward:  61.71\n",
      "30.9 action:  [-0.064, -0.9969] n_targets:  1 reward:  57.96\n",
      "31.5 action:  [-0.0516, -0.9934] n_targets:  1 reward:  56.71\n",
      "31.7 action:  [-0.0359, -0.9964] n_targets:  1 reward:  57.75\n",
      "32.3 action:  [-0.041, -0.9912] n_targets:  1 reward:  51.48\n",
      "33.5 action:  [-0.1561, -0.9837] n_targets:  1 reward:  54.25\n",
      "34.3 action:  [0.0008, -0.9985] n_targets:  1 reward:  69.24\n",
      "35.7 action:  [0.184, -0.9907] n_targets:  1 reward:  51.21\n",
      "40.1 action:  [-0.2634, -0.9972] n_targets:  1 reward:  50.81\n",
      "41.1 action:  [0.0834, -0.996] n_targets:  1 reward:  55.68\n",
      "42.5 action:  [0.0256, -0.9889] n_targets:  1 reward:  51.47\n",
      "42.7 action:  [0.1119, -0.9897] n_targets:  1 reward:  53.53\n",
      "44.1 action:  [-0.0017, -0.995] n_targets:  1 reward:  54.09\n",
      "46.1 action:  [0.1627, -0.9988] n_targets:  1 reward:  55.96\n",
      "47.3 action:  [-0.4084, -0.9857] n_targets:  1 reward:  55.61\n",
      "48.5 action:  [-0.1619, -0.9976] n_targets:  1 reward:  51.2\n",
      "49.7 action:  [0.233, -0.9957] n_targets:  1 reward:  51.38\n",
      "50.3 action:  [-0.1138, -0.9961] n_targets:  1 reward:  60.18\n",
      "50.5 action:  [-0.0182, -0.9987] n_targets:  1 reward:  60.52\n",
      "51.9 action:  [-0.1135, -0.9973] n_targets:  1 reward:  50.66\n",
      "52.1 action:  [-0.313, -0.9969] n_targets:  1 reward:  65.31\n",
      "52.3 action:  [0.1196, -0.9987] n_targets:  1 reward:  55.24\n",
      "52.5 action:  [-0.3954, -0.9979] n_targets:  1 reward:  56.92\n",
      "53.5 action:  [-0.2016, -0.9969] n_targets:  1 reward:  56.96\n",
      "54.1 action:  [-0.0155, -0.9924] n_targets:  2 reward:  107.24\n",
      "54.7 action:  [-0.2914, -0.9886] n_targets:  1 reward:  56.09\n",
      "55.1 action:  [-0.3644, -0.9978] n_targets:  1 reward:  50.38\n",
      "55.3 action:  [-0.117, -0.9961] n_targets:  1 reward:  51.39\n",
      "56.3 action:  [-0.0334, -0.9989] n_targets:  1 reward:  54.1\n",
      "56.5 action:  [0.1114, -0.9941] n_targets:  1 reward:  50.43\n",
      "57.1 action:  [0.0893, -0.9975] n_targets:  2 reward:  148.01\n",
      "59.1 action:  [0.1449, -0.9885] n_targets:  1 reward:  53.57\n",
      "59.7 action:  [-0.1631, -0.9986] n_targets:  1 reward:  62.49\n",
      "60.1 action:  [0.0681, -0.9846] n_targets:  1 reward:  67.32\n",
      "61.9 action:  [-0.3248, -0.9913] n_targets:  1 reward:  51.43\n",
      "62.9 action:  [-0.2149, -0.9975] n_targets:  1 reward:  58.19\n",
      "63.5 action:  [-0.1205, -0.9973] n_targets:  1 reward:  50.97\n",
      "64.1 action:  [-0.1737, -0.9873] n_targets:  1 reward:  51.77\n",
      "64.3 action:  [-0.0989, -0.9923] n_targets:  1 reward:  50.82\n",
      "64.9 action:  [-0.1802, -0.9954] n_targets:  1 reward:  56.68\n",
      "65.7 action:  [-0.0104, -0.9972] n_targets:  1 reward:  55.76\n",
      "65.9 action:  [-0.1852, -0.9973] n_targets:  1 reward:  55.0\n",
      "66.9 action:  [-0.1096, -0.9896] n_targets:  1 reward:  55.29\n",
      "67.3 action:  [-0.0567, -0.9985] n_targets:  2 reward:  107.68\n",
      "67.7 action:  [-0.0648, -0.9894] n_targets:  1 reward:  52.96\n",
      "70.9 action:  [-0.0995, -0.9864] n_targets:  1 reward:  57.62\n",
      "75.1 action:  [-0.136, -0.9874] n_targets:  1 reward:  68.41\n",
      "76.3 action:  [-0.1182, -0.989] n_targets:  1 reward:  66.04\n",
      "80.3 action:  [-0.1739, -0.9934] n_targets:  1 reward:  52.07\n",
      "80.5 action:  [0.0418, -0.999] n_targets:  2 reward:  132.46\n",
      "81.3 action:  [0.179, -0.9919] n_targets:  1 reward:  51.64\n",
      "81.7 action:  [-0.1868, -0.997] n_targets:  1 reward:  58.03\n",
      "82.1 action:  [0.2731, -0.9907] n_targets:  1 reward:  62.47\n",
      "85.7 action:  [-0.0933, -0.9815] n_targets:  1 reward:  52.84\n",
      "87.5 action:  [0.0744, -0.9994] n_targets:  1 reward:  55.64\n",
      "87.9 action:  [-0.1603, -0.9824] n_targets:  1 reward:  53.15\n",
      "90.7 action:  [0.1169, -0.997] n_targets:  1 reward:  50.94\n",
      "93.1 action:  [-0.0567, -0.991] n_targets:  1 reward:  56.29\n",
      "93.5 action:  [0.3545, -0.9905] n_targets:  1 reward:  61.35\n",
      "93.7 action:  [-0.2508, -0.9864] n_targets:  1 reward:  60.82\n",
      "95.3 action:  [0.2975, -0.9924] n_targets:  1 reward:  52.18\n",
      "95.7 action:  [0.0637, -0.9935] n_targets:  1 reward:  60.13\n",
      "97.9 action:  [0.289, -0.9885] n_targets:  1 reward:  54.74\n",
      "98.5 action:  [0.2448, -0.9891] n_targets:  2 reward:  114.93\n",
      "98.9 action:  [-0.3411, -0.9974] n_targets:  1 reward:  50.65\n",
      "99.3 action:  [0.1815, -0.994] n_targets:  1 reward:  53.39\n",
      "99.9 action:  [-0.0014, -0.9983] n_targets:  2 reward:  105.91\n",
      "100.3 action:  [-0.2489, -0.9913] n_targets:  2 reward:  107.26\n",
      "100.7 action:  [-0.1445, -0.9841] n_targets:  1 reward:  53.22\n",
      "101.3 action:  [-0.0379, -0.9913] n_targets:  1 reward:  51.37\n",
      "101.5 action:  [-0.104, -0.9914] n_targets:  1 reward:  52.45\n",
      "101.9 action:  [-0.1162, -0.9991] n_targets:  1 reward:  60.58\n",
      "ALPHA (entropy-related):  tensor([0.1841], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.19287 0.19195 0.19094 0.18984 0.18886 0.18805 0.18719 0.18618 0.18521\n",
      " 0.18413]\n",
      "Episode: 114, Episode Reward: 6030.570910135904\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  284\n",
      "0.7 action:  [-0.0178, -0.9936] n_targets:  1 reward:  50.16\n",
      "2.1 action:  [-0.3167, -0.9812] n_targets:  1 reward:  57.95\n",
      "3.1 action:  [0.0734, -0.9868] n_targets:  1 reward:  54.12\n",
      "3.9 action:  [0.1742, -0.9854] n_targets:  1 reward:  64.63\n",
      "4.1 action:  [0.1121, -0.9969] n_targets:  1 reward:  51.4\n",
      "5.7 action:  [0.2101, -0.9944] n_targets:  1 reward:  53.77\n",
      "5.9 action:  [-0.0082, -0.9927] n_targets:  1 reward:  53.93\n",
      "7.3 action:  [-0.1662, -0.997] n_targets:  1 reward:  53.99\n",
      "7.7 action:  [-0.0812, -0.9925] n_targets:  1 reward:  54.05\n",
      "9.1 action:  [0.2496, -0.9983] n_targets:  1 reward:  50.64\n",
      "10.3 action:  [0.2438, -0.9968] n_targets:  1 reward:  65.33\n",
      "10.5 action:  [0.0516, -0.9851] n_targets:  1 reward:  50.25\n",
      "12.7 action:  [0.1081, -0.9933] n_targets:  2 reward:  136.83\n",
      "13.9 action:  [0.0807, -0.9988] n_targets:  1 reward:  54.14\n",
      "14.5 action:  [-0.0861, -0.9974] n_targets:  1 reward:  58.28\n",
      "14.7 action:  [-0.1034, -0.9955] n_targets:  1 reward:  55.95\n",
      "17.9 action:  [0.0686, -0.9977] n_targets:  1 reward:  50.96\n",
      "18.7 action:  [0.233, -0.9964] n_targets:  1 reward:  53.91\n",
      "19.9 action:  [-0.1205, -0.9938] n_targets:  2 reward:  123.31\n",
      "21.1 action:  [0.2462, -0.9986] n_targets:  1 reward:  55.38\n",
      "21.9 action:  [-0.0009, -0.9835] n_targets:  1 reward:  57.5\n",
      "22.9 action:  [0.1382, -0.9973] n_targets:  2 reward:  107.85\n",
      "23.3 action:  [0.0896, -0.9849] n_targets:  1 reward:  58.67\n",
      "24.7 action:  [-0.046, -0.9863] n_targets:  1 reward:  67.94\n",
      "25.9 action:  [-0.2546, -0.9878] n_targets:  2 reward:  100.56\n",
      "26.5 action:  [-0.1156, -0.9868] n_targets:  1 reward:  54.08\n",
      "28.8 action:  [-0.2606, -0.9826] n_targets:  1 reward:  59.74\n",
      "31.2 action:  [0.2376, -0.9937] n_targets:  1 reward:  54.14\n",
      "31.6 action:  [0.0097, -0.9956] n_targets:  1 reward:  52.43\n",
      "32.3 action:  [-0.1919, -0.9911] n_targets:  1 reward:  53.75\n",
      "34.9 action:  [0.0485, -0.9899] n_targets:  1 reward:  53.05\n",
      "35.9 action:  [-0.1845, -0.9958] n_targets:  2 reward:  105.64\n",
      "38.3 action:  [-0.2036, -0.9942] n_targets:  1 reward:  50.67\n",
      "41.3 action:  [0.0867, -0.991] n_targets:  1 reward:  60.31\n",
      "43.1 action:  [-0.1161, -0.9951] n_targets:  2 reward:  110.23\n",
      "46.3 action:  [0.1233, -0.9969] n_targets:  1 reward:  71.92\n",
      "47.1 action:  [0.1892, -0.9981] n_targets:  1 reward:  50.54\n",
      "47.9 action:  [0.1453, -0.9948] n_targets:  1 reward:  59.75\n",
      "53.3 action:  [-0.0195, -0.9916] n_targets:  1 reward:  50.58\n",
      "54.3 action:  [-0.0753, -0.9961] n_targets:  1 reward:  55.92\n",
      "55.3 action:  [0.0406, -0.9891] n_targets:  2 reward:  121.09\n",
      "57.9 action:  [0.1055, -0.9966] n_targets:  1 reward:  50.89\n",
      "58.5 action:  [-0.1326, -0.9976] n_targets:  1 reward:  55.81\n",
      "60.1 action:  [-0.0867, -0.9951] n_targets:  1 reward:  53.07\n",
      "60.3 action:  [-0.0981, -0.9895] n_targets:  1 reward:  55.29\n",
      "60.5 action:  [-0.0797, -0.9985] n_targets:  1 reward:  57.85\n",
      "64.1 action:  [0.0469, -0.9967] n_targets:  1 reward:  52.31\n",
      "66.9 action:  [-0.1155, -0.997] n_targets:  2 reward:  108.54\n",
      "68.1 action:  [-0.0313, -0.9988] n_targets:  1 reward:  50.76\n",
      "68.3 action:  [-0.3673, -0.9965] n_targets:  1 reward:  52.92\n",
      "68.5 action:  [-0.0879, -0.9937] n_targets:  2 reward:  116.0\n",
      "68.7 action:  [-0.0315, -0.9985] n_targets:  1 reward:  51.05\n",
      "70.7 action:  [0.0164, -0.998] n_targets:  1 reward:  56.47\n",
      "70.9 action:  [-0.125, -0.9887] n_targets:  1 reward:  60.44\n",
      "71.1 action:  [-0.0685, -0.991] n_targets:  1 reward:  55.36\n",
      "71.7 action:  [0.1081, -0.9982] n_targets:  1 reward:  50.58\n",
      "72.1 action:  [-0.1718, -0.9904] n_targets:  1 reward:  55.39\n",
      "73.9 action:  [-0.0414, -0.9939] n_targets:  1 reward:  57.14\n",
      "75.1 action:  [-0.0387, -0.9812] n_targets:  1 reward:  67.66\n",
      "76.1 action:  [-0.2039, -0.9967] n_targets:  3 reward:  165.97\n",
      "76.5 action:  [0.1694, -0.9983] n_targets:  1 reward:  55.54\n",
      "78.1 action:  [-0.1247, -0.9971] n_targets:  2 reward:  111.59\n",
      "78.9 action:  [-0.1973, -0.982] n_targets:  1 reward:  51.21\n",
      "80.1 action:  [-0.0797, -0.9936] n_targets:  1 reward:  67.13\n",
      "80.5 action:  [-0.2814, -0.9989] n_targets:  1 reward:  53.43\n",
      "81.3 action:  [0.0479, -0.9974] n_targets:  1 reward:  61.46\n",
      "82.1 action:  [-0.3275, -0.9972] n_targets:  1 reward:  55.62\n",
      "85.7 action:  [-0.228, -0.9913] n_targets:  1 reward:  53.5\n",
      "86.1 action:  [-0.1458, -0.9976] n_targets:  1 reward:  50.27\n",
      "87.1 action:  [-0.2449, -0.9948] n_targets:  1 reward:  52.68\n",
      "87.3 action:  [-0.1835, -0.9976] n_targets:  1 reward:  51.64\n",
      "88.3 action:  [0.0165, -0.9965] n_targets:  1 reward:  55.26\n",
      "92.5 action:  [-0.2371, -0.997] n_targets:  1 reward:  53.85\n",
      "94.7 action:  [-0.2705, -0.9988] n_targets:  1 reward:  50.92\n",
      "95.5 action:  [0.0038, -0.9952] n_targets:  1 reward:  92.8\n",
      "96.7 action:  [-0.1107, -0.9956] n_targets:  1 reward:  63.63\n",
      "97.3 action:  [-0.0904, -0.996] n_targets:  1 reward:  54.4\n",
      "97.5 action:  [-0.1949, -0.9842] n_targets:  1 reward:  53.62\n",
      "101.3 action:  [0.0, -0.9925] n_targets:  1 reward:  51.56\n",
      "ALPHA (entropy-related):  tensor([0.1831], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.19195 0.19094 0.18984 0.18886 0.18805 0.18719 0.18618 0.18521 0.18413\n",
      " 0.18307]\n",
      "Episode: 115, Episode Reward: 5118.975893656412\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  285\n",
      "0.2 action:  [-0.225, -0.9971] n_targets:  3 reward:  229.33\n",
      "2.8 action:  [-0.2126, -0.9964] n_targets:  1 reward:  59.92\n",
      "5.4 action:  [-0.0695, -0.999] n_targets:  2 reward:  124.16\n",
      "5.6 action:  [-0.0387, -0.9951] n_targets:  1 reward:  55.86\n",
      "6.2 action:  [0.0728, -0.9944] n_targets:  2 reward:  105.89\n",
      "10.4 action:  [0.0772, -0.995] n_targets:  1 reward:  54.95\n",
      "13.4 action:  [0.2586, -0.9946] n_targets:  1 reward:  50.39\n",
      "15.4 action:  [-0.2829, -0.9895] n_targets:  1 reward:  50.86\n",
      "16.2 action:  [0.1951, -0.9981] n_targets:  1 reward:  56.74\n",
      "17.0 action:  [-0.2171, -0.9979] n_targets:  1 reward:  50.06\n",
      "17.4 action:  [0.0565, -0.997] n_targets:  1 reward:  55.67\n",
      "17.8 action:  [-0.0261, -0.9915] n_targets:  2 reward:  114.03\n",
      "19.2 action:  [-0.2005, -0.9969] n_targets:  1 reward:  54.84\n",
      "21.4 action:  [-0.2115, -0.9974] n_targets:  1 reward:  51.32\n",
      "22.0 action:  [-0.0437, -0.9899] n_targets:  1 reward:  53.02\n",
      "22.4 action:  [0.0405, -0.9983] n_targets:  1 reward:  53.39\n",
      "23.4 action:  [0.2142, -0.9888] n_targets:  2 reward:  112.81\n",
      "24.8 action:  [-0.0021, -0.9983] n_targets:  1 reward:  56.07\n",
      "25.2 action:  [0.0097, -0.9928] n_targets:  1 reward:  55.78\n",
      "25.6 action:  [0.1009, -0.9981] n_targets:  2 reward:  110.35\n",
      "29.2 action:  [0.0029, -0.9964] n_targets:  1 reward:  50.96\n",
      "29.8 action:  [-0.0973, -0.9844] n_targets:  1 reward:  74.6\n",
      "30.6 action:  [-0.0286, -0.9921] n_targets:  1 reward:  52.33\n",
      "30.8 action:  [0.0821, -0.997] n_targets:  1 reward:  51.32\n",
      "31.0 action:  [0.0138, -0.9971] n_targets:  1 reward:  51.42\n",
      "31.2 action:  [0.1003, -0.9941] n_targets:  1 reward:  54.14\n",
      "34.8 action:  [-0.1146, -0.9977] n_targets:  1 reward:  51.54\n",
      "35.0 action:  [0.119, -0.9979] n_targets:  1 reward:  56.03\n",
      "35.2 action:  [-0.1737, -0.9843] n_targets:  1 reward:  51.22\n",
      "36.2 action:  [0.1378, -0.9973] n_targets:  1 reward:  52.77\n",
      "38.6 action:  [-0.0773, -0.9971] n_targets:  1 reward:  62.62\n",
      "42.4 action:  [0.171, -0.9964] n_targets:  1 reward:  73.01\n",
      "43.8 action:  [-0.0239, -0.9917] n_targets:  1 reward:  52.33\n",
      "44.0 action:  [-0.1406, -0.9972] n_targets:  1 reward:  51.74\n",
      "45.7 action:  [0.1414, -0.9974] n_targets:  1 reward:  50.52\n",
      "46.7 action:  [-0.1637, -0.9878] n_targets:  1 reward:  53.26\n",
      "47.1 action:  [-0.129, -0.999] n_targets:  1 reward:  52.06\n",
      "47.7 action:  [0.1993, -0.9932] n_targets:  1 reward:  52.5\n",
      "48.3 action:  [0.0545, -0.9934] n_targets:  1 reward:  60.25\n",
      "49.5 action:  [0.0848, -0.9963] n_targets:  1 reward:  52.68\n",
      "49.9 action:  [0.0718, -0.9887] n_targets:  1 reward:  60.24\n",
      "50.1 action:  [0.0293, -0.9955] n_targets:  1 reward:  50.07\n",
      "50.7 action:  [-0.2887, -0.9921] n_targets:  2 reward:  105.99\n",
      "51.1 action:  [0.023, -0.9941] n_targets:  1 reward:  51.36\n",
      "52.1 action:  [-0.1699, -0.9887] n_targets:  1 reward:  53.01\n",
      "52.5 action:  [-0.0303, -0.9969] n_targets:  1 reward:  51.71\n",
      "53.1 action:  [0.0528, -0.9947] n_targets:  1 reward:  55.56\n",
      "54.5 action:  [-0.2534, -0.9946] n_targets:  1 reward:  53.32\n",
      "55.7 action:  [-0.1691, -0.9864] n_targets:  1 reward:  52.26\n",
      "57.5 action:  [-0.2389, -0.9944] n_targets:  2 reward:  132.37\n",
      "60.7 action:  [-0.129, -0.9931] n_targets:  1 reward:  58.43\n",
      "61.7 action:  [0.2798, -0.9988] n_targets:  1 reward:  55.69\n",
      "62.7 action:  [-0.3284, -0.9902] n_targets:  1 reward:  51.96\n",
      "64.8 action:  [0.1443, -0.9994] n_targets:  2 reward:  111.62\n",
      "65.0 action:  [0.1497, -0.981] n_targets:  1 reward:  53.87\n",
      "65.8 action:  [0.0976, -0.9988] n_targets:  1 reward:  55.43\n",
      "66.2 action:  [0.0893, -0.9956] n_targets:  1 reward:  56.72\n",
      "66.4 action:  [0.2052, -0.9884] n_targets:  2 reward:  105.25\n",
      "67.0 action:  [0.0011, -0.985] n_targets:  1 reward:  55.63\n",
      "67.6 action:  [-0.0106, -0.9915] n_targets:  1 reward:  53.56\n",
      "68.2 action:  [-0.2481, -0.9961] n_targets:  1 reward:  52.44\n",
      "70.0 action:  [0.2197, -0.9917] n_targets:  1 reward:  57.53\n",
      "70.6 action:  [-0.0175, -0.9871] n_targets:  1 reward:  54.09\n",
      "73.2 action:  [-0.1911, -0.9949] n_targets:  1 reward:  53.35\n",
      "73.8 action:  [-0.0623, -0.9891] n_targets:  1 reward:  53.03\n",
      "77.0 action:  [-0.1132, -0.9933] n_targets:  1 reward:  67.88\n",
      "79.8 action:  [-0.0561, -0.9868] n_targets:  1 reward:  52.48\n",
      "80.0 action:  [-0.0013, -0.9823] n_targets:  1 reward:  57.89\n",
      "82.0 action:  [0.0196, -0.9943] n_targets:  1 reward:  70.58\n",
      "83.7 action:  [-0.0327, -0.9973] n_targets:  2 reward:  112.49\n",
      "85.1 action:  [0.0014, -0.9878] n_targets:  1 reward:  58.01\n",
      "85.7 action:  [0.1518, -0.9969] n_targets:  1 reward:  52.98\n",
      "85.9 action:  [0.0227, -0.9982] n_targets:  1 reward:  56.1\n",
      "88.1 action:  [-0.1034, -0.9973] n_targets:  2 reward:  110.01\n",
      "88.3 action:  [-0.0844, -0.987] n_targets:  2 reward:  119.17\n",
      "88.7 action:  [-0.1558, -0.9963] n_targets:  1 reward:  58.25\n",
      "90.6 action:  [-0.0341, -0.9934] n_targets:  1 reward:  50.42\n",
      "91.8 action:  [-0.3092, -0.9944] n_targets:  1 reward:  50.33\n",
      "93.0 action:  [0.0553, -0.9966] n_targets:  1 reward:  59.61\n",
      "93.6 action:  [-0.0575, -0.9974] n_targets:  1 reward:  53.61\n",
      "94.8 action:  [-0.35, -0.9992] n_targets:  1 reward:  55.17\n",
      "95.2 action:  [-0.0656, -0.9984] n_targets:  2 reward:  100.42\n",
      "95.4 action:  [-0.0892, -0.9971] n_targets:  1 reward:  57.39\n",
      "95.8 action:  [0.1417, -0.9911] n_targets:  2 reward:  105.44\n",
      "96.2 action:  [-0.1346, -0.9936] n_targets:  1 reward:  53.31\n",
      "96.8 action:  [-0.199, -0.9946] n_targets:  1 reward:  52.46\n",
      "99.0 action:  [0.3203, -0.9977] n_targets:  1 reward:  59.45\n",
      "100.2 action:  [-0.0768, -0.994] n_targets:  1 reward:  52.76\n",
      "100.4 action:  [-0.1938, -0.9983] n_targets:  1 reward:  63.02\n",
      "101.4 action:  [-0.1763, -0.9997] n_targets:  2 reward:  102.82\n",
      "ALPHA (entropy-related):  tensor([0.1822], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.19094 0.18984 0.18886 0.18805 0.18719 0.18618 0.18521 0.18413 0.18307\n",
      " 0.18222]\n",
      "Episode: 116, Episode Reward: 5989.300641377767\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  286\n",
      "0.1 action:  [-0.2448, -0.9967] n_targets:  2 reward:  129.21\n",
      "0.3 action:  [0.0093, -0.9839] n_targets:  1 reward:  54.68\n",
      "0.5 action:  [-0.0668, -0.9928] n_targets:  1 reward:  58.18\n",
      "1.5 action:  [-0.4836, -0.9938] n_targets:  2 reward:  109.53\n",
      "2.1 action:  [0.1123, -0.9966] n_targets:  1 reward:  54.33\n",
      "5.3 action:  [-0.1337, -0.9984] n_targets:  4 reward:  231.34\n",
      "5.9 action:  [0.0118, -0.993] n_targets:  1 reward:  51.9\n",
      "7.3 action:  [-0.0984, -0.9858] n_targets:  1 reward:  51.54\n",
      "7.5 action:  [-0.2275, -0.9965] n_targets:  1 reward:  58.85\n",
      "8.1 action:  [-0.0143, -0.9984] n_targets:  1 reward:  53.82\n",
      "9.9 action:  [-0.1878, -0.9967] n_targets:  2 reward:  114.9\n",
      "10.5 action:  [-0.1062, -0.992] n_targets:  1 reward:  54.57\n",
      "10.9 action:  [-0.0105, -0.9919] n_targets:  1 reward:  66.03\n",
      "12.5 action:  [0.0007, -0.9804] n_targets:  1 reward:  50.87\n",
      "18.5 action:  [0.1352, -0.9812] n_targets:  1 reward:  51.55\n",
      "19.1 action:  [-0.0436, -0.9964] n_targets:  1 reward:  53.67\n",
      "19.3 action:  [-0.2311, -0.9925] n_targets:  1 reward:  50.92\n",
      "22.5 action:  [-0.1399, -0.9932] n_targets:  1 reward:  52.46\n",
      "24.1 action:  [-0.1173, -0.9901] n_targets:  1 reward:  54.71\n",
      "25.5 action:  [-0.054, -0.9984] n_targets:  1 reward:  58.07\n",
      "27.9 action:  [-0.105, -0.9938] n_targets:  1 reward:  61.11\n",
      "28.7 action:  [-0.1803, -0.9984] n_targets:  1 reward:  51.83\n",
      "29.3 action:  [-0.1725, -0.9923] n_targets:  1 reward:  55.86\n",
      "29.5 action:  [0.048, -0.9915] n_targets:  1 reward:  54.62\n",
      "32.0 action:  [-0.0911, -0.9942] n_targets:  2 reward:  111.39\n",
      "34.0 action:  [-0.1014, -0.9971] n_targets:  1 reward:  51.65\n",
      "34.4 action:  [0.0955, -0.9965] n_targets:  1 reward:  64.18\n",
      "34.6 action:  [0.0201, -0.9956] n_targets:  1 reward:  50.84\n",
      "39.6 action:  [-0.08, -0.9982] n_targets:  1 reward:  51.57\n",
      "40.0 action:  [-0.1621, -0.9976] n_targets:  1 reward:  59.07\n",
      "41.2 action:  [-0.1001, -0.9969] n_targets:  2 reward:  103.63\n",
      "43.0 action:  [-0.1868, -0.9904] n_targets:  1 reward:  58.31\n",
      "44.6 action:  [0.158, -0.9992] n_targets:  1 reward:  58.37\n",
      "45.8 action:  [0.0827, -0.9901] n_targets:  1 reward:  54.01\n",
      "47.6 action:  [-0.1733, -0.9951] n_targets:  1 reward:  59.0\n",
      "48.8 action:  [-0.0412, -0.9926] n_targets:  2 reward:  105.62\n",
      "51.3 action:  [0.1133, -0.9922] n_targets:  1 reward:  59.75\n",
      "51.5 action:  [-0.2567, -0.9943] n_targets:  1 reward:  55.0\n",
      "55.1 action:  [-0.3087, -0.9944] n_targets:  1 reward:  58.48\n",
      "55.3 action:  [0.089, -0.9901] n_targets:  1 reward:  52.67\n",
      "55.5 action:  [-0.0206, -0.995] n_targets:  1 reward:  52.71\n",
      "55.7 action:  [-0.2169, -0.9971] n_targets:  1 reward:  50.45\n",
      "56.7 action:  [-0.2076, -0.993] n_targets:  2 reward:  107.2\n",
      "57.7 action:  [-0.0874, -0.9988] n_targets:  1 reward:  59.62\n",
      "58.1 action:  [-0.0169, -0.9926] n_targets:  1 reward:  58.4\n",
      "60.1 action:  [-0.0034, -0.9889] n_targets:  2 reward:  109.34\n",
      "60.5 action:  [0.1475, -0.9964] n_targets:  1 reward:  53.07\n",
      "60.7 action:  [-0.1287, -0.9949] n_targets:  2 reward:  108.15\n",
      "60.9 action:  [0.154, -0.9932] n_targets:  1 reward:  53.23\n",
      "61.9 action:  [-0.067, -0.9962] n_targets:  1 reward:  51.64\n",
      "62.3 action:  [0.0911, -0.9992] n_targets:  1 reward:  53.83\n",
      "62.7 action:  [-0.1757, -0.9903] n_targets:  1 reward:  54.38\n",
      "63.1 action:  [-0.1057, -0.9951] n_targets:  2 reward:  123.32\n",
      "65.0 action:  [-0.2199, -0.9875] n_targets:  1 reward:  54.7\n",
      "65.6 action:  [-0.0405, -0.9826] n_targets:  1 reward:  56.2\n",
      "66.0 action:  [-0.0881, -0.9984] n_targets:  1 reward:  56.16\n",
      "67.0 action:  [-0.2745, -0.9946] n_targets:  1 reward:  59.37\n",
      "68.0 action:  [-0.065, -0.9915] n_targets:  1 reward:  50.19\n",
      "68.4 action:  [-0.2822, -0.9957] n_targets:  1 reward:  50.33\n",
      "68.6 action:  [0.1821, -0.9968] n_targets:  1 reward:  59.32\n",
      "72.2 action:  [-0.0445, -0.9862] n_targets:  1 reward:  63.48\n",
      "73.4 action:  [-0.176, -0.992] n_targets:  1 reward:  53.36\n",
      "87.7 action:  [0.146, -0.9917] n_targets:  4 reward:  228.87\n",
      "88.5 action:  [-0.2127, -0.9979] n_targets:  1 reward:  55.28\n",
      "88.9 action:  [0.0653, -0.9919] n_targets:  2 reward:  104.56\n",
      "90.1 action:  [0.0301, -0.9961] n_targets:  1 reward:  51.55\n",
      "92.3 action:  [-0.375, -0.9962] n_targets:  1 reward:  56.01\n",
      "92.8 action:  [-0.0284, -0.9928] n_targets:  1 reward:  51.19\n",
      "95.6 action:  [-0.2291, -0.992] n_targets:  1 reward:  70.8\n",
      "95.8 action:  [-0.4164, -0.9949] n_targets:  1 reward:  57.39\n",
      "96.7 action:  [0.2642, -0.9893] n_targets:  1 reward:  54.81\n",
      "100.2 action:  [-0.0321, -0.9873] n_targets:  1 reward:  53.77\n",
      "101.2 action:  [0.0716, -0.9988] n_targets:  1 reward:  51.98\n",
      "101.8 action:  [-0.0403, -0.9953] n_targets:  1 reward:  51.58\n",
      "ALPHA (entropy-related):  tensor([0.1815], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.18984 0.18886 0.18805 0.18719 0.18618 0.18521 0.18413 0.18307 0.18222\n",
      " 0.18148]\n",
      "Episode: 117, Episode Reward: 5064.372416178385\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  287\n",
      "0.1 action:  [0.3095, -0.9987] n_targets:  1 reward:  81.53\n",
      "2.6 action:  [0.2332, -0.9956] n_targets:  1 reward:  60.3\n",
      "3.6 action:  [-0.2214, -0.998] n_targets:  1 reward:  50.02\n",
      "5.2 action:  [-0.0445, -0.987] n_targets:  1 reward:  58.23\n",
      "7.0 action:  [0.1711, -0.9981] n_targets:  1 reward:  50.28\n",
      "7.4 action:  [-0.4957, -0.9962] n_targets:  2 reward:  109.0\n",
      "7.6 action:  [-0.0515, -0.9971] n_targets:  1 reward:  58.73\n",
      "7.8 action:  [-0.121, -0.9885] n_targets:  1 reward:  50.68\n",
      "9.2 action:  [-0.1333, -0.9972] n_targets:  2 reward:  118.48\n",
      "9.8 action:  [-0.1368, -0.9972] n_targets:  1 reward:  50.16\n",
      "11.3 action:  [-0.1595, -0.9842] n_targets:  1 reward:  52.95\n",
      "12.7 action:  [-0.4343, -0.9832] n_targets:  1 reward:  50.25\n",
      "15.4 action:  [0.2092, -0.9976] n_targets:  1 reward:  58.49\n",
      "15.6 action:  [-0.2561, -0.9982] n_targets:  1 reward:  50.67\n",
      "16.8 action:  [-0.2583, -0.9943] n_targets:  1 reward:  51.21\n",
      "18.0 action:  [-0.0413, -0.9993] n_targets:  1 reward:  52.86\n",
      "18.6 action:  [-0.0057, -0.9975] n_targets:  1 reward:  50.52\n",
      "21.4 action:  [-0.0912, -0.9977] n_targets:  1 reward:  51.36\n",
      "22.0 action:  [-0.4439, -0.9901] n_targets:  1 reward:  60.87\n",
      "23.2 action:  [-0.1386, -0.9957] n_targets:  1 reward:  52.23\n",
      "23.4 action:  [-0.0181, -0.9889] n_targets:  1 reward:  59.15\n",
      "24.8 action:  [0.1146, -0.9932] n_targets:  2 reward:  101.69\n",
      "26.0 action:  [0.0475, -0.9974] n_targets:  1 reward:  52.08\n",
      "26.6 action:  [-0.1152, -0.9858] n_targets:  2 reward:  111.54\n",
      "26.8 action:  [-0.4645, -0.9958] n_targets:  1 reward:  54.77\n",
      "27.0 action:  [0.1543, -0.9953] n_targets:  1 reward:  50.09\n",
      "27.4 action:  [0.044, -0.9948] n_targets:  1 reward:  51.57\n",
      "27.8 action:  [-0.0335, -0.9946] n_targets:  1 reward:  51.63\n",
      "30.2 action:  [0.022, -0.9969] n_targets:  1 reward:  60.47\n",
      "30.6 action:  [-0.0324, -0.9984] n_targets:  1 reward:  53.72\n",
      "31.8 action:  [-0.3883, -0.9921] n_targets:  1 reward:  59.73\n",
      "34.4 action:  [-0.0235, -0.9835] n_targets:  1 reward:  55.9\n",
      "37.8 action:  [-0.135, -0.9867] n_targets:  1 reward:  55.4\n",
      "38.0 action:  [-0.1883, -0.9969] n_targets:  2 reward:  101.02\n",
      "39.4 action:  [-0.263, -0.9988] n_targets:  1 reward:  53.88\n",
      "42.6 action:  [-0.1289, -0.9889] n_targets:  1 reward:  53.46\n",
      "43.4 action:  [0.0416, -0.9868] n_targets:  1 reward:  54.79\n",
      "44.4 action:  [-0.1841, -0.9874] n_targets:  1 reward:  55.85\n",
      "46.0 action:  [-0.09, -0.9942] n_targets:  1 reward:  60.16\n",
      "46.2 action:  [0.107, -0.9866] n_targets:  1 reward:  51.72\n",
      "48.8 action:  [0.003, -0.9883] n_targets:  1 reward:  64.5\n",
      "49.0 action:  [-0.1827, -0.9994] n_targets:  1 reward:  56.37\n",
      "50.0 action:  [-0.1305, -0.9886] n_targets:  1 reward:  58.69\n",
      "50.8 action:  [-0.117, -0.9878] n_targets:  2 reward:  128.36\n",
      "51.0 action:  [-0.3963, -0.9974] n_targets:  1 reward:  53.38\n",
      "51.8 action:  [-0.3954, -0.9941] n_targets:  1 reward:  58.72\n",
      "53.4 action:  [-0.2226, -0.9864] n_targets:  1 reward:  52.89\n",
      "54.6 action:  [0.0214, -0.9975] n_targets:  1 reward:  58.06\n",
      "54.8 action:  [-0.1524, -0.9967] n_targets:  2 reward:  106.01\n",
      "55.0 action:  [0.0987, -0.9915] n_targets:  1 reward:  50.45\n",
      "57.6 action:  [0.0226, -0.9908] n_targets:  1 reward:  54.71\n",
      "58.6 action:  [0.0167, -0.9934] n_targets:  1 reward:  51.89\n",
      "61.0 action:  [-0.0558, -0.9921] n_targets:  1 reward:  56.01\n",
      "61.2 action:  [-0.0143, -0.9889] n_targets:  1 reward:  50.26\n",
      "61.4 action:  [0.15, -0.9956] n_targets:  1 reward:  55.33\n",
      "62.8 action:  [0.0195, -0.9865] n_targets:  1 reward:  50.43\n",
      "66.4 action:  [-0.0205, -0.995] n_targets:  1 reward:  61.69\n",
      "67.4 action:  [-0.0775, -0.9895] n_targets:  2 reward:  123.24\n",
      "71.6 action:  [-0.2205, -0.9906] n_targets:  1 reward:  53.03\n",
      "72.4 action:  [-0.1899, -0.9981] n_targets:  1 reward:  58.0\n",
      "75.2 action:  [0.0364, -0.9962] n_targets:  1 reward:  50.58\n",
      "76.2 action:  [-0.1513, -0.996] n_targets:  1 reward:  52.66\n",
      "77.0 action:  [-0.2966, -0.9994] n_targets:  4 reward:  252.86\n",
      "77.8 action:  [0.1926, -0.9928] n_targets:  1 reward:  52.13\n",
      "78.6 action:  [-0.2479, -0.9921] n_targets:  1 reward:  53.8\n",
      "79.4 action:  [-0.157, -0.9961] n_targets:  2 reward:  108.38\n",
      "82.6 action:  [-0.2715, -0.9985] n_targets:  1 reward:  53.8\n",
      "84.4 action:  [-0.1523, -0.9891] n_targets:  2 reward:  109.27\n",
      "84.6 action:  [-0.1808, -0.9994] n_targets:  1 reward:  55.89\n",
      "85.6 action:  [-0.0724, -0.9894] n_targets:  1 reward:  59.36\n",
      "86.8 action:  [-0.3517, -0.9975] n_targets:  1 reward:  50.2\n",
      "87.4 action:  [0.0655, -0.9966] n_targets:  1 reward:  50.88\n",
      "89.1 action:  [0.0515, -0.9932] n_targets:  2 reward:  104.64\n",
      "93.5 action:  [-0.1885, -0.998] n_targets:  1 reward:  62.2\n",
      "93.8 action:  [-0.4199, -0.9931] n_targets:  1 reward:  53.98\n",
      "95.2 action:  [0.1785, -0.9917] n_targets:  1 reward:  59.56\n",
      "96.0 action:  [0.0052, -0.9963] n_targets:  1 reward:  58.29\n",
      "100.6 action:  [-0.027, -0.9984] n_targets:  1 reward:  53.98\n",
      "102.0 action:  [-0.0111, -0.9959] n_targets:  1 reward:  50.94\n",
      "ALPHA (entropy-related):  tensor([0.1806], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.18886 0.18805 0.18719 0.18618 0.18521 0.18413 0.18307 0.18222 0.18148\n",
      " 0.18063]\n",
      "Episode: 118, Episode Reward: 5162.870713551838\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  288\n",
      "0.2 action:  [0.1523, -0.9956] n_targets:  2 reward:  107.28\n",
      "0.4 action:  [-0.1088, -0.9939] n_targets:  1 reward:  52.99\n",
      "1.6 action:  [-0.1263, -0.988] n_targets:  1 reward:  59.36\n",
      "1.8 action:  [-0.0337, -0.996] n_targets:  1 reward:  58.66\n",
      "2.2 action:  [-0.0831, -0.9918] n_targets:  1 reward:  60.7\n",
      "4.2 action:  [-0.144, -0.998] n_targets:  1 reward:  62.88\n",
      "4.8 action:  [-0.2554, -0.9898] n_targets:  1 reward:  50.36\n",
      "7.8 action:  [-0.0591, -0.9901] n_targets:  1 reward:  69.54\n",
      "8.2 action:  [-0.1785, -0.9983] n_targets:  1 reward:  50.45\n",
      "10.0 action:  [-0.0351, -0.999] n_targets:  1 reward:  52.47\n",
      "10.8 action:  [0.0584, -0.9925] n_targets:  1 reward:  51.28\n",
      "11.2 action:  [0.1077, -0.9909] n_targets:  1 reward:  62.03\n",
      "12.6 action:  [0.0502, -0.9946] n_targets:  1 reward:  51.4\n",
      "13.6 action:  [0.0928, -0.9986] n_targets:  1 reward:  61.19\n",
      "14.0 action:  [-0.1672, -0.9976] n_targets:  1 reward:  58.89\n",
      "14.4 action:  [0.0262, -0.9842] n_targets:  1 reward:  53.65\n",
      "15.0 action:  [-0.0909, -0.9835] n_targets:  1 reward:  62.96\n",
      "16.4 action:  [-0.0985, -0.9979] n_targets:  3 reward:  156.54\n",
      "19.6 action:  [-0.1366, -0.9801] n_targets:  1 reward:  56.98\n",
      "21.0 action:  [0.1855, -0.997] n_targets:  1 reward:  54.7\n",
      "22.2 action:  [0.0119, -0.9892] n_targets:  1 reward:  58.29\n",
      "23.2 action:  [-0.3553, -0.9977] n_targets:  1 reward:  50.53\n",
      "24.0 action:  [-0.2686, -0.9974] n_targets:  1 reward:  51.13\n",
      "24.6 action:  [0.2489, -0.9989] n_targets:  1 reward:  54.82\n",
      "26.6 action:  [0.0663, -0.9912] n_targets:  3 reward:  180.29\n",
      "27.2 action:  [-0.158, -0.9978] n_targets:  1 reward:  58.18\n",
      "27.4 action:  [0.1897, -0.9945] n_targets:  1 reward:  54.25\n",
      "27.9 action:  [-0.1607, -0.9966] n_targets:  1 reward:  54.15\n",
      "28.7 action:  [-0.0599, -0.9914] n_targets:  1 reward:  57.19\n",
      "30.9 action:  [-0.0185, -0.9811] n_targets:  1 reward:  57.52\n",
      "31.9 action:  [-0.1426, -0.9976] n_targets:  1 reward:  51.18\n",
      "32.7 action:  [-0.0097, -0.991] n_targets:  1 reward:  51.57\n",
      "33.5 action:  [0.3117, -0.9976] n_targets:  1 reward:  55.57\n",
      "35.0 action:  [0.1059, -0.9987] n_targets:  2 reward:  105.97\n",
      "37.2 action:  [-0.1841, -0.9983] n_targets:  1 reward:  51.84\n",
      "37.8 action:  [-0.0004, -0.9952] n_targets:  1 reward:  50.9\n",
      "38.2 action:  [0.187, -0.9881] n_targets:  1 reward:  52.17\n",
      "39.6 action:  [-0.0191, -0.9955] n_targets:  1 reward:  58.93\n",
      "41.2 action:  [-0.1078, -0.9988] n_targets:  2 reward:  144.92\n",
      "41.6 action:  [-0.0198, -0.9985] n_targets:  1 reward:  50.47\n",
      "43.6 action:  [-0.0392, -0.9959] n_targets:  1 reward:  58.63\n",
      "44.9 action:  [0.0501, -0.9986] n_targets:  1 reward:  50.25\n",
      "45.3 action:  [0.1041, -0.9969] n_targets:  1 reward:  57.61\n",
      "46.1 action:  [-0.0188, -0.997] n_targets:  1 reward:  53.58\n",
      "46.9 action:  [-0.125, -0.9924] n_targets:  1 reward:  66.71\n",
      "48.9 action:  [-0.1179, -0.9959] n_targets:  1 reward:  52.74\n",
      "49.1 action:  [-0.1499, -0.992] n_targets:  1 reward:  57.37\n",
      "50.9 action:  [-0.0673, -0.9951] n_targets:  2 reward:  116.19\n",
      "53.7 action:  [0.1521, -0.9921] n_targets:  1 reward:  53.16\n",
      "54.7 action:  [0.124, -0.9961] n_targets:  1 reward:  55.56\n",
      "56.1 action:  [-0.3525, -0.9923] n_targets:  1 reward:  53.78\n",
      "56.5 action:  [-0.0583, -0.9954] n_targets:  1 reward:  59.64\n",
      "56.9 action:  [0.1167, -0.9895] n_targets:  1 reward:  55.45\n",
      "57.9 action:  [-0.015, -0.9973] n_targets:  1 reward:  63.37\n",
      "61.0 action:  [0.3357, -0.9975] n_targets:  1 reward:  54.8\n",
      "61.4 action:  [0.0031, -0.9908] n_targets:  2 reward:  105.42\n",
      "62.6 action:  [0.1049, -0.9986] n_targets:  2 reward:  113.36\n",
      "64.8 action:  [-0.0652, -0.9985] n_targets:  2 reward:  104.9\n",
      "68.0 action:  [0.0704, -0.9897] n_targets:  1 reward:  50.9\n",
      "69.4 action:  [-0.3278, -0.9907] n_targets:  1 reward:  50.44\n",
      "69.8 action:  [-0.0328, -0.9881] n_targets:  1 reward:  52.82\n",
      "70.4 action:  [-0.1162, -0.9925] n_targets:  1 reward:  53.4\n",
      "71.6 action:  [-0.0971, -0.9983] n_targets:  1 reward:  57.61\n",
      "76.2 action:  [-0.0742, -0.9983] n_targets:  1 reward:  51.99\n",
      "76.8 action:  [-0.0014, -0.9925] n_targets:  1 reward:  70.62\n",
      "79.0 action:  [0.0407, -0.9843] n_targets:  1 reward:  54.08\n",
      "79.8 action:  [-0.1344, -0.997] n_targets:  1 reward:  53.54\n",
      "80.0 action:  [-0.2937, -0.9951] n_targets:  2 reward:  106.12\n",
      "81.0 action:  [-0.0043, -0.9984] n_targets:  1 reward:  58.77\n",
      "81.6 action:  [-0.1216, -0.999] n_targets:  1 reward:  57.49\n",
      "82.6 action:  [-0.0416, -0.9884] n_targets:  2 reward:  110.67\n",
      "87.0 action:  [0.0948, -0.9983] n_targets:  1 reward:  55.23\n",
      "90.6 action:  [-0.439, -0.9808] n_targets:  1 reward:  51.01\n",
      "91.7 action:  [0.0144, -0.9997] n_targets:  1 reward:  58.64\n",
      "92.5 action:  [-0.109, -0.9985] n_targets:  2 reward:  116.07\n",
      "92.9 action:  [0.0246, -0.9919] n_targets:  1 reward:  56.52\n",
      "93.1 action:  [-0.2223, -0.9805] n_targets:  1 reward:  56.21\n",
      "100.8 action:  [-0.4076, -0.9926] n_targets:  1 reward:  51.8\n",
      "ALPHA (entropy-related):  tensor([0.1799], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.18805 0.18719 0.18618 0.18521 0.18413 0.18307 0.18222 0.18148 0.18063\n",
      " 0.17987]\n",
      "Episode: 119, Episode Reward: 5150.666651407877\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  289\n",
      "0.1 action:  [-0.0206, -0.9921] n_targets:  2 reward:  134.61\n",
      "0.3 action:  [0.0595, -0.9974] n_targets:  1 reward:  52.56\n",
      "2.7 action:  [-0.2617, -0.998] n_targets:  1 reward:  50.29\n",
      "3.5 action:  [0.0622, -0.9974] n_targets:  1 reward:  59.07\n",
      "3.7 action:  [-0.058, -0.9901] n_targets:  1 reward:  52.16\n",
      "4.7 action:  [0.3388, -0.9962] n_targets:  1 reward:  51.14\n",
      "5.5 action:  [-0.0425, -0.9851] n_targets:  1 reward:  53.52\n",
      "6.1 action:  [-0.3234, -0.9962] n_targets:  1 reward:  52.82\n",
      "6.3 action:  [0.1661, -0.9929] n_targets:  1 reward:  53.63\n",
      "6.5 action:  [-0.0902, -0.9986] n_targets:  1 reward:  52.9\n",
      "7.3 action:  [-0.2775, -0.9978] n_targets:  1 reward:  56.95\n",
      "8.9 action:  [-0.0357, -0.9968] n_targets:  1 reward:  58.89\n",
      "9.5 action:  [-0.2029, -0.9966] n_targets:  1 reward:  50.36\n",
      "9.7 action:  [0.0701, -0.9983] n_targets:  1 reward:  50.62\n",
      "10.3 action:  [-0.2415, -0.9936] n_targets:  1 reward:  55.05\n",
      "14.5 action:  [-0.215, -0.9984] n_targets:  1 reward:  59.08\n",
      "15.7 action:  [-0.3565, -0.9873] n_targets:  1 reward:  51.92\n",
      "15.9 action:  [-0.2179, -0.9989] n_targets:  1 reward:  56.8\n",
      "16.5 action:  [-0.0119, -0.9993] n_targets:  1 reward:  61.42\n",
      "17.9 action:  [0.1835, -0.9971] n_targets:  1 reward:  59.34\n",
      "19.5 action:  [0.0061, -0.9917] n_targets:  2 reward:  115.63\n",
      "19.7 action:  [-0.4909, -0.9863] n_targets:  1 reward:  56.16\n",
      "21.7 action:  [0.1756, -0.9986] n_targets:  1 reward:  55.04\n",
      "24.2 action:  [-0.0747, -0.9966] n_targets:  2 reward:  108.09\n",
      "25.6 action:  [-0.0609, -0.9979] n_targets:  1 reward:  54.94\n",
      "26.2 action:  [-0.0728, -0.9956] n_targets:  2 reward:  107.66\n",
      "27.0 action:  [0.0837, -0.9908] n_targets:  2 reward:  109.49\n",
      "29.7 action:  [-0.0933, -0.9935] n_targets:  1 reward:  51.26\n",
      "31.9 action:  [0.0671, -0.9975] n_targets:  1 reward:  84.95\n",
      "32.3 action:  [0.0844, -0.9923] n_targets:  1 reward:  50.66\n",
      "38.1 action:  [0.1043, -0.9818] n_targets:  1 reward:  67.41\n",
      "38.5 action:  [-0.0335, -0.9944] n_targets:  1 reward:  50.21\n",
      "39.3 action:  [0.0315, -0.9819] n_targets:  1 reward:  56.94\n",
      "41.1 action:  [-0.3656, -0.9991] n_targets:  1 reward:  62.37\n",
      "41.3 action:  [-0.1814, -0.9968] n_targets:  1 reward:  61.0\n",
      "41.5 action:  [0.0377, -0.9893] n_targets:  1 reward:  56.03\n",
      "43.3 action:  [0.0353, -0.9922] n_targets:  1 reward:  53.09\n",
      "44.3 action:  [0.2505, -0.9866] n_targets:  1 reward:  50.38\n",
      "44.9 action:  [-0.0312, -0.9975] n_targets:  1 reward:  55.28\n",
      "45.7 action:  [-0.0448, -0.9959] n_targets:  1 reward:  53.66\n",
      "48.5 action:  [0.4578, -0.9932] n_targets:  2 reward:  110.31\n",
      "48.7 action:  [0.0168, -0.9944] n_targets:  1 reward:  55.96\n",
      "49.8 action:  [-0.3648, -0.9895] n_targets:  1 reward:  55.31\n",
      "50.6 action:  [0.0723, -0.9947] n_targets:  1 reward:  52.89\n",
      "52.4 action:  [0.2516, -0.9932] n_targets:  1 reward:  53.86\n",
      "53.6 action:  [0.0232, -0.9943] n_targets:  1 reward:  70.54\n",
      "54.4 action:  [0.2375, -0.9946] n_targets:  3 reward:  178.07\n",
      "54.6 action:  [0.0137, -0.9976] n_targets:  2 reward:  113.23\n",
      "56.4 action:  [0.2608, -0.9985] n_targets:  1 reward:  54.27\n",
      "57.4 action:  [-0.164, -0.9994] n_targets:  1 reward:  56.89\n",
      "57.6 action:  [-0.0633, -0.996] n_targets:  1 reward:  60.58\n",
      "59.2 action:  [0.402, -0.993] n_targets:  1 reward:  56.55\n",
      "60.8 action:  [-0.1595, -0.9957] n_targets:  2 reward:  107.21\n",
      "61.2 action:  [-0.4631, -0.9813] n_targets:  1 reward:  60.91\n",
      "64.8 action:  [-0.0393, -0.9913] n_targets:  1 reward:  51.77\n",
      "68.2 action:  [-0.0963, -0.9994] n_targets:  1 reward:  53.6\n",
      "68.4 action:  [-0.0589, -0.9904] n_targets:  1 reward:  52.62\n",
      "70.4 action:  [-0.1784, -0.9907] n_targets:  1 reward:  56.76\n",
      "71.4 action:  [-0.039, -0.9968] n_targets:  1 reward:  50.66\n",
      "73.2 action:  [-0.1262, -0.9868] n_targets:  3 reward:  183.31\n",
      "74.6 action:  [-0.0318, -0.9891] n_targets:  1 reward:  53.93\n",
      "75.2 action:  [0.1559, -0.9943] n_targets:  1 reward:  53.11\n",
      "76.4 action:  [-0.022, -0.9976] n_targets:  1 reward:  51.01\n",
      "77.4 action:  [0.0717, -0.9977] n_targets:  2 reward:  159.0\n",
      "78.6 action:  [-0.1078, -0.9957] n_targets:  1 reward:  51.43\n",
      "80.0 action:  [-0.0729, -0.995] n_targets:  1 reward:  54.78\n",
      "86.6 action:  [0.0003, -0.9986] n_targets:  1 reward:  57.64\n",
      "87.0 action:  [-0.2053, -0.9992] n_targets:  1 reward:  58.67\n",
      "89.0 action:  [0.125, -0.9918] n_targets:  1 reward:  51.09\n",
      "89.8 action:  [0.1457, -0.9917] n_targets:  1 reward:  55.28\n",
      "90.2 action:  [-0.112, -0.9972] n_targets:  1 reward:  61.42\n",
      "90.6 action:  [0.1736, -0.9911] n_targets:  1 reward:  50.67\n",
      "92.2 action:  [0.0694, -0.9938] n_targets:  1 reward:  56.43\n",
      "93.8 action:  [0.342, -0.9865] n_targets:  2 reward:  110.76\n",
      "97.6 action:  [0.0183, -0.9872] n_targets:  2 reward:  161.56\n",
      "100.0 action:  [-0.3402, -0.9918] n_targets:  1 reward:  55.96\n",
      "100.8 action:  [0.0509, -0.9956] n_targets:  1 reward:  57.42\n",
      "101.0 action:  [-0.0021, -0.994] n_targets:  1 reward:  50.23\n",
      "ALPHA (entropy-related):  tensor([0.1790], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.18719 0.18618 0.18521 0.18413 0.18307 0.18222 0.18148 0.18063 0.17987\n",
      " 0.17898]\n",
      "Last 100 ALPHA: [0.60419 0.59106 0.5783  0.5658  0.55365 0.54182 0.53022 0.51899 0.50808\n",
      " 0.49747 0.48716 0.47703 0.4672  0.45771 0.44833 0.43925 0.43039 0.42165\n",
      " 0.41309 0.40474 0.39652 0.38867 0.38105 0.37368 0.36641 0.35933 0.35272\n",
      " 0.34622 0.33993 0.33402 0.32853 0.32323 0.3181  0.3132  0.30834 0.30365\n",
      " 0.29916 0.2949  0.29077 0.28672 0.28285 0.27889 0.27518 0.27152 0.26775\n",
      " 0.26443 0.26134 0.2583  0.25516 0.25228 0.24955 0.24708 0.24469 0.24231\n",
      " 0.23982 0.2372  0.23469 0.23254 0.23052 0.22838 0.22627 0.22428 0.22241\n",
      " 0.22068 0.21899 0.21737 0.21588 0.21455 0.21316 0.21174 0.21035 0.2088\n",
      " 0.20702 0.20546 0.20408 0.20271 0.20141 0.20007 0.19875 0.19767 0.19656\n",
      " 0.19547 0.19465 0.19375 0.19287 0.19195 0.19094 0.18984 0.18886 0.18805\n",
      " 0.18719 0.18618 0.18521 0.18413 0.18307 0.18222 0.18148 0.18063 0.17987\n",
      " 0.17898]\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  290\n",
      "0.1 action:  [-0.078, -0.9947] n_targets:  1 reward:  87.97\n",
      "1.3 action:  [-0.1138, -0.9942] n_targets:  1 reward:  51.09\n",
      "5.3 action:  [-0.1151, -0.9939] n_targets:  1 reward:  50.38\n",
      "6.7 action:  [-0.1123, -0.9939] n_targets:  1 reward:  57.94\n",
      "7.9 action:  [-0.1157, -0.9939] n_targets:  1 reward:  57.82\n",
      "8.9 action:  [-0.1175, -0.994] n_targets:  1 reward:  51.33\n",
      "9.9 action:  [-0.1174, -0.9942] n_targets:  1 reward:  50.8\n",
      "13.9 action:  [-0.1066, -0.9937] n_targets:  1 reward:  63.72\n",
      "15.5 action:  [-0.1157, -0.9939] n_targets:  1 reward:  57.88\n",
      "16.7 action:  [-0.1155, -0.994] n_targets:  1 reward:  54.99\n",
      "17.1 action:  [-0.1125, -0.9938] n_targets:  1 reward:  59.33\n",
      "19.7 action:  [-0.1136, -0.994] n_targets:  1 reward:  58.25\n",
      "22.7 action:  [-0.1107, -0.9941] n_targets:  1 reward:  59.73\n",
      "22.9 action:  [-0.1094, -0.9941] n_targets:  2 reward:  109.4\n",
      "23.9 action:  [-0.1148, -0.9941] n_targets:  1 reward:  60.5\n",
      "24.7 action:  [-0.1176, -0.9941] n_targets:  2 reward:  115.43\n",
      "25.5 action:  [-0.1101, -0.9928] n_targets:  1 reward:  58.16\n",
      "27.3 action:  [-0.115, -0.9937] n_targets:  1 reward:  55.36\n",
      "28.3 action:  [-0.1154, -0.994] n_targets:  2 reward:  112.43\n",
      "28.5 action:  [-0.1164, -0.9941] n_targets:  1 reward:  50.72\n",
      "28.9 action:  [-0.1157, -0.9941] n_targets:  1 reward:  55.07\n",
      "29.7 action:  [-0.1082, -0.9941] n_targets:  2 reward:  105.71\n",
      "32.5 action:  [-0.1067, -0.9937] n_targets:  1 reward:  55.99\n",
      "32.7 action:  [-0.1157, -0.994] n_targets:  1 reward:  60.21\n",
      "34.1 action:  [-0.1089, -0.9937] n_targets:  1 reward:  61.7\n",
      "34.5 action:  [-0.1041, -0.9937] n_targets:  1 reward:  50.4\n",
      "36.1 action:  [-0.1075, -0.9935] n_targets:  1 reward:  59.86\n",
      "37.5 action:  [-0.1169, -0.9941] n_targets:  1 reward:  56.73\n",
      "38.5 action:  [-0.072, -0.9926] n_targets:  1 reward:  56.41\n",
      "39.1 action:  [-0.1122, -0.9942] n_targets:  1 reward:  63.02\n",
      "39.5 action:  [-0.1134, -0.994] n_targets:  1 reward:  53.27\n",
      "41.1 action:  [-0.1107, -0.9939] n_targets:  1 reward:  54.85\n",
      "43.5 action:  [-0.1077, -0.9937] n_targets:  1 reward:  52.28\n",
      "45.3 action:  [-0.1172, -0.9939] n_targets:  1 reward:  55.39\n",
      "46.3 action:  [-0.117, -0.9941] n_targets:  1 reward:  57.58\n",
      "47.3 action:  [-0.1155, -0.994] n_targets:  2 reward:  112.09\n",
      "49.9 action:  [-0.1161, -0.994] n_targets:  1 reward:  59.51\n",
      "50.9 action:  [-0.1006, -0.9939] n_targets:  1 reward:  57.65\n",
      "51.3 action:  [-0.0688, -0.9931] n_targets:  1 reward:  53.95\n",
      "52.5 action:  [-0.1034, -0.9937] n_targets:  1 reward:  81.7\n",
      "56.9 action:  [-0.1152, -0.9941] n_targets:  1 reward:  50.36\n",
      "57.1 action:  [-0.1129, -0.9937] n_targets:  1 reward:  61.41\n",
      "64.5 action:  [-0.1191, -0.994] n_targets:  1 reward:  50.21\n",
      "65.1 action:  [-0.1115, -0.9936] n_targets:  1 reward:  50.21\n",
      "65.3 action:  [-0.0573, -0.9923] n_targets:  1 reward:  66.15\n",
      "66.7 action:  [-0.1156, -0.9937] n_targets:  1 reward:  50.27\n",
      "71.7 action:  [-0.1155, -0.9938] n_targets:  2 reward:  103.19\n",
      "73.5 action:  [-0.1108, -0.9935] n_targets:  2 reward:  113.2\n",
      "78.3 action:  [-0.0715, -0.9935] n_targets:  1 reward:  53.98\n",
      "83.5 action:  [-0.1167, -0.994] n_targets:  1 reward:  55.92\n",
      "84.3 action:  [-0.1158, -0.994] n_targets:  1 reward:  56.13\n",
      "84.5 action:  [-0.1156, -0.994] n_targets:  1 reward:  52.51\n",
      "86.5 action:  [-0.1151, -0.994] n_targets:  1 reward:  62.95\n",
      "87.1 action:  [-0.1125, -0.994] n_targets:  2 reward:  105.16\n",
      "88.9 action:  [-0.1153, -0.9938] n_targets:  1 reward:  56.56\n",
      "89.3 action:  [-0.1067, -0.993] n_targets:  1 reward:  54.88\n",
      "90.3 action:  [-0.0676, -0.9928] n_targets:  1 reward:  54.66\n",
      "91.7 action:  [-0.1074, -0.9939] n_targets:  2 reward:  129.53\n",
      "92.1 action:  [-0.1136, -0.9938] n_targets:  1 reward:  56.96\n",
      "92.7 action:  [-0.118, -0.9942] n_targets:  1 reward:  50.51\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  291\n",
      "1.1 action:  [-0.1157, -0.994] n_targets:  1 reward:  54.44\n",
      "4.7 action:  [-0.1077, -0.9936] n_targets:  1 reward:  59.84\n",
      "6.3 action:  [-0.1161, -0.9941] n_targets:  1 reward:  58.55\n",
      "7.3 action:  [-0.114, -0.9938] n_targets:  1 reward:  50.64\n",
      "8.3 action:  [-0.1077, -0.9937] n_targets:  1 reward:  60.76\n",
      "8.7 action:  [-0.1164, -0.9939] n_targets:  2 reward:  104.16\n",
      "9.3 action:  [-0.1155, -0.9939] n_targets:  1 reward:  61.87\n",
      "9.5 action:  [-0.1159, -0.9942] n_targets:  1 reward:  52.65\n",
      "10.5 action:  [-0.1145, -0.9941] n_targets:  1 reward:  56.11\n",
      "10.7 action:  [-0.1089, -0.9941] n_targets:  1 reward:  57.01\n",
      "11.3 action:  [-0.1125, -0.9937] n_targets:  1 reward:  55.12\n",
      "11.5 action:  [-0.1146, -0.9938] n_targets:  1 reward:  53.27\n",
      "13.9 action:  [-0.1127, -0.9941] n_targets:  1 reward:  55.57\n",
      "18.1 action:  [-0.1104, -0.9936] n_targets:  1 reward:  55.21\n",
      "18.7 action:  [-0.1121, -0.9936] n_targets:  1 reward:  56.84\n",
      "18.9 action:  [-0.1141, -0.9937] n_targets:  1 reward:  50.41\n",
      "21.1 action:  [-0.1167, -0.9942] n_targets:  1 reward:  52.78\n",
      "21.5 action:  [-0.1116, -0.994] n_targets:  1 reward:  51.45\n",
      "21.9 action:  [-0.1116, -0.9941] n_targets:  1 reward:  54.43\n",
      "22.3 action:  [-0.1171, -0.9941] n_targets:  1 reward:  52.49\n",
      "22.5 action:  [-0.1171, -0.9941] n_targets:  1 reward:  51.2\n",
      "22.9 action:  [-0.1153, -0.9941] n_targets:  2 reward:  107.05\n",
      "24.5 action:  [-0.1173, -0.9941] n_targets:  1 reward:  50.27\n",
      "24.9 action:  [-0.0534, -0.993] n_targets:  1 reward:  53.29\n",
      "25.7 action:  [-0.0584, -0.9933] n_targets:  1 reward:  61.62\n",
      "28.3 action:  [-0.1172, -0.994] n_targets:  1 reward:  53.93\n",
      "32.3 action:  [-0.1154, -0.9941] n_targets:  1 reward:  53.53\n",
      "33.7 action:  [-0.1154, -0.9941] n_targets:  1 reward:  54.1\n",
      "35.1 action:  [-0.1088, -0.9936] n_targets:  1 reward:  57.17\n",
      "35.9 action:  [-0.1083, -0.9937] n_targets:  1 reward:  55.04\n",
      "36.5 action:  [-0.1084, -0.9938] n_targets:  2 reward:  107.79\n",
      "37.1 action:  [-0.1126, -0.9938] n_targets:  1 reward:  57.5\n",
      "38.7 action:  [-0.1092, -0.9933] n_targets:  2 reward:  112.35\n",
      "41.3 action:  [-0.1136, -0.9941] n_targets:  2 reward:  108.73\n",
      "41.9 action:  [-0.1169, -0.994] n_targets:  1 reward:  54.94\n",
      "43.9 action:  [-0.1129, -0.9939] n_targets:  1 reward:  54.83\n",
      "45.1 action:  [-0.1164, -0.994] n_targets:  2 reward:  107.96\n",
      "47.5 action:  [-0.1163, -0.9941] n_targets:  1 reward:  53.52\n",
      "48.1 action:  [-0.1159, -0.994] n_targets:  1 reward:  54.18\n",
      "48.7 action:  [-0.113, -0.9938] n_targets:  1 reward:  58.26\n",
      "49.9 action:  [-0.1089, -0.9937] n_targets:  1 reward:  52.7\n",
      "51.3 action:  [-0.0526, -0.9923] n_targets:  1 reward:  55.89\n",
      "54.1 action:  [-0.1148, -0.994] n_targets:  1 reward:  57.98\n",
      "54.7 action:  [-0.1136, -0.994] n_targets:  1 reward:  53.14\n",
      "60.3 action:  [-0.107, -0.9935] n_targets:  1 reward:  60.69\n",
      "61.3 action:  [-0.1125, -0.9938] n_targets:  1 reward:  51.25\n",
      "64.1 action:  [-0.0955, -0.9935] n_targets:  1 reward:  51.22\n",
      "64.5 action:  [-0.0567, -0.9923] n_targets:  1 reward:  58.1\n",
      "65.5 action:  [-0.113, -0.9941] n_targets:  1 reward:  54.68\n",
      "65.7 action:  [-0.1166, -0.994] n_targets:  1 reward:  80.04\n",
      "72.1 action:  [-0.104, -0.9936] n_targets:  1 reward:  54.55\n",
      "73.9 action:  [-0.1104, -0.9938] n_targets:  2 reward:  104.71\n",
      "74.7 action:  [-0.1057, -0.9933] n_targets:  1 reward:  55.62\n",
      "74.9 action:  [-0.1178, -0.9933] n_targets:  1 reward:  51.68\n",
      "75.3 action:  [-0.1072, -0.9934] n_targets:  1 reward:  58.98\n",
      "75.5 action:  [-0.1051, -0.9932] n_targets:  1 reward:  52.75\n",
      "76.7 action:  [-0.1033, -0.9931] n_targets:  1 reward:  52.13\n",
      "78.1 action:  [-0.1125, -0.9941] n_targets:  2 reward:  132.14\n",
      "80.7 action:  [-0.1134, -0.9938] n_targets:  1 reward:  54.75\n",
      "81.7 action:  [-0.112, -0.9941] n_targets:  1 reward:  51.78\n",
      "81.9 action:  [-0.1114, -0.9941] n_targets:  1 reward:  52.52\n",
      "82.9 action:  [-0.1161, -0.9938] n_targets:  1 reward:  51.66\n",
      "86.3 action:  [-0.1164, -0.9938] n_targets:  1 reward:  59.12\n",
      "87.3 action:  [-0.1173, -0.994] n_targets:  1 reward:  51.37\n",
      "87.7 action:  [-0.1174, -0.994] n_targets:  1 reward:  55.94\n",
      "89.1 action:  [-0.1167, -0.9939] n_targets:  1 reward:  50.26\n",
      "96.3 action:  [-0.1145, -0.994] n_targets:  1 reward:  51.42\n",
      "97.9 action:  [-0.1171, -0.9941] n_targets:  1 reward:  51.3\n",
      "98.9 action:  [-0.1147, -0.9939] n_targets:  1 reward:  51.78\n",
      "100.1 action:  [-0.1164, -0.9939] n_targets:  1 reward:  53.03\n",
      "Best average reward: 5468.409524281821, Current average reward: 4107.727509816486\n",
      "Evaluation rewards: [0.0, 0.0, 1720.9753100077312, 1085.8211957613628, 4981.922852198283, 5070.855417569475, 5468.409524281821, 4107.727509816486]\n",
      "Episode: 120, Episode Reward: 5319.034755706787\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  292\n",
      "0.2 action:  [-0.0329, -0.9918] n_targets:  1 reward:  50.41\n",
      "1.8 action:  [-0.1597, -0.9972] n_targets:  1 reward:  52.24\n",
      "4.0 action:  [-0.1138, -0.9853] n_targets:  1 reward:  56.4\n",
      "4.2 action:  [-0.068, -0.9992] n_targets:  1 reward:  50.42\n",
      "5.0 action:  [-0.3757, -0.9816] n_targets:  1 reward:  53.02\n",
      "6.0 action:  [-0.2952, -0.9842] n_targets:  1 reward:  55.03\n",
      "6.8 action:  [0.1357, -0.9933] n_targets:  1 reward:  75.06\n",
      "10.6 action:  [-0.103, -0.9851] n_targets:  1 reward:  54.15\n",
      "10.8 action:  [-0.3936, -0.9908] n_targets:  1 reward:  50.9\n",
      "12.4 action:  [-0.0344, -0.9979] n_targets:  1 reward:  77.38\n",
      "12.8 action:  [-0.055, -0.9982] n_targets:  1 reward:  53.44\n",
      "13.5 action:  [-0.1424, -0.9917] n_targets:  1 reward:  60.65\n",
      "15.1 action:  [0.1886, -0.9951] n_targets:  1 reward:  55.55\n",
      "17.5 action:  [-0.0478, -0.9989] n_targets:  1 reward:  56.99\n",
      "21.9 action:  [-0.2647, -0.9936] n_targets:  1 reward:  59.42\n",
      "22.9 action:  [-0.3695, -0.9989] n_targets:  2 reward:  115.44\n",
      "23.7 action:  [-0.2808, -0.9872] n_targets:  1 reward:  50.28\n",
      "24.5 action:  [-0.0404, -0.9937] n_targets:  1 reward:  63.31\n",
      "24.7 action:  [-0.012, -0.9854] n_targets:  1 reward:  55.74\n",
      "25.3 action:  [-0.208, -0.9981] n_targets:  2 reward:  116.5\n",
      "26.7 action:  [0.1929, -0.9961] n_targets:  1 reward:  79.42\n",
      "27.7 action:  [-0.1747, -0.9874] n_targets:  1 reward:  50.78\n",
      "28.1 action:  [-0.1436, -0.9923] n_targets:  1 reward:  53.79\n",
      "28.9 action:  [-0.1071, -0.9992] n_targets:  1 reward:  52.19\n",
      "29.7 action:  [-0.1452, -0.997] n_targets:  1 reward:  58.68\n",
      "30.6 action:  [-0.0268, -0.9919] n_targets:  2 reward:  112.48\n",
      "31.6 action:  [-0.1537, -0.9939] n_targets:  1 reward:  51.14\n",
      "31.8 action:  [-0.2591, -0.9984] n_targets:  1 reward:  50.74\n",
      "32.0 action:  [0.1101, -0.9982] n_targets:  2 reward:  110.22\n",
      "32.6 action:  [-0.2303, -0.9892] n_targets:  1 reward:  53.38\n",
      "35.8 action:  [0.2251, -0.9933] n_targets:  1 reward:  54.97\n",
      "37.4 action:  [0.1853, -0.9944] n_targets:  2 reward:  141.05\n",
      "38.8 action:  [-0.1816, -0.9961] n_targets:  1 reward:  59.0\n",
      "39.2 action:  [-0.0595, -0.9959] n_targets:  1 reward:  50.94\n",
      "39.4 action:  [-0.2949, -0.997] n_targets:  1 reward:  58.66\n",
      "40.2 action:  [-0.1987, -0.9964] n_targets:  1 reward:  58.2\n",
      "41.4 action:  [0.0511, -0.9992] n_targets:  1 reward:  52.79\n",
      "41.6 action:  [0.0558, -0.9826] n_targets:  2 reward:  104.66\n",
      "42.4 action:  [-0.1178, -0.9898] n_targets:  2 reward:  104.12\n",
      "42.8 action:  [0.2158, -0.9902] n_targets:  1 reward:  56.31\n",
      "43.2 action:  [-0.1501, -0.988] n_targets:  2 reward:  115.49\n",
      "44.6 action:  [0.0113, -0.9801] n_targets:  1 reward:  50.95\n",
      "45.0 action:  [-0.0398, -0.9859] n_targets:  1 reward:  50.13\n",
      "46.4 action:  [-0.0473, -0.9945] n_targets:  1 reward:  53.93\n",
      "47.0 action:  [0.1623, -0.9964] n_targets:  1 reward:  70.4\n",
      "47.2 action:  [-0.1877, -0.9944] n_targets:  1 reward:  55.8\n",
      "47.6 action:  [-0.2775, -0.992] n_targets:  1 reward:  54.77\n",
      "49.4 action:  [0.1843, -0.9831] n_targets:  1 reward:  51.64\n",
      "49.6 action:  [-0.1431, -0.9942] n_targets:  2 reward:  110.93\n",
      "50.8 action:  [-0.0071, -0.9898] n_targets:  1 reward:  50.02\n",
      "51.8 action:  [-0.1482, -0.9934] n_targets:  1 reward:  55.33\n",
      "52.8 action:  [0.1878, -0.9932] n_targets:  1 reward:  50.13\n",
      "54.0 action:  [0.1708, -0.9972] n_targets:  1 reward:  55.79\n",
      "56.2 action:  [0.0218, -0.9958] n_targets:  1 reward:  52.68\n",
      "57.8 action:  [0.0724, -0.9982] n_targets:  1 reward:  57.93\n",
      "58.8 action:  [-0.1188, -0.9902] n_targets:  1 reward:  53.26\n",
      "59.4 action:  [-0.0688, -0.9942] n_targets:  1 reward:  57.23\n",
      "59.6 action:  [0.2322, -0.9963] n_targets:  2 reward:  107.88\n",
      "63.2 action:  [0.1947, -0.9914] n_targets:  1 reward:  51.3\n",
      "64.2 action:  [-0.2562, -0.9956] n_targets:  2 reward:  118.22\n",
      "66.8 action:  [0.012, -0.9942] n_targets:  1 reward:  51.65\n",
      "67.0 action:  [0.0197, -0.9907] n_targets:  1 reward:  53.26\n",
      "72.8 action:  [0.0506, -0.9935] n_targets:  1 reward:  54.18\n",
      "74.2 action:  [0.0096, -0.9923] n_targets:  1 reward:  59.12\n",
      "75.2 action:  [0.2231, -0.991] n_targets:  1 reward:  50.02\n",
      "80.6 action:  [-0.0921, -0.9845] n_targets:  1 reward:  52.91\n",
      "81.2 action:  [-0.1253, -0.9963] n_targets:  1 reward:  58.03\n",
      "81.8 action:  [-0.1525, -0.9975] n_targets:  2 reward:  110.02\n",
      "83.4 action:  [-0.2801, -0.9991] n_targets:  2 reward:  109.13\n",
      "86.6 action:  [0.0587, -0.9937] n_targets:  1 reward:  51.48\n",
      "87.0 action:  [0.1012, -0.9958] n_targets:  1 reward:  56.34\n",
      "87.4 action:  [-0.011, -0.9986] n_targets:  2 reward:  108.66\n",
      "88.6 action:  [-0.2344, -0.9972] n_targets:  1 reward:  54.36\n",
      "89.2 action:  [-0.0215, -0.9932] n_targets:  1 reward:  51.74\n",
      "90.8 action:  [-0.3218, -0.9959] n_targets:  1 reward:  57.54\n",
      "91.4 action:  [-0.2042, -0.9903] n_targets:  1 reward:  52.3\n",
      "92.0 action:  [-0.1746, -0.9978] n_targets:  1 reward:  52.59\n",
      "94.2 action:  [-0.2096, -0.9802] n_targets:  1 reward:  60.57\n",
      "96.2 action:  [0.0567, -0.9969] n_targets:  2 reward:  116.44\n",
      "96.6 action:  [-0.113, -0.9849] n_targets:  1 reward:  53.29\n",
      "96.8 action:  [0.0594, -0.9943] n_targets:  1 reward:  50.58\n",
      "97.2 action:  [0.1056, -0.9892] n_targets:  1 reward:  59.58\n",
      "101.4 action:  [-0.0627, -0.9979] n_targets:  1 reward:  54.78\n",
      "102.0 action:  [0.2899, -0.997] n_targets:  1 reward:  55.42\n",
      "102.2 action:  [0.0848, -0.9877] n_targets:  3 reward:  156.13\n",
      "ALPHA (entropy-related):  tensor([0.1782], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.18618 0.18521 0.18413 0.18307 0.18222 0.18148 0.18063 0.17987 0.17898\n",
      " 0.17815]\n",
      "Episode: 121, Episode Reward: 5689.790236155191\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  293\n",
      "0.2 action:  [-0.1329, -0.9977] n_targets:  1 reward:  62.45\n",
      "0.6 action:  [-0.2032, -0.9891] n_targets:  1 reward:  56.29\n",
      "2.2 action:  [-0.0527, -0.9952] n_targets:  1 reward:  54.8\n",
      "4.2 action:  [-0.1186, -0.9983] n_targets:  1 reward:  50.1\n",
      "4.6 action:  [-0.0964, -0.9911] n_targets:  2 reward:  102.8\n",
      "8.0 action:  [0.1556, -0.9967] n_targets:  1 reward:  56.03\n",
      "8.6 action:  [0.0746, -0.9892] n_targets:  1 reward:  57.69\n",
      "8.8 action:  [0.1725, -0.9902] n_targets:  1 reward:  55.33\n",
      "10.2 action:  [-0.0346, -0.9922] n_targets:  3 reward:  178.6\n",
      "13.6 action:  [0.0049, -0.9892] n_targets:  1 reward:  54.89\n",
      "15.0 action:  [-0.3338, -0.9948] n_targets:  1 reward:  56.68\n",
      "16.8 action:  [-0.2074, -0.9914] n_targets:  2 reward:  111.03\n",
      "17.6 action:  [-0.147, -0.9984] n_targets:  1 reward:  57.27\n",
      "18.0 action:  [0.0957, -0.9923] n_targets:  1 reward:  50.91\n",
      "18.8 action:  [-0.1385, -0.9946] n_targets:  1 reward:  58.41\n",
      "19.0 action:  [-0.1811, -0.9979] n_targets:  1 reward:  54.63\n",
      "19.8 action:  [-0.0488, -0.9978] n_targets:  1 reward:  53.17\n",
      "20.2 action:  [0.075, -0.9995] n_targets:  1 reward:  54.02\n",
      "23.4 action:  [-0.0748, -0.9963] n_targets:  1 reward:  51.32\n",
      "25.8 action:  [-0.0245, -0.998] n_targets:  1 reward:  75.42\n",
      "26.4 action:  [-0.0269, -0.9959] n_targets:  1 reward:  58.08\n",
      "27.2 action:  [-0.3219, -0.9966] n_targets:  1 reward:  54.26\n",
      "27.4 action:  [-0.2605, -0.9986] n_targets:  1 reward:  59.29\n",
      "28.2 action:  [-0.123, -0.9925] n_targets:  1 reward:  57.26\n",
      "31.0 action:  [-0.3801, -0.9966] n_targets:  1 reward:  55.35\n",
      "32.4 action:  [-0.1853, -0.9863] n_targets:  1 reward:  54.46\n",
      "32.6 action:  [0.0357, -0.9973] n_targets:  1 reward:  58.04\n",
      "34.0 action:  [-0.2428, -0.9848] n_targets:  1 reward:  54.22\n",
      "34.2 action:  [-0.3276, -0.9893] n_targets:  2 reward:  111.28\n",
      "36.0 action:  [-0.0158, -0.9827] n_targets:  1 reward:  57.06\n",
      "37.4 action:  [-0.2835, -0.9865] n_targets:  1 reward:  53.86\n",
      "37.6 action:  [-0.1542, -0.9856] n_targets:  2 reward:  111.53\n",
      "38.6 action:  [-0.0967, -0.9971] n_targets:  1 reward:  54.04\n",
      "39.0 action:  [0.0827, -0.9949] n_targets:  1 reward:  52.37\n",
      "39.2 action:  [-0.2596, -0.9837] n_targets:  1 reward:  56.11\n",
      "41.4 action:  [-0.0951, -0.9929] n_targets:  1 reward:  55.77\n",
      "41.8 action:  [-0.0632, -0.9946] n_targets:  1 reward:  56.58\n",
      "42.6 action:  [0.0039, -0.9962] n_targets:  1 reward:  52.56\n",
      "43.2 action:  [-0.1686, -0.9949] n_targets:  1 reward:  52.74\n",
      "46.0 action:  [-0.2489, -0.9877] n_targets:  1 reward:  60.58\n",
      "46.2 action:  [-0.0367, -0.9887] n_targets:  1 reward:  52.67\n",
      "46.8 action:  [-0.4714, -0.9919] n_targets:  1 reward:  59.05\n",
      "47.4 action:  [0.0826, -0.9996] n_targets:  1 reward:  58.64\n",
      "49.0 action:  [-0.0918, -0.9904] n_targets:  1 reward:  50.39\n",
      "49.4 action:  [-0.1102, -0.9963] n_targets:  1 reward:  52.01\n",
      "49.8 action:  [-0.1499, -0.9963] n_targets:  1 reward:  56.65\n",
      "50.6 action:  [-0.3243, -0.9834] n_targets:  1 reward:  54.43\n",
      "52.4 action:  [-0.1969, -0.9953] n_targets:  1 reward:  64.75\n",
      "53.0 action:  [-0.0133, -0.9816] n_targets:  1 reward:  57.09\n",
      "55.0 action:  [-0.1688, -0.9807] n_targets:  1 reward:  52.99\n",
      "57.6 action:  [-0.159, -0.9972] n_targets:  1 reward:  52.72\n",
      "60.4 action:  [-0.4373, -0.9956] n_targets:  1 reward:  51.87\n",
      "61.2 action:  [-0.2382, -0.9878] n_targets:  1 reward:  53.5\n",
      "63.4 action:  [-0.0302, -0.9937] n_targets:  1 reward:  58.88\n",
      "64.0 action:  [-0.1033, -0.989] n_targets:  1 reward:  53.99\n",
      "64.2 action:  [-0.3476, -0.9804] n_targets:  1 reward:  52.23\n",
      "68.8 action:  [-0.0151, -0.9886] n_targets:  1 reward:  58.44\n",
      "69.6 action:  [-0.2184, -0.9985] n_targets:  1 reward:  50.94\n",
      "70.6 action:  [-0.0243, -0.994] n_targets:  1 reward:  52.51\n",
      "71.6 action:  [-0.3168, -0.9933] n_targets:  1 reward:  54.67\n",
      "72.8 action:  [-0.2315, -0.9958] n_targets:  1 reward:  57.73\n",
      "74.2 action:  [-0.0776, -0.9847] n_targets:  1 reward:  52.97\n",
      "76.4 action:  [0.0945, -0.9838] n_targets:  1 reward:  53.97\n",
      "77.4 action:  [0.0927, -0.9944] n_targets:  1 reward:  56.42\n",
      "78.2 action:  [0.1594, -0.9944] n_targets:  1 reward:  53.37\n",
      "82.4 action:  [-0.3207, -0.9949] n_targets:  1 reward:  60.83\n",
      "83.8 action:  [-0.0956, -0.9863] n_targets:  1 reward:  57.73\n",
      "84.0 action:  [-0.2296, -0.9936] n_targets:  1 reward:  53.73\n",
      "85.0 action:  [-0.052, -0.9962] n_targets:  1 reward:  51.05\n",
      "85.8 action:  [0.0775, -0.9937] n_targets:  1 reward:  53.25\n",
      "86.6 action:  [0.0957, -0.9854] n_targets:  1 reward:  71.69\n",
      "87.4 action:  [-0.1119, -0.9984] n_targets:  1 reward:  51.19\n",
      "87.6 action:  [-0.0481, -0.9979] n_targets:  2 reward:  107.95\n",
      "88.8 action:  [-0.2091, -0.9979] n_targets:  1 reward:  53.65\n",
      "91.0 action:  [-0.1497, -0.9988] n_targets:  1 reward:  54.49\n",
      "91.4 action:  [-0.2054, -0.9995] n_targets:  1 reward:  52.0\n",
      "92.4 action:  [0.2173, -0.9886] n_targets:  1 reward:  59.76\n",
      "93.2 action:  [0.3674, -0.994] n_targets:  3 reward:  215.09\n",
      "93.4 action:  [-0.0245, -0.9882] n_targets:  1 reward:  50.42\n",
      "93.6 action:  [-0.1362, -0.9984] n_targets:  1 reward:  52.28\n",
      "94.0 action:  [-0.0237, -0.9867] n_targets:  2 reward:  107.09\n",
      "94.8 action:  [-0.104, -0.9994] n_targets:  1 reward:  86.6\n",
      "95.0 action:  [-0.0191, -0.9964] n_targets:  1 reward:  51.18\n",
      "95.4 action:  [-0.5669, -0.9916] n_targets:  1 reward:  50.92\n",
      "98.2 action:  [0.1438, -0.9902] n_targets:  1 reward:  53.13\n",
      "98.4 action:  [0.3563, -0.995] n_targets:  1 reward:  53.9\n",
      "98.6 action:  [-0.1051, -0.9939] n_targets:  1 reward:  52.48\n",
      "98.8 action:  [0.0672, -0.9893] n_targets:  1 reward:  60.14\n",
      "ALPHA (entropy-related):  tensor([0.1776], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.18521 0.18413 0.18307 0.18222 0.18148 0.18063 0.17987 0.17898 0.17815\n",
      " 0.17757]\n",
      "Episode: 122, Episode Reward: 5510.704151153564\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  294\n",
      "0.2 action:  [-0.3107, -0.9965] n_targets:  1 reward:  69.17\n",
      "1.6 action:  [-0.0882, -0.9915] n_targets:  1 reward:  55.28\n",
      "2.6 action:  [-0.1776, -0.9935] n_targets:  1 reward:  54.36\n",
      "3.4 action:  [0.2569, -0.9962] n_targets:  2 reward:  118.35\n",
      "8.4 action:  [-0.1657, -0.9862] n_targets:  1 reward:  57.86\n",
      "8.8 action:  [-0.0347, -0.9942] n_targets:  1 reward:  53.17\n",
      "9.2 action:  [-0.086, -0.9983] n_targets:  1 reward:  56.47\n",
      "9.8 action:  [-0.1995, -0.996] n_targets:  1 reward:  54.34\n",
      "11.2 action:  [-0.3676, -0.9983] n_targets:  2 reward:  103.57\n",
      "14.6 action:  [-0.1394, -0.9972] n_targets:  1 reward:  51.44\n",
      "17.0 action:  [-0.0189, -0.9973] n_targets:  1 reward:  72.72\n",
      "17.4 action:  [-0.1768, -0.9971] n_targets:  1 reward:  57.28\n",
      "19.0 action:  [0.0571, -0.9959] n_targets:  1 reward:  56.39\n",
      "19.6 action:  [-0.4762, -0.9963] n_targets:  1 reward:  60.06\n",
      "20.2 action:  [0.1847, -0.9917] n_targets:  1 reward:  58.28\n",
      "20.6 action:  [0.0183, -0.9985] n_targets:  1 reward:  53.65\n",
      "21.6 action:  [-0.1608, -0.9989] n_targets:  2 reward:  104.76\n",
      "22.2 action:  [-0.2279, -0.9819] n_targets:  2 reward:  102.29\n",
      "23.6 action:  [-0.0777, -0.9966] n_targets:  1 reward:  62.36\n",
      "25.0 action:  [0.0704, -0.9947] n_targets:  1 reward:  51.25\n",
      "25.4 action:  [-0.3794, -0.9905] n_targets:  1 reward:  53.38\n",
      "27.0 action:  [-0.3066, -0.9945] n_targets:  1 reward:  51.88\n",
      "27.4 action:  [-0.0131, -0.9911] n_targets:  1 reward:  63.31\n",
      "28.0 action:  [-0.0822, -0.9902] n_targets:  2 reward:  114.62\n",
      "28.8 action:  [-0.1687, -0.9942] n_targets:  1 reward:  57.92\n",
      "29.0 action:  [-0.3877, -0.9982] n_targets:  1 reward:  52.74\n",
      "29.4 action:  [0.1717, -0.9943] n_targets:  1 reward:  56.36\n",
      "29.8 action:  [0.3228, -0.9954] n_targets:  2 reward:  103.63\n",
      "30.0 action:  [-0.3317, -0.9953] n_targets:  1 reward:  53.36\n",
      "33.0 action:  [-0.1868, -0.9956] n_targets:  1 reward:  50.12\n",
      "33.6 action:  [0.28, -0.9899] n_targets:  1 reward:  58.84\n",
      "34.4 action:  [0.2382, -0.9958] n_targets:  1 reward:  55.46\n",
      "34.6 action:  [-0.0057, -0.9888] n_targets:  1 reward:  50.34\n",
      "37.0 action:  [0.0064, -0.9905] n_targets:  1 reward:  74.23\n",
      "37.2 action:  [-0.2687, -0.9809] n_targets:  1 reward:  52.99\n",
      "37.6 action:  [0.0981, -0.9957] n_targets:  1 reward:  52.36\n",
      "38.2 action:  [0.3143, -0.9844] n_targets:  1 reward:  52.72\n",
      "38.4 action:  [-0.4156, -0.9849] n_targets:  1 reward:  50.23\n",
      "41.6 action:  [-0.3077, -0.9959] n_targets:  1 reward:  54.65\n",
      "44.8 action:  [-0.1351, -0.9856] n_targets:  1 reward:  50.58\n",
      "45.2 action:  [-0.175, -0.9953] n_targets:  1 reward:  55.78\n",
      "45.8 action:  [0.0319, -0.9933] n_targets:  1 reward:  50.98\n",
      "46.4 action:  [0.1005, -0.994] n_targets:  1 reward:  50.75\n",
      "47.2 action:  [-0.0649, -0.996] n_targets:  1 reward:  50.33\n",
      "50.6 action:  [0.0152, -0.9978] n_targets:  1 reward:  64.26\n",
      "50.8 action:  [-0.2035, -0.9949] n_targets:  1 reward:  61.3\n",
      "51.4 action:  [-0.1199, -0.9972] n_targets:  1 reward:  51.06\n",
      "51.6 action:  [-0.0545, -0.9994] n_targets:  1 reward:  60.43\n",
      "52.0 action:  [0.0139, -0.9973] n_targets:  1 reward:  51.1\n",
      "52.4 action:  [-0.0723, -0.9926] n_targets:  1 reward:  54.44\n",
      "54.2 action:  [-0.1611, -0.9975] n_targets:  1 reward:  54.29\n",
      "54.4 action:  [0.1506, -0.9974] n_targets:  1 reward:  50.24\n",
      "55.2 action:  [0.0192, -0.9944] n_targets:  2 reward:  107.03\n",
      "55.6 action:  [0.0308, -0.9893] n_targets:  1 reward:  57.06\n",
      "56.8 action:  [0.1242, -0.9982] n_targets:  1 reward:  53.0\n",
      "57.6 action:  [0.2347, -0.9941] n_targets:  1 reward:  53.69\n",
      "58.6 action:  [0.105, -0.9952] n_targets:  1 reward:  50.2\n",
      "59.4 action:  [-0.0963, -0.9967] n_targets:  1 reward:  51.86\n",
      "59.6 action:  [0.0028, -0.9977] n_targets:  1 reward:  55.99\n",
      "61.4 action:  [-0.052, -0.9971] n_targets:  1 reward:  50.03\n",
      "62.2 action:  [0.0164, -0.9984] n_targets:  1 reward:  53.61\n",
      "63.4 action:  [0.0574, -0.9922] n_targets:  1 reward:  67.16\n",
      "63.8 action:  [-0.1228, -0.9998] n_targets:  1 reward:  54.9\n",
      "64.4 action:  [0.274, -0.999] n_targets:  1 reward:  55.52\n",
      "64.8 action:  [-0.0167, -0.9978] n_targets:  1 reward:  53.76\n",
      "65.8 action:  [-0.0145, -0.997] n_targets:  1 reward:  50.99\n",
      "67.0 action:  [-0.1429, -0.9978] n_targets:  1 reward:  84.46\n",
      "67.8 action:  [0.1184, -0.9928] n_targets:  1 reward:  56.17\n",
      "69.2 action:  [0.1328, -0.9826] n_targets:  1 reward:  50.0\n",
      "69.8 action:  [-0.0873, -0.9955] n_targets:  1 reward:  56.99\n",
      "70.4 action:  [-0.165, -0.9972] n_targets:  1 reward:  52.15\n",
      "71.2 action:  [-0.0792, -0.9975] n_targets:  1 reward:  54.66\n",
      "71.4 action:  [0.2631, -0.9981] n_targets:  1 reward:  54.35\n",
      "72.2 action:  [-0.2954, -0.9912] n_targets:  1 reward:  56.16\n",
      "72.6 action:  [-0.1508, -0.9971] n_targets:  1 reward:  60.36\n",
      "74.8 action:  [0.0592, -0.9958] n_targets:  1 reward:  57.71\n",
      "80.9 action:  [-0.0622, -0.9996] n_targets:  1 reward:  51.08\n",
      "82.5 action:  [0.1852, -0.9941] n_targets:  1 reward:  60.76\n",
      "83.7 action:  [0.0509, -0.9832] n_targets:  1 reward:  55.44\n",
      "84.3 action:  [0.1571, -0.9949] n_targets:  1 reward:  52.12\n",
      "85.5 action:  [-0.2708, -0.9985] n_targets:  1 reward:  51.49\n",
      "86.7 action:  [0.0392, -0.9806] n_targets:  1 reward:  60.75\n",
      "91.9 action:  [-0.1055, -0.9975] n_targets:  1 reward:  51.26\n",
      "94.3 action:  [-0.0981, -0.9968] n_targets:  1 reward:  63.82\n",
      "95.7 action:  [-0.0419, -0.9994] n_targets:  1 reward:  50.47\n",
      "95.9 action:  [0.0463, -0.9911] n_targets:  1 reward:  53.61\n",
      "96.1 action:  [0.0145, -0.998] n_targets:  1 reward:  53.67\n",
      "96.7 action:  [-0.2026, -0.9936] n_targets:  1 reward:  51.01\n",
      "97.7 action:  [-0.0079, -0.9864] n_targets:  3 reward:  164.16\n",
      "98.1 action:  [-0.0762, -0.9978] n_targets:  1 reward:  58.9\n",
      "99.7 action:  [-0.0386, -0.9978] n_targets:  1 reward:  54.65\n",
      "100.3 action:  [-0.0346, -0.991] n_targets:  1 reward:  54.9\n",
      "101.1 action:  [0.059, -0.9945] n_targets:  1 reward:  59.79\n",
      "102.1 action:  [-0.1807, -0.9975] n_targets:  1 reward:  57.83\n",
      "ALPHA (entropy-related):  tensor([0.1772], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.18413 0.18307 0.18222 0.18148 0.18063 0.17987 0.17898 0.17815 0.17757\n",
      " 0.17721]\n",
      "Episode: 123, Episode Reward: 5721.255341847737\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  295\n",
      "0.1 action:  [-0.1152, -0.9975] n_targets:  2 reward:  118.05\n",
      "0.7 action:  [-0.4582, -0.9839] n_targets:  1 reward:  50.59\n",
      "1.1 action:  [-0.4056, -0.9975] n_targets:  1 reward:  58.32\n",
      "2.9 action:  [-0.2677, -0.9987] n_targets:  1 reward:  50.18\n",
      "3.1 action:  [-0.2478, -0.9968] n_targets:  1 reward:  50.81\n",
      "4.5 action:  [0.0741, -0.9838] n_targets:  1 reward:  52.24\n",
      "4.9 action:  [0.1204, -0.9896] n_targets:  1 reward:  50.35\n",
      "6.5 action:  [-0.292, -0.9909] n_targets:  2 reward:  100.93\n",
      "7.1 action:  [-0.0637, -0.9804] n_targets:  1 reward:  55.7\n",
      "7.7 action:  [-0.1502, -0.9967] n_targets:  1 reward:  50.35\n",
      "8.5 action:  [-0.2101, -0.9955] n_targets:  1 reward:  53.39\n",
      "9.5 action:  [-0.2867, -0.9989] n_targets:  2 reward:  118.91\n",
      "10.5 action:  [-0.1299, -0.9913] n_targets:  1 reward:  58.87\n",
      "10.9 action:  [-0.0495, -0.9954] n_targets:  2 reward:  115.08\n",
      "12.5 action:  [0.0684, -0.9827] n_targets:  1 reward:  51.81\n",
      "14.1 action:  [-0.0806, -0.9935] n_targets:  1 reward:  56.66\n",
      "14.9 action:  [-0.2347, -0.9905] n_targets:  1 reward:  51.6\n",
      "15.5 action:  [0.0871, -0.9984] n_targets:  1 reward:  58.98\n",
      "17.9 action:  [-0.1795, -0.9987] n_targets:  1 reward:  58.62\n",
      "20.1 action:  [0.0677, -0.9973] n_targets:  1 reward:  50.8\n",
      "20.5 action:  [-0.0608, -0.9965] n_targets:  1 reward:  54.41\n",
      "21.1 action:  [-0.2121, -0.9899] n_targets:  1 reward:  60.26\n",
      "21.3 action:  [0.0036, -0.9951] n_targets:  1 reward:  50.57\n",
      "21.7 action:  [-0.0003, -0.9963] n_targets:  1 reward:  61.79\n",
      "21.9 action:  [-0.2811, -0.989] n_targets:  1 reward:  55.66\n",
      "23.1 action:  [0.0973, -0.9943] n_targets:  1 reward:  51.06\n",
      "23.9 action:  [-0.186, -0.9932] n_targets:  2 reward:  112.16\n",
      "26.5 action:  [-0.2943, -0.9988] n_targets:  1 reward:  52.98\n",
      "26.9 action:  [-0.301, -0.9946] n_targets:  1 reward:  57.48\n",
      "29.3 action:  [0.0084, -0.9934] n_targets:  1 reward:  56.34\n",
      "30.3 action:  [-0.097, -0.9901] n_targets:  1 reward:  52.82\n",
      "32.1 action:  [0.0857, -0.9983] n_targets:  1 reward:  53.07\n",
      "32.9 action:  [-0.1125, -0.9991] n_targets:  1 reward:  56.18\n",
      "33.7 action:  [-0.0377, -0.9991] n_targets:  1 reward:  57.23\n",
      "33.9 action:  [-0.288, -0.9836] n_targets:  1 reward:  51.34\n",
      "34.3 action:  [-0.1511, -0.9893] n_targets:  1 reward:  56.44\n",
      "35.9 action:  [-0.1965, -0.9957] n_targets:  1 reward:  58.48\n",
      "38.1 action:  [-0.3567, -0.9994] n_targets:  1 reward:  51.67\n",
      "38.9 action:  [-0.0918, -0.9832] n_targets:  1 reward:  50.63\n",
      "40.5 action:  [0.0081, -0.9924] n_targets:  1 reward:  52.14\n",
      "40.9 action:  [-0.0922, -0.99] n_targets:  1 reward:  53.38\n",
      "41.5 action:  [-0.0427, -0.9984] n_targets:  1 reward:  61.06\n",
      "42.1 action:  [0.1151, -0.9945] n_targets:  2 reward:  110.98\n",
      "42.5 action:  [0.0886, -0.9935] n_targets:  2 reward:  121.09\n",
      "46.1 action:  [-0.1611, -0.9974] n_targets:  1 reward:  55.88\n",
      "46.7 action:  [-0.2615, -0.9959] n_targets:  1 reward:  59.99\n",
      "47.9 action:  [-0.078, -0.9972] n_targets:  1 reward:  52.48\n",
      "51.1 action:  [0.0247, -0.998] n_targets:  1 reward:  60.18\n",
      "53.3 action:  [-0.0221, -0.995] n_targets:  1 reward:  59.13\n",
      "53.7 action:  [0.0518, -0.9985] n_targets:  1 reward:  52.51\n",
      "54.3 action:  [-0.0798, -0.9978] n_targets:  1 reward:  52.72\n",
      "55.1 action:  [-0.1263, -0.9818] n_targets:  1 reward:  59.64\n",
      "56.5 action:  [0.1653, -0.9944] n_targets:  1 reward:  53.73\n",
      "58.1 action:  [0.0909, -0.9874] n_targets:  1 reward:  51.52\n",
      "58.5 action:  [0.1149, -0.9965] n_targets:  1 reward:  55.45\n",
      "60.1 action:  [-0.182, -0.9917] n_targets:  1 reward:  59.05\n",
      "60.5 action:  [-0.3026, -0.9957] n_targets:  1 reward:  60.42\n",
      "62.7 action:  [0.0267, -0.9983] n_targets:  1 reward:  82.85\n",
      "62.9 action:  [-0.051, -0.9966] n_targets:  1 reward:  53.63\n",
      "67.5 action:  [0.01, -0.9966] n_targets:  1 reward:  59.61\n",
      "68.7 action:  [-0.0059, -0.9941] n_targets:  2 reward:  107.38\n",
      "69.5 action:  [0.0177, -0.9902] n_targets:  2 reward:  117.85\n",
      "69.7 action:  [-0.0902, -0.9928] n_targets:  1 reward:  57.69\n",
      "69.9 action:  [0.0512, -0.9948] n_targets:  2 reward:  106.75\n",
      "70.1 action:  [0.0715, -0.996] n_targets:  1 reward:  52.38\n",
      "70.5 action:  [0.0099, -0.9978] n_targets:  1 reward:  53.52\n",
      "71.1 action:  [0.0615, -0.9953] n_targets:  1 reward:  51.09\n",
      "71.9 action:  [-0.1264, -0.9964] n_targets:  1 reward:  61.83\n",
      "74.5 action:  [-0.1017, -0.9989] n_targets:  1 reward:  56.37\n",
      "76.1 action:  [0.0095, -0.996] n_targets:  1 reward:  54.01\n",
      "76.9 action:  [-0.0305, -0.9916] n_targets:  1 reward:  52.43\n",
      "78.7 action:  [-0.0292, -0.9979] n_targets:  1 reward:  51.24\n",
      "78.9 action:  [0.1675, -0.9865] n_targets:  1 reward:  51.02\n",
      "80.1 action:  [0.1388, -0.9941] n_targets:  1 reward:  57.22\n",
      "81.1 action:  [-0.1242, -0.9903] n_targets:  1 reward:  56.22\n",
      "81.3 action:  [0.0159, -0.9966] n_targets:  1 reward:  56.3\n",
      "81.5 action:  [0.1388, -0.9961] n_targets:  1 reward:  59.62\n",
      "83.1 action:  [0.0795, -0.9949] n_targets:  1 reward:  52.6\n",
      "83.7 action:  [-0.2139, -0.9972] n_targets:  1 reward:  58.81\n",
      "84.9 action:  [0.0315, -0.9952] n_targets:  1 reward:  60.84\n",
      "86.3 action:  [-0.0777, -0.9978] n_targets:  1 reward:  52.16\n",
      "86.5 action:  [-0.0934, -0.9981] n_targets:  2 reward:  107.07\n",
      "88.3 action:  [0.0873, -0.9978] n_targets:  1 reward:  58.98\n",
      "88.5 action:  [-0.3042, -0.9947] n_targets:  1 reward:  55.21\n",
      "89.1 action:  [-0.0407, -0.9993] n_targets:  1 reward:  58.5\n",
      "90.9 action:  [-0.3092, -0.9976] n_targets:  1 reward:  53.48\n",
      "93.3 action:  [0.126, -0.996] n_targets:  1 reward:  56.81\n",
      "94.7 action:  [0.4104, -0.9971] n_targets:  1 reward:  50.07\n",
      "95.3 action:  [-0.0168, -0.9952] n_targets:  2 reward:  118.81\n",
      "95.7 action:  [-0.1579, -0.9885] n_targets:  1 reward:  53.12\n",
      "96.1 action:  [-0.151, -0.9829] n_targets:  1 reward:  54.89\n",
      "99.3 action:  [0.1628, -0.9902] n_targets:  1 reward:  55.23\n",
      "99.5 action:  [0.1492, -0.9884] n_targets:  1 reward:  60.09\n",
      "99.7 action:  [-0.1881, -0.9869] n_targets:  1 reward:  52.25\n",
      "100.5 action:  [0.0309, -0.9962] n_targets:  2 reward:  119.05\n",
      "100.9 action:  [-0.117, -0.9899] n_targets:  1 reward:  50.14\n",
      "101.3 action:  [-0.1515, -0.9877] n_targets:  2 reward:  107.1\n",
      "ALPHA (entropy-related):  tensor([0.1767], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.18307 0.18222 0.18148 0.18063 0.17987 0.17898 0.17815 0.17757 0.17721\n",
      " 0.17672]\n",
      "Episode: 124, Episode Reward: 6178.349976857502\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  296\n",
      "0.1 action:  [-0.1507, -0.998] n_targets:  2 reward:  144.25\n",
      "0.9 action:  [0.0225, -0.9969] n_targets:  2 reward:  103.07\n",
      "1.5 action:  [-0.0051, -0.9978] n_targets:  2 reward:  120.24\n",
      "5.1 action:  [-0.0121, -0.995] n_targets:  1 reward:  50.57\n",
      "7.7 action:  [-0.0457, -0.9928] n_targets:  2 reward:  135.97\n",
      "8.3 action:  [-0.1693, -0.9961] n_targets:  1 reward:  53.33\n",
      "8.7 action:  [-0.2487, -0.994] n_targets:  1 reward:  66.82\n",
      "10.5 action:  [0.0376, -0.9862] n_targets:  1 reward:  51.93\n",
      "15.3 action:  [0.0128, -0.9973] n_targets:  1 reward:  61.34\n",
      "15.7 action:  [0.0715, -0.9952] n_targets:  1 reward:  62.0\n",
      "16.7 action:  [0.0871, -0.9933] n_targets:  1 reward:  58.15\n",
      "18.1 action:  [-0.0408, -0.9972] n_targets:  1 reward:  54.28\n",
      "19.3 action:  [0.0046, -0.9983] n_targets:  2 reward:  104.33\n",
      "19.9 action:  [-0.1243, -0.9954] n_targets:  1 reward:  52.3\n",
      "20.9 action:  [-0.1038, -0.994] n_targets:  1 reward:  54.66\n",
      "21.3 action:  [-0.1526, -0.9946] n_targets:  2 reward:  108.2\n",
      "21.5 action:  [0.0606, -0.9911] n_targets:  1 reward:  58.89\n",
      "22.9 action:  [0.251, -0.9863] n_targets:  1 reward:  51.43\n",
      "23.3 action:  [-0.287, -0.9941] n_targets:  1 reward:  55.55\n",
      "23.7 action:  [-0.2078, -0.9912] n_targets:  1 reward:  58.15\n",
      "24.5 action:  [-0.1937, -0.9989] n_targets:  1 reward:  59.12\n",
      "25.3 action:  [-0.0528, -0.9855] n_targets:  1 reward:  59.22\n",
      "28.1 action:  [0.1297, -0.9955] n_targets:  1 reward:  51.06\n",
      "31.9 action:  [0.1027, -0.9886] n_targets:  1 reward:  51.0\n",
      "32.1 action:  [-0.0851, -0.9993] n_targets:  1 reward:  50.12\n",
      "35.3 action:  [0.0782, -0.9976] n_targets:  2 reward:  110.9\n",
      "35.7 action:  [-0.0119, -0.9896] n_targets:  1 reward:  53.0\n",
      "35.9 action:  [-0.1631, -0.9925] n_targets:  1 reward:  61.95\n",
      "36.1 action:  [0.0264, -0.9994] n_targets:  1 reward:  52.74\n",
      "36.7 action:  [-0.1273, -0.9949] n_targets:  1 reward:  64.5\n",
      "38.5 action:  [0.1228, -0.9942] n_targets:  1 reward:  51.44\n",
      "40.1 action:  [0.0504, -0.9975] n_targets:  1 reward:  54.58\n",
      "40.7 action:  [-0.1561, -0.996] n_targets:  1 reward:  61.22\n",
      "42.1 action:  [-0.3175, -0.9857] n_targets:  1 reward:  65.57\n",
      "43.9 action:  [-0.0081, -0.988] n_targets:  1 reward:  53.52\n",
      "44.5 action:  [-0.0192, -0.9871] n_targets:  1 reward:  52.5\n",
      "44.9 action:  [-0.0508, -0.9922] n_targets:  2 reward:  108.86\n",
      "45.3 action:  [0.001, -0.9938] n_targets:  1 reward:  60.01\n",
      "46.3 action:  [-0.0979, -0.9959] n_targets:  1 reward:  61.63\n",
      "46.9 action:  [0.0012, -0.9918] n_targets:  1 reward:  52.86\n",
      "48.7 action:  [-0.0271, -0.9893] n_targets:  1 reward:  56.91\n",
      "49.1 action:  [-0.3492, -0.9897] n_targets:  1 reward:  53.19\n",
      "49.9 action:  [0.1899, -0.9983] n_targets:  1 reward:  52.44\n",
      "50.1 action:  [-0.0101, -0.997] n_targets:  1 reward:  51.54\n",
      "51.5 action:  [0.1405, -0.9912] n_targets:  1 reward:  56.18\n",
      "56.9 action:  [-0.0489, -0.9962] n_targets:  1 reward:  55.49\n",
      "57.3 action:  [0.1694, -0.9831] n_targets:  1 reward:  56.62\n",
      "58.3 action:  [0.08, -0.9941] n_targets:  1 reward:  58.22\n",
      "59.3 action:  [0.0068, -0.9916] n_targets:  1 reward:  55.85\n",
      "59.9 action:  [-0.1299, -0.9927] n_targets:  1 reward:  50.34\n",
      "61.3 action:  [0.1252, -0.9926] n_targets:  1 reward:  56.05\n",
      "62.7 action:  [-0.1025, -0.9911] n_targets:  1 reward:  65.13\n",
      "62.9 action:  [0.0373, -0.9938] n_targets:  1 reward:  59.17\n",
      "63.3 action:  [0.2205, -0.9847] n_targets:  1 reward:  54.29\n",
      "64.1 action:  [-0.2841, -0.995] n_targets:  1 reward:  56.67\n",
      "64.7 action:  [-0.0728, -0.996] n_targets:  1 reward:  50.97\n",
      "64.9 action:  [0.0204, -0.9961] n_targets:  1 reward:  61.49\n",
      "66.1 action:  [-0.1053, -0.9821] n_targets:  1 reward:  52.2\n",
      "66.5 action:  [-0.1179, -0.9971] n_targets:  1 reward:  54.97\n",
      "67.3 action:  [-0.223, -0.9915] n_targets:  1 reward:  62.62\n",
      "68.1 action:  [-0.0673, -0.9954] n_targets:  1 reward:  53.24\n",
      "69.3 action:  [0.0596, -0.9988] n_targets:  1 reward:  55.4\n",
      "70.3 action:  [0.0117, -0.9945] n_targets:  1 reward:  54.26\n",
      "72.7 action:  [-0.2181, -0.9991] n_targets:  1 reward:  52.9\n",
      "74.5 action:  [0.2738, -0.9991] n_targets:  1 reward:  59.31\n",
      "74.9 action:  [0.1231, -0.9863] n_targets:  1 reward:  54.04\n",
      "75.1 action:  [-0.0717, -0.9916] n_targets:  1 reward:  50.6\n",
      "77.5 action:  [0.2267, -0.9835] n_targets:  1 reward:  54.04\n",
      "83.1 action:  [0.0085, -0.9933] n_targets:  1 reward:  58.6\n",
      "85.1 action:  [-0.1854, -0.9982] n_targets:  1 reward:  55.88\n",
      "86.1 action:  [0.1437, -0.994] n_targets:  1 reward:  53.92\n",
      "87.9 action:  [-0.0745, -0.997] n_targets:  1 reward:  51.0\n",
      "89.7 action:  [-0.1986, -0.9859] n_targets:  3 reward:  170.89\n",
      "90.3 action:  [-0.0542, -0.9963] n_targets:  1 reward:  60.31\n",
      "90.9 action:  [0.0998, -0.9964] n_targets:  1 reward:  57.21\n",
      "92.9 action:  [-0.1886, -0.9991] n_targets:  1 reward:  50.47\n",
      "93.5 action:  [0.0174, -0.9962] n_targets:  1 reward:  61.35\n",
      "94.7 action:  [-0.2021, -0.9935] n_targets:  1 reward:  58.31\n",
      "96.1 action:  [0.1447, -0.9992] n_targets:  1 reward:  59.6\n",
      "97.5 action:  [0.2073, -0.9965] n_targets:  1 reward:  50.94\n",
      "97.9 action:  [0.1354, -0.9805] n_targets:  1 reward:  52.46\n",
      "98.5 action:  [0.0669, -0.9957] n_targets:  1 reward:  62.28\n",
      "101.9 action:  [-0.1914, -0.9939] n_targets:  1 reward:  58.04\n",
      "ALPHA (entropy-related):  tensor([0.1763], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.18222 0.18148 0.18063 0.17987 0.17898 0.17815 0.17757 0.17721 0.17672\n",
      " 0.17633]\n",
      "Episode: 125, Episode Reward: 5256.646545410156\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  297\n",
      "1.8 action:  [-0.259, -0.9943] n_targets:  1 reward:  57.8\n",
      "3.2 action:  [-0.1318, -0.9921] n_targets:  1 reward:  58.57\n",
      "3.4 action:  [-0.1991, -0.9946] n_targets:  1 reward:  50.93\n",
      "4.2 action:  [-0.231, -0.9963] n_targets:  1 reward:  64.68\n",
      "6.2 action:  [-0.1376, -0.9976] n_targets:  1 reward:  56.39\n",
      "7.4 action:  [0.1504, -0.9991] n_targets:  1 reward:  55.44\n",
      "8.6 action:  [-0.0927, -0.9925] n_targets:  1 reward:  71.21\n",
      "10.2 action:  [-0.1588, -0.9864] n_targets:  1 reward:  51.38\n",
      "14.2 action:  [-0.0515, -0.9903] n_targets:  1 reward:  57.14\n",
      "14.6 action:  [0.0185, -0.9957] n_targets:  1 reward:  61.3\n",
      "17.0 action:  [-0.1878, -0.9867] n_targets:  1 reward:  57.95\n",
      "17.2 action:  [-0.1689, -0.9938] n_targets:  1 reward:  50.17\n",
      "17.4 action:  [0.1149, -0.9939] n_targets:  2 reward:  103.01\n",
      "17.8 action:  [0.0061, -0.9876] n_targets:  1 reward:  55.29\n",
      "18.6 action:  [-0.0293, -0.9988] n_targets:  1 reward:  52.77\n",
      "19.6 action:  [-0.258, -0.9959] n_targets:  1 reward:  51.05\n",
      "21.0 action:  [-0.098, -0.9952] n_targets:  1 reward:  58.22\n",
      "22.2 action:  [-0.1254, -0.9921] n_targets:  1 reward:  56.44\n",
      "23.6 action:  [-0.2683, -0.9975] n_targets:  1 reward:  57.0\n",
      "25.0 action:  [-0.2061, -0.991] n_targets:  1 reward:  60.9\n",
      "26.6 action:  [0.0459, -0.9925] n_targets:  1 reward:  63.05\n",
      "27.8 action:  [0.0056, -0.9987] n_targets:  2 reward:  110.14\n",
      "31.4 action:  [-0.2227, -0.9915] n_targets:  1 reward:  65.24\n",
      "31.8 action:  [-0.0591, -0.9986] n_targets:  1 reward:  54.39\n",
      "34.0 action:  [0.241, -0.9897] n_targets:  1 reward:  54.8\n",
      "35.0 action:  [-0.0696, -0.995] n_targets:  1 reward:  62.11\n",
      "36.2 action:  [0.3804, -0.9986] n_targets:  1 reward:  50.56\n",
      "37.2 action:  [0.1098, -0.9836] n_targets:  1 reward:  54.07\n",
      "38.0 action:  [0.1316, -0.9871] n_targets:  1 reward:  55.63\n",
      "40.2 action:  [-0.1408, -0.9921] n_targets:  1 reward:  62.51\n",
      "42.0 action:  [-0.1889, -0.9951] n_targets:  1 reward:  51.71\n",
      "43.2 action:  [0.0323, -0.9979] n_targets:  1 reward:  56.48\n",
      "43.6 action:  [0.0125, -0.997] n_targets:  1 reward:  57.45\n",
      "44.2 action:  [0.0364, -0.9946] n_targets:  1 reward:  56.39\n",
      "45.2 action:  [0.0161, -0.9814] n_targets:  1 reward:  51.16\n",
      "45.6 action:  [0.0976, -0.9971] n_targets:  1 reward:  57.13\n",
      "46.4 action:  [-0.0212, -0.9964] n_targets:  1 reward:  51.1\n",
      "46.6 action:  [0.2021, -0.997] n_targets:  1 reward:  50.41\n",
      "48.4 action:  [0.0365, -0.9954] n_targets:  1 reward:  55.1\n",
      "49.2 action:  [-0.0216, -0.9832] n_targets:  1 reward:  58.95\n",
      "52.2 action:  [-0.0226, -0.9913] n_targets:  1 reward:  77.84\n",
      "53.0 action:  [0.0675, -0.9954] n_targets:  1 reward:  58.52\n",
      "55.4 action:  [0.147, -0.9833] n_targets:  1 reward:  58.25\n",
      "56.6 action:  [-0.1256, -0.9954] n_targets:  2 reward:  126.54\n",
      "57.0 action:  [0.2841, -0.9916] n_targets:  1 reward:  71.94\n",
      "58.6 action:  [0.0295, -0.9949] n_targets:  1 reward:  55.23\n",
      "59.6 action:  [-0.0565, -0.9978] n_targets:  1 reward:  51.55\n",
      "60.2 action:  [0.0057, -0.9985] n_targets:  1 reward:  53.64\n",
      "60.6 action:  [0.2716, -0.9973] n_targets:  1 reward:  54.37\n",
      "61.8 action:  [-0.0053, -0.9968] n_targets:  1 reward:  54.26\n",
      "63.6 action:  [-0.0912, -0.9982] n_targets:  1 reward:  56.63\n",
      "64.0 action:  [-0.0428, -0.993] n_targets:  1 reward:  52.66\n",
      "66.2 action:  [0.1849, -0.9892] n_targets:  1 reward:  57.84\n",
      "68.2 action:  [0.0978, -0.9965] n_targets:  2 reward:  127.82\n",
      "69.6 action:  [0.0088, -0.9937] n_targets:  1 reward:  56.3\n",
      "70.0 action:  [0.58, -0.9955] n_targets:  1 reward:  70.78\n",
      "70.4 action:  [-0.1792, -0.9961] n_targets:  2 reward:  105.51\n",
      "71.8 action:  [-0.0212, -0.9956] n_targets:  1 reward:  74.94\n",
      "72.0 action:  [0.1085, -0.9986] n_targets:  1 reward:  56.08\n",
      "77.6 action:  [-0.1602, -0.9939] n_targets:  1 reward:  57.84\n",
      "79.0 action:  [-0.0585, -0.991] n_targets:  1 reward:  54.15\n",
      "81.2 action:  [0.0875, -0.9969] n_targets:  1 reward:  56.25\n",
      "81.4 action:  [0.0251, -0.9979] n_targets:  2 reward:  103.18\n",
      "81.6 action:  [-0.2406, -0.9988] n_targets:  1 reward:  52.0\n",
      "81.8 action:  [-0.2009, -0.9978] n_targets:  1 reward:  53.73\n",
      "84.0 action:  [0.1416, -0.9858] n_targets:  1 reward:  54.07\n",
      "85.2 action:  [-0.1402, -0.9909] n_targets:  1 reward:  51.74\n",
      "85.4 action:  [-0.0306, -0.9945] n_targets:  1 reward:  57.93\n",
      "87.0 action:  [0.1137, -0.9988] n_targets:  1 reward:  61.15\n",
      "87.2 action:  [-0.0056, -0.9859] n_targets:  1 reward:  53.18\n",
      "87.8 action:  [0.0252, -0.9969] n_targets:  2 reward:  108.45\n",
      "89.4 action:  [-0.1614, -0.9891] n_targets:  1 reward:  56.61\n",
      "94.0 action:  [0.0465, -0.9941] n_targets:  1 reward:  54.37\n",
      "ALPHA (entropy-related):  tensor([0.1761], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.18148 0.18063 0.17987 0.17898 0.17815 0.17757 0.17721 0.17672 0.17633\n",
      " 0.17611]\n",
      "Episode: 126, Episode Reward: 4561.3961842854815\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  298\n",
      "0.1 action:  [-0.2059, -0.9892] n_targets:  3 reward:  217.34\n",
      "1.5 action:  [-0.3359, -0.9943] n_targets:  1 reward:  54.76\n",
      "1.9 action:  [0.0568, -0.9953] n_targets:  1 reward:  59.3\n",
      "2.7 action:  [0.056, -0.9973] n_targets:  1 reward:  51.07\n",
      "2.9 action:  [-0.0222, -0.9976] n_targets:  1 reward:  50.95\n",
      "3.7 action:  [-0.0725, -0.9969] n_targets:  2 reward:  110.44\n",
      "4.1 action:  [0.0177, -0.9922] n_targets:  1 reward:  58.09\n",
      "5.1 action:  [-0.1815, -0.9931] n_targets:  1 reward:  55.02\n",
      "6.5 action:  [-0.1709, -0.9995] n_targets:  1 reward:  60.8\n",
      "7.1 action:  [-0.1945, -0.9982] n_targets:  1 reward:  54.38\n",
      "7.9 action:  [-0.051, -0.9922] n_targets:  1 reward:  57.63\n",
      "9.7 action:  [0.1253, -0.9849] n_targets:  1 reward:  56.84\n",
      "10.1 action:  [-0.0703, -0.9941] n_targets:  1 reward:  55.13\n",
      "10.9 action:  [0.0881, -0.9854] n_targets:  1 reward:  65.67\n",
      "11.3 action:  [0.0448, -0.9905] n_targets:  2 reward:  115.35\n",
      "12.7 action:  [-0.068, -0.9893] n_targets:  1 reward:  59.66\n",
      "13.9 action:  [-0.3567, -0.9989] n_targets:  1 reward:  59.62\n",
      "14.5 action:  [-0.0895, -0.9971] n_targets:  1 reward:  51.78\n",
      "16.9 action:  [-0.1495, -0.9913] n_targets:  1 reward:  53.1\n",
      "17.3 action:  [-0.1546, -0.9917] n_targets:  1 reward:  54.62\n",
      "21.5 action:  [-0.0218, -0.9964] n_targets:  1 reward:  51.44\n",
      "21.9 action:  [-0.168, -0.9933] n_targets:  1 reward:  55.83\n",
      "22.5 action:  [0.0617, -0.9953] n_targets:  1 reward:  56.61\n",
      "23.9 action:  [-0.104, -0.9974] n_targets:  1 reward:  54.94\n",
      "24.5 action:  [-0.3744, -0.9964] n_targets:  1 reward:  58.78\n",
      "24.9 action:  [0.043, -0.9907] n_targets:  1 reward:  61.33\n",
      "25.1 action:  [0.0203, -0.992] n_targets:  1 reward:  52.24\n",
      "29.9 action:  [0.0553, -0.9851] n_targets:  1 reward:  52.33\n",
      "30.7 action:  [0.0189, -0.9898] n_targets:  1 reward:  56.01\n",
      "30.9 action:  [-0.1389, -0.99] n_targets:  1 reward:  61.78\n",
      "32.7 action:  [0.0927, -0.9956] n_targets:  1 reward:  51.68\n",
      "32.9 action:  [-0.0688, -0.9946] n_targets:  1 reward:  54.43\n",
      "34.5 action:  [-0.3165, -0.9966] n_targets:  1 reward:  82.5\n",
      "34.9 action:  [0.0414, -0.9955] n_targets:  1 reward:  56.19\n",
      "35.1 action:  [-0.1541, -0.9948] n_targets:  1 reward:  58.68\n",
      "35.3 action:  [0.0009, -0.9981] n_targets:  1 reward:  60.52\n",
      "36.5 action:  [-0.2386, -0.9808] n_targets:  1 reward:  53.56\n",
      "36.9 action:  [0.0164, -0.9959] n_targets:  1 reward:  58.74\n",
      "37.1 action:  [-0.0621, -0.998] n_targets:  1 reward:  54.56\n",
      "37.7 action:  [-0.3331, -0.9951] n_targets:  1 reward:  53.96\n",
      "38.5 action:  [-0.1577, -0.9979] n_targets:  1 reward:  54.9\n",
      "40.7 action:  [-0.2958, -0.9974] n_targets:  1 reward:  60.89\n",
      "42.9 action:  [0.0332, -0.9985] n_targets:  1 reward:  54.81\n",
      "43.3 action:  [0.0596, -0.9939] n_targets:  1 reward:  56.71\n",
      "43.5 action:  [-0.0101, -0.991] n_targets:  1 reward:  60.27\n",
      "48.1 action:  [-0.1455, -0.9921] n_targets:  1 reward:  55.91\n",
      "49.3 action:  [-0.1315, -0.9961] n_targets:  1 reward:  50.74\n",
      "51.7 action:  [-0.1056, -0.9898] n_targets:  1 reward:  53.1\n",
      "52.1 action:  [0.0602, -0.9816] n_targets:  1 reward:  57.23\n",
      "53.1 action:  [0.09, -0.9943] n_targets:  1 reward:  58.44\n",
      "53.3 action:  [-0.1803, -0.9816] n_targets:  1 reward:  55.81\n",
      "53.5 action:  [-0.0313, -0.992] n_targets:  1 reward:  51.74\n",
      "55.3 action:  [-0.0665, -0.9974] n_targets:  1 reward:  55.92\n",
      "55.5 action:  [-0.1437, -0.9898] n_targets:  1 reward:  52.97\n",
      "55.9 action:  [-0.1133, -0.9856] n_targets:  1 reward:  55.17\n",
      "56.7 action:  [0.0553, -0.992] n_targets:  1 reward:  72.01\n",
      "58.1 action:  [-0.058, -0.9982] n_targets:  1 reward:  52.08\n",
      "58.3 action:  [0.1372, -0.9958] n_targets:  1 reward:  50.79\n",
      "60.7 action:  [-0.1274, -0.9854] n_targets:  1 reward:  56.5\n",
      "68.1 action:  [0.2471, -0.9894] n_targets:  1 reward:  56.33\n",
      "68.7 action:  [0.0775, -0.9946] n_targets:  3 reward:  159.41\n",
      "68.9 action:  [0.0327, -0.9847] n_targets:  1 reward:  55.06\n",
      "70.9 action:  [0.0019, -0.9912] n_targets:  1 reward:  51.15\n",
      "72.1 action:  [-0.0264, -0.9806] n_targets:  2 reward:  156.88\n",
      "73.5 action:  [-0.2002, -0.9962] n_targets:  1 reward:  57.44\n",
      "74.1 action:  [-0.1201, -0.9988] n_targets:  1 reward:  52.59\n",
      "74.5 action:  [0.0481, -0.9932] n_targets:  1 reward:  55.57\n",
      "76.9 action:  [-0.1119, -0.9986] n_targets:  1 reward:  54.98\n",
      "77.3 action:  [0.0006, -0.9962] n_targets:  1 reward:  56.31\n",
      "78.3 action:  [0.0653, -0.9939] n_targets:  2 reward:  108.81\n",
      "79.3 action:  [0.2833, -0.9838] n_targets:  1 reward:  56.38\n",
      "79.9 action:  [-0.154, -0.9906] n_targets:  1 reward:  50.94\n",
      "81.3 action:  [-0.3556, -0.9982] n_targets:  1 reward:  54.88\n",
      "82.1 action:  [-0.0039, -0.9964] n_targets:  1 reward:  51.35\n",
      "85.1 action:  [-0.1267, -0.9818] n_targets:  1 reward:  83.52\n",
      "86.9 action:  [-0.069, -0.9933] n_targets:  1 reward:  56.85\n",
      "89.1 action:  [-0.0601, -0.9982] n_targets:  1 reward:  56.5\n",
      "90.3 action:  [-0.1218, -0.9978] n_targets:  1 reward:  50.78\n",
      "91.1 action:  [-0.1126, -0.9964] n_targets:  1 reward:  50.25\n",
      "91.7 action:  [-0.4737, -0.9949] n_targets:  1 reward:  52.17\n",
      "92.5 action:  [-0.1772, -0.9957] n_targets:  1 reward:  55.78\n",
      "93.9 action:  [-0.2907, -0.994] n_targets:  1 reward:  50.01\n",
      "95.1 action:  [-0.1894, -0.997] n_targets:  1 reward:  52.72\n",
      "96.5 action:  [-0.2985, -0.9989] n_targets:  1 reward:  50.26\n",
      "97.5 action:  [0.0863, -0.9895] n_targets:  1 reward:  59.4\n",
      "98.1 action:  [-0.0995, -0.9954] n_targets:  1 reward:  50.12\n",
      "98.9 action:  [-0.0139, -0.9984] n_targets:  2 reward:  107.51\n",
      "100.7 action:  [-0.1938, -0.9967] n_targets:  1 reward:  53.65\n",
      "ALPHA (entropy-related):  tensor([0.1757], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.18063 0.17987 0.17898 0.17815 0.17757 0.17721 0.17672 0.17633 0.17611\n",
      " 0.17569]\n",
      "Episode: 127, Episode Reward: 5521.218747456868\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  299\n",
      "0.6 action:  [-0.1405, -0.9962] n_targets:  1 reward:  50.19\n",
      "4.4 action:  [-0.4577, -0.9944] n_targets:  1 reward:  54.34\n",
      "5.8 action:  [-0.0767, -0.9926] n_targets:  1 reward:  51.35\n",
      "6.0 action:  [-0.0743, -0.9943] n_targets:  2 reward:  104.09\n",
      "7.0 action:  [-0.0129, -0.9971] n_targets:  1 reward:  55.3\n",
      "8.4 action:  [-0.2011, -0.9947] n_targets:  1 reward:  62.37\n",
      "8.8 action:  [-0.1566, -0.9854] n_targets:  1 reward:  57.36\n",
      "10.0 action:  [-0.2435, -0.9959] n_targets:  1 reward:  62.16\n",
      "11.4 action:  [0.0708, -0.9928] n_targets:  1 reward:  58.24\n",
      "12.4 action:  [-0.1222, -0.9952] n_targets:  1 reward:  55.79\n",
      "12.6 action:  [-0.1521, -0.9986] n_targets:  1 reward:  55.49\n",
      "14.0 action:  [-0.0496, -0.9944] n_targets:  1 reward:  50.28\n",
      "15.0 action:  [-0.1032, -0.9932] n_targets:  1 reward:  74.1\n",
      "16.6 action:  [-0.2033, -0.9954] n_targets:  1 reward:  52.29\n",
      "17.6 action:  [-0.1069, -0.9953] n_targets:  1 reward:  52.63\n",
      "20.2 action:  [-0.3147, -0.9967] n_targets:  1 reward:  54.84\n",
      "20.8 action:  [-0.1007, -0.9972] n_targets:  1 reward:  55.7\n",
      "22.4 action:  [-0.1864, -0.9947] n_targets:  1 reward:  50.32\n",
      "24.0 action:  [-0.3665, -0.9971] n_targets:  1 reward:  54.17\n",
      "24.6 action:  [0.2455, -0.9958] n_targets:  1 reward:  52.01\n",
      "26.6 action:  [0.0205, -0.9822] n_targets:  1 reward:  55.87\n",
      "28.2 action:  [-0.0292, -0.9984] n_targets:  1 reward:  50.19\n",
      "29.6 action:  [-0.2153, -0.9897] n_targets:  1 reward:  56.95\n",
      "31.9 action:  [-0.1129, -0.996] n_targets:  1 reward:  52.73\n",
      "32.7 action:  [-0.0146, -0.9984] n_targets:  1 reward:  52.9\n",
      "33.5 action:  [-0.0697, -0.9966] n_targets:  1 reward:  51.41\n",
      "34.5 action:  [0.1614, -0.9855] n_targets:  1 reward:  59.25\n",
      "38.5 action:  [-0.0712, -0.9962] n_targets:  2 reward:  115.99\n",
      "39.9 action:  [-0.0851, -0.9921] n_targets:  1 reward:  61.55\n",
      "41.1 action:  [-0.044, -0.9893] n_targets:  2 reward:  113.4\n",
      "41.9 action:  [-0.1568, -0.9978] n_targets:  2 reward:  109.94\n",
      "42.1 action:  [-0.0475, -0.9883] n_targets:  1 reward:  51.37\n",
      "42.7 action:  [-0.0097, -0.9954] n_targets:  2 reward:  115.65\n",
      "42.9 action:  [-0.0726, -0.989] n_targets:  2 reward:  122.13\n",
      "43.3 action:  [0.3017, -0.9975] n_targets:  1 reward:  54.32\n",
      "44.1 action:  [-0.1438, -0.9921] n_targets:  1 reward:  51.43\n",
      "45.3 action:  [-0.0356, -0.994] n_targets:  3 reward:  156.34\n",
      "45.5 action:  [0.0055, -0.997] n_targets:  1 reward:  51.78\n",
      "46.1 action:  [0.015, -0.9955] n_targets:  1 reward:  56.34\n",
      "46.9 action:  [-0.0755, -0.992] n_targets:  1 reward:  57.38\n",
      "47.5 action:  [-0.1472, -0.9973] n_targets:  1 reward:  56.5\n",
      "47.7 action:  [0.0912, -0.9974] n_targets:  1 reward:  50.52\n",
      "48.9 action:  [-0.128, -0.9855] n_targets:  1 reward:  55.03\n",
      "49.3 action:  [-0.1055, -0.9981] n_targets:  1 reward:  54.22\n",
      "49.7 action:  [0.0905, -0.9964] n_targets:  1 reward:  55.28\n",
      "51.5 action:  [-0.0038, -0.9935] n_targets:  1 reward:  61.52\n",
      "51.7 action:  [-0.0887, -0.9968] n_targets:  1 reward:  55.62\n",
      "52.1 action:  [0.1858, -0.9903] n_targets:  1 reward:  59.34\n",
      "53.7 action:  [-0.055, -0.996] n_targets:  1 reward:  58.85\n",
      "55.3 action:  [-0.0407, -0.9965] n_targets:  1 reward:  54.72\n",
      "55.7 action:  [-0.1159, -0.9892] n_targets:  2 reward:  109.19\n",
      "57.3 action:  [-0.0425, -0.9991] n_targets:  1 reward:  53.54\n",
      "60.1 action:  [-0.1582, -0.9837] n_targets:  3 reward:  164.7\n",
      "61.3 action:  [-0.0062, -0.9895] n_targets:  1 reward:  57.49\n",
      "62.7 action:  [0.1412, -0.9921] n_targets:  1 reward:  61.25\n",
      "65.1 action:  [0.0823, -0.9984] n_targets:  1 reward:  52.61\n",
      "66.5 action:  [0.0192, -0.9807] n_targets:  1 reward:  53.54\n",
      "69.6 action:  [0.1504, -0.9986] n_targets:  1 reward:  54.38\n",
      "71.2 action:  [-0.2053, -0.995] n_targets:  1 reward:  52.69\n",
      "72.0 action:  [-0.3102, -0.9936] n_targets:  1 reward:  56.75\n",
      "72.6 action:  [-0.0248, -0.9986] n_targets:  1 reward:  55.05\n",
      "73.4 action:  [-0.188, -0.9956] n_targets:  1 reward:  59.41\n",
      "74.2 action:  [0.2858, -0.9912] n_targets:  1 reward:  65.03\n",
      "75.2 action:  [-0.2094, -0.9953] n_targets:  1 reward:  59.13\n",
      "76.0 action:  [0.296, -0.9934] n_targets:  2 reward:  106.38\n",
      "80.8 action:  [-0.1794, -0.9902] n_targets:  1 reward:  56.94\n",
      "82.2 action:  [0.3098, -0.9973] n_targets:  1 reward:  55.71\n",
      "86.8 action:  [0.2192, -0.9848] n_targets:  1 reward:  54.82\n",
      "88.8 action:  [0.0102, -0.9941] n_targets:  1 reward:  51.22\n",
      "89.0 action:  [0.2438, -0.998] n_targets:  1 reward:  54.42\n",
      "90.0 action:  [0.1807, -0.9987] n_targets:  1 reward:  50.67\n",
      "90.6 action:  [-0.1338, -0.9913] n_targets:  1 reward:  54.42\n",
      "91.6 action:  [0.1443, -0.9938] n_targets:  1 reward:  51.78\n",
      "92.0 action:  [0.3232, -0.9901] n_targets:  1 reward:  58.82\n",
      "92.6 action:  [-0.1709, -0.994] n_targets:  1 reward:  52.39\n",
      "93.0 action:  [-0.1233, -0.9894] n_targets:  1 reward:  50.53\n",
      "94.3 action:  [-0.2115, -0.9994] n_targets:  1 reward:  67.43\n",
      "96.7 action:  [-0.1132, -0.9876] n_targets:  1 reward:  54.39\n",
      "97.1 action:  [0.1855, -0.9975] n_targets:  1 reward:  51.12\n",
      "98.9 action:  [-0.0512, -0.9913] n_targets:  3 reward:  212.54\n",
      "101.7 action:  [0.1966, -0.9966] n_targets:  1 reward:  56.86\n",
      "ALPHA (entropy-related):  tensor([0.1751], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.17987 0.17898 0.17815 0.17757 0.17721 0.17672 0.17633 0.17611 0.17569\n",
      " 0.17513]\n",
      "Episode: 128, Episode Reward: 5316.767929077148\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  300\n",
      "0.4 action:  [-0.0264, -0.9927] n_targets:  3 reward:  189.12\n",
      "1.9 action:  [-0.1773, -0.9855] n_targets:  1 reward:  52.14\n",
      "3.5 action:  [0.128, -0.9884] n_targets:  1 reward:  50.27\n",
      "4.5 action:  [0.0577, -0.9948] n_targets:  1 reward:  52.88\n",
      "5.7 action:  [-0.2767, -0.997] n_targets:  2 reward:  106.41\n",
      "6.5 action:  [0.1948, -0.9954] n_targets:  1 reward:  53.93\n",
      "6.7 action:  [0.0313, -0.9827] n_targets:  1 reward:  57.86\n",
      "8.5 action:  [0.2297, -0.9927] n_targets:  1 reward:  57.89\n",
      "11.3 action:  [-0.0473, -0.9968] n_targets:  1 reward:  66.99\n",
      "12.7 action:  [0.1809, -0.9951] n_targets:  1 reward:  54.61\n",
      "14.9 action:  [0.3574, -0.9983] n_targets:  1 reward:  53.37\n",
      "15.1 action:  [0.1449, -0.9871] n_targets:  1 reward:  51.61\n",
      "15.5 action:  [-0.0173, -0.9858] n_targets:  1 reward:  53.75\n",
      "18.7 action:  [0.2176, -0.9952] n_targets:  1 reward:  50.01\n",
      "19.1 action:  [0.0052, -0.9814] n_targets:  1 reward:  56.92\n",
      "19.3 action:  [-0.1782, -0.9976] n_targets:  1 reward:  50.32\n",
      "22.7 action:  [-0.0773, -0.9914] n_targets:  1 reward:  53.83\n",
      "25.5 action:  [-0.0924, -0.9979] n_targets:  1 reward:  50.72\n",
      "26.3 action:  [0.2005, -0.9921] n_targets:  1 reward:  54.25\n",
      "27.5 action:  [-0.2692, -0.9907] n_targets:  1 reward:  51.33\n",
      "28.3 action:  [-0.0567, -0.9984] n_targets:  1 reward:  60.93\n",
      "28.9 action:  [-0.1574, -0.9994] n_targets:  1 reward:  60.98\n",
      "30.7 action:  [-0.1727, -0.9968] n_targets:  2 reward:  118.7\n",
      "32.9 action:  [-0.2179, -0.9809] n_targets:  1 reward:  57.93\n",
      "33.1 action:  [0.0335, -0.9994] n_targets:  1 reward:  53.71\n",
      "34.3 action:  [-0.0677, -0.987] n_targets:  1 reward:  89.59\n",
      "35.5 action:  [-0.1437, -0.9982] n_targets:  1 reward:  50.64\n",
      "36.5 action:  [-0.2517, -0.9926] n_targets:  1 reward:  53.48\n",
      "38.3 action:  [-0.0267, -0.9983] n_targets:  1 reward:  50.84\n",
      "39.3 action:  [-0.1699, -0.9975] n_targets:  1 reward:  59.62\n",
      "40.3 action:  [-0.063, -0.9938] n_targets:  1 reward:  57.97\n",
      "42.1 action:  [-0.1796, -0.9861] n_targets:  1 reward:  50.92\n",
      "42.5 action:  [-0.2357, -0.995] n_targets:  1 reward:  59.64\n",
      "43.9 action:  [-0.2437, -0.9952] n_targets:  1 reward:  50.12\n",
      "44.9 action:  [-0.1382, -0.9994] n_targets:  1 reward:  56.75\n",
      "46.9 action:  [-0.0195, -0.9981] n_targets:  1 reward:  54.93\n",
      "48.1 action:  [-0.1455, -0.9939] n_targets:  1 reward:  53.55\n",
      "49.3 action:  [-0.2561, -0.9983] n_targets:  1 reward:  54.49\n",
      "49.5 action:  [-0.1707, -0.9966] n_targets:  1 reward:  50.56\n",
      "49.7 action:  [0.1006, -0.9881] n_targets:  1 reward:  65.06\n",
      "50.9 action:  [0.2515, -0.9941] n_targets:  1 reward:  51.08\n",
      "52.1 action:  [0.2696, -0.9967] n_targets:  1 reward:  53.72\n",
      "53.3 action:  [-0.2531, -0.9909] n_targets:  1 reward:  52.35\n",
      "54.1 action:  [-0.0206, -0.9928] n_targets:  1 reward:  58.15\n",
      "54.3 action:  [-0.1326, -0.9972] n_targets:  1 reward:  51.63\n",
      "56.3 action:  [0.106, -0.9975] n_targets:  2 reward:  132.26\n",
      "57.5 action:  [-0.239, -0.9914] n_targets:  1 reward:  51.24\n",
      "58.3 action:  [-0.0205, -0.981] n_targets:  1 reward:  51.49\n",
      "58.9 action:  [-0.0716, -0.9938] n_targets:  2 reward:  107.67\n",
      "59.7 action:  [0.1737, -0.9964] n_targets:  1 reward:  51.02\n",
      "63.7 action:  [0.0525, -0.9957] n_targets:  1 reward:  55.15\n",
      "64.7 action:  [0.0208, -0.9981] n_targets:  1 reward:  64.45\n",
      "64.9 action:  [-0.0821, -0.987] n_targets:  2 reward:  107.77\n",
      "65.5 action:  [-0.1795, -0.9936] n_targets:  1 reward:  58.28\n",
      "69.9 action:  [0.0284, -0.9958] n_targets:  1 reward:  53.7\n",
      "70.7 action:  [-0.3183, -0.9935] n_targets:  1 reward:  57.6\n",
      "72.5 action:  [0.121, -0.9993] n_targets:  1 reward:  50.92\n",
      "73.1 action:  [-0.0739, -0.9987] n_targets:  1 reward:  50.2\n",
      "73.5 action:  [0.1756, -0.9887] n_targets:  1 reward:  56.69\n",
      "74.5 action:  [0.1698, -0.9995] n_targets:  1 reward:  54.4\n",
      "74.7 action:  [0.1209, -0.9927] n_targets:  2 reward:  110.32\n",
      "76.3 action:  [-0.1432, -0.9967] n_targets:  1 reward:  53.82\n",
      "76.7 action:  [-0.3248, -0.9884] n_targets:  1 reward:  57.9\n",
      "77.7 action:  [0.107, -0.989] n_targets:  1 reward:  57.42\n",
      "78.3 action:  [0.1144, -0.9903] n_targets:  1 reward:  51.35\n",
      "78.5 action:  [0.1756, -0.9971] n_targets:  1 reward:  52.71\n",
      "79.7 action:  [-0.0505, -0.9858] n_targets:  1 reward:  55.45\n",
      "81.0 action:  [-0.0974, -0.9908] n_targets:  3 reward:  166.67\n",
      "81.2 action:  [0.1945, -0.9884] n_targets:  2 reward:  142.31\n",
      "81.4 action:  [-0.1295, -0.9974] n_targets:  1 reward:  50.57\n",
      "82.6 action:  [-0.0477, -0.9959] n_targets:  1 reward:  55.37\n",
      "83.2 action:  [0.043, -0.9955] n_targets:  1 reward:  53.91\n",
      "84.6 action:  [-0.3596, -0.9958] n_targets:  1 reward:  55.35\n",
      "85.4 action:  [-0.4228, -0.9886] n_targets:  1 reward:  52.29\n",
      "86.6 action:  [-0.0976, -0.9867] n_targets:  1 reward:  51.64\n",
      "87.2 action:  [-0.1567, -0.9975] n_targets:  1 reward:  54.24\n",
      "87.4 action:  [0.05, -0.9895] n_targets:  1 reward:  76.96\n",
      "88.4 action:  [0.1675, -0.9953] n_targets:  1 reward:  51.95\n",
      "88.8 action:  [-0.274, -0.9871] n_targets:  1 reward:  50.55\n",
      "89.6 action:  [0.0682, -0.9903] n_targets:  1 reward:  55.52\n",
      "91.4 action:  [0.2335, -0.9888] n_targets:  1 reward:  55.58\n",
      "92.6 action:  [-0.0515, -0.9966] n_targets:  1 reward:  58.9\n",
      "94.0 action:  [0.1437, -0.9979] n_targets:  1 reward:  53.82\n",
      "94.4 action:  [0.1733, -0.9986] n_targets:  1 reward:  56.97\n",
      "94.6 action:  [-0.1566, -0.99] n_targets:  1 reward:  61.28\n",
      "94.8 action:  [-0.3038, -0.9972] n_targets:  1 reward:  50.85\n",
      "96.2 action:  [0.0177, -0.9866] n_targets:  1 reward:  51.12\n",
      "96.8 action:  [0.3007, -0.9941] n_targets:  1 reward:  58.23\n",
      "99.4 action:  [-0.2676, -0.9968] n_targets:  1 reward:  57.24\n",
      "101.2 action:  [0.2594, -0.9922] n_targets:  1 reward:  53.34\n",
      "ALPHA (entropy-related):  tensor([0.1745], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.17898 0.17815 0.17757 0.17721 0.17672 0.17633 0.17611 0.17569 0.17513\n",
      " 0.17452]\n",
      "Episode: 129, Episode Reward: 5662.045917510986\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  301\n",
      "0.2 action:  [-0.0573, -0.9963] n_targets:  3 reward:  191.93\n",
      "1.8 action:  [0.0417, -0.9878] n_targets:  1 reward:  57.02\n",
      "2.0 action:  [-0.1433, -0.9904] n_targets:  1 reward:  50.02\n",
      "3.8 action:  [-0.0905, -0.993] n_targets:  1 reward:  57.26\n",
      "4.0 action:  [-0.1307, -0.994] n_targets:  1 reward:  51.47\n",
      "5.4 action:  [-0.0443, -0.9884] n_targets:  1 reward:  50.99\n",
      "6.6 action:  [-0.088, -0.9966] n_targets:  1 reward:  56.4\n",
      "10.2 action:  [-0.1741, -0.9952] n_targets:  1 reward:  53.17\n",
      "11.2 action:  [0.003, -0.9966] n_targets:  1 reward:  53.96\n",
      "13.8 action:  [0.0983, -0.9986] n_targets:  1 reward:  54.83\n",
      "14.4 action:  [-0.0141, -0.9968] n_targets:  1 reward:  61.51\n",
      "15.0 action:  [-0.0843, -0.9923] n_targets:  1 reward:  54.91\n",
      "16.4 action:  [0.2619, -0.9806] n_targets:  2 reward:  117.97\n",
      "21.4 action:  [0.0029, -0.9974] n_targets:  1 reward:  57.88\n",
      "24.0 action:  [-0.1905, -0.9974] n_targets:  3 reward:  161.36\n",
      "25.2 action:  [0.0459, -0.9958] n_targets:  1 reward:  52.75\n",
      "25.6 action:  [-0.0543, -0.9956] n_targets:  1 reward:  50.33\n",
      "27.0 action:  [-0.0384, -0.9964] n_targets:  1 reward:  50.85\n",
      "27.2 action:  [-0.0413, -0.9978] n_targets:  1 reward:  53.31\n",
      "27.4 action:  [0.1738, -0.9986] n_targets:  2 reward:  108.43\n",
      "29.4 action:  [0.1399, -0.9989] n_targets:  1 reward:  54.05\n",
      "29.8 action:  [-0.1794, -0.9979] n_targets:  1 reward:  51.02\n",
      "30.6 action:  [-0.0905, -0.9925] n_targets:  1 reward:  87.87\n",
      "30.8 action:  [0.1779, -0.9962] n_targets:  1 reward:  58.88\n",
      "31.4 action:  [-0.042, -0.9981] n_targets:  1 reward:  50.26\n",
      "31.6 action:  [-0.25, -0.9912] n_targets:  1 reward:  52.23\n",
      "32.2 action:  [-0.0437, -0.9971] n_targets:  1 reward:  56.34\n",
      "33.2 action:  [-0.1028, -0.9968] n_targets:  1 reward:  56.01\n",
      "33.4 action:  [0.2922, -0.9828] n_targets:  1 reward:  53.95\n",
      "34.0 action:  [-0.2392, -0.9857] n_targets:  1 reward:  55.44\n",
      "35.6 action:  [-0.0902, -0.9928] n_targets:  1 reward:  60.85\n",
      "35.8 action:  [0.1046, -0.9928] n_targets:  1 reward:  59.61\n",
      "38.4 action:  [0.259, -0.9965] n_targets:  1 reward:  54.99\n",
      "39.4 action:  [0.1062, -0.9922] n_targets:  1 reward:  52.43\n",
      "42.8 action:  [0.1544, -0.999] n_targets:  2 reward:  113.06\n",
      "43.2 action:  [0.017, -0.9971] n_targets:  1 reward:  60.33\n",
      "43.4 action:  [0.016, -0.9985] n_targets:  1 reward:  51.39\n",
      "44.2 action:  [-0.2044, -0.9957] n_targets:  1 reward:  53.31\n",
      "44.4 action:  [0.1524, -0.9986] n_targets:  1 reward:  56.77\n",
      "45.6 action:  [-0.0106, -0.9923] n_targets:  1 reward:  50.89\n",
      "46.4 action:  [0.1585, -0.9949] n_targets:  1 reward:  56.54\n",
      "46.6 action:  [0.1744, -0.9995] n_targets:  1 reward:  56.26\n",
      "50.4 action:  [0.1022, -0.998] n_targets:  1 reward:  56.0\n",
      "50.6 action:  [-0.1229, -0.995] n_targets:  1 reward:  51.44\n",
      "51.0 action:  [-0.2396, -0.9983] n_targets:  1 reward:  52.71\n",
      "57.2 action:  [-0.0923, -0.9958] n_targets:  1 reward:  52.75\n",
      "58.8 action:  [-0.1632, -0.9919] n_targets:  1 reward:  50.09\n",
      "59.6 action:  [-0.0684, -0.991] n_targets:  1 reward:  53.29\n",
      "59.8 action:  [0.0742, -0.9992] n_targets:  1 reward:  53.1\n",
      "61.8 action:  [-0.175, -0.9937] n_targets:  1 reward:  72.35\n",
      "63.6 action:  [0.0019, -0.9944] n_targets:  1 reward:  54.22\n",
      "68.0 action:  [-0.1383, -0.9992] n_targets:  1 reward:  56.06\n",
      "68.2 action:  [-0.1991, -0.9851] n_targets:  1 reward:  57.22\n",
      "73.0 action:  [0.0766, -0.9876] n_targets:  1 reward:  61.75\n",
      "73.4 action:  [-0.1962, -0.9907] n_targets:  1 reward:  56.7\n",
      "74.8 action:  [0.012, -0.9864] n_targets:  1 reward:  60.52\n",
      "75.8 action:  [-0.0823, -0.998] n_targets:  2 reward:  106.8\n",
      "76.6 action:  [-0.3863, -0.9943] n_targets:  1 reward:  56.6\n",
      "76.8 action:  [-0.2693, -0.9971] n_targets:  1 reward:  52.27\n",
      "77.2 action:  [-0.0975, -0.9952] n_targets:  1 reward:  55.45\n",
      "77.4 action:  [0.1832, -0.9936] n_targets:  1 reward:  53.46\n",
      "77.8 action:  [0.1672, -0.9985] n_targets:  2 reward:  117.67\n",
      "79.2 action:  [0.0593, -0.9938] n_targets:  1 reward:  57.41\n",
      "79.8 action:  [-0.142, -0.982] n_targets:  2 reward:  110.49\n",
      "80.0 action:  [-0.0931, -0.9973] n_targets:  1 reward:  55.87\n",
      "81.2 action:  [-0.2063, -0.9985] n_targets:  1 reward:  50.25\n",
      "82.0 action:  [-0.2342, -0.9948] n_targets:  1 reward:  70.97\n",
      "83.0 action:  [-0.4145, -0.9834] n_targets:  1 reward:  70.46\n",
      "91.6 action:  [-0.1518, -0.9979] n_targets:  2 reward:  110.65\n",
      "92.0 action:  [0.0157, -0.9915] n_targets:  1 reward:  56.37\n",
      "92.4 action:  [-0.1342, -0.9937] n_targets:  1 reward:  50.63\n",
      "92.6 action:  [-0.2837, -0.9978] n_targets:  1 reward:  53.88\n",
      "94.6 action:  [-0.4548, -0.9885] n_targets:  1 reward:  56.38\n",
      "95.6 action:  [-0.4973, -0.9971] n_targets:  1 reward:  61.76\n",
      "96.0 action:  [-0.0442, -0.9984] n_targets:  2 reward:  119.12\n",
      "96.8 action:  [-0.1254, -0.9839] n_targets:  1 reward:  54.56\n",
      "97.8 action:  [-0.1245, -0.9973] n_targets:  2 reward:  117.17\n",
      "98.6 action:  [-0.3188, -0.9988] n_targets:  1 reward:  52.27\n",
      "ALPHA (entropy-related):  tensor([0.1739], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.17815 0.17757 0.17721 0.17672 0.17633 0.17611 0.17569 0.17513 0.17452\n",
      " 0.17392]\n",
      "Episode: 130, Episode Reward: 5127.540528615315\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  302\n",
      "0.2 action:  [0.0808, -0.9923] n_targets:  2 reward:  141.51\n",
      "1.0 action:  [0.2867, -0.9978] n_targets:  1 reward:  50.14\n",
      "2.8 action:  [0.0933, -0.9952] n_targets:  1 reward:  54.35\n",
      "3.0 action:  [-0.2715, -0.9976] n_targets:  1 reward:  66.35\n",
      "3.2 action:  [-0.3213, -0.9874] n_targets:  1 reward:  50.55\n",
      "6.4 action:  [-0.1704, -0.9918] n_targets:  1 reward:  55.78\n",
      "7.2 action:  [0.138, -0.9912] n_targets:  1 reward:  59.07\n",
      "7.4 action:  [-0.0219, -0.9844] n_targets:  1 reward:  53.29\n",
      "8.4 action:  [-0.1852, -0.9889] n_targets:  1 reward:  54.36\n",
      "10.0 action:  [0.1352, -0.9824] n_targets:  1 reward:  51.17\n",
      "10.2 action:  [0.0301, -0.9979] n_targets:  1 reward:  54.54\n",
      "10.6 action:  [0.2913, -0.997] n_targets:  1 reward:  50.84\n",
      "11.0 action:  [0.1419, -0.9987] n_targets:  1 reward:  52.42\n",
      "11.8 action:  [-0.1839, -0.9875] n_targets:  2 reward:  152.04\n",
      "12.2 action:  [-0.0118, -0.9871] n_targets:  1 reward:  52.61\n",
      "12.4 action:  [0.1601, -0.9934] n_targets:  1 reward:  53.87\n",
      "12.6 action:  [-0.1286, -0.9935] n_targets:  1 reward:  51.85\n",
      "13.6 action:  [-0.1919, -0.9861] n_targets:  1 reward:  52.41\n",
      "16.6 action:  [0.0166, -0.9932] n_targets:  1 reward:  60.67\n",
      "18.2 action:  [-0.0722, -0.998] n_targets:  1 reward:  54.39\n",
      "20.4 action:  [-0.1356, -0.9984] n_targets:  2 reward:  127.0\n",
      "21.0 action:  [0.2077, -0.987] n_targets:  1 reward:  54.22\n",
      "21.6 action:  [-0.0543, -0.9982] n_targets:  1 reward:  60.82\n",
      "22.0 action:  [-0.1813, -0.993] n_targets:  1 reward:  52.41\n",
      "24.2 action:  [-0.0745, -0.9866] n_targets:  1 reward:  61.65\n",
      "28.8 action:  [-0.241, -0.9978] n_targets:  1 reward:  50.75\n",
      "29.8 action:  [-0.2271, -0.9922] n_targets:  1 reward:  56.79\n",
      "32.4 action:  [-0.0777, -0.9943] n_targets:  1 reward:  50.3\n",
      "33.2 action:  [-0.4319, -0.9898] n_targets:  2 reward:  154.38\n",
      "33.8 action:  [-0.0131, -0.9985] n_targets:  1 reward:  58.99\n",
      "34.2 action:  [0.0696, -0.9942] n_targets:  1 reward:  56.44\n",
      "38.2 action:  [0.4327, -0.9989] n_targets:  1 reward:  52.17\n",
      "39.0 action:  [0.1359, -0.9928] n_targets:  1 reward:  66.34\n",
      "41.0 action:  [-0.0379, -0.9842] n_targets:  1 reward:  61.28\n",
      "41.4 action:  [-0.0232, -0.9997] n_targets:  1 reward:  56.51\n",
      "43.4 action:  [-0.3937, -0.9968] n_targets:  1 reward:  52.77\n",
      "44.5 action:  [-0.0287, -0.999] n_targets:  1 reward:  58.28\n",
      "44.7 action:  [0.0681, -0.995] n_targets:  1 reward:  54.89\n",
      "45.3 action:  [-0.0157, -0.9972] n_targets:  1 reward:  53.98\n",
      "46.7 action:  [0.1475, -0.995] n_targets:  1 reward:  62.18\n",
      "48.7 action:  [-0.1547, -0.9928] n_targets:  1 reward:  51.26\n",
      "49.5 action:  [0.4022, -0.9983] n_targets:  1 reward:  54.0\n",
      "51.3 action:  [-0.2345, -0.9939] n_targets:  1 reward:  52.95\n",
      "51.7 action:  [0.0661, -0.9986] n_targets:  1 reward:  56.99\n",
      "51.9 action:  [-0.1926, -0.9977] n_targets:  1 reward:  50.36\n",
      "52.3 action:  [-0.3528, -0.9993] n_targets:  1 reward:  59.56\n",
      "52.5 action:  [-0.3323, -0.9971] n_targets:  1 reward:  55.08\n",
      "52.7 action:  [-0.3495, -0.9932] n_targets:  1 reward:  59.54\n",
      "52.9 action:  [-0.0504, -0.9993] n_targets:  1 reward:  51.78\n",
      "53.9 action:  [-0.0606, -0.9976] n_targets:  1 reward:  51.41\n",
      "55.1 action:  [0.0163, -0.9943] n_targets:  1 reward:  60.64\n",
      "55.3 action:  [-0.3854, -0.9921] n_targets:  1 reward:  51.52\n",
      "55.5 action:  [-0.1327, -0.9895] n_targets:  1 reward:  50.53\n",
      "55.9 action:  [0.0752, -0.997] n_targets:  1 reward:  57.77\n",
      "56.5 action:  [-0.1203, -0.9955] n_targets:  1 reward:  68.51\n",
      "58.5 action:  [-0.133, -0.9984] n_targets:  1 reward:  58.64\n",
      "58.7 action:  [0.0676, -0.9959] n_targets:  1 reward:  59.51\n",
      "59.5 action:  [-0.0663, -0.9966] n_targets:  3 reward:  162.44\n",
      "60.3 action:  [0.0165, -0.9988] n_targets:  1 reward:  57.84\n",
      "61.3 action:  [-0.2402, -0.9853] n_targets:  1 reward:  54.19\n",
      "61.9 action:  [-0.1281, -0.9934] n_targets:  1 reward:  54.96\n",
      "65.7 action:  [-0.0865, -0.9954] n_targets:  1 reward:  57.2\n",
      "66.3 action:  [-0.1097, -0.9988] n_targets:  1 reward:  54.61\n",
      "67.1 action:  [0.1488, -0.9955] n_targets:  1 reward:  53.76\n",
      "67.3 action:  [-0.0814, -0.9982] n_targets:  1 reward:  58.89\n",
      "69.5 action:  [-0.2303, -0.9932] n_targets:  3 reward:  200.07\n",
      "70.1 action:  [-0.1658, -0.9972] n_targets:  1 reward:  59.89\n",
      "70.7 action:  [-0.2385, -0.9971] n_targets:  1 reward:  54.24\n",
      "71.5 action:  [0.0134, -0.9964] n_targets:  1 reward:  53.24\n",
      "71.7 action:  [-0.2234, -0.9963] n_targets:  1 reward:  55.19\n",
      "73.9 action:  [-0.0782, -0.9971] n_targets:  1 reward:  51.21\n",
      "74.9 action:  [0.0103, -0.9978] n_targets:  1 reward:  51.56\n",
      "76.1 action:  [0.2688, -0.9985] n_targets:  1 reward:  55.73\n",
      "76.9 action:  [0.0373, -0.9888] n_targets:  1 reward:  52.81\n",
      "77.7 action:  [-0.3073, -0.9954] n_targets:  1 reward:  52.11\n",
      "78.3 action:  [-0.2342, -0.9952] n_targets:  1 reward:  59.85\n",
      "79.9 action:  [-0.2244, -0.9909] n_targets:  1 reward:  52.43\n",
      "80.5 action:  [-0.1556, -0.9885] n_targets:  1 reward:  50.37\n",
      "80.7 action:  [-0.1923, -0.9865] n_targets:  2 reward:  106.17\n",
      "81.3 action:  [-0.3608, -0.9974] n_targets:  1 reward:  50.51\n",
      "82.5 action:  [-0.2573, -0.994] n_targets:  1 reward:  50.64\n",
      "82.9 action:  [0.0852, -0.9884] n_targets:  1 reward:  52.52\n",
      "83.7 action:  [-0.2745, -0.9968] n_targets:  2 reward:  146.74\n",
      "85.9 action:  [-0.1575, -0.9973] n_targets:  1 reward:  50.61\n",
      "87.7 action:  [0.0159, -0.9963] n_targets:  2 reward:  106.65\n",
      "88.5 action:  [-0.115, -0.991] n_targets:  1 reward:  50.2\n",
      "88.9 action:  [-0.0837, -0.9915] n_targets:  1 reward:  54.31\n",
      "89.5 action:  [0.1202, -0.9981] n_targets:  1 reward:  61.64\n",
      "89.9 action:  [-0.0437, -0.9977] n_targets:  1 reward:  50.57\n",
      "90.5 action:  [0.1531, -0.9952] n_targets:  1 reward:  50.13\n",
      "91.9 action:  [0.1553, -0.9978] n_targets:  1 reward:  58.38\n",
      "92.5 action:  [-0.0764, -0.9916] n_targets:  1 reward:  58.11\n",
      "94.3 action:  [0.0326, -0.9904] n_targets:  1 reward:  51.52\n",
      "94.9 action:  [0.0068, -0.997] n_targets:  1 reward:  55.73\n",
      "96.7 action:  [-0.149, -0.9976] n_targets:  1 reward:  51.18\n",
      "97.3 action:  [-0.0117, -0.9954] n_targets:  2 reward:  115.11\n",
      "97.7 action:  [0.0869, -0.998] n_targets:  2 reward:  111.05\n",
      "98.5 action:  [0.2219, -0.9927] n_targets:  2 reward:  104.99\n",
      "99.1 action:  [-0.1172, -0.9873] n_targets:  1 reward:  57.99\n",
      "ALPHA (entropy-related):  tensor([0.1733], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.17757 0.17721 0.17672 0.17633 0.17611 0.17569 0.17513 0.17452 0.17392\n",
      " 0.17326]\n",
      "Episode: 131, Episode Reward: 6421.776842753092\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  303\n",
      "0.1 action:  [-0.1332, -0.9913] n_targets:  2 reward:  123.31\n",
      "0.7 action:  [-0.2311, -0.9961] n_targets:  2 reward:  114.87\n",
      "1.9 action:  [-0.0294, -0.9929] n_targets:  1 reward:  51.4\n",
      "4.5 action:  [-0.2008, -0.9855] n_targets:  1 reward:  57.72\n",
      "4.7 action:  [-0.1605, -0.9872] n_targets:  1 reward:  50.41\n",
      "5.5 action:  [-0.0695, -0.9922] n_targets:  1 reward:  53.81\n",
      "7.1 action:  [-0.2624, -0.9922] n_targets:  1 reward:  50.45\n",
      "8.5 action:  [-0.2825, -0.9982] n_targets:  2 reward:  109.62\n",
      "9.1 action:  [-0.1163, -0.9991] n_targets:  3 reward:  169.33\n",
      "9.7 action:  [0.127, -0.9986] n_targets:  1 reward:  58.88\n",
      "9.9 action:  [-0.195, -0.9894] n_targets:  1 reward:  57.42\n",
      "10.1 action:  [-0.1618, -0.9903] n_targets:  1 reward:  58.16\n",
      "10.3 action:  [0.1187, -0.9943] n_targets:  1 reward:  50.5\n",
      "10.9 action:  [0.1659, -0.9914] n_targets:  1 reward:  53.99\n",
      "12.5 action:  [0.1969, -0.9983] n_targets:  1 reward:  57.03\n",
      "14.7 action:  [-0.1067, -0.9952] n_targets:  2 reward:  115.32\n",
      "15.9 action:  [0.1307, -0.9969] n_targets:  1 reward:  54.73\n",
      "16.7 action:  [0.142, -0.9888] n_targets:  1 reward:  73.96\n",
      "19.0 action:  [-0.1004, -0.9991] n_targets:  1 reward:  54.23\n",
      "19.6 action:  [-0.0109, -0.9897] n_targets:  1 reward:  61.07\n",
      "20.6 action:  [0.1686, -0.9875] n_targets:  1 reward:  52.93\n",
      "20.8 action:  [0.0889, -0.9979] n_targets:  1 reward:  57.19\n",
      "21.2 action:  [-0.0413, -0.9961] n_targets:  1 reward:  68.48\n",
      "22.4 action:  [0.1421, -0.9942] n_targets:  1 reward:  58.54\n",
      "22.8 action:  [0.3103, -0.9974] n_targets:  2 reward:  110.65\n",
      "23.6 action:  [0.1696, -0.9869] n_targets:  1 reward:  58.57\n",
      "24.8 action:  [0.1403, -0.9832] n_targets:  1 reward:  57.62\n",
      "25.6 action:  [0.0705, -0.9976] n_targets:  1 reward:  68.85\n",
      "26.2 action:  [-0.0314, -0.9984] n_targets:  1 reward:  56.8\n",
      "27.8 action:  [0.3411, -0.9913] n_targets:  1 reward:  50.97\n",
      "28.0 action:  [-0.0955, -0.9977] n_targets:  1 reward:  51.36\n",
      "28.8 action:  [-0.2791, -0.9989] n_targets:  2 reward:  109.33\n",
      "29.0 action:  [-0.0399, -0.9905] n_targets:  1 reward:  51.93\n",
      "29.4 action:  [0.2539, -0.9948] n_targets:  1 reward:  57.95\n",
      "32.6 action:  [-0.315, -0.9902] n_targets:  1 reward:  55.38\n",
      "34.6 action:  [0.0418, -0.9989] n_targets:  1 reward:  52.52\n",
      "38.8 action:  [0.096, -0.9947] n_targets:  1 reward:  53.28\n",
      "39.6 action:  [-0.0433, -0.9911] n_targets:  2 reward:  109.75\n",
      "40.4 action:  [-0.0575, -0.9946] n_targets:  1 reward:  66.07\n",
      "42.8 action:  [-0.2149, -0.9984] n_targets:  1 reward:  54.15\n",
      "46.0 action:  [-0.0805, -0.9918] n_targets:  1 reward:  54.51\n",
      "46.6 action:  [-0.1292, -0.9861] n_targets:  1 reward:  55.83\n",
      "47.4 action:  [0.0003, -0.9971] n_targets:  1 reward:  52.96\n",
      "52.0 action:  [0.0796, -0.9989] n_targets:  1 reward:  59.12\n",
      "53.4 action:  [-0.1243, -0.9974] n_targets:  1 reward:  59.79\n",
      "54.0 action:  [-0.1851, -0.9943] n_targets:  1 reward:  56.84\n",
      "54.4 action:  [-0.025, -0.9866] n_targets:  1 reward:  52.47\n",
      "55.2 action:  [-0.0928, -0.9862] n_targets:  1 reward:  58.78\n",
      "57.0 action:  [0.0009, -0.9984] n_targets:  1 reward:  51.18\n",
      "58.2 action:  [-0.2929, -0.9967] n_targets:  1 reward:  68.54\n",
      "64.6 action:  [-0.3917, -0.9935] n_targets:  1 reward:  54.92\n",
      "66.8 action:  [0.0209, -0.9814] n_targets:  2 reward:  107.1\n",
      "68.8 action:  [0.0059, -0.9922] n_targets:  1 reward:  51.84\n",
      "69.6 action:  [0.0039, -0.9878] n_targets:  1 reward:  51.58\n",
      "70.8 action:  [-0.2569, -0.9887] n_targets:  1 reward:  67.07\n",
      "71.2 action:  [-0.2926, -0.999] n_targets:  1 reward:  60.65\n",
      "71.6 action:  [-0.1555, -0.9973] n_targets:  1 reward:  56.51\n",
      "71.8 action:  [-0.207, -0.9943] n_targets:  1 reward:  55.69\n",
      "73.6 action:  [0.0958, -0.999] n_targets:  2 reward:  117.53\n",
      "73.8 action:  [-0.0285, -0.9983] n_targets:  1 reward:  51.84\n",
      "75.0 action:  [-0.1113, -0.9968] n_targets:  2 reward:  109.2\n",
      "75.2 action:  [0.0082, -0.9971] n_targets:  1 reward:  57.75\n",
      "76.8 action:  [0.0312, -0.9861] n_targets:  2 reward:  106.36\n",
      "79.0 action:  [0.1766, -0.9987] n_targets:  3 reward:  191.9\n",
      "79.2 action:  [-0.2497, -0.9962] n_targets:  1 reward:  56.54\n",
      "79.8 action:  [-0.1602, -0.9884] n_targets:  1 reward:  57.86\n",
      "82.2 action:  [-0.1254, -0.9972] n_targets:  1 reward:  51.37\n",
      "82.8 action:  [-0.0866, -0.9996] n_targets:  1 reward:  61.53\n",
      "83.0 action:  [-0.0995, -0.9918] n_targets:  1 reward:  52.31\n",
      "83.8 action:  [-0.0486, -0.9925] n_targets:  1 reward:  52.37\n",
      "84.0 action:  [-0.4234, -0.9985] n_targets:  1 reward:  54.87\n",
      "84.6 action:  [0.1564, -0.9978] n_targets:  1 reward:  51.3\n",
      "84.8 action:  [-0.0081, -0.9891] n_targets:  1 reward:  57.28\n",
      "86.2 action:  [-0.2791, -0.9989] n_targets:  1 reward:  50.91\n",
      "86.4 action:  [-0.2283, -0.9858] n_targets:  1 reward:  56.53\n",
      "90.8 action:  [-0.1228, -0.9811] n_targets:  1 reward:  50.81\n",
      "91.4 action:  [-0.0344, -0.9951] n_targets:  1 reward:  55.15\n",
      "92.6 action:  [0.1274, -0.9936] n_targets:  1 reward:  50.96\n",
      "92.8 action:  [-0.0054, -0.993] n_targets:  1 reward:  54.47\n",
      "93.2 action:  [-0.0044, -0.9986] n_targets:  1 reward:  59.1\n",
      "94.0 action:  [-0.1554, -0.9963] n_targets:  1 reward:  59.24\n",
      "94.2 action:  [-0.1665, -0.9965] n_targets:  1 reward:  52.62\n",
      "94.4 action:  [-0.241, -0.998] n_targets:  1 reward:  51.85\n",
      "94.6 action:  [-0.0019, -0.9981] n_targets:  1 reward:  51.52\n",
      "95.0 action:  [-0.3565, -0.9911] n_targets:  1 reward:  52.19\n",
      "95.2 action:  [-0.2654, -0.9991] n_targets:  1 reward:  58.91\n",
      "95.8 action:  [-0.0571, -0.9819] n_targets:  2 reward:  113.01\n",
      "96.2 action:  [-0.0558, -0.9993] n_targets:  1 reward:  51.72\n",
      "96.4 action:  [-0.0635, -0.9887] n_targets:  1 reward:  60.02\n",
      "98.0 action:  [-0.0663, -0.9946] n_targets:  1 reward:  55.87\n",
      "99.0 action:  [-0.0847, -0.9975] n_targets:  1 reward:  51.11\n",
      "ALPHA (entropy-related):  tensor([0.1727], grad_fn=<ExpBackward0>)\n",
      "Last 10 ALPHA: [0.17721 0.17672 0.17633 0.17611 0.17569 0.17513 0.17452 0.17392 0.17326\n",
      " 0.17274]\n",
      "Episode: 132, Episode Reward: 6019.93835957845\n",
      "============================================================\n",
      "TIME before resetting: 102.39999999999846\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.64\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  2\n",
      "current flash_on_interval:  3\n",
      "current num_obs_ff:  5\n",
      "current reward_boundary:  75\n",
      "current max_in_memory_time:  1\n",
      "\n",
      " episode:  304\n",
      "0.1 action:  [-0.0835, -0.9952] n_targets:  2 reward:  155.04\n",
      "0.3 action:  [-0.1275, -0.9905] n_targets:  1 reward:  51.94\n",
      "4.2 action:  [0.1386, -0.9841] n_targets:  1 reward:  50.72\n",
      "4.6 action:  [-0.0104, -0.9941] n_targets:  1 reward:  68.4\n",
      "5.4 action:  [-0.2377, -0.9949] n_targets:  1 reward:  50.96\n",
      "6.4 action:  [-0.2195, -0.9996] n_targets:  1 reward:  59.74\n",
      "8.6 action:  [0.0289, -0.9899] n_targets:  1 reward:  51.27\n",
      "9.2 action:  [-0.2198, -0.9945] n_targets:  1 reward:  50.55\n",
      "16.0 action:  [-0.1438, -0.9906] n_targets:  2 reward:  108.08\n",
      "16.4 action:  [-0.0606, -0.992] n_targets:  1 reward:  52.62\n",
      "20.8 action:  [-0.2051, -0.994] n_targets:  1 reward:  54.94\n",
      "21.2 action:  [-0.2401, -0.9921] n_targets:  1 reward:  54.24\n",
      "21.8 action:  [-0.2119, -0.9949] n_targets:  1 reward:  52.08\n",
      "22.0 action:  [-0.1878, -0.9942] n_targets:  1 reward:  55.74\n",
      "22.4 action:  [-0.0896, -0.9867] n_targets:  1 reward:  64.43\n",
      "22.8 action:  [0.1081, -0.9967] n_targets:  1 reward:  55.24\n",
      "24.0 action:  [0.0354, -0.993] n_targets:  1 reward:  56.38\n",
      "25.2 action:  [0.0094, -0.9976] n_targets:  1 reward:  50.73\n",
      "26.8 action:  [0.1943, -0.9868] n_targets:  4 reward:  257.12\n",
      "27.0 action:  [-0.0465, -0.9901] n_targets:  2 reward:  114.39\n",
      "28.2 action:  [-0.3604, -0.9949] n_targets:  1 reward:  53.4\n",
      "28.4 action:  [0.0911, -0.9934] n_targets:  1 reward:  51.1\n",
      "28.6 action:  [-0.3068, -0.9845] n_targets:  1 reward:  50.41\n",
      "28.8 action:  [-0.2464, -0.9853] n_targets:  1 reward:  51.92\n",
      "31.6 action:  [0.0026, -0.9947] n_targets:  1 reward:  50.1\n",
      "32.0 action:  [0.0994, -0.9973] n_targets:  1 reward:  53.55\n",
      "32.2 action:  [-0.0958, -0.9933] n_targets:  1 reward:  59.95\n",
      "33.8 action:  [0.0587, -0.9968] n_targets:  1 reward:  55.32\n",
      "34.8 action:  [-0.1845, -0.9832] n_targets:  1 reward:  52.2\n",
      "35.4 action:  [0.1882, -0.9896] n_targets:  1 reward:  55.22\n",
      "35.8 action:  [-0.1617, -0.9992] n_targets:  1 reward:  51.82\n",
      "37.6 action:  [-0.2553, -0.9934] n_targets:  1 reward:  59.3\n",
      "40.4 action:  [0.0554, -0.9955] n_targets:  1 reward:  53.24\n",
      "45.4 action:  [0.1421, -0.9902] n_targets:  1 reward:  50.48\n",
      "45.6 action:  [-0.2131, -0.9957] n_targets:  1 reward:  55.8\n",
      "46.6 action:  [-0.0667, -0.9949] n_targets:  1 reward:  54.18\n",
      "48.8 action:  [0.0217, -0.9853] n_targets:  1 reward:  51.57\n",
      "49.0 action:  [-0.0396, -0.9956] n_targets:  1 reward:  51.46\n",
      "49.8 action:  [-0.0506, -0.9979] n_targets:  1 reward:  55.64\n",
      "51.0 action:  [-0.1423, -0.9987] n_targets:  1 reward:  50.52\n",
      "51.6 action:  [-0.0411, -0.9879] n_targets:  1 reward:  51.4\n",
      "52.6 action:  [0.0373, -0.9917] n_targets:  1 reward:  50.73\n",
      "52.8 action:  [-0.0507, -0.9977] n_targets:  1 reward:  53.5\n",
      "53.6 action:  [-0.3369, -0.9986] n_targets:  1 reward:  52.22\n",
      "53.8 action:  [0.1488, -0.9982] n_targets:  1 reward:  54.99\n",
      "58.6 action:  [0.2262, -0.9988] n_targets:  1 reward:  54.82\n",
      "59.0 action:  [-0.0972, -0.9921] n_targets:  1 reward:  62.16\n",
      "59.8 action:  [-0.1958, -0.9933] n_targets:  1 reward:  54.1\n",
      "60.6 action:  [0.1119, -0.9959] n_targets:  2 reward:  107.52\n",
      "61.2 action:  [-0.3174, -0.9964] n_targets:  3 reward:  189.84\n",
      "61.4 action:  [-0.116, -0.9947] n_targets:  1 reward:  57.21\n",
      "62.4 action:  [0.0289, -0.9985] n_targets:  1 reward:  56.19\n",
      "66.0 action:  [-0.2143, -0.9975] n_targets:  1 reward:  56.93\n",
      "66.4 action:  [0.1333, -0.9893] n_targets:  1 reward:  51.55\n",
      "67.4 action:  [0.0022, -0.9929] n_targets:  1 reward:  53.92\n",
      "68.0 action:  [0.0809, -0.9995] n_targets:  1 reward:  69.07\n",
      "69.6 action:  [-0.0741, -0.9888] n_targets:  1 reward:  59.31\n",
      "69.8 action:  [-0.2799, -0.994] n_targets:  1 reward:  56.15\n",
      "71.6 action:  [-0.2426, -0.9989] n_targets:  1 reward:  55.68\n",
      "73.2 action:  [-0.0178, -0.9921] n_targets:  1 reward:  52.32\n",
      "74.0 action:  [-0.281, -0.9995] n_targets:  2 reward:  143.15\n",
      "75.2 action:  [-0.0262, -0.995] n_targets:  1 reward:  59.22\n",
      "75.4 action:  [-0.2508, -0.9948] n_targets:  2 reward:  122.2\n",
      "75.6 action:  [-0.3608, -0.999] n_targets:  1 reward:  57.65\n",
      "77.2 action:  [-0.2053, -0.9825] n_targets:  1 reward:  59.66\n",
      "78.0 action:  [-0.1631, -0.9918] n_targets:  1 reward:  57.91\n",
      "78.2 action:  [-0.0587, -0.9931] n_targets:  1 reward:  51.9\n",
      "84.2 action:  [-0.2331, -0.9972] n_targets:  1 reward:  55.85\n",
      "85.2 action:  [-0.1589, -0.9967] n_targets:  2 reward:  119.27\n",
      "87.2 action:  [0.0454, -0.9893] n_targets:  1 reward:  54.03\n"
     ]
    }
   ],
   "source": [
    "lm.streamline_everything(currentTrial_for_animation=None, num_trials_for_animation=None, duration=[10, 40],\n",
    "                         model_exists_ok=False,\n",
    "                         best_model_postcurriculum_exists_ok=True,\n",
    "                         to_train_agent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b30ef4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: failed to retrieve env params. Will use the env params passed in. Error message: [Errno 2] No such file or directory: 'RL_models/LSTM_stored_models/all_agents/oct_13/dv0_dw0_w0_memT1/env_params.txt'\n",
      "Collecting new agent data......\n",
      "TIME before resetting: 0\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  1\n",
      "current dt:  0.2\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  1\n",
      "2.8 action:  [-0.5419, -0.9946] n_targets:  1 reward:  100\n",
      "8.4 action:  [-0.5411, -0.9947] n_targets:  1 reward:  100\n",
      "10.4 action:  [-0.5429, -0.9947] n_targets:  2 reward:  200\n",
      "23.2 action:  [-0.5406, -0.9945] n_targets:  1 reward:  100\n",
      "47.6 action:  [-0.5345, -0.9944] n_targets:  1 reward:  100\n",
      "58.4 action:  [-0.5339, -0.9945] n_targets:  1 reward:  100\n",
      "175.6 action:  [-0.5412, -0.9946] n_targets:  1 reward:  100\n",
      "2025-10-13 13:41:59,451 - INFO - Firefly capture rate: 0.0350\n",
      "Warnings: currently, only ff in obs at each step are used in ff_dataframe. All ff are labeled 'visible' regardless of their actual time since last visible.\n",
      "It is possible that the LSTM agent has the memory of ff in the past, but the code needs to be modified to reflect that. For planning analysis, info of in-memory ff is not needed.\n",
      "made ff_dataframe\n",
      "Number of frames is: 148\n",
      "Number of frames for the animation is: 148\n",
      "Saving animation as: RL_models/LSTM_stored_models/all_collected_data/processed_data/oct_13/dv0_dw0_w0_memT1/individual_data_sessions/data_0/no_cost__2-4_rate_1.04.mp4\n",
      "2025-10-13 13:42:00,296 - INFO - Animation.save using <class 'matplotlib.animation.FFMpegWriter'>\n",
      "2025-10-13 13:42:00,297 - INFO - MovieWriter._run: running command: ffmpeg -f rawvideo -vcodec rawvideo -s 640x480 -pix_fmt rgba -framerate 4 -loglevel error -i pipe: -vcodec h264 -pix_fmt yuv420p -y RL_models/LSTM_stored_models/all_collected_data/processed_data/oct_13/dv0_dw0_w0_memT1/individual_data_sessions/data_0/no_cost__2-4_rate_1.04.mp4\n",
      "Animation is saved at: RL_models/LSTM_stored_models/all_collected_data/processed_data/oct_13/dv0_dw0_w0_memT1/individual_data_sessions/data_0/no_cost__2-4_rate_1.04.mp4\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm.streamline_making_animation(currentTrial_for_animation=None, num_trials_for_animation=None, duration=[10, 40], n_steps=1000, video_dir=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34b1b4e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video controls  >\n",
       " <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\">\n",
       " Your browser does not support the video tag.\n",
       " </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "Video(lm.video_path_name, embed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d203ba7c",
   "metadata": {},
   "source": [
    "## see animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dacb0bd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'machine_learning.RL.SB3.rl_for_multiff_class' from '/user_data/cicid/Multifirefly-Project/multiff_analysis/multiff_code/methods/machine_learning/RL/SB3/rl_for_multiff_class.py'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(rl_for_multiff_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c58f858",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cicid/miniconda3/envs/multiff_clean/lib/python3.11/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\n",
      "/user_data/cicid/Multifirefly-Project/multiff_analysis/multiff_code/methods/machine_learning/RL/lstm/LSTM_functions.py:551: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.soft_q_net1.load_state_dict(torch.load(\n",
      "/user_data/cicid/Multifirefly-Project/multiff_analysis/multiff_code/methods/machine_learning/RL/lstm/LSTM_functions.py:553: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.soft_q_net2.load_state_dict(torch.load(\n",
      "/user_data/cicid/Multifirefly-Project/multiff_analysis/multiff_code/methods/machine_learning/RL/lstm/LSTM_functions.py:555: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.policy_net.load_state_dict(torch.load(\n",
      "/user_data/cicid/Multifirefly-Project/multiff_analysis/multiff_code/methods/machine_learning/RL/lstm/lstm_for_multiff_class.py:253: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return lambda b: torch.load(io.BytesIO(b), map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soft Q Network (1,2):  QNetworkLSTM2(\n",
      "  (linear1): Linear(in_features=49, out_features=128, bias=True)\n",
      "  (lstm1): LSTM(128, 128, dropout=0.2)\n",
      "  (linear2): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "Policy Network:  SAC_PolicyNetworkLSTM(\n",
      "  (linear1): Linear(in_features=45, out_features=128, bias=True)\n",
      "  (linear2): Linear(in_features=47, out_features=128, bias=True)\n",
      "  (lstm1): LSTM(128, 128, dropout=0.2)\n",
      "  (linear3): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (mean_linear): Linear(in_features=128, out_features=2, bias=True)\n",
      "  (log_std_linear): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n",
      "Soft Q Network (1,2):  QNetworkLSTM2(\n",
      "  (linear1): Linear(in_features=49, out_features=128, bias=True)\n",
      "  (lstm1): LSTM(128, 128, dropout=0.2)\n",
      "  (linear2): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "Policy Network:  SAC_PolicyNetworkLSTM(\n",
      "  (linear1): Linear(in_features=45, out_features=128, bias=True)\n",
      "  (linear2): Linear(in_features=47, out_features=128, bias=True)\n",
      "  (lstm1): LSTM(128, 128, dropout=0.2)\n",
      "  (linear3): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (mean_linear): Linear(in_features=128, out_features=2, bias=True)\n",
      "  (log_std_linear): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n",
      "length of replay buffer: 0\n",
      "Warning: failed to retrieve env params. Will use the env params passed in. Error message: [Errno 2] No such file or directory: 'RL_models/LSTM_stored_models/all_agents/oct_12/dv0_dw0_w0_memT1/env_params.txt'\n",
      "Collecting new agent data......\n",
      "Removed all files in the folder: RL_models/LSTM_stored_models/all_collected_data/processed_data/oct_12/dv0_dw0_w0_memT1/individual_data_sessions/data_0\n",
      "TIME before resetting: 0\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  1\n",
      "current dt:  0.2\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  1\n",
      "2025-10-13 12:50:39,101 - INFO - Firefly capture rate: 0.0000\n",
      "Warnings: currently, only ff in obs at each step are used in ff_dataframe. All ff are labeled 'visible' regardless of their actual time since last visible.\n",
      "It is possible that the LSTM agent has the memory of ff in the past, but the code needs to be modified to reflect that. For planning analysis, info of in-memory ff is not needed.\n",
      "made ff_dataframe\n",
      "No closest stop to capture found\n",
      "CurrentTrial and num_trials are set to be None because of the following error: index 0 is out of bounds for axis 0 with size 0\n",
      "Number of frames is: 148\n",
      "Number of frames for the animation is: 148\n",
      "Saving animation as: RL_models/LSTM_stored_models/all_collected_data/processed_data/oct_12/dv0_dw0_w0_memT1/individual_data_sessions/data_0/dv0_dw0_w0_memT1__10s_to_40s_rate_1.0.mp4\n",
      "2025-10-13 12:50:39,463 - INFO - Animation.save using <class 'matplotlib.animation.FFMpegWriter'>\n",
      "2025-10-13 12:50:39,464 - INFO - MovieWriter._run: running command: ffmpeg -f rawvideo -vcodec rawvideo -s 640x480 -pix_fmt rgba -framerate 5 -loglevel error -i pipe: -vcodec h264 -pix_fmt yuv420p -y RL_models/LSTM_stored_models/all_collected_data/processed_data/oct_12/dv0_dw0_w0_memT1/individual_data_sessions/data_0/dv0_dw0_w0_memT1__10s_to_40s_rate_1.0.mp4\n",
      "Animation is saved at: RL_models/LSTM_stored_models/all_collected_data/processed_data/oct_12/dv0_dw0_w0_memT1/individual_data_sessions/data_0/dv0_dw0_w0_memT1__10s_to_40s_rate_1.0.mp4\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm.make_env(**lm.env_kwargs)\n",
    "lm.make_agent()\n",
    "# lm.make_initial_env_for_curriculum_training()\n",
    "lm.load_best_model_postcurriculum()\n",
    "lm.streamline_making_animation(currentTrial_for_animation=None, num_trials_for_animation=None, duration=[10, 40], n_steps=1000, video_dir=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8aaec4be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.ff_caught_T_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7f9c1d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video controls  >\n",
       " <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\">\n",
       " Your browser does not support the video tag.\n",
       " </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "Video(lm.video_path_name, embed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbeca84c",
   "metadata": {},
   "source": [
    "# class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1f89f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # retrieve old model\n",
    "# env_kwargs = {'num_obs_ff': 2,\n",
    "#               'add_action_to_obs': True,\n",
    "#               'angular_terminal_vel': 1,\n",
    "#               \"reward_per_ff\": 100,\n",
    "#           \n",
    "#               \"dv_cost_factor\": 0,\n",
    "#               \"dw_cost_factor\": 0,\n",
    "#               \"w_cost_factor\": 0,\n",
    "#               \"dt\": 0.25,\n",
    "#               \"flash_on_interval\": 0.3\n",
    "#             }\n",
    "\n",
    "\n",
    "# lm = lstm_for_multiff_class.LSTMforMultifirefly(overall_folder='RL_models/LSTM_stored_models/all_agents/gen_-1/',\n",
    "#                                                 model_folder_name='RL_models/LSTM_stored_models/all_agents/gen_-1/LSTM_Jan_2',\n",
    "#                                                 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a56b5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env_kwargs = {'num_obs_ff': 3,\n",
    "              'add_action_to_obs': True,\n",
    "              'angular_terminal_vel': 1,\n",
    "              \"reward_per_ff\": 80,\n",
    "          \n",
    "              \"dv_cost_factor\": 0,\n",
    "              \"dw_cost_factor\": 0,\n",
    "              \"w_cost_factor\": 0,\n",
    "              \"dt\": 0.25,\n",
    "              \"flash_on_interval\": 0.3\n",
    "            }   \n",
    "lm = lstm_for_multiff_class.LSTMforMultifirefly(overall_folder='RL_models/LSTM_stored_models/all_agents/gen_1_3ff/',\n",
    "                                                **env_kwargs)\n",
    "\n",
    "                                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0abce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lm = lstm_for_multiff_class.LSTMforMultifirefly()\n",
    "\n",
    "env_kwargs = {\n",
    "              'num_obs_ff': 10,\n",
    "              'add_action_to_obs': True,\n",
    "              'angular_terminal_vel': 1,\n",
    "              \"dt\": 0.25,\n",
    "              \"flash_on_interval\": 0.3,\n",
    "          \n",
    "              \"dv_cost_factor\": 0,\n",
    "              \"dw_cost_factor\": 0,\n",
    "              \"w_cost_factor\": 0,\n",
    "            }   \n",
    "\n",
    "lm = lstm_for_multiff_class.LSTMforMultifirefly(overall_folder='RL_models/LSTM_stored_models/all_agents/gen_8_env2/',\n",
    "                                                \n",
    "                                                **env_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c3f071",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.make_env(**env_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe77f492",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.make_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eab7131",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.train_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b67996e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.save_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ec1db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.load_agent(load_replay_buffer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac06133",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.test_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d97fcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.monkey_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c954f3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.ff_dataframe['time']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b573d59",
   "metadata": {},
   "source": [
    "## animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172ebfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curriculum_training kwargs\n",
    "env_kwargs = {\n",
    "              \n",
    "              'angular_terminal_vel': 1,\n",
    "              \"reward_per_ff\": 100,\n",
    "          \n",
    "              \"dv_cost_factor\": 0,\n",
    "              \"dw_cost_factor\": 0,\n",
    "              \"w_cost_factor\": 0,\n",
    "              \"dt\": 0.25,\n",
    "              \"flash_on_interval\": 2.1\n",
    "            }\n",
    "lm.env_kwargs.update(env_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6537ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curriculum_training kwargs\n",
    "env_kwargs = {\n",
    "              \n",
    "              'angular_terminal_vel': 1,\n",
    "              \"reward_per_ff\": 100,\n",
    "          \n",
    "              \"dv_cost_factor\": 0,\n",
    "              \"dw_cost_factor\": 0,\n",
    "              \"w_cost_factor\": 0,\n",
    "              \"dt\": 0.25,\n",
    "              \"flash_on_interval\": 2.1\n",
    "            }\n",
    "lm.env_kwargs.update(env_kwargs)\n",
    "\n",
    "lm.streamline_making_animation(currentTrial_for_animation=None, num_trials_for_animation=None, duration=[10, 40], n_steps=1000, video_dir=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PkFUlYfELK_e",
   "metadata": {
    "id": "PkFUlYfELK_e"
   },
   "source": [
    "# Optuna (LSTM)\n",
    "\n",
    "(my own codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0vtL5qnLQ0e",
   "metadata": {
    "id": "d0vtL5qnLQ0e"
   },
   "source": [
    "##### sample_sac_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CM9Yhsb7LQ0e",
   "metadata": {
    "id": "CM9Yhsb7LQ0e"
   },
   "outputs": [],
   "source": [
    "def sample_sac_params(trial):\n",
    "    \"\"\"\n",
    "    Sampler for SAC hyperparams.\n",
    "    :param trial: (optuna.trial)\n",
    "    :return: (dict)\n",
    "    \"\"\"\n",
    "  \n",
    "    gamma = 1.0 - trial.suggest_float(\"1-gamma\", 1e-4, 0.1, log=True)\n",
    "    soft_q_lr = trial.suggest_float(\"soft_q_lr\", 1e-5, 1, log=True)\n",
    "    policy_lr = trial.suggest_float(\"policy_lr\", 1e-5, 1, log=True)\n",
    "    alpha_lr  = trial.suggest_float(\"alpha_lr\", 1e-5, 1, log=True)\n",
    "    batch_size  = trial.suggest_categorical('batch_size', [5, 10, 15, 20, 25, 30])\n",
    "    update_itr = trial.suggest_categorical('update_itr', [1, 2, 3, 5])\n",
    "    hidden_dim = trial.suggest_categorical('hidden_dim', [16, 32, 64, 100, 150, 200, 256])\n",
    "    reward_scale = trial.suggest_categorical('reward_scale', [1, 3, 5, 10, 15, 20]) # I updated after running\n",
    "    target_entropy = trial.suggest_categorical('target_entropy', [-1, -2, -3, -5, -8, -10]) # I updated after running\n",
    "    soft_tau= trial.suggest_float(\"soft_tau\", 1e-6, 1, log=True)\n",
    "    #activation_fn\n",
    "\n",
    "\n",
    "    return {\n",
    "        'gamma': gamma,\n",
    "        'soft_q_lr':soft_q_lr,\n",
    "        'policy_lr':policy_lr,\n",
    "        'alpha_lr':alpha_lr,\n",
    "        'batch_size': batch_size,\n",
    "        'update_itr':update_itr,\n",
    "        'hidden_dim': hidden_dim,\n",
    "        'reward_scale':reward_scale,\n",
    "        'target_entropy':target_entropy,\n",
    "        'soft_tau':soft_tau\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "w8snZtlopA-9",
   "metadata": {
    "id": "w8snZtlopA-9"
   },
   "source": [
    "## put_in_fixed_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1RFcdEk5pAmv",
   "metadata": {
    "id": "1RFcdEk5pAmv"
   },
   "outputs": [],
   "source": [
    "def put_in_fixed_params():\n",
    "    return {\n",
    "        'model_folder_name':  None, \n",
    "        'train_freq': 100, \n",
    "        'batch_size': 10, \n",
    "        'update_itr': 1,\n",
    "        'num_train_episodes': 50, \n",
    "        'eval_eps_freq': 10, \n",
    "        'max_steps_per_eps': 1024, \n",
    "        'auto_entropy': True, \n",
    "        'DETERMINISTIC': False, \n",
    "        'num_eval_episodes': 3, \n",
    "        'print_episode_reward':  True}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9TMGTKP3LTVV",
   "metadata": {
    "id": "9TMGTKP3LTVV"
   },
   "source": [
    "## objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NhoGtlzHqxKY",
   "metadata": {
    "id": "NhoGtlzHqxKY"
   },
   "outputs": [],
   "source": [
    "def objective(trial: optuna.Trial) -> float: \n",
    "  try:\n",
    "    # Sample hyperparameters\n",
    "    kwargs = sample_sac_params(trial)\n",
    "    kwargs = put_in_fixed_params()\n",
    "\n",
    "    num_train_episodes = kwargs['num_train_episodes'] \n",
    "    eval_eps_freq = kwargs['eval_eps_freq']\n",
    "    max_steps_per_eps = kwargs['max_steps_per_eps'] \n",
    "    auto_entropy = kwargs['auto_entropy'] \n",
    "    num_eval_episodes = kwargs['num_eval_episodes'] \n",
    "    print_episode_reward = kwargs['print_episode_reward']\n",
    "\n",
    "\n",
    "    env = env_for_lstm.EnvForLSTM()\n",
    "    sac_model = LSTM_functions.SAC_Trainer(**kwargs)\n",
    "    lm2 = lstm_for_multiff_class.LSTMforMultifirefly()\n",
    "    lm2.sac_model = sac_model\n",
    "    sac_model, best_avg_reward_record, alpha_df = lm2.train_LSTM_agent(env, device,\n",
    "                                                         num_train_episodes=num_train_episodes, eval_eps_freq=eval_eps_freq, max_steps_per_eps=max_steps_per_eps, \n",
    "                                                         num_eval_episodes=num_eval_episodes, print_episode_reward=print_episode_reward, auto_entropy=auto_entropy)\n",
    "\n",
    "  except ValueError as e:\n",
    "    # Sometimes, random hyperparams can generate NaN\n",
    "      print(e)\n",
    "\n",
    "  return best_avg_reward_record"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zWLQGb9cW2TQ",
   "metadata": {
    "id": "zWLQGb9cW2TQ"
   },
   "source": [
    "## run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "L4wV8ncwW2TQ",
   "metadata": {
    "id": "L4wV8ncwW2TQ"
   },
   "outputs": [],
   "source": [
    "N_TRIALS = 100\n",
    "N_STARTUP_TRIALS = 5\n",
    "\n",
    "# Set pytorch num threads to 1 for faster training\n",
    "torch.set_num_threads(1)\n",
    " \n",
    "sampler = TPESampler(n_startup_trials=N_STARTUP_TRIALS)\n",
    "## Do not prune before 1/3 of the max budget is used\n",
    "# pruner = MedianPruner(n_startup_trials=N_STARTUP_TRIALS, n_warmup_steps=N_EVALUATIONS 2//3 3)\n",
    "\n",
    "study = optuna.create_study(sampler=sampler, direction=\"maximize\")\n",
    "try:\n",
    "    study.optimize(objective, n_trials=N_TRIALS)\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "print(\"  User attrs:\")\n",
    "for key, value in trial.user_attrs.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QvPXVRFfIbVD",
   "metadata": {
    "id": "QvPXVRFfIbVD"
   },
   "source": [
    "# Animation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Z5vjDrdHrSn2",
   "metadata": {
    "id": "Z5vjDrdHrSn2"
   },
   "source": [
    "## collect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yDZgRZMNHnZl",
   "metadata": {
    "id": "yDZgRZMNHnZl"
   },
   "outputs": [],
   "source": [
    "env = env_for_sb3.CollectInformationLSTM()\n",
    "env.flash_on_interval = 0.3\n",
    "env.distance2center_cost = 0\n",
    "sac_model.load_model(model_folder_name)\n",
    "\n",
    "env._run_agent_to_collect_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JlxfIO9YFFHe",
   "metadata": {
    "id": "JlxfIO9YFFHe"
   },
   "source": [
    "## prepare for animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6DqTSaOz6s38",
   "metadata": {
    "id": "6DqTSaOz6s38"
   },
   "outputs": [],
   "source": [
    "currentTrial = 2\n",
    "num_trials = 7\n",
    "k = 1\n",
    "fig, ax = plt.subplots()\n",
    "num_frames, anim_monkey_info, flash_on_ff_dict, alive_ff_dict, believed_ff_dict, new_num_trials, ff_dataframe_anim \\\n",
    "            = animation_utils.prepare_for_animation(ff_dataframe, ff_caught_T_new, ff_life_sorted, ff_believed_position_sorted, \n",
    "            ff_real_position_sorted, ff_flash_sorted, monkey_information, k=k, currentTrial=currentTrial, num_trials=num_trials)\n",
    "print(\"Number of frames is:\", num_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AesCxkpsFbVn",
   "metadata": {
    "id": "AesCxkpsFbVn"
   },
   "source": [
    "## make animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pUcR1X-0-OQo",
   "metadata": {
    "id": "pUcR1X-0-OQo"
   },
   "outputs": [],
   "source": [
    "animate_func = partial(animation_func.animate, ax=ax, anim_monkey_info=anim_monkey_info, ff_dataframe_anim=ff_dataframe_anim, ff_real_position_sorted=ff_real_position_sorted, \\\n",
    "                         flash_on_ff_dict=flash_on_ff_dict, alive_ff_dict=alive_ff_dict, believed_ff_dict=believed_ff_dict, margin = 400)\n",
    "anim = animation.FuncAnimation(fig, animate_func, frames=num_frames, interval=100, repeat=True) \n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-BX7XwsnFc4k",
   "metadata": {
    "id": "-BX7XwsnFc4k"
   },
   "source": [
    "## make animation with annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aE9lp9YZ-H7c",
   "metadata": {
    "id": "aE9lp9YZ-H7c"
   },
   "outputs": [],
   "source": [
    "annotation_info = animation_utils.make_annotation_info(caught_ff_num+1, max_point_index, n_ff_in_a_row, visible_before_last_one_trials, disappear_latest_trials, \\\n",
    "                                        ignore_sudden_flash_indices, give_up_after_trying_indices, try_a_few_times_indices)\n",
    "animate_annotated_func = partial(animation_func.animate_annotated, ax=ax, anim_monkey_info=anim_monkey_info, margin=margin, ff_dataframe=ff_dataframe, \\\n",
    "                                   flash_on_ff_dict=flash_on_ff_dict, alive_ff_dict=alive_ff_dict, believed_ff_dict=believed_ff_dict, ff_caught_T_new=ff_caught_T_new, annotation_info=annotation_info)\n",
    "anim_annotated = animation.FuncAnimation(fig, animate_annotated_func, frames=num_frames, interval=100, repeat=True) \n",
    "HTML(anim_annotated.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h8mO9eLF5U7Y",
   "metadata": {
    "id": "h8mO9eLF5U7Y"
   },
   "source": [
    "# Debug"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [
    {
     "file_id": "1FShUys0iUmi3huyQwtaEdyhGivLG2R_5",
     "timestamp": 1681095282536
    }
   ],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "multiff_clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
