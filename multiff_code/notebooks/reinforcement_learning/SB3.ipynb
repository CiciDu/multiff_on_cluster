{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a76cf94f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/cicid/miniconda3/envs/multiff_clean/bin/ffmpeg'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutil.which(\"ffmpeg\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1t_MmbnQxz3f",
   "metadata": {
    "id": "1t_MmbnQxz3f"
   },
   "source": [
    "## Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370f1462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: neo in /opt/miniconda3/envs/multiff_clean/lib/python3.11/site-packages (0.14.2)\n",
      "Requirement already satisfied: packaging in /opt/miniconda3/envs/multiff_clean/lib/python3.11/site-packages (from neo) (25.0)\n",
      "Requirement already satisfied: numpy>=1.24.4 in /opt/miniconda3/envs/multiff_clean/lib/python3.11/site-packages (from neo) (1.26.4)\n",
      "Requirement already satisfied: quantities>=0.16.1 in /opt/miniconda3/envs/multiff_clean/lib/python3.11/site-packages (from neo) (0.16.2)\n",
      "Requirement already satisfied: matplotlib_scalebar in /opt/miniconda3/envs/multiff_clean/lib/python3.11/site-packages (0.9.0)\n",
      "Requirement already satisfied: matplotlib in /opt/miniconda3/envs/multiff_clean/lib/python3.11/site-packages (from matplotlib_scalebar) (3.10.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/miniconda3/envs/multiff_clean/lib/python3.11/site-packages (from matplotlib->matplotlib_scalebar) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/miniconda3/envs/multiff_clean/lib/python3.11/site-packages (from matplotlib->matplotlib_scalebar) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/miniconda3/envs/multiff_clean/lib/python3.11/site-packages (from matplotlib->matplotlib_scalebar) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/miniconda3/envs/multiff_clean/lib/python3.11/site-packages (from matplotlib->matplotlib_scalebar) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in /opt/miniconda3/envs/multiff_clean/lib/python3.11/site-packages (from matplotlib->matplotlib_scalebar) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/miniconda3/envs/multiff_clean/lib/python3.11/site-packages (from matplotlib->matplotlib_scalebar) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /opt/miniconda3/envs/multiff_clean/lib/python3.11/site-packages (from matplotlib->matplotlib_scalebar) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/miniconda3/envs/multiff_clean/lib/python3.11/site-packages (from matplotlib->matplotlib_scalebar) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/miniconda3/envs/multiff_clean/lib/python3.11/site-packages (from matplotlib->matplotlib_scalebar) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/miniconda3/envs/multiff_clean/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib->matplotlib_scalebar) (1.17.0)\n",
      "Collecting ffmpeg\n",
      "  Using cached ffmpeg-1.4-py3-none-any.whl\n",
      "Installing collected packages: ffmpeg\n",
      "Successfully installed ffmpeg-1.4\n",
      "Requirement already satisfied: wheel==0.38.4 in /opt/miniconda3/envs/multiff_clean/lib/python3.11/site-packages (0.38.4)\n",
      "Collecting setuptools==65.5.0\n",
      "  Using cached setuptools-65.5.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Using cached setuptools-65.5.0-py3-none-any.whl (1.2 MB)\n",
      "Installing collected packages: setuptools\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 72.1.0\n",
      "    Uninstalling setuptools-72.1.0:\n",
      "      Successfully uninstalled setuptools-72.1.0\n",
      "Successfully installed setuptools-65.5.0\n",
      "Requirement already satisfied: stable_baselines3 in /opt/miniconda3/envs/multiff_clean/lib/python3.11/site-packages (2.7.0)\n",
      "Requirement already satisfied: gymnasium<1.3.0,>=0.29.1 in /opt/miniconda3/envs/multiff_clean/lib/python3.11/site-packages (from stable_baselines3) (1.2.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.20 in /opt/miniconda3/envs/multiff_clean/lib/python3.11/site-packages (from stable_baselines3) (1.26.4)\n",
      "Requirement already satisfied: torch<3.0,>=2.3 in /opt/miniconda3/envs/multiff_clean/lib/python3.11/site-packages (from stable_baselines3) (2.5.1)\n",
      "Requirement already satisfied: cloudpickle in /opt/miniconda3/envs/multiff_clean/lib/python3.11/site-packages (from stable_baselines3) (3.1.1)\n",
      "Requirement already satisfied: pandas in /opt/miniconda3/envs/multiff_clean/lib/python3.11/site-packages (from stable_baselines3) (2.3.2)\n",
      "Requirement already satisfied: matplotlib in /opt/miniconda3/envs/multiff_clean/lib/python3.11/site-packages (from stable_baselines3) (3.10.5)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /opt/miniconda3/envs/multiff_clean/lib/python3.11/site-packages (from gymnasium<1.3.0,>=0.29.1->stable_baselines3) (4.14.1)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /opt/miniconda3/envs/multiff_clean/lib/python3.11/site-packages (from gymnasium<1.3.0,>=0.29.1->stable_baselines3) (0.0.4)\n",
      "Requirement already satisfied: filelock in /opt/miniconda3/envs/multiff_clean/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable_baselines3) (3.13.1)\n",
      "Requirement already satisfied: networkx in /opt/miniconda3/envs/multiff_clean/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable_baselines3) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/miniconda3/envs/multiff_clean/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable_baselines3) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/miniconda3/envs/multiff_clean/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable_baselines3) (2024.6.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/miniconda3/envs/multiff_clean/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable_baselines3) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/miniconda3/envs/multiff_clean/lib/python3.11/site-packages (from sympy==1.13.1->torch<3.0,>=2.3->stable_baselines3) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniconda3/envs/multiff_clean/lib/python3.11/site-packages (from jinja2->torch<3.0,>=2.3->stable_baselines3) (3.0.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/miniconda3/envs/multiff_clean/lib/python3.11/site-packages (from matplotlib->stable_baselines3) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/miniconda3/envs/multiff_clean/lib/python3.11/site-packages (from matplotlib->stable_baselines3) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/miniconda3/envs/multiff_clean/lib/python3.11/site-packages (from matplotlib->stable_baselines3) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/miniconda3/envs/multiff_clean/lib/python3.11/site-packages (from matplotlib->stable_baselines3) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/miniconda3/envs/multiff_clean/lib/python3.11/site-packages (from matplotlib->stable_baselines3) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /opt/miniconda3/envs/multiff_clean/lib/python3.11/site-packages (from matplotlib->stable_baselines3) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/miniconda3/envs/multiff_clean/lib/python3.11/site-packages (from matplotlib->stable_baselines3) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/miniconda3/envs/multiff_clean/lib/python3.11/site-packages (from matplotlib->stable_baselines3) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/miniconda3/envs/multiff_clean/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib->stable_baselines3) (1.17.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/miniconda3/envs/multiff_clean/lib/python3.11/site-packages (from pandas->stable_baselines3) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/miniconda3/envs/multiff_clean/lib/python3.11/site-packages (from pandas->stable_baselines3) (2025.2)\n",
      "Collecting optuna\n",
      "  Downloading optuna-4.5.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting alembic>=1.5.0 (from optuna)\n",
      "  Downloading alembic-1.16.5-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting colorlog (from optuna)\n",
      "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy in /opt/miniconda3/envs/multiff_clean/lib/python3.11/site-packages (from optuna) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/miniconda3/envs/multiff_clean/lib/python3.11/site-packages (from optuna) (25.0)\n",
      "Collecting sqlalchemy>=1.4.2 (from optuna)\n",
      "  Downloading sqlalchemy-2.0.43-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: tqdm in /opt/miniconda3/envs/multiff_clean/lib/python3.11/site-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in /opt/miniconda3/envs/multiff_clean/lib/python3.11/site-packages (from optuna) (6.0.2)\n",
      "Collecting Mako (from alembic>=1.5.0->optuna)\n",
      "  Downloading mako-1.3.10-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in /opt/miniconda3/envs/multiff_clean/lib/python3.11/site-packages (from alembic>=1.5.0->optuna) (4.14.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /opt/miniconda3/envs/multiff_clean/lib/python3.11/site-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
      "Downloading optuna-4.5.0-py3-none-any.whl (400 kB)\n",
      "Downloading alembic-1.16.5-py3-none-any.whl (247 kB)\n",
      "Downloading sqlalchemy-2.0.43-cp311-cp311-macosx_11_0_arm64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
      "Downloading mako-1.3.10-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: sqlalchemy, Mako, colorlog, alembic, optuna\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/5\u001b[0m [optuna]2m4/5\u001b[0m [optuna]]\n",
      "\u001b[1A\u001b[2KSuccessfully installed Mako-1.3.10 alembic-1.16.5 colorlog-6.9.0 optuna-4.5.0 sqlalchemy-2.0.43\n"
     ]
    }
   ],
   "source": [
    "# !pip install neo\n",
    "# !pip install matplotlib_scalebar\n",
    "# !pip install ffmpeg\n",
    "# !pip install wheel==0.38.4\n",
    "# !pip3 install setuptools==65.5.0\n",
    "# !pip install stable_baselines3\n",
    "# !pip install optuna"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9904b001",
   "metadata": {
    "id": "9904b001"
   },
   "source": [
    "## import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78d7921b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32160,
     "status": "ok",
     "timestamp": 1684945803461,
     "user": {
      "displayName": "Cici Du",
      "userId": "17701548280142155870"
     },
     "user_tz": -480
    },
    "id": "78d7921b",
    "outputId": "68f69138-fbc7-40e8-db4e-f3bea17f3a31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pathlib import Path\n",
    "import os, sys\n",
    "for p in [Path.cwd()] + list(Path.cwd().parents):\n",
    "    if p.name == 'Multifirefly-Project':\n",
    "        os.chdir(p)\n",
    "        sys.path.insert(0, str(p / 'multiff_analysis/multiff_code/methods'))\n",
    "        break\n",
    "\n",
    " \n",
    "\n",
    "from data_wrangling import specific_utils, process_monkey_information\n",
    "from pattern_discovery import pattern_by_trials, make_ff_dataframe, pattern_by_trials, organize_patterns_and_features\n",
    "from visualization.matplotlib_tools import plot_trials, plot_polar, additional_plots, plot_behaviors_utils, plot_statistics\n",
    "from visualization.animation import animation_func, animation_utils, animation_class\n",
    "from machine_learning.RL.env_related import env_for_lstm, env_utils, base_env, collect_agent_data, process_agent_data, env_for_sb3\n",
    "from machine_learning.RL.lstm import GRU_functions, LSTM_functions\n",
    "from machine_learning.RL.SB3 import interpret_neural_network, sb3_for_multiff_class, rl_for_multiff_utils, SB3_functions\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import optuna\n",
    "import matplotlib\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib import rc\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common import results_plotter\n",
    "from stable_baselines3.common.results_plotter import plot_results\n",
    "from stable_baselines3.common.callbacks import StopTrainingOnNoModelImprovement\n",
    "from optuna.pruners import MedianPruner\n",
    "from optuna.samplers import TPESampler\n",
    "from IPython.display import HTML\n",
    "from functools import partial\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "from os.path import exists\n",
    "import random\n",
    "from importlib import reload\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnNoModelImprovement, StopTrainingOnRewardThreshold\n",
    "import gc\n",
    "\n",
    "plt.rcParams[\"animation.html\"] = \"html5\"\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "rc('animation', html='jshtml')\n",
    "matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n",
    "matplotlib.rcParams['animation.embed_limit'] = 2**128\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "import os, shutil\n",
    "\n",
    "matplotlib.rcParams[\"animation.ffmpeg_path\"] = shutil.which(\"ffmpeg\")\n",
    "matplotlib.rcParams[\"animation.writer\"] = \"ffmpeg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749d36d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2b4cb6e9",
   "metadata": {
    "id": "2b4cb6e9"
   },
   "source": [
    "## basic parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64f61c2",
   "metadata": {
    "id": "d64f61c2"
   },
   "outputs": [],
   "source": [
    "overall_folder = \"RL_models/SB3_stored_models/all_agents/gen_5/\"\n",
    "os.makedirs(overall_folder, exist_ok=True)\n",
    "PLAYER = \"agent\"\n",
    "# NEW_DATASET = True\n",
    "# MONKEY_DATA = False\n",
    "# NO_PLOT_NEEDED = True\n",
    "# raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Bruno/data_0219\"\n",
    "# data_folder_name = \"all_monkey_data/raw_monkey_data/monkey_Bruno/data_0219/processed_data\"\n",
    "#data_num = 19\n",
    "\n",
    "\n",
    "# # for agent\n",
    "# PLAYER = \"agent\"\n",
    "# NEW_DATASET = True\n",
    "# MONKEY_DATA = False\n",
    "# NO_PLOT_NEEDED = True\n",
    "# data_folder_name = \"env.multiff_analysis/RL_models/LSTM_July_29\"\n",
    "# data_num = 721\n",
    "# trial_total_num = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ac8034",
   "metadata": {},
   "source": [
    "# Streamline training agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d2542d",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3eac088",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder_name = \"RL_models/SB3_stored_models/all_agents/regular\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d7fc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_folder_name = \"RL_models/SB3_stored_models/all_agents/temp_10_11\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928cab3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder_name = \"RL_models/SB3_stored_models/all_agents/oct_12_6\"\n",
    "os.makedirs(model_folder_name, exist_ok=True)\n",
    "\n",
    "env_kwargs = {}\n",
    "env = env_for_sb3.EnvForSB3(**env_kwargs)\n",
    "env = Monitor(env, model_folder_name)\n",
    "\n",
    "# For direct training\n",
    "sac_model = SAC(\"MlpPolicy\", \n",
    "            env,\n",
    "            gamma=0.998,\n",
    "            learning_rate=0.0015,\n",
    "            batch_size=1024,\n",
    "            target_update_interval=50,\n",
    "            buffer_size=1000000,\n",
    "            learning_starts=10000,\n",
    "            train_freq=10,\n",
    "            ent_coef='auto',\n",
    "            policy_kwargs=dict(activation_fn=nn.Tanh, net_arch=[128, 128])\n",
    "            )\n",
    "\n",
    "\n",
    "callback = SB3_functions.SaveOnBestTrainingRewardCallback(check_freq=20000, model_folder_name=model_folder_name)\n",
    "#timesteps = 50000000\n",
    "timesteps = 200000\n",
    "sac_model.learn(total_timesteps=int(timesteps), callback=callback)\n",
    "plot_results([model_folder_name], timesteps, results_plotter.X_TIMESTEPS, \"env.MultiFF\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cf5546",
   "metadata": {},
   "source": [
    "## Use class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efbb0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_folder_name: RL_models/SB3_stored_models/all_agents/oct_13\n",
      "Made agent with the following params: {'learning_rate': 0.0003, 'batch_size': 1024, 'target_update_interval': 50, 'buffer_size': 1000000, 'learning_starts': 10000, 'train_freq': TrainFreq(frequency=10, unit=<TrainFrequencyUnit.STEP: 'step'>), 'gradient_steps': 10, 'ent_coef': 'auto', 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>, 'net_arch': [256, 128], 'use_sde': False}, 'gamma': 0.998}\n",
      "There was an error retrieving agent or replay_buffer in RL_models/SB3_stored_models/all_agents/oct_13/best_model.zip. Error message [Errno 2] No such file or directory: 'RL_models/SB3_stored_models/all_agents/oct_13/best_model.zip.zip'\n",
      "Failed to load existing agent. Need to train a new agent. Error:  There was an error retrieving agent or replay_buffer in RL_models/SB3_stored_models/all_agents/oct_13/best_model.zip. Error message [Errno 2] No such file or directory: 'RL_models/SB3_stored_models/all_agents/oct_13/best_model.zip.zip'\n",
      "There was an error retrieving agent or replay_buffer in RL_models/SB3_stored_models/all_agents/env1_relu/best_model_after_curriculum/best_model.zip. Error message [Errno 2] No such file or directory: 'RL_models/SB3_stored_models/all_agents/env1_relu/best_model_after_curriculum/best_model.zip.zip'\n",
      "Need to train a new best_model_after_curriculum\n",
      "Starting curriculum training\n",
      "Made agent with the following params: {'learning_rate': 0.0015, 'batch_size': 1024, 'target_update_interval': 50, 'buffer_size': 1000000, 'learning_starts': 10000, 'train_freq': TrainFreq(frequency=10, unit=<TrainFrequencyUnit.STEP: 'step'>), 'gradient_steps': 1, 'ent_coef': 'auto', 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>, 'net_arch': [256, 128], 'use_sde': False}, 'gamma': 0.998}\n",
      "reward_threshold: 819.2000000000002\n",
      "TIME before resetting: 0\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.32\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  0.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.32\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  1\n",
      "26.3 action:  [-0.1579, -0.991] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  1 ff for 102.49999999999845 s: -------------------> 0.01\n",
      "Total reward for the episode:  80.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.32\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  2\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  0.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.32\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  3\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  0.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.32\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  4\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  0.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.32\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  5\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  0.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.32\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  6\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  0.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.32\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  7\n",
      "TIME before resetting: 82.49999999999959\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.32\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  8\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  0.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.32\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  9\n",
      "Eval num_timesteps=8000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 1025.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  0.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.32\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  10\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  0.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.32\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  11\n",
      "57.8 action:  [-0.1326, -0.9804] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  1 ff for 102.49999999999845 s: -------------------> 0.01\n",
      "Total reward for the episode:  80.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.32\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  12\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  0.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.32\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  13\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  0.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.32\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  14\n",
      "15.1 action:  [0.128, -0.9865] n_targets:  2 reward:  160\n",
      "Firely capture rate for the episode:  2 ff for 102.49999999999845 s: -------------------> 0.02\n",
      "Total reward for the episode:  160.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.32\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  15\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  0.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.32\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  16\n",
      "37.4 action:  [-0.1874, -0.9936] n_targets:  1 reward:  80\n",
      "47.0 action:  [0.1969, -0.9971] n_targets:  1 reward:  80\n",
      "73.9 action:  [-0.21, -0.986] n_targets:  1 reward:  80\n",
      "TIME before resetting: 82.49999999999959\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.32\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  17\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  0.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.32\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  18\n",
      "Eval num_timesteps=16000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 1025.00 +/- 0.00\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  0.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.32\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  19\n",
      "79.9 action:  [-0.1344, -0.997] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  1 ff for 102.49999999999845 s: -------------------> 0.01\n",
      "Total reward for the episode:  80.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.32\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  20\n",
      "17.9 action:  [-0.1601, -0.9847] n_targets:  1 reward:  80\n",
      "23.0 action:  [-0.1679, -0.9972] n_targets:  1 reward:  80\n",
      "30.0 action:  [0.0523, -0.9854] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  3 ff for 102.49999999999845 s: -------------------> 0.03\n",
      "Total reward for the episode:  240.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.32\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  21\n",
      "13.9 action:  [0.258, -0.9885] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  1 ff for 102.49999999999845 s: -------------------> 0.01\n",
      "Total reward for the episode:  80.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.32\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  22\n",
      "21.0 action:  [0.0245, -0.9956] n_targets:  1 reward:  80\n",
      "59.3 action:  [-0.237, -0.9932] n_targets:  1 reward:  80\n",
      "84.4 action:  [0.2039, -0.9824] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  3 ff for 102.49999999999845 s: -------------------> 0.03\n",
      "Total reward for the episode:  240.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.32\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  23\n",
      "92.4 action:  [-0.0589, -0.9956] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  1 ff for 102.49999999999845 s: -------------------> 0.01\n",
      "Total reward for the episode:  80.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.32\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  24\n",
      "36.0 action:  [-0.0504, -0.9997] n_targets:  1 reward:  80\n",
      "54.1 action:  [-0.0878, -0.9989] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  2 ff for 102.49999999999845 s: -------------------> 0.02\n",
      "Total reward for the episode:  160.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.32\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  25\n",
      "17.9 action:  [-0.184, -0.9846] n_targets:  1 reward:  80\n",
      "TIME before resetting: 82.49999999999959\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.32\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  26\n",
      "22.7 action:  [0.0524, -0.992] n_targets:  1 reward:  80\n",
      "65.7 action:  [-0.2326, -0.9993] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  2 ff for 102.49999999999845 s: -------------------> 0.02\n",
      "Total reward for the episode:  160.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.32\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  27\n",
      "Eval num_timesteps=24000, episode_reward=160.00 +/- 0.00\n",
      "Episode length: 1025.00 +/- 0.00\n",
      "New best mean reward!\n",
      "26.7 action:  [-0.013, -0.9953] n_targets:  1 reward:  80\n",
      "93.4 action:  [0.0385, -0.9947] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  2 ff for 102.49999999999845 s: -------------------> 0.02\n",
      "Total reward for the episode:  160.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.32\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  28\n",
      "44.3 action:  [-0.0741, -0.9969] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  1 ff for 102.49999999999845 s: -------------------> 0.01\n",
      "Total reward for the episode:  80.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.32\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  29\n",
      "100.2 action:  [-0.0296, -0.9991] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  1 ff for 102.49999999999845 s: -------------------> 0.01\n",
      "Total reward for the episode:  80.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.32\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  30\n",
      "101.2 action:  [-0.077, -0.9999] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  1 ff for 102.49999999999845 s: -------------------> 0.01\n",
      "Total reward for the episode:  80.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.32\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  31\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  0.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.32\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  32\n",
      "9.7 action:  [-0.1939, -0.9934] n_targets:  1 reward:  80\n",
      "18.4 action:  [-0.2619, -0.9999] n_targets:  1 reward:  80\n",
      "46.6 action:  [-0.0773, -0.9892] n_targets:  2 reward:  160\n",
      "82.5 action:  [0.2554, -0.9934] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  5 ff for 102.49999999999845 s: -------------------> 0.05\n",
      "Total reward for the episode:  400.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.32\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  33\n",
      "54.2 action:  [-0.1385, -1.0] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  1 ff for 102.49999999999845 s: -------------------> 0.01\n",
      "Total reward for the episode:  80.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.32\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  34\n",
      "TIME before resetting: 82.49999999999959\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.32\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  35\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  0.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.32\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  36\n",
      "Eval num_timesteps=32000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 1025.00 +/- 0.00\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  0.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.32\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  37\n",
      "47.1 action:  [-0.2547, -0.9884] n_targets:  2 reward:  160\n",
      "Firely capture rate for the episode:  2 ff for 102.49999999999845 s: -------------------> 0.02\n",
      "Total reward for the episode:  160.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.32\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  38\n",
      "21.9 action:  [-0.1745, -0.9807] n_targets:  1 reward:  80\n",
      "41.6 action:  [-0.2905, -0.9818] n_targets:  1 reward:  80\n",
      "75.4 action:  [0.1511, -0.9903] n_targets:  1 reward:  80\n",
      "76.3 action:  [-0.0999, -0.9843] n_targets:  1 reward:  80\n",
      "86.6 action:  [-0.2345, -0.9943] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  5 ff for 102.49999999999845 s: -------------------> 0.05\n",
      "Total reward for the episode:  400.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.32\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  39\n",
      "21.0 action:  [0.0924, -0.9953] n_targets:  1 reward:  80\n",
      "56.6 action:  [-0.2067, -0.9952] n_targets:  1 reward:  80\n",
      "73.1 action:  [0.1002, -0.9898] n_targets:  1 reward:  80\n",
      "82.4 action:  [-0.1646, -0.9893] n_targets:  1 reward:  80\n",
      "97.3 action:  [0.198, -0.991] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  5 ff for 102.49999999999845 s: -------------------> 0.05\n",
      "Total reward for the episode:  400.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.32\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  40\n",
      "11.8 action:  [-0.2442, -0.9856] n_targets:  1 reward:  80\n",
      "25.7 action:  [-0.2419, -0.9811] n_targets:  1 reward:  80\n",
      "76.1 action:  [0.2316, -0.9938] n_targets:  1 reward:  80\n",
      "83.2 action:  [0.1149, -0.9949] n_targets:  2 reward:  160\n",
      "Firely capture rate for the episode:  5 ff for 102.49999999999845 s: -------------------> 0.05\n",
      "Total reward for the episode:  400.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.32\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  41\n",
      "16.8 action:  [-0.1797, -0.9841] n_targets:  1 reward:  80\n",
      "22.6 action:  [-0.32, -0.9956] n_targets:  1 reward:  80\n",
      "37.7 action:  [-0.3087, -0.9998] n_targets:  1 reward:  80\n",
      "67.2 action:  [-0.1716, -0.9948] n_targets:  2 reward:  160\n",
      "69.9 action:  [-0.2348, -0.9983] n_targets:  2 reward:  160\n",
      "88.7 action:  [0.0522, -0.9862] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  8 ff for 102.49999999999845 s: -------------------> 0.08\n",
      "Total reward for the episode:  640.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.32\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  42\n",
      "11.7 action:  [-0.1071, -0.9959] n_targets:  1 reward:  80\n",
      "24.5 action:  [0.1413, -0.9942] n_targets:  1 reward:  80\n",
      "28.2 action:  [0.0625, -0.997] n_targets:  1 reward:  80\n",
      "60.5 action:  [0.146, -0.9892] n_targets:  1 reward:  80\n",
      "90.3 action:  [-0.0814, -0.9939] n_targets:  1 reward:  80\n",
      "90.5 action:  [0.2049, -0.9996] n_targets:  1 reward:  80\n",
      "92.8 action:  [-0.2617, -0.9964] n_targets:  1 reward:  80\n",
      "97.0 action:  [-0.2329, -0.9847] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  8 ff for 102.49999999999845 s: -------------------> 0.08\n",
      "Total reward for the episode:  640.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.32\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  43\n",
      "16.5 action:  [0.226, -0.9901] n_targets:  1 reward:  80\n",
      "24.2 action:  [0.2283, -1.0] n_targets:  1 reward:  80\n",
      "36.8 action:  [-0.2922, -0.9978] n_targets:  1 reward:  80\n",
      "49.1 action:  [0.0983, -0.9984] n_targets:  1 reward:  80\n",
      "49.9 action:  [0.1314, -0.9851] n_targets:  1 reward:  80\n",
      "72.1 action:  [-0.155, -0.9847] n_targets:  1 reward:  80\n",
      "78.2 action:  [0.1863, -0.9888] n_targets:  1 reward:  80\n",
      "TIME before resetting: 82.49999999999959\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.32\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  44\n",
      "20.5 action:  [0.1962, -0.9913] n_targets:  1 reward:  80\n",
      "24.2 action:  [-0.3002, -0.9949] n_targets:  1 reward:  80\n",
      "41.6 action:  [0.1864, -0.9902] n_targets:  1 reward:  80\n",
      "47.5 action:  [0.1659, -0.9825] n_targets:  1 reward:  80\n",
      "64.2 action:  [-0.0775, -0.9966] n_targets:  1 reward:  80\n",
      "75.4 action:  [0.2302, -0.999] n_targets:  1 reward:  80\n",
      "89.5 action:  [0.0733, -1.0] n_targets:  2 reward:  160\n",
      "Firely capture rate for the episode:  8 ff for 102.49999999999845 s: -------------------> 0.08\n",
      "Total reward for the episode:  640.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.32\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  45\n",
      "Eval num_timesteps=40000, episode_reward=640.00 +/- 0.00\n",
      "Episode length: 1025.00 +/- 0.00\n",
      "New best mean reward!\n",
      "30.4 action:  [-0.0564, -0.9899] n_targets:  1 reward:  80\n",
      "34.2 action:  [-0.3181, -0.9976] n_targets:  1 reward:  80\n",
      "64.7 action:  [0.297, -0.999] n_targets:  1 reward:  80\n",
      "74.0 action:  [0.1542, -0.9853] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  4 ff for 102.49999999999845 s: -------------------> 0.04\n",
      "Total reward for the episode:  320.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.32\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  46\n",
      "14.6 action:  [0.2494, -0.9947] n_targets:  1 reward:  80\n",
      "15.5 action:  [0.1692, -0.9986] n_targets:  1 reward:  80\n",
      "17.6 action:  [0.067, -0.992] n_targets:  1 reward:  80\n",
      "20.2 action:  [0.1389, -0.9941] n_targets:  1 reward:  80\n",
      "44.5 action:  [-0.2187, -0.9868] n_targets:  1 reward:  80\n",
      "48.2 action:  [0.0679, -0.9999] n_targets:  1 reward:  80\n",
      "55.7 action:  [-0.1472, -0.9927] n_targets:  1 reward:  80\n",
      "77.8 action:  [0.0421, -0.9934] n_targets:  1 reward:  80\n",
      "78.3 action:  [0.2362, -0.9969] n_targets:  1 reward:  80\n",
      "88.0 action:  [-0.2493, -0.9935] n_targets:  1 reward:  80\n",
      "89.6 action:  [0.138, -0.9825] n_targets:  2 reward:  160\n",
      "Firely capture rate for the episode:  12 ff for 102.49999999999845 s: -------------------> 0.12\n",
      "Total reward for the episode:  960.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.32\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  47\n",
      "8.2 action:  [0.2091, -0.9988] n_targets:  1 reward:  80\n",
      "17.2 action:  [-0.1202, -0.9837] n_targets:  1 reward:  80\n",
      "22.7 action:  [0.0452, -0.9824] n_targets:  2 reward:  160\n",
      "37.7 action:  [-0.1962, -0.9817] n_targets:  2 reward:  160\n",
      "46.9 action:  [-0.0727, -0.9848] n_targets:  1 reward:  80\n",
      "51.7 action:  [0.1663, -0.9849] n_targets:  1 reward:  80\n",
      "61.9 action:  [-0.1239, -0.9967] n_targets:  1 reward:  80\n",
      "84.2 action:  [-0.2377, -0.9998] n_targets:  1 reward:  80\n",
      "86.3 action:  [0.1067, -0.9987] n_targets:  1 reward:  80\n",
      "95.4 action:  [0.3172, -0.993] n_targets:  1 reward:  80\n",
      "99.1 action:  [-0.0813, -0.995] n_targets:  1 reward:  80\n",
      "102.1 action:  [0.2181, -0.9986] n_targets:  2 reward:  160\n",
      "Firely capture rate for the episode:  15 ff for 102.49999999999845 s: -------------------> 0.15\n",
      "Total reward for the episode:  1200.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.32\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  48\n",
      "1.4 action:  [0.028, -0.9937] n_targets:  1 reward:  80\n",
      "5.9 action:  [0.2511, -0.9982] n_targets:  1 reward:  80\n",
      "8.2 action:  [0.0615, -0.9853] n_targets:  1 reward:  80\n",
      "21.6 action:  [-0.1884, -0.9888] n_targets:  1 reward:  80\n",
      "32.4 action:  [0.0893, -0.9827] n_targets:  1 reward:  80\n",
      "43.1 action:  [0.0255, -0.996] n_targets:  1 reward:  80\n",
      "50.2 action:  [-0.3119, -0.9868] n_targets:  1 reward:  80\n",
      "51.2 action:  [-0.1294, -0.9993] n_targets:  1 reward:  80\n",
      "56.2 action:  [-0.0677, -0.9981] n_targets:  1 reward:  80\n",
      "63.3 action:  [-0.1651, -0.9838] n_targets:  1 reward:  80\n",
      "70.6 action:  [-0.2334, -0.9975] n_targets:  1 reward:  80\n",
      "72.7 action:  [0.1811, -0.9978] n_targets:  1 reward:  80\n",
      "89.8 action:  [-0.2455, -0.9818] n_targets:  1 reward:  80\n",
      "98.2 action:  [0.0564, -0.9944] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  14 ff for 102.49999999999845 s: -------------------> 0.14\n",
      "Total reward for the episode:  1120.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.32\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  49\n",
      "21.3 action:  [-0.2038, -0.9939] n_targets:  1 reward:  80\n",
      "29.0 action:  [0.1099, -0.9975] n_targets:  1 reward:  80\n",
      "32.2 action:  [0.2774, -0.9998] n_targets:  2 reward:  160\n",
      "33.0 action:  [0.3166, -0.9805] n_targets:  2 reward:  160\n",
      "40.5 action:  [-0.2476, -0.9993] n_targets:  1 reward:  80\n",
      "45.9 action:  [-0.2894, -0.9865] n_targets:  1 reward:  80\n",
      "46.4 action:  [-0.1984, -0.9997] n_targets:  1 reward:  80\n",
      "54.8 action:  [-0.0665, -0.993] n_targets:  1 reward:  80\n",
      "58.8 action:  [0.2304, -0.989] n_targets:  1 reward:  80\n",
      "59.8 action:  [0.0187, -0.9972] n_targets:  1 reward:  80\n",
      "84.6 action:  [-0.1334, -0.9947] n_targets:  1 reward:  80\n",
      "89.5 action:  [-0.216, -0.9898] n_targets:  2 reward:  160\n",
      "93.6 action:  [0.0576, -0.9939] n_targets:  1 reward:  80\n",
      "98.0 action:  [-0.1217, -0.9894] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  17 ff for 102.49999999999845 s: -------------------> 0.17\n",
      "Total reward for the episode:  1360.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.32\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  50\n",
      "2.1 action:  [0.2529, -0.9985] n_targets:  1 reward:  80\n",
      "10.3 action:  [0.2747, -0.9996] n_targets:  1 reward:  80\n",
      "12.1 action:  [-0.1525, -0.9865] n_targets:  1 reward:  80\n",
      "21.9 action:  [-0.1973, -0.9828] n_targets:  2 reward:  160\n",
      "28.5 action:  [-0.1283, -0.9933] n_targets:  1 reward:  80\n",
      "39.0 action:  [0.0443, -0.9975] n_targets:  1 reward:  80\n",
      "45.3 action:  [-0.0979, -0.9928] n_targets:  1 reward:  80\n",
      "48.8 action:  [0.0543, -0.9967] n_targets:  1 reward:  80\n",
      "53.2 action:  [0.0941, -0.9995] n_targets:  1 reward:  80\n",
      "54.3 action:  [-0.2594, -0.9919] n_targets:  1 reward:  80\n",
      "69.3 action:  [-0.1834, -0.9852] n_targets:  2 reward:  160\n",
      "78.1 action:  [0.2753, -0.997] n_targets:  1 reward:  80\n",
      "80.8 action:  [-0.2897, -0.9994] n_targets:  1 reward:  80\n",
      "83.6 action:  [0.042, -0.9978] n_targets:  1 reward:  80\n",
      "88.4 action:  [0.1951, -0.9961] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  17 ff for 102.49999999999845 s: -------------------> 0.17\n",
      "Total reward for the episode:  1360.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.32\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  51\n",
      "10.4 action:  [0.0285, -0.9923] n_targets:  1 reward:  80\n",
      "15.8 action:  [0.3012, -0.9876] n_targets:  1 reward:  80\n",
      "16.4 action:  [-0.1261, -0.995] n_targets:  1 reward:  80\n",
      "27.0 action:  [-0.1162, -0.9967] n_targets:  1 reward:  80\n",
      "82.9 action:  [-0.0005, -0.9802] n_targets:  3 reward:  240\n",
      "85.7 action:  [-0.0768, -0.9917] n_targets:  1 reward:  80\n",
      "95.2 action:  [-0.1597, -0.9838] n_targets:  1 reward:  80\n",
      "102.4 action:  [-0.2428, -0.9947] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  10 ff for 102.49999999999845 s: -------------------> 0.1\n",
      "Total reward for the episode:  800.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.32\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  52\n",
      "0.2 action:  [0.2301, -0.9802] n_targets:  1 reward:  80\n",
      "15.6 action:  [0.047, -0.9982] n_targets:  1 reward:  80\n",
      "22.9 action:  [-0.26, -0.9963] n_targets:  1 reward:  80\n",
      "23.7 action:  [-0.2773, -0.9873] n_targets:  1 reward:  80\n",
      "31.9 action:  [-0.1681, -0.99] n_targets:  1 reward:  80\n",
      "45.9 action:  [-0.1881, -0.9922] n_targets:  1 reward:  80\n",
      "50.8 action:  [-0.105, -0.9938] n_targets:  1 reward:  80\n",
      "68.7 action:  [-0.0839, -0.9993] n_targets:  1 reward:  80\n",
      "81.0 action:  [0.2664, -0.9839] n_targets:  1 reward:  80\n",
      "TIME before resetting: 82.49999999999959\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.32\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  53\n",
      "12.2 action:  [-0.2597, -0.9808] n_targets:  1 reward:  80\n",
      "24.2 action:  [-0.2537, -0.9869] n_targets:  1 reward:  80\n",
      "41.0 action:  [-0.136, -0.9867] n_targets:  1 reward:  80\n",
      "46.4 action:  [0.0893, -0.995] n_targets:  1 reward:  80\n",
      "54.9 action:  [-0.0253, -0.9882] n_targets:  1 reward:  80\n",
      "56.4 action:  [-0.2857, -0.9808] n_targets:  2 reward:  160\n",
      "61.5 action:  [-0.0628, -0.9848] n_targets:  1 reward:  80\n",
      "63.4 action:  [0.2714, -0.984] n_targets:  1 reward:  80\n",
      "67.8 action:  [-0.2713, -0.9919] n_targets:  1 reward:  80\n",
      "69.8 action:  [0.0916, -0.9833] n_targets:  1 reward:  80\n",
      "75.7 action:  [0.1293, -0.9942] n_targets:  1 reward:  80\n",
      "78.6 action:  [0.2227, -0.9839] n_targets:  1 reward:  80\n",
      "89.7 action:  [-0.0158, -0.9871] n_targets:  1 reward:  80\n",
      "96.1 action:  [-0.1842, -0.9844] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  15 ff for 102.49999999999845 s: -------------------> 0.15\n",
      "Total reward for the episode:  1200.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.32\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  54\n",
      "Eval num_timesteps=48000, episode_reward=1200.00 +/- 0.00\n",
      "Episode length: 1025.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Saved agent: RL_models/SB3_stored_models/all_agents/env1_relu/best_model_after_curriculum/best_model\n",
      "Saved replay buffer: RL_models/SB3_stored_models/all_agents/env1_relu/best_model_after_curriculum/buffer\n",
      "Current dt: 0.1\n",
      "Current gamma: 0.998\n",
      "Current angular_terminal_vel: 0.16\n",
      "reward_threshold: 819.2000000000002\n",
      "TIME before resetting: 0\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.16\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  55\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  0.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.16\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  56\n",
      "73.3 action:  [0.1199, -0.9931] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  1 ff for 102.49999999999845 s: -------------------> 0.01\n",
      "Total reward for the episode:  80.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.16\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  57\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  0.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.16\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  58\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  0.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.16\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  59\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  0.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.16\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  60\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  0.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.16\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  61\n",
      "5.3 action:  [-0.0747, -0.9834] n_targets:  1 reward:  80\n",
      "47.0 action:  [-0.0655, -0.9851] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  2 ff for 102.49999999999845 s: -------------------> 0.02\n",
      "Total reward for the episode:  160.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.16\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  62\n",
      "TIME before resetting: 82.49999999999959\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.16\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  63\n",
      "13.0 action:  [0.0195, -0.9922] n_targets:  1 reward:  80\n",
      "20.8 action:  [-0.1184, -0.9908] n_targets:  1 reward:  80\n",
      "34.3 action:  [-0.0025, -0.9858] n_targets:  1 reward:  80\n",
      "51.5 action:  [0.154, -0.9927] n_targets:  1 reward:  80\n",
      "52.6 action:  [-0.0981, -0.9865] n_targets:  1 reward:  80\n",
      "53.1 action:  [0.1588, -0.9942] n_targets:  1 reward:  80\n",
      "62.0 action:  [0.137, -0.9941] n_targets:  1 reward:  80\n",
      "77.1 action:  [-0.0144, -0.9838] n_targets:  2 reward:  160\n",
      "85.4 action:  [-0.071, -0.9847] n_targets:  1 reward:  80\n",
      "89.5 action:  [0.1333, -0.9902] n_targets:  2 reward:  160\n",
      "92.7 action:  [-0.106, -0.9894] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  13 ff for 102.49999999999845 s: -------------------> 0.13\n",
      "Total reward for the episode:  1040.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.16\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  64\n",
      "Eval num_timesteps=8000, episode_reward=1040.00 +/- 0.00\n",
      "Episode length: 1025.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Saved agent: RL_models/SB3_stored_models/all_agents/env1_relu/best_model_after_curriculum/best_model\n",
      "Saved replay buffer: RL_models/SB3_stored_models/all_agents/env1_relu/best_model_after_curriculum/buffer\n",
      "Current dt: 0.1\n",
      "Current gamma: 0.998\n",
      "Current angular_terminal_vel: 0.08\n",
      "reward_threshold: 819.2000000000002\n",
      "TIME before resetting: 0\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.08\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  65\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  0.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.08\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  66\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  0.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.08\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  67\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  0.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.08\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  68\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  0.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.08\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  69\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  0.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.08\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  70\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  0.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.08\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  71\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  0.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.08\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  72\n",
      "TIME before resetting: 82.49999999999959\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.08\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  73\n",
      "28.7 action:  [-0.0491, -0.9885] n_targets:  1 reward:  80\n",
      "31.9 action:  [-0.0068, -0.9884] n_targets:  1 reward:  80\n",
      "32.3 action:  [-0.0026, -0.9904] n_targets:  1 reward:  80\n",
      "44.9 action:  [-0.0207, -0.9984] n_targets:  1 reward:  80\n",
      "70.0 action:  [-0.0624, -0.9816] n_targets:  1 reward:  80\n",
      "85.9 action:  [0.0482, -0.986] n_targets:  2 reward:  160\n",
      "90.8 action:  [0.0756, -0.9923] n_targets:  1 reward:  80\n",
      "100.8 action:  [0.0669, -0.9824] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  9 ff for 102.49999999999845 s: -------------------> 0.09\n",
      "Total reward for the episode:  720.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.08\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  74\n",
      "Eval num_timesteps=8000, episode_reward=720.00 +/- 0.00\n",
      "Episode length: 1025.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  0.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.08\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  75\n",
      "102.0 action:  [-0.079, -0.9954] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  1 ff for 102.49999999999845 s: -------------------> 0.01\n",
      "Total reward for the episode:  80.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.08\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  76\n",
      "18.9 action:  [-0.0511, -0.9918] n_targets:  1 reward:  80\n",
      "39.5 action:  [0.0736, -0.9973] n_targets:  1 reward:  80\n",
      "57.4 action:  [0.0548, -0.9882] n_targets:  1 reward:  80\n",
      "89.7 action:  [0.0301, -0.9971] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  4 ff for 102.49999999999845 s: -------------------> 0.04\n",
      "Total reward for the episode:  320.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.08\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  77\n",
      "1.7 action:  [-0.0375, -0.9919] n_targets:  1 reward:  80\n",
      "18.9 action:  [0.0198, -0.9876] n_targets:  2 reward:  160\n",
      "34.7 action:  [-0.02, -0.9949] n_targets:  1 reward:  80\n",
      "44.6 action:  [0.0724, -0.992] n_targets:  1 reward:  80\n",
      "52.8 action:  [0.0183, -0.998] n_targets:  1 reward:  80\n",
      "60.3 action:  [0.064, -0.9871] n_targets:  1 reward:  80\n",
      "69.5 action:  [0.015, -0.984] n_targets:  1 reward:  80\n",
      "85.6 action:  [0.0339, -0.9808] n_targets:  1 reward:  80\n",
      "93.1 action:  [-0.076, -0.9918] n_targets:  1 reward:  80\n",
      "101.2 action:  [-0.0589, -0.9888] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  11 ff for 102.49999999999845 s: -------------------> 0.11\n",
      "Total reward for the episode:  880.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.08\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  78\n",
      "3.3 action:  [0.0305, -0.9899] n_targets:  1 reward:  80\n",
      "5.2 action:  [-0.0042, -0.9945] n_targets:  1 reward:  80\n",
      "26.2 action:  [-0.0606, -0.9847] n_targets:  1 reward:  80\n",
      "36.1 action:  [-0.0318, -0.9991] n_targets:  3 reward:  240\n",
      "49.7 action:  [-0.0685, -0.9864] n_targets:  1 reward:  80\n",
      "54.4 action:  [-0.0276, -0.9928] n_targets:  1 reward:  80\n",
      "59.4 action:  [-0.0581, -0.9985] n_targets:  1 reward:  80\n",
      "62.2 action:  [0.0024, -0.9814] n_targets:  1 reward:  80\n",
      "77.4 action:  [-0.0403, -0.9992] n_targets:  1 reward:  80\n",
      "84.8 action:  [0.0238, -0.9941] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  12 ff for 102.49999999999845 s: -------------------> 0.12\n",
      "Total reward for the episode:  960.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.08\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  79\n",
      "3.1 action:  [-0.0138, -0.9918] n_targets:  1 reward:  80\n",
      "9.0 action:  [0.0219, -0.9995] n_targets:  3 reward:  240\n",
      "22.2 action:  [0.0421, -0.9941] n_targets:  1 reward:  80\n",
      "48.1 action:  [-0.0643, -0.9921] n_targets:  1 reward:  80\n",
      "80.3 action:  [-0.067, -0.9997] n_targets:  1 reward:  80\n",
      "97.4 action:  [-0.0026, -0.9984] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  8 ff for 102.49999999999845 s: -------------------> 0.08\n",
      "Total reward for the episode:  640.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.08\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  80\n",
      "12.2 action:  [0.0556, -0.9999] n_targets:  1 reward:  80\n",
      "28.8 action:  [0.0731, -0.9983] n_targets:  1 reward:  80\n",
      "31.3 action:  [0.0125, -0.9951] n_targets:  2 reward:  160\n",
      "46.4 action:  [-0.0286, -0.9804] n_targets:  1 reward:  80\n",
      "56.2 action:  [0.0026, -0.985] n_targets:  1 reward:  80\n",
      "57.0 action:  [-0.0048, -0.994] n_targets:  1 reward:  80\n",
      "73.7 action:  [-0.0029, -0.9849] n_targets:  1 reward:  80\n",
      "84.4 action:  [0.0465, -0.9978] n_targets:  1 reward:  80\n",
      "87.6 action:  [-0.0564, -0.9818] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  10 ff for 102.49999999999845 s: -------------------> 0.1\n",
      "Total reward for the episode:  800.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.08\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  81\n",
      "2.0 action:  [0.0672, -0.9988] n_targets:  1 reward:  80\n",
      "6.7 action:  [0.0519, -0.9899] n_targets:  2 reward:  160\n",
      "11.4 action:  [-0.0396, -0.9889] n_targets:  1 reward:  80\n",
      "12.1 action:  [0.0639, -0.9952] n_targets:  1 reward:  80\n",
      "21.4 action:  [-0.0481, -0.9996] n_targets:  1 reward:  80\n",
      "27.8 action:  [-0.0292, -0.9843] n_targets:  1 reward:  80\n",
      "28.8 action:  [0.066, -0.9893] n_targets:  1 reward:  80\n",
      "32.5 action:  [0.0451, -0.9868] n_targets:  1 reward:  80\n",
      "35.8 action:  [0.0293, -0.9818] n_targets:  1 reward:  80\n",
      "65.8 action:  [-0.0433, -0.9892] n_targets:  1 reward:  80\n",
      "80.9 action:  [0.0531, -0.9839] n_targets:  1 reward:  80\n",
      "81.9 action:  [-0.0241, -0.9804] n_targets:  1 reward:  80\n",
      "TIME before resetting: 82.49999999999959\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.08\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  82\n",
      "59.8 action:  [-0.0089, -0.9814] n_targets:  2 reward:  160\n",
      "62.8 action:  [-0.0024, -0.982] n_targets:  1 reward:  80\n",
      "81.0 action:  [-0.0209, -0.9976] n_targets:  1 reward:  80\n",
      "85.7 action:  [0.0061, -0.9931] n_targets:  1 reward:  80\n",
      "88.9 action:  [-0.0764, -0.9836] n_targets:  2 reward:  160\n",
      "92.4 action:  [0.0221, -0.984] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  8 ff for 102.49999999999845 s: -------------------> 0.08\n",
      "Total reward for the episode:  640.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.08\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  83\n",
      "Eval num_timesteps=16000, episode_reward=640.00 +/- 0.00\n",
      "Episode length: 1025.00 +/- 0.00\n",
      "8.9 action:  [0.0328, -0.9809] n_targets:  1 reward:  80\n",
      "58.6 action:  [-0.024, -0.9953] n_targets:  1 reward:  80\n",
      "74.6 action:  [-0.0471, -0.9965] n_targets:  1 reward:  80\n",
      "77.2 action:  [0.0254, -0.9871] n_targets:  1 reward:  80\n",
      "81.6 action:  [-0.0741, -0.9957] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  5 ff for 102.49999999999845 s: -------------------> 0.05\n",
      "Total reward for the episode:  400.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.08\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  84\n",
      "7.6 action:  [0.0402, -0.9885] n_targets:  1 reward:  80\n",
      "11.6 action:  [-0.043, -0.9966] n_targets:  1 reward:  80\n",
      "16.0 action:  [0.0344, -0.9966] n_targets:  2 reward:  160\n",
      "21.2 action:  [-0.0556, -0.9838] n_targets:  1 reward:  80\n",
      "53.6 action:  [-0.0632, -0.9909] n_targets:  1 reward:  80\n",
      "91.8 action:  [0.0536, -0.9865] n_targets:  1 reward:  80\n",
      "93.9 action:  [-0.0751, -0.9927] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  8 ff for 102.49999999999845 s: -------------------> 0.08\n",
      "Total reward for the episode:  640.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.08\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  85\n",
      "11.6 action:  [-0.0235, -0.9919] n_targets:  1 reward:  80\n",
      "12.4 action:  [0.0773, -0.9829] n_targets:  1 reward:  80\n",
      "21.8 action:  [0.0144, -0.9984] n_targets:  1 reward:  80\n",
      "25.3 action:  [0.0175, -0.9836] n_targets:  1 reward:  80\n",
      "26.9 action:  [0.0431, -0.986] n_targets:  1 reward:  80\n",
      "30.8 action:  [0.0527, -0.9988] n_targets:  1 reward:  80\n",
      "47.8 action:  [0.0529, -0.9879] n_targets:  1 reward:  80\n",
      "51.1 action:  [0.0274, -0.9941] n_targets:  1 reward:  80\n",
      "55.6 action:  [-0.0268, -0.9856] n_targets:  1 reward:  80\n",
      "61.7 action:  [0.0719, -0.9859] n_targets:  1 reward:  80\n",
      "101.9 action:  [0.0024, -0.9993] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  11 ff for 102.49999999999845 s: -------------------> 0.11\n",
      "Total reward for the episode:  880.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.08\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  86\n",
      "7.1 action:  [-0.0247, -0.9832] n_targets:  2 reward:  160\n",
      "9.6 action:  [-0.0589, -0.9974] n_targets:  2 reward:  160\n",
      "19.6 action:  [0.0067, -0.9908] n_targets:  1 reward:  80\n",
      "21.8 action:  [0.0577, -0.9963] n_targets:  1 reward:  80\n",
      "27.2 action:  [-0.0068, -0.9925] n_targets:  1 reward:  80\n",
      "29.3 action:  [0.0423, -0.9943] n_targets:  1 reward:  80\n",
      "30.9 action:  [-0.0787, -0.9878] n_targets:  1 reward:  80\n",
      "40.2 action:  [-0.0732, -0.9983] n_targets:  3 reward:  240\n",
      "56.6 action:  [-0.0474, -0.9935] n_targets:  1 reward:  80\n",
      "77.0 action:  [0.0523, -0.9987] n_targets:  2 reward:  160\n",
      "87.6 action:  [0.0166, -0.9987] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  16 ff for 102.49999999999845 s: -------------------> 0.16\n",
      "Total reward for the episode:  1280.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.08\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  87\n",
      "5.1 action:  [0.046, -0.9968] n_targets:  2 reward:  160\n",
      "39.4 action:  [-0.0635, -0.9997] n_targets:  1 reward:  80\n",
      "44.1 action:  [-0.0708, -0.9902] n_targets:  1 reward:  80\n",
      "48.8 action:  [-0.0211, -0.9922] n_targets:  1 reward:  80\n",
      "50.4 action:  [-0.0347, -0.9849] n_targets:  2 reward:  160\n",
      "55.3 action:  [-0.028, -0.9954] n_targets:  1 reward:  80\n",
      "62.5 action:  [0.0578, -0.9963] n_targets:  1 reward:  80\n",
      "63.9 action:  [-0.0519, -0.994] n_targets:  1 reward:  80\n",
      "89.1 action:  [0.0211, -0.9965] n_targets:  2 reward:  160\n",
      "90.6 action:  [-0.039, -0.9921] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  13 ff for 102.49999999999845 s: -------------------> 0.13\n",
      "Total reward for the episode:  1040.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.08\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  88\n",
      "32.4 action:  [-0.0571, -0.9984] n_targets:  1 reward:  80\n",
      "39.8 action:  [-0.0261, -0.9805] n_targets:  1 reward:  80\n",
      "47.7 action:  [0.0749, -0.9956] n_targets:  1 reward:  80\n",
      "66.2 action:  [-0.0411, -0.991] n_targets:  1 reward:  80\n",
      "73.1 action:  [-0.006, -0.9944] n_targets:  1 reward:  80\n",
      "96.0 action:  [-0.0484, -0.999] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  6 ff for 102.49999999999845 s: -------------------> 0.06\n",
      "Total reward for the episode:  480.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.08\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  89\n",
      "3.4 action:  [0.0184, -0.9948] n_targets:  2 reward:  160\n",
      "6.9 action:  [0.049, -0.9952] n_targets:  1 reward:  80\n",
      "14.6 action:  [-0.0564, -0.9934] n_targets:  1 reward:  80\n",
      "28.6 action:  [-0.0298, -0.9877] n_targets:  1 reward:  80\n",
      "70.3 action:  [0.0085, -0.999] n_targets:  1 reward:  80\n",
      "76.4 action:  [-0.0379, -0.9866] n_targets:  1 reward:  80\n",
      "79.3 action:  [0.0697, -0.9935] n_targets:  1 reward:  80\n",
      "80.8 action:  [-0.0761, -0.9979] n_targets:  1 reward:  80\n",
      "99.3 action:  [-0.0275, -0.9948] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  10 ff for 102.49999999999845 s: -------------------> 0.1\n",
      "Total reward for the episode:  800.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.08\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  90\n",
      "21.2 action:  [0.052, -0.9842] n_targets:  1 reward:  80\n",
      "44.3 action:  [0.0775, -0.9981] n_targets:  1 reward:  80\n",
      "47.5 action:  [-0.0471, -0.9978] n_targets:  1 reward:  80\n",
      "58.6 action:  [-0.0656, -0.9983] n_targets:  1 reward:  80\n",
      "78.2 action:  [0.0767, -0.9968] n_targets:  1 reward:  80\n",
      "TIME before resetting: 82.49999999999959\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.08\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  91\n",
      "5.6 action:  [0.0447, -0.9893] n_targets:  1 reward:  80\n",
      "53.2 action:  [-0.0095, -0.9955] n_targets:  1 reward:  80\n",
      "56.7 action:  [-0.0054, -0.9935] n_targets:  1 reward:  80\n",
      "67.1 action:  [0.0486, -0.9813] n_targets:  1 reward:  80\n",
      "75.6 action:  [0.037, -0.9924] n_targets:  1 reward:  80\n",
      "77.8 action:  [0.0364, -0.9807] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  6 ff for 102.49999999999845 s: -------------------> 0.06\n",
      "Total reward for the episode:  480.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.08\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  92\n",
      "Eval num_timesteps=24000, episode_reward=480.00 +/- 0.00\n",
      "Episode length: 1025.00 +/- 0.00\n",
      "19.3 action:  [-0.077, -0.9832] n_targets:  1 reward:  80\n",
      "27.8 action:  [0.0594, -0.9972] n_targets:  1 reward:  80\n",
      "30.3 action:  [0.026, -0.9905] n_targets:  1 reward:  80\n",
      "37.8 action:  [-0.0187, -0.9832] n_targets:  2 reward:  160\n",
      "53.6 action:  [0.0069, -0.989] n_targets:  1 reward:  80\n",
      "59.8 action:  [-0.0331, -0.9937] n_targets:  1 reward:  80\n",
      "87.3 action:  [-0.0539, -0.9919] n_targets:  1 reward:  80\n",
      "94.2 action:  [0.0672, -0.9844] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  9 ff for 102.49999999999845 s: -------------------> 0.09\n",
      "Total reward for the episode:  720.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.08\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  93\n",
      "17.3 action:  [0.0498, -0.9933] n_targets:  2 reward:  160\n",
      "42.7 action:  [-0.0121, -0.9837] n_targets:  1 reward:  80\n",
      "59.8 action:  [-0.0695, -0.9996] n_targets:  1 reward:  80\n",
      "71.7 action:  [0.0665, -0.987] n_targets:  2 reward:  160\n",
      "75.7 action:  [-0.0609, -0.9967] n_targets:  2 reward:  160\n",
      "87.3 action:  [-0.0428, -0.9904] n_targets:  1 reward:  80\n",
      "100.8 action:  [0.0188, -0.9905] n_targets:  2 reward:  160\n",
      "Firely capture rate for the episode:  11 ff for 102.49999999999845 s: -------------------> 0.11\n",
      "Total reward for the episode:  880.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.08\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  94\n",
      "5.1 action:  [-0.0395, -0.9936] n_targets:  2 reward:  160\n",
      "28.6 action:  [0.0744, -0.9984] n_targets:  1 reward:  80\n",
      "46.5 action:  [-0.0413, -0.9829] n_targets:  1 reward:  80\n",
      "59.1 action:  [0.0231, -0.9923] n_targets:  1 reward:  80\n",
      "61.5 action:  [0.0264, -0.9878] n_targets:  2 reward:  160\n",
      "87.3 action:  [0.0227, -0.9991] n_targets:  1 reward:  80\n",
      "97.2 action:  [-0.0748, -0.9988] n_targets:  1 reward:  80\n",
      "98.3 action:  [0.0683, -0.998] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  10 ff for 102.49999999999845 s: -------------------> 0.1\n",
      "Total reward for the episode:  800.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.08\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  95\n",
      "6.1 action:  [-0.0575, -0.9974] n_targets:  1 reward:  80\n",
      "26.9 action:  [-0.0054, -0.9908] n_targets:  1 reward:  80\n",
      "28.6 action:  [0.0299, -0.9978] n_targets:  1 reward:  80\n",
      "34.9 action:  [0.0719, -0.9981] n_targets:  1 reward:  80\n",
      "45.3 action:  [-0.043, -0.9988] n_targets:  1 reward:  80\n",
      "45.9 action:  [-0.0327, -0.9868] n_targets:  1 reward:  80\n",
      "48.7 action:  [-0.0524, -0.9958] n_targets:  1 reward:  80\n",
      "50.5 action:  [0.0062, -0.9946] n_targets:  2 reward:  160\n",
      "53.2 action:  [0.046, -0.993] n_targets:  1 reward:  80\n",
      "55.9 action:  [0.0112, -0.9866] n_targets:  1 reward:  80\n",
      "61.4 action:  [-0.0187, -0.9886] n_targets:  1 reward:  80\n",
      "79.5 action:  [0.0484, -0.9979] n_targets:  1 reward:  80\n",
      "97.8 action:  [0.0023, -0.9971] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  14 ff for 102.49999999999845 s: -------------------> 0.14\n",
      "Total reward for the episode:  1120.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.08\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  96\n",
      "7.4 action:  [-0.0468, -0.9924] n_targets:  1 reward:  80\n",
      "19.9 action:  [-0.0652, -0.9974] n_targets:  1 reward:  80\n",
      "25.4 action:  [0.0518, -0.9952] n_targets:  2 reward:  160\n",
      "31.5 action:  [0.0667, -0.9804] n_targets:  1 reward:  80\n",
      "33.4 action:  [0.0498, -0.9945] n_targets:  1 reward:  80\n",
      "38.5 action:  [-0.0437, -1.0] n_targets:  2 reward:  160\n",
      "42.1 action:  [0.0039, -0.994] n_targets:  1 reward:  80\n",
      "52.5 action:  [-0.0173, -0.9943] n_targets:  1 reward:  80\n",
      "63.3 action:  [0.009, -0.9997] n_targets:  1 reward:  80\n",
      "65.7 action:  [-0.0285, -0.9996] n_targets:  2 reward:  160\n",
      "69.8 action:  [-0.0635, -0.9922] n_targets:  1 reward:  80\n",
      "74.7 action:  [-0.0408, -0.9961] n_targets:  1 reward:  80\n",
      "75.9 action:  [-0.0166, -0.9971] n_targets:  1 reward:  80\n",
      "78.0 action:  [-0.0446, -0.9982] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  17 ff for 102.49999999999845 s: -------------------> 0.17\n",
      "Total reward for the episode:  1360.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.08\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  97\n",
      "14.2 action:  [0.0552, -0.9997] n_targets:  1 reward:  80\n",
      "32.5 action:  [0.0352, -0.9959] n_targets:  1 reward:  80\n",
      "38.1 action:  [0.063, -0.9885] n_targets:  1 reward:  80\n",
      "41.0 action:  [0.0437, -0.9869] n_targets:  1 reward:  80\n",
      "43.9 action:  [0.0065, -0.9952] n_targets:  1 reward:  80\n",
      "47.9 action:  [0.0354, -0.9951] n_targets:  1 reward:  80\n",
      "56.0 action:  [-0.041, -0.9809] n_targets:  1 reward:  80\n",
      "64.0 action:  [0.0526, -0.9882] n_targets:  1 reward:  80\n",
      "65.2 action:  [0.0429, -0.9959] n_targets:  1 reward:  80\n",
      "83.0 action:  [0.0008, -0.9973] n_targets:  1 reward:  80\n",
      "91.8 action:  [-0.0646, -0.9858] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  11 ff for 102.49999999999845 s: -------------------> 0.11\n",
      "Total reward for the episode:  880.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.08\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  98\n",
      "11.9 action:  [0.0518, -0.9957] n_targets:  1 reward:  80\n",
      "30.8 action:  [0.0125, -0.9994] n_targets:  1 reward:  80\n",
      "33.0 action:  [-0.0651, -0.9995] n_targets:  1 reward:  80\n",
      "35.7 action:  [0.0282, -0.9887] n_targets:  1 reward:  80\n",
      "38.6 action:  [0.0406, -0.9831] n_targets:  1 reward:  80\n",
      "42.8 action:  [0.0011, -0.9995] n_targets:  2 reward:  160\n",
      "58.9 action:  [-0.0293, -0.9829] n_targets:  1 reward:  80\n",
      "65.5 action:  [0.0736, -0.9827] n_targets:  1 reward:  80\n",
      "71.2 action:  [-0.0265, -0.9942] n_targets:  1 reward:  80\n",
      "84.3 action:  [0.0579, -0.9933] n_targets:  1 reward:  80\n",
      "92.9 action:  [-0.0406, -0.999] n_targets:  1 reward:  80\n",
      "102.5 action:  [-0.0582, -0.9917] n_targets:  2 reward:  160\n",
      "Firely capture rate for the episode:  14 ff for 102.49999999999845 s: -------------------> 0.14\n",
      "Total reward for the episode:  1120.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.08\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  99\n",
      "19.1 action:  [0.0494, -0.9933] n_targets:  1 reward:  80\n",
      "29.7 action:  [0.0177, -0.9889] n_targets:  1 reward:  80\n",
      "42.7 action:  [0.0448, -0.9977] n_targets:  1 reward:  80\n",
      "45.8 action:  [-0.0645, -0.9976] n_targets:  1 reward:  80\n",
      "57.3 action:  [0.0773, -0.9982] n_targets:  1 reward:  80\n",
      "68.0 action:  [0.0183, -0.999] n_targets:  1 reward:  80\n",
      "69.6 action:  [0.0517, -0.9909] n_targets:  1 reward:  80\n",
      "80.4 action:  [-0.0059, -0.9997] n_targets:  2 reward:  160\n",
      "TIME before resetting: 82.49999999999959\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.08\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  100\n",
      "53.2 action:  [0.0378, -0.9838] n_targets:  1 reward:  80\n",
      "55.8 action:  [-0.0174, -0.9879] n_targets:  1 reward:  80\n",
      "71.7 action:  [0.0689, -0.9926] n_targets:  1 reward:  80\n",
      "96.9 action:  [0.0322, -0.983] n_targets:  1 reward:  80\n",
      "99.0 action:  [0.0413, -0.9871] n_targets:  1 reward:  80\n",
      "99.9 action:  [-0.0422, -0.9988] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  6 ff for 102.49999999999845 s: -------------------> 0.06\n",
      "Total reward for the episode:  480.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.08\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  101\n",
      "Eval num_timesteps=32000, episode_reward=480.00 +/- 0.00\n",
      "Episode length: 1025.00 +/- 0.00\n",
      "6.6 action:  [-0.0328, -0.9993] n_targets:  1 reward:  80\n",
      "15.2 action:  [0.0661, -0.9983] n_targets:  1 reward:  80\n",
      "33.4 action:  [-0.0, -0.9955] n_targets:  2 reward:  160\n",
      "38.8 action:  [-0.0718, -0.9901] n_targets:  1 reward:  80\n",
      "39.9 action:  [-0.0325, -0.9852] n_targets:  1 reward:  80\n",
      "45.8 action:  [-0.0526, -0.9845] n_targets:  1 reward:  80\n",
      "50.3 action:  [-0.0635, -0.9984] n_targets:  1 reward:  80\n",
      "65.7 action:  [-0.0631, -0.9869] n_targets:  1 reward:  80\n",
      "73.9 action:  [-0.0418, -0.9966] n_targets:  2 reward:  160\n",
      "92.4 action:  [-0.0293, -0.9955] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  12 ff for 102.49999999999845 s: -------------------> 0.12\n",
      "Total reward for the episode:  960.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.08\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  102\n",
      "11.8 action:  [0.0202, -0.9931] n_targets:  1 reward:  80\n",
      "16.6 action:  [-0.0515, -0.9892] n_targets:  1 reward:  80\n",
      "17.5 action:  [-0.0415, -0.9903] n_targets:  1 reward:  80\n",
      "57.5 action:  [0.006, -0.9959] n_targets:  1 reward:  80\n",
      "71.8 action:  [-0.0017, -0.9914] n_targets:  1 reward:  80\n",
      "73.2 action:  [0.0563, -0.9986] n_targets:  1 reward:  80\n",
      "74.5 action:  [-0.0045, -0.9899] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  7 ff for 102.49999999999845 s: -------------------> 0.07\n",
      "Total reward for the episode:  560.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.08\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  103\n",
      "12.8 action:  [0.062, -0.9954] n_targets:  2 reward:  160\n",
      "18.6 action:  [-0.0644, -0.9875] n_targets:  1 reward:  80\n",
      "41.4 action:  [0.0696, -0.9968] n_targets:  1 reward:  80\n",
      "52.0 action:  [-0.0674, -0.9922] n_targets:  1 reward:  80\n",
      "70.7 action:  [0.0262, -0.9997] n_targets:  1 reward:  80\n",
      "79.7 action:  [-0.0402, -0.9997] n_targets:  2 reward:  160\n",
      "91.2 action:  [0.0135, -0.9928] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  9 ff for 102.49999999999845 s: -------------------> 0.09\n",
      "Total reward for the episode:  720.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.08\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  104\n",
      "5.9 action:  [-0.0489, -0.996] n_targets:  1 reward:  80\n",
      "14.2 action:  [-0.0068, -0.9995] n_targets:  1 reward:  80\n",
      "24.1 action:  [0.0041, -0.9967] n_targets:  2 reward:  160\n",
      "37.8 action:  [0.0345, -0.9944] n_targets:  1 reward:  80\n",
      "41.6 action:  [-0.0468, -0.9952] n_targets:  1 reward:  80\n",
      "52.5 action:  [0.0783, -0.9995] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  7 ff for 102.49999999999845 s: -------------------> 0.07\n",
      "Total reward for the episode:  560.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.08\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  105\n",
      "12.4 action:  [-0.0432, -0.9977] n_targets:  1 reward:  80\n",
      "13.1 action:  [-0.0757, -0.9917] n_targets:  1 reward:  80\n",
      "40.4 action:  [-0.0045, -0.9984] n_targets:  1 reward:  80\n",
      "49.5 action:  [-0.0644, -0.9953] n_targets:  1 reward:  80\n",
      "58.5 action:  [-0.0129, -0.9985] n_targets:  1 reward:  80\n",
      "78.1 action:  [-0.0484, -1.0] n_targets:  1 reward:  80\n",
      "81.5 action:  [-0.05, -0.9845] n_targets:  1 reward:  80\n",
      "84.1 action:  [0.0659, -0.985] n_targets:  1 reward:  80\n",
      "87.3 action:  [-0.0227, -0.9993] n_targets:  1 reward:  80\n",
      "95.4 action:  [-0.0598, -0.9976] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  10 ff for 102.49999999999845 s: -------------------> 0.1\n",
      "Total reward for the episode:  800.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.08\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  106\n",
      "7.2 action:  [-0.0368, -0.9908] n_targets:  1 reward:  80\n",
      "10.1 action:  [-0.0653, -0.9826] n_targets:  1 reward:  80\n",
      "19.9 action:  [-0.0627, -1.0] n_targets:  1 reward:  80\n",
      "22.6 action:  [-0.0371, -0.9937] n_targets:  1 reward:  80\n",
      "29.3 action:  [0.0198, -0.9983] n_targets:  1 reward:  80\n",
      "31.2 action:  [-0.0143, -0.9997] n_targets:  1 reward:  80\n",
      "41.2 action:  [0.0362, -0.9997] n_targets:  1 reward:  80\n",
      "48.2 action:  [-0.0697, -0.9958] n_targets:  1 reward:  80\n",
      "65.0 action:  [-0.0049, -0.9989] n_targets:  1 reward:  80\n",
      "86.3 action:  [0.0449, -0.9897] n_targets:  1 reward:  80\n",
      "86.6 action:  [-0.0365, -0.9963] n_targets:  1 reward:  80\n",
      "88.9 action:  [0.0009, -0.9885] n_targets:  1 reward:  80\n",
      "92.3 action:  [-0.0065, -0.9993] n_targets:  1 reward:  80\n",
      "99.5 action:  [-0.0257, -0.9973] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  14 ff for 102.49999999999845 s: -------------------> 0.14\n",
      "Total reward for the episode:  1120.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.08\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  107\n",
      "5.1 action:  [-0.0594, -0.9995] n_targets:  1 reward:  80\n",
      "28.2 action:  [-0.0448, -0.9999] n_targets:  1 reward:  80\n",
      "36.7 action:  [0.0749, -0.9997] n_targets:  1 reward:  80\n",
      "68.5 action:  [-0.0749, -0.9889] n_targets:  1 reward:  80\n",
      "77.0 action:  [0.0123, -0.9947] n_targets:  1 reward:  80\n",
      "80.8 action:  [-0.0341, -0.9994] n_targets:  1 reward:  80\n",
      "82.6 action:  [-0.0067, -0.9968] n_targets:  1 reward:  80\n",
      "94.5 action:  [-0.0465, -0.9932] n_targets:  1 reward:  80\n",
      "101.8 action:  [0.0311, -0.9971] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  9 ff for 102.49999999999845 s: -------------------> 0.09\n",
      "Total reward for the episode:  720.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.08\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  108\n",
      "5.5 action:  [0.0539, -0.9962] n_targets:  1 reward:  80\n",
      "6.2 action:  [0.0334, -0.9932] n_targets:  1 reward:  80\n",
      "7.9 action:  [0.0347, -1.0] n_targets:  3 reward:  240\n",
      "24.3 action:  [-0.0032, -0.9998] n_targets:  1 reward:  80\n",
      "46.5 action:  [-0.0791, -0.9962] n_targets:  1 reward:  80\n",
      "48.7 action:  [-0.0623, -0.9905] n_targets:  1 reward:  80\n",
      "53.5 action:  [-0.0348, -0.9973] n_targets:  1 reward:  80\n",
      "67.4 action:  [-0.0381, -0.9898] n_targets:  1 reward:  80\n",
      "71.7 action:  [-0.0313, -0.9805] n_targets:  1 reward:  80\n",
      "73.8 action:  [-0.0643, -0.9938] n_targets:  2 reward:  160\n",
      "80.6 action:  [0.0658, -0.9856] n_targets:  1 reward:  80\n",
      "TIME before resetting: 82.49999999999959\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.08\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  109\n",
      "3.8 action:  [0.0271, -0.9979] n_targets:  1 reward:  80\n",
      "15.7 action:  [-0.0208, -0.9999] n_targets:  2 reward:  160\n",
      "26.3 action:  [0.0191, -0.9806] n_targets:  1 reward:  80\n",
      "41.1 action:  [0.0549, -0.9998] n_targets:  1 reward:  80\n",
      "68.6 action:  [-0.0534, -0.9941] n_targets:  1 reward:  80\n",
      "75.5 action:  [-0.0115, -0.9996] n_targets:  2 reward:  160\n",
      "79.9 action:  [-0.0067, -0.985] n_targets:  1 reward:  80\n",
      "84.5 action:  [0.0482, -0.9961] n_targets:  2 reward:  160\n",
      "86.7 action:  [-0.0771, -0.9974] n_targets:  1 reward:  80\n",
      "90.5 action:  [0.0795, -0.9909] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  13 ff for 102.49999999999845 s: -------------------> 0.13\n",
      "Total reward for the episode:  1040.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.08\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  110\n",
      "Eval num_timesteps=40000, episode_reward=1040.00 +/- 0.00\n",
      "Episode length: 1025.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Saved agent: RL_models/SB3_stored_models/all_agents/env1_relu/best_model_after_curriculum/best_model\n",
      "Saved replay buffer: RL_models/SB3_stored_models/all_agents/env1_relu/best_model_after_curriculum/buffer\n",
      "Current dt: 0.1\n",
      "Current gamma: 0.998\n",
      "Current angular_terminal_vel: 0.04\n",
      "reward_threshold: 819.2000000000002\n",
      "TIME before resetting: 0\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.04\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  111\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  0.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.04\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  112\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  0.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.04\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  113\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  0.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.04\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  114\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  0.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.04\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  115\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  0.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.04\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  116\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  0.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.04\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  117\n",
      "39.7 action:  [-0.0322, -0.9943] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  1 ff for 102.49999999999845 s: -------------------> 0.01\n",
      "Total reward for the episode:  80.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.04\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  118\n",
      "TIME before resetting: 82.49999999999959\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.04\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  119\n",
      "8.1 action:  [-0.0254, -0.9982] n_targets:  1 reward:  80\n",
      "28.7 action:  [-0.0378, -1.0] n_targets:  1 reward:  80\n",
      "41.5 action:  [-0.0066, -0.998] n_targets:  1 reward:  80\n",
      "47.6 action:  [-0.0122, -0.9957] n_targets:  1 reward:  80\n",
      "53.8 action:  [0.0268, -0.9985] n_targets:  1 reward:  80\n",
      "60.4 action:  [0.0047, -0.9855] n_targets:  2 reward:  160\n",
      "73.1 action:  [-0.0355, -0.9986] n_targets:  2 reward:  160\n",
      "77.2 action:  [0.0311, -0.9952] n_targets:  1 reward:  80\n",
      "80.9 action:  [-0.0346, -0.9916] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  11 ff for 102.49999999999845 s: -------------------> 0.11\n",
      "Total reward for the episode:  880.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.04\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  120\n",
      "Eval num_timesteps=8000, episode_reward=880.00 +/- 0.00\n",
      "Episode length: 1025.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Saved agent: RL_models/SB3_stored_models/all_agents/env1_relu/best_model_after_curriculum/best_model\n",
      "Saved replay buffer: RL_models/SB3_stored_models/all_agents/env1_relu/best_model_after_curriculum/buffer\n",
      "Current dt: 0.1\n",
      "Current gamma: 0.998\n",
      "Current angular_terminal_vel: 0.02\n",
      "reward_threshold: 819.2000000000002\n",
      "TIME before resetting: 0\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  121\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  0.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  122\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  0.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  123\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  0.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  124\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  0.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  125\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  0.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  126\n",
      "45.3 action:  [0.0049, -0.9961] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  1 ff for 102.49999999999845 s: -------------------> 0.01\n",
      "Total reward for the episode:  80.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  127\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  0.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  128\n",
      "TIME before resetting: 82.49999999999959\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  129\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  0.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  130\n",
      "Eval num_timesteps=8000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 1025.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  0.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  131\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  0.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  132\n",
      "46.8 action:  [-0.0138, -0.994] n_targets:  1 reward:  80\n",
      "60.4 action:  [0.0122, -0.9873] n_targets:  1 reward:  80\n",
      "63.4 action:  [-0.0126, -0.9994] n_targets:  1 reward:  80\n",
      "80.3 action:  [-0.0114, -0.9991] n_targets:  1 reward:  80\n",
      "84.7 action:  [0.0085, -0.9959] n_targets:  1 reward:  80\n",
      "87.3 action:  [0.0195, -0.9915] n_targets:  2 reward:  160\n",
      "91.1 action:  [0.0046, -0.9999] n_targets:  1 reward:  80\n",
      "95.2 action:  [-0.0152, -0.9934] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  9 ff for 102.49999999999845 s: -------------------> 0.09\n",
      "Total reward for the episode:  720.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  133\n",
      "4.6 action:  [-0.0094, -0.9983] n_targets:  1 reward:  80\n",
      "20.8 action:  [0.0031, -0.9996] n_targets:  1 reward:  80\n",
      "65.0 action:  [-0.0062, -0.9997] n_targets:  1 reward:  80\n",
      "76.7 action:  [0.0076, -0.9984] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  4 ff for 102.49999999999845 s: -------------------> 0.04\n",
      "Total reward for the episode:  320.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  134\n",
      "10.4 action:  [0.0192, -0.995] n_targets:  1 reward:  80\n",
      "62.9 action:  [0.0002, -0.982] n_targets:  2 reward:  160\n",
      "66.3 action:  [0.0164, -0.9989] n_targets:  1 reward:  80\n",
      "75.5 action:  [-0.0189, -0.9889] n_targets:  1 reward:  80\n",
      "77.9 action:  [-0.0117, -0.9878] n_targets:  1 reward:  80\n",
      "99.2 action:  [-0.0193, -0.9913] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  7 ff for 102.49999999999845 s: -------------------> 0.07\n",
      "Total reward for the episode:  560.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  135\n",
      "8.6 action:  [-0.003, -0.9917] n_targets:  1 reward:  80\n",
      "19.9 action:  [0.009, -0.9913] n_targets:  1 reward:  80\n",
      "23.5 action:  [-0.015, -0.9999] n_targets:  1 reward:  80\n",
      "95.9 action:  [-0.0122, -0.9975] n_targets:  1 reward:  80\n",
      "101.6 action:  [-0.0013, -0.9992] n_targets:  1 reward:  80\n",
      "102.4 action:  [-0.0047, -0.9992] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  6 ff for 102.49999999999845 s: -------------------> 0.06\n",
      "Total reward for the episode:  480.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  136\n",
      "30.6 action:  [0.0189, -0.9924] n_targets:  1 reward:  80\n",
      "69.5 action:  [-0.0023, -0.999] n_targets:  1 reward:  80\n",
      "79.9 action:  [0.0097, -0.9999] n_targets:  1 reward:  80\n",
      "92.2 action:  [0.0195, -0.995] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  4 ff for 102.49999999999845 s: -------------------> 0.04\n",
      "Total reward for the episode:  320.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  137\n",
      "2.1 action:  [-0.0149, -0.9943] n_targets:  1 reward:  80\n",
      "32.7 action:  [0.0085, -0.9974] n_targets:  1 reward:  80\n",
      "54.4 action:  [0.019, -1.0] n_targets:  1 reward:  80\n",
      "64.9 action:  [0.0092, -0.9999] n_targets:  1 reward:  80\n",
      "79.7 action:  [0.0197, -0.9988] n_targets:  1 reward:  80\n",
      "81.7 action:  [-0.0073, -0.9995] n_targets:  1 reward:  80\n",
      "TIME before resetting: 82.49999999999959\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  138\n",
      "16.2 action:  [0.0102, -0.9997] n_targets:  1 reward:  80\n",
      "52.2 action:  [0.0173, -0.9991] n_targets:  1 reward:  80\n",
      "64.2 action:  [-0.0044, -0.9943] n_targets:  1 reward:  80\n",
      "70.3 action:  [-0.0015, -0.9958] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  4 ff for 102.49999999999845 s: -------------------> 0.04\n",
      "Total reward for the episode:  320.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  139\n",
      "Eval num_timesteps=16000, episode_reward=320.00 +/- 0.00\n",
      "Episode length: 1025.00 +/- 0.00\n",
      "New best mean reward!\n",
      "3.3 action:  [0.0021, -0.9995] n_targets:  1 reward:  80\n",
      "10.6 action:  [0.0048, -0.9936] n_targets:  1 reward:  80\n",
      "12.3 action:  [0.0051, -0.9974] n_targets:  1 reward:  80\n",
      "22.7 action:  [0.0169, -0.9986] n_targets:  2 reward:  160\n",
      "29.8 action:  [-0.0171, -0.9974] n_targets:  1 reward:  80\n",
      "34.9 action:  [-0.0143, -0.999] n_targets:  1 reward:  80\n",
      "75.8 action:  [0.0063, -0.9843] n_targets:  1 reward:  80\n",
      "82.6 action:  [0.0006, -0.9977] n_targets:  2 reward:  160\n",
      "97.6 action:  [-0.0045, -0.998] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  11 ff for 102.49999999999845 s: -------------------> 0.11\n",
      "Total reward for the episode:  880.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  140\n",
      "2.6 action:  [-0.0175, -0.9953] n_targets:  1 reward:  80\n",
      "60.8 action:  [-0.0097, -0.9996] n_targets:  1 reward:  80\n",
      "76.6 action:  [-0.0181, -0.9997] n_targets:  1 reward:  80\n",
      "79.8 action:  [0.0114, -0.9961] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  4 ff for 102.49999999999845 s: -------------------> 0.04\n",
      "Total reward for the episode:  320.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  141\n",
      "7.0 action:  [-0.0104, -0.9868] n_targets:  1 reward:  80\n",
      "17.7 action:  [0.0196, -0.9947] n_targets:  1 reward:  80\n",
      "19.5 action:  [-0.0166, -1.0] n_targets:  1 reward:  80\n",
      "21.8 action:  [-0.0192, -0.9976] n_targets:  2 reward:  160\n",
      "39.1 action:  [0.0007, -0.9908] n_targets:  1 reward:  80\n",
      "44.3 action:  [0.0019, -0.9988] n_targets:  1 reward:  80\n",
      "48.4 action:  [0.0004, -0.9997] n_targets:  1 reward:  80\n",
      "50.9 action:  [-0.0022, -0.9979] n_targets:  1 reward:  80\n",
      "62.5 action:  [-0.0119, -0.995] n_targets:  1 reward:  80\n",
      "69.1 action:  [0.0181, -0.9989] n_targets:  1 reward:  80\n",
      "73.4 action:  [0.0086, -1.0] n_targets:  1 reward:  80\n",
      "88.3 action:  [-0.0161, -0.9999] n_targets:  1 reward:  80\n",
      "91.5 action:  [-0.0051, -0.981] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  14 ff for 102.49999999999845 s: -------------------> 0.14\n",
      "Total reward for the episode:  1120.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  142\n",
      "7.0 action:  [0.0074, -0.9819] n_targets:  1 reward:  80\n",
      "47.6 action:  [-0.0101, -0.9992] n_targets:  1 reward:  80\n",
      "66.1 action:  [0.0166, -0.9998] n_targets:  1 reward:  80\n",
      "69.0 action:  [0.0004, -0.9963] n_targets:  1 reward:  80\n",
      "88.9 action:  [-0.0002, -0.9981] n_targets:  2 reward:  160\n",
      "Firely capture rate for the episode:  6 ff for 102.49999999999845 s: -------------------> 0.06\n",
      "Total reward for the episode:  480.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  143\n",
      "4.7 action:  [-0.013, -0.9994] n_targets:  1 reward:  80\n",
      "7.1 action:  [0.0111, -0.9999] n_targets:  1 reward:  80\n",
      "33.4 action:  [-0.0089, -1.0] n_targets:  1 reward:  80\n",
      "49.0 action:  [0.0103, -0.9982] n_targets:  1 reward:  80\n",
      "56.4 action:  [0.0029, -0.9996] n_targets:  1 reward:  80\n",
      "61.4 action:  [0.0066, -0.9995] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  6 ff for 102.49999999999845 s: -------------------> 0.06\n",
      "Total reward for the episode:  480.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  144\n",
      "23.2 action:  [-0.007, -0.9826] n_targets:  1 reward:  80\n",
      "26.1 action:  [0.0005, -0.996] n_targets:  1 reward:  80\n",
      "29.8 action:  [-0.0017, -0.9992] n_targets:  1 reward:  80\n",
      "44.1 action:  [-0.0073, -0.9999] n_targets:  1 reward:  80\n",
      "63.0 action:  [-0.0008, -0.9981] n_targets:  1 reward:  80\n",
      "68.9 action:  [-0.0147, -0.9981] n_targets:  1 reward:  80\n",
      "82.1 action:  [-0.0198, -0.9944] n_targets:  1 reward:  80\n",
      "86.3 action:  [-0.0066, -0.9994] n_targets:  1 reward:  80\n",
      "100.3 action:  [-0.0127, -0.9806] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  9 ff for 102.49999999999845 s: -------------------> 0.09\n",
      "Total reward for the episode:  720.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  145\n",
      "3.2 action:  [-0.0004, -0.9978] n_targets:  1 reward:  80\n",
      "14.9 action:  [-0.003, -0.9987] n_targets:  1 reward:  80\n",
      "52.4 action:  [-0.0027, -0.9997] n_targets:  1 reward:  80\n",
      "53.1 action:  [0.0115, -0.9948] n_targets:  1 reward:  80\n",
      "54.4 action:  [-0.0064, -1.0] n_targets:  1 reward:  80\n",
      "61.2 action:  [0.0109, -0.9966] n_targets:  1 reward:  80\n",
      "74.4 action:  [-0.0031, -0.9994] n_targets:  1 reward:  80\n",
      "83.2 action:  [-0.0198, -0.9878] n_targets:  1 reward:  80\n",
      "100.7 action:  [0.0148, -0.989] n_targets:  2 reward:  160\n",
      "Firely capture rate for the episode:  10 ff for 102.49999999999845 s: -------------------> 0.1\n",
      "Total reward for the episode:  800.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  146\n",
      "22.4 action:  [0.0158, -0.999] n_targets:  1 reward:  80\n",
      "54.3 action:  [0.0157, -0.9979] n_targets:  1 reward:  80\n",
      "58.9 action:  [0.0192, -0.9999] n_targets:  1 reward:  80\n",
      "TIME before resetting: 82.49999999999959\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  147\n",
      "13.0 action:  [-0.0121, -0.9928] n_targets:  1 reward:  80\n",
      "43.0 action:  [-0.008, -0.9971] n_targets:  1 reward:  80\n",
      "73.4 action:  [-0.0145, -0.9997] n_targets:  1 reward:  80\n",
      "78.4 action:  [0.0136, -0.9953] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  4 ff for 102.49999999999845 s: -------------------> 0.04\n",
      "Total reward for the episode:  320.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  148\n",
      "Eval num_timesteps=24000, episode_reward=320.00 +/- 0.00\n",
      "Episode length: 1025.00 +/- 0.00\n",
      "2.4 action:  [0.0095, -0.9994] n_targets:  1 reward:  80\n",
      "9.9 action:  [0.002, -0.9989] n_targets:  1 reward:  80\n",
      "17.2 action:  [0.0193, -0.9966] n_targets:  1 reward:  80\n",
      "32.7 action:  [0.0131, -0.9994] n_targets:  1 reward:  80\n",
      "37.2 action:  [-0.0077, -0.9961] n_targets:  1 reward:  80\n",
      "50.5 action:  [-0.0164, -0.9927] n_targets:  1 reward:  80\n",
      "52.0 action:  [0.0109, -0.9922] n_targets:  1 reward:  80\n",
      "53.2 action:  [0.0113, -0.9933] n_targets:  1 reward:  80\n",
      "64.7 action:  [0.002, -0.9991] n_targets:  1 reward:  80\n",
      "89.2 action:  [0.0164, -0.9955] n_targets:  1 reward:  80\n",
      "93.9 action:  [-0.0011, -0.9964] n_targets:  1 reward:  80\n",
      "95.8 action:  [-0.0005, -0.9908] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  12 ff for 102.49999999999845 s: -------------------> 0.12\n",
      "Total reward for the episode:  960.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  149\n",
      "13.6 action:  [0.0151, -0.9848] n_targets:  1 reward:  80\n",
      "27.8 action:  [0.0079, -0.9993] n_targets:  1 reward:  80\n",
      "33.0 action:  [-0.007, -0.9995] n_targets:  1 reward:  80\n",
      "40.3 action:  [0.0148, -1.0] n_targets:  2 reward:  160\n",
      "43.5 action:  [0.0023, -0.9949] n_targets:  1 reward:  80\n",
      "57.6 action:  [0.0191, -0.9933] n_targets:  1 reward:  80\n",
      "63.7 action:  [0.0084, -0.9957] n_targets:  1 reward:  80\n",
      "73.6 action:  [-0.006, -0.9917] n_targets:  1 reward:  80\n",
      "87.5 action:  [-0.0084, -0.9998] n_targets:  1 reward:  80\n",
      "99.9 action:  [0.0065, -0.9998] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  11 ff for 102.49999999999845 s: -------------------> 0.11\n",
      "Total reward for the episode:  880.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  150\n",
      "13.4 action:  [0.0175, -0.9958] n_targets:  1 reward:  80\n",
      "16.4 action:  [0.0114, -0.9981] n_targets:  1 reward:  80\n",
      "29.3 action:  [-0.0006, -0.9822] n_targets:  1 reward:  80\n",
      "41.5 action:  [-0.0019, -0.9993] n_targets:  1 reward:  80\n",
      "50.4 action:  [0.0004, -0.9999] n_targets:  1 reward:  80\n",
      "63.2 action:  [-0.007, -0.998] n_targets:  1 reward:  80\n",
      "67.4 action:  [0.0071, -0.9993] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  7 ff for 102.49999999999845 s: -------------------> 0.07\n",
      "Total reward for the episode:  560.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  151\n",
      "6.3 action:  [0.0086, -0.9991] n_targets:  2 reward:  160\n",
      "21.2 action:  [-0.0183, -0.9871] n_targets:  1 reward:  80\n",
      "25.4 action:  [0.0118, -0.9811] n_targets:  1 reward:  80\n",
      "40.8 action:  [-0.0138, -0.9985] n_targets:  1 reward:  80\n",
      "49.6 action:  [-0.0055, -0.9969] n_targets:  1 reward:  80\n",
      "54.5 action:  [0.0119, -0.9998] n_targets:  1 reward:  80\n",
      "56.6 action:  [-0.0173, -0.9973] n_targets:  1 reward:  80\n",
      "81.7 action:  [0.0011, -0.9949] n_targets:  1 reward:  80\n",
      "97.5 action:  [-0.0166, -0.9994] n_targets:  1 reward:  80\n",
      "99.1 action:  [-0.0093, -0.9946] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  11 ff for 102.49999999999845 s: -------------------> 0.11\n",
      "Total reward for the episode:  880.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  152\n",
      "3.2 action:  [0.0098, -0.9823] n_targets:  1 reward:  80\n",
      "7.7 action:  [0.009, -0.9996] n_targets:  1 reward:  80\n",
      "35.5 action:  [0.0005, -0.9986] n_targets:  1 reward:  80\n",
      "43.4 action:  [-0.0068, -0.9804] n_targets:  1 reward:  80\n",
      "50.2 action:  [0.0198, -0.9983] n_targets:  1 reward:  80\n",
      "86.3 action:  [0.0102, -0.9982] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  6 ff for 102.49999999999845 s: -------------------> 0.06\n",
      "Total reward for the episode:  480.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  153\n",
      "3.6 action:  [-0.0052, -0.9959] n_targets:  1 reward:  80\n",
      "5.7 action:  [-0.0089, -0.9917] n_targets:  1 reward:  80\n",
      "10.3 action:  [-0.0005, -0.9999] n_targets:  1 reward:  80\n",
      "17.0 action:  [-0.0121, -0.9912] n_targets:  1 reward:  80\n",
      "35.7 action:  [0.0185, -0.995] n_targets:  1 reward:  80\n",
      "39.3 action:  [0.0196, -0.996] n_targets:  1 reward:  80\n",
      "48.1 action:  [0.0083, -0.9995] n_targets:  1 reward:  80\n",
      "56.4 action:  [0.0059, -0.9814] n_targets:  1 reward:  80\n",
      "57.6 action:  [0.0189, -0.9995] n_targets:  1 reward:  80\n",
      "60.3 action:  [-0.0099, -0.992] n_targets:  1 reward:  80\n",
      "71.9 action:  [-0.0039, -0.997] n_targets:  1 reward:  80\n",
      "94.2 action:  [0.0115, -0.999] n_targets:  1 reward:  80\n",
      "97.5 action:  [0.0177, -0.9962] n_targets:  1 reward:  80\n",
      "100.2 action:  [-0.0168, -0.9978] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  14 ff for 102.49999999999845 s: -------------------> 0.14\n",
      "Total reward for the episode:  1120.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  154\n",
      "5.9 action:  [0.0189, -0.997] n_targets:  1 reward:  80\n",
      "38.1 action:  [0.0131, -0.9999] n_targets:  2 reward:  160\n",
      "42.6 action:  [0.0071, -0.9986] n_targets:  1 reward:  80\n",
      "62.3 action:  [0.0083, -0.9995] n_targets:  1 reward:  80\n",
      "70.0 action:  [-0.0023, -0.9952] n_targets:  1 reward:  80\n",
      "73.8 action:  [0.0094, -0.9996] n_targets:  1 reward:  80\n",
      "75.4 action:  [0.0059, -0.9996] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  8 ff for 102.49999999999845 s: -------------------> 0.08\n",
      "Total reward for the episode:  640.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  155\n",
      "23.6 action:  [0.0027, -0.9997] n_targets:  1 reward:  80\n",
      "45.1 action:  [0.0122, -0.984] n_targets:  2 reward:  160\n",
      "61.5 action:  [-0.0167, -0.9933] n_targets:  1 reward:  80\n",
      "TIME before resetting: 82.49999999999959\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  156\n",
      "64.0 action:  [-0.0088, -0.9987] n_targets:  2 reward:  160\n",
      "77.1 action:  [0.0138, -0.995] n_targets:  2 reward:  160\n",
      "96.7 action:  [-0.0105, -0.9973] n_targets:  1 reward:  80\n",
      "99.7 action:  [0.0022, -0.9979] n_targets:  1 reward:  80\n",
      "101.5 action:  [0.0097, -0.9967] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  7 ff for 102.49999999999845 s: -------------------> 0.07\n",
      "Total reward for the episode:  560.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  157\n",
      "Eval num_timesteps=32000, episode_reward=560.00 +/- 0.00\n",
      "Episode length: 1025.00 +/- 0.00\n",
      "New best mean reward!\n",
      "22.6 action:  [0.0185, -0.9968] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  1 ff for 102.49999999999845 s: -------------------> 0.01\n",
      "Total reward for the episode:  80.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  158\n",
      "26.6 action:  [-0.0187, -0.993] n_targets:  1 reward:  80\n",
      "27.4 action:  [-0.0166, -0.9953] n_targets:  1 reward:  80\n",
      "28.7 action:  [-0.0129, -0.9843] n_targets:  1 reward:  80\n",
      "31.5 action:  [0.0108, -0.9892] n_targets:  1 reward:  80\n",
      "34.9 action:  [-0.0104, -0.9984] n_targets:  1 reward:  80\n",
      "53.0 action:  [0.018, -1.0] n_targets:  1 reward:  80\n",
      "61.3 action:  [-0.0012, -0.9907] n_targets:  1 reward:  80\n",
      "75.4 action:  [-0.0171, -0.9857] n_targets:  1 reward:  80\n",
      "91.6 action:  [0.0092, -0.999] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  9 ff for 102.49999999999845 s: -------------------> 0.09\n",
      "Total reward for the episode:  720.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  159\n",
      "10.8 action:  [0.015, -0.9992] n_targets:  1 reward:  80\n",
      "18.9 action:  [-0.0101, -0.9906] n_targets:  1 reward:  80\n",
      "23.1 action:  [-0.0123, -0.9932] n_targets:  1 reward:  80\n",
      "30.5 action:  [0.0126, -0.9823] n_targets:  1 reward:  80\n",
      "34.2 action:  [-0.0177, -0.997] n_targets:  1 reward:  80\n",
      "54.4 action:  [-0.0069, -0.9985] n_targets:  1 reward:  80\n",
      "62.6 action:  [-0.0194, -0.9993] n_targets:  1 reward:  80\n",
      "70.4 action:  [-0.0105, -0.9963] n_targets:  1 reward:  80\n",
      "76.8 action:  [0.0059, -0.9989] n_targets:  1 reward:  80\n",
      "90.8 action:  [-0.0054, -0.9954] n_targets:  1 reward:  80\n",
      "91.1 action:  [0.0077, -0.9984] n_targets:  1 reward:  80\n",
      "95.5 action:  [0.018, -0.9976] n_targets:  2 reward:  160\n",
      "Firely capture rate for the episode:  13 ff for 102.49999999999845 s: -------------------> 0.13\n",
      "Total reward for the episode:  1040.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  160\n",
      "2.6 action:  [-0.006, -0.9975] n_targets:  1 reward:  80\n",
      "8.2 action:  [-0.0002, -0.9841] n_targets:  1 reward:  80\n",
      "14.7 action:  [0.0059, -0.997] n_targets:  1 reward:  80\n",
      "17.9 action:  [0.0177, -0.9949] n_targets:  3 reward:  240\n",
      "37.1 action:  [-0.0142, -1.0] n_targets:  1 reward:  80\n",
      "70.5 action:  [-0.0002, -0.9917] n_targets:  1 reward:  80\n",
      "75.9 action:  [0.0089, -0.9977] n_targets:  1 reward:  80\n",
      "86.6 action:  [-0.0167, -0.9939] n_targets:  1 reward:  80\n",
      "90.9 action:  [0.0115, -0.9956] n_targets:  1 reward:  80\n",
      "101.2 action:  [0.0152, -0.9906] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  12 ff for 102.49999999999845 s: -------------------> 0.12\n",
      "Total reward for the episode:  960.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  161\n",
      "13.9 action:  [-0.0094, -0.9974] n_targets:  1 reward:  80\n",
      "16.7 action:  [-0.0032, -1.0] n_targets:  1 reward:  80\n",
      "22.1 action:  [-0.0042, -0.9977] n_targets:  1 reward:  80\n",
      "24.0 action:  [0.011, -0.9964] n_targets:  1 reward:  80\n",
      "29.8 action:  [-0.0098, -0.9888] n_targets:  1 reward:  80\n",
      "38.5 action:  [0.0121, -0.9992] n_targets:  2 reward:  160\n",
      "40.5 action:  [0.0024, -0.9996] n_targets:  1 reward:  80\n",
      "43.2 action:  [-0.0077, -0.996] n_targets:  1 reward:  80\n",
      "52.5 action:  [0.0199, -0.9992] n_targets:  1 reward:  80\n",
      "57.5 action:  [-0.016, -0.9982] n_targets:  1 reward:  80\n",
      "58.2 action:  [-0.013, -0.9998] n_targets:  1 reward:  80\n",
      "62.2 action:  [0.0103, -0.9999] n_targets:  2 reward:  160\n",
      "75.1 action:  [0.0192, -0.9926] n_targets:  1 reward:  80\n",
      "81.3 action:  [0.0116, -0.9991] n_targets:  3 reward:  240\n",
      "88.3 action:  [0.0165, -0.9992] n_targets:  1 reward:  80\n",
      "89.6 action:  [-0.0148, -1.0] n_targets:  1 reward:  80\n",
      "92.3 action:  [0.0111, -0.9927] n_targets:  1 reward:  80\n",
      "96.2 action:  [-0.0182, -0.9958] n_targets:  1 reward:  80\n",
      "100.0 action:  [-0.0079, -0.9971] n_targets:  2 reward:  160\n",
      "Firely capture rate for the episode:  24 ff for 102.49999999999845 s: -------------------> 0.23\n",
      "Total reward for the episode:  1920.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  162\n",
      "5.2 action:  [0.0036, -0.9912] n_targets:  1 reward:  80\n",
      "8.4 action:  [0.0023, -0.9886] n_targets:  1 reward:  80\n",
      "24.2 action:  [-0.0045, -0.9828] n_targets:  1 reward:  80\n",
      "54.8 action:  [-0.0043, -0.9989] n_targets:  1 reward:  80\n",
      "57.1 action:  [-0.0168, -0.9994] n_targets:  1 reward:  80\n",
      "93.2 action:  [-0.0064, -0.99] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  6 ff for 102.49999999999845 s: -------------------> 0.06\n",
      "Total reward for the episode:  480.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  163\n",
      "6.9 action:  [0.0141, -0.9967] n_targets:  1 reward:  80\n",
      "10.6 action:  [0.019, -0.9981] n_targets:  1 reward:  80\n",
      "12.4 action:  [0.0161, -0.9998] n_targets:  1 reward:  80\n",
      "17.7 action:  [0.0119, -0.9973] n_targets:  1 reward:  80\n",
      "34.5 action:  [-0.0053, -0.9995] n_targets:  1 reward:  80\n",
      "35.9 action:  [0.0138, -0.9896] n_targets:  1 reward:  80\n",
      "58.1 action:  [-0.016, -0.9893] n_targets:  1 reward:  80\n",
      "77.8 action:  [0.0159, -0.9986] n_targets:  1 reward:  80\n",
      "82.7 action:  [0.008, -0.9954] n_targets:  2 reward:  160\n",
      "92.9 action:  [0.0172, -0.9942] n_targets:  1 reward:  80\n",
      "94.0 action:  [0.0179, -0.9957] n_targets:  1 reward:  80\n",
      "96.3 action:  [-0.0055, -0.9834] n_targets:  1 reward:  80\n",
      "102.1 action:  [0.0199, -0.9962] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  14 ff for 102.49999999999845 s: -------------------> 0.14\n",
      "Total reward for the episode:  1120.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  164\n",
      "0.2 action:  [0.0166, -0.9966] n_targets:  1 reward:  80\n",
      "20.0 action:  [0.0164, -0.985] n_targets:  1 reward:  80\n",
      "33.3 action:  [-0.0006, -0.9992] n_targets:  1 reward:  80\n",
      "46.7 action:  [-0.0109, -0.9997] n_targets:  2 reward:  160\n",
      "52.5 action:  [0.0186, -0.9995] n_targets:  1 reward:  80\n",
      "60.3 action:  [-0.0107, -0.9998] n_targets:  1 reward:  80\n",
      "65.1 action:  [-0.0171, -0.9995] n_targets:  1 reward:  80\n",
      "68.1 action:  [-0.019, -0.9999] n_targets:  1 reward:  80\n",
      "77.0 action:  [-0.0159, -0.984] n_targets:  1 reward:  80\n",
      "78.6 action:  [-0.0143, -0.997] n_targets:  1 reward:  80\n",
      "TIME before resetting: 82.49999999999959\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  165\n",
      "9.1 action:  [-0.0184, -0.9999] n_targets:  1 reward:  80\n",
      "26.4 action:  [0.0007, -0.9951] n_targets:  1 reward:  80\n",
      "37.0 action:  [0.0019, -0.9981] n_targets:  1 reward:  80\n",
      "61.2 action:  [0.0149, -0.9999] n_targets:  1 reward:  80\n",
      "65.2 action:  [0.0004, -0.9987] n_targets:  1 reward:  80\n",
      "87.6 action:  [-0.012, -0.9994] n_targets:  1 reward:  80\n",
      "90.5 action:  [0.0199, -0.9968] n_targets:  1 reward:  80\n",
      "97.5 action:  [0.0068, -0.9814] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  8 ff for 102.49999999999845 s: -------------------> 0.08\n",
      "Total reward for the episode:  640.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  166\n",
      "Eval num_timesteps=40000, episode_reward=640.00 +/- 0.00\n",
      "Episode length: 1025.00 +/- 0.00\n",
      "New best mean reward!\n",
      "6.6 action:  [0.0078, -0.9964] n_targets:  1 reward:  80\n",
      "7.3 action:  [-0.0069, -0.9837] n_targets:  1 reward:  80\n",
      "12.7 action:  [-0.006, -0.9999] n_targets:  1 reward:  80\n",
      "16.4 action:  [0.007, -0.9996] n_targets:  1 reward:  80\n",
      "18.5 action:  [-0.0119, -0.9948] n_targets:  1 reward:  80\n",
      "23.5 action:  [-0.0102, -0.9968] n_targets:  1 reward:  80\n",
      "24.5 action:  [0.0116, -0.989] n_targets:  1 reward:  80\n",
      "35.4 action:  [0.0093, -1.0] n_targets:  1 reward:  80\n",
      "47.7 action:  [-0.0034, -0.9876] n_targets:  1 reward:  80\n",
      "53.3 action:  [0.0086, -0.9951] n_targets:  2 reward:  160\n",
      "57.0 action:  [0.017, -0.9982] n_targets:  1 reward:  80\n",
      "66.1 action:  [-0.0191, -0.9994] n_targets:  1 reward:  80\n",
      "71.5 action:  [-0.0193, -0.9994] n_targets:  1 reward:  80\n",
      "74.1 action:  [-0.0139, -0.9978] n_targets:  1 reward:  80\n",
      "95.3 action:  [-0.0038, -0.9983] n_targets:  1 reward:  80\n",
      "102.1 action:  [0.0024, -0.9997] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  17 ff for 102.49999999999845 s: -------------------> 0.17\n",
      "Total reward for the episode:  1360.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  167\n",
      "12.5 action:  [-0.0193, -0.9941] n_targets:  1 reward:  80\n",
      "25.0 action:  [0.0044, -0.9886] n_targets:  2 reward:  160\n",
      "26.4 action:  [-0.0011, -1.0] n_targets:  3 reward:  240\n",
      "37.5 action:  [-0.0118, -0.9945] n_targets:  1 reward:  80\n",
      "65.0 action:  [-0.0103, -0.9997] n_targets:  1 reward:  80\n",
      "80.6 action:  [0.0141, -1.0] n_targets:  1 reward:  80\n",
      "81.1 action:  [0.0191, -0.9929] n_targets:  1 reward:  80\n",
      "84.6 action:  [0.0022, -0.9982] n_targets:  1 reward:  80\n",
      "95.2 action:  [0.0108, -0.9912] n_targets:  1 reward:  80\n",
      "98.9 action:  [0.013, -0.9996] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  13 ff for 102.49999999999845 s: -------------------> 0.13\n",
      "Total reward for the episode:  1040.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  168\n",
      "8.2 action:  [-0.0037, -0.9968] n_targets:  1 reward:  80\n",
      "18.2 action:  [0.0092, -0.9911] n_targets:  1 reward:  80\n",
      "23.5 action:  [0.0043, -0.996] n_targets:  1 reward:  80\n",
      "47.4 action:  [-0.0031, -0.985] n_targets:  1 reward:  80\n",
      "56.6 action:  [-0.0134, -0.9997] n_targets:  2 reward:  160\n",
      "69.3 action:  [-0.0188, -0.9975] n_targets:  1 reward:  80\n",
      "75.3 action:  [0.0133, -0.9993] n_targets:  1 reward:  80\n",
      "77.4 action:  [0.006, -0.9982] n_targets:  1 reward:  80\n",
      "80.6 action:  [0.0146, -0.9998] n_targets:  1 reward:  80\n",
      "84.6 action:  [0.0087, -0.9981] n_targets:  2 reward:  160\n",
      "88.7 action:  [0.0198, -0.9993] n_targets:  1 reward:  80\n",
      "97.6 action:  [0.0029, -0.9999] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  14 ff for 102.49999999999845 s: -------------------> 0.14\n",
      "Total reward for the episode:  1120.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  169\n",
      "25.9 action:  [0.0052, -0.9997] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  1 ff for 102.49999999999845 s: -------------------> 0.01\n",
      "Total reward for the episode:  80.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  170\n",
      "9.6 action:  [0.014, -0.9915] n_targets:  1 reward:  80\n",
      "24.8 action:  [-0.0161, -0.9983] n_targets:  1 reward:  80\n",
      "50.9 action:  [-0.0085, -0.99] n_targets:  1 reward:  80\n",
      "51.7 action:  [-0.0028, -0.9807] n_targets:  1 reward:  80\n",
      "54.7 action:  [0.0027, -0.9966] n_targets:  1 reward:  80\n",
      "56.4 action:  [-0.0105, -0.9992] n_targets:  1 reward:  80\n",
      "57.1 action:  [0.0187, -0.9918] n_targets:  1 reward:  80\n",
      "67.8 action:  [-0.0015, -0.9918] n_targets:  1 reward:  80\n",
      "77.1 action:  [-0.0051, -0.9926] n_targets:  1 reward:  80\n",
      "89.3 action:  [0.008, -0.9904] n_targets:  1 reward:  80\n",
      "91.0 action:  [0.0075, -0.9985] n_targets:  1 reward:  80\n",
      "102.0 action:  [0.003, -0.9942] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  12 ff for 102.49999999999845 s: -------------------> 0.12\n",
      "Total reward for the episode:  960.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  171\n",
      "10.8 action:  [-0.0015, -0.9828] n_targets:  1 reward:  80\n",
      "17.6 action:  [-0.0081, -0.9992] n_targets:  1 reward:  80\n",
      "22.7 action:  [0.0008, -0.9998] n_targets:  1 reward:  80\n",
      "38.7 action:  [-0.0004, -0.9844] n_targets:  1 reward:  80\n",
      "40.0 action:  [0.0193, -0.9987] n_targets:  1 reward:  80\n",
      "41.5 action:  [0.0112, -0.9956] n_targets:  1 reward:  80\n",
      "46.5 action:  [-0.0089, -0.9974] n_targets:  1 reward:  80\n",
      "69.3 action:  [-0.0158, -0.9998] n_targets:  2 reward:  160\n",
      "80.5 action:  [-0.0022, -0.9943] n_targets:  1 reward:  80\n",
      "84.8 action:  [-0.0172, -0.9879] n_targets:  1 reward:  80\n",
      "85.4 action:  [0.0137, -0.9802] n_targets:  1 reward:  80\n",
      "98.8 action:  [0.0188, -0.9942] n_targets:  1 reward:  80\n",
      "101.0 action:  [-0.0197, -0.9998] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  14 ff for 102.49999999999845 s: -------------------> 0.14\n",
      "Total reward for the episode:  1120.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  172\n",
      "7.0 action:  [-0.0001, -0.9999] n_targets:  1 reward:  80\n",
      "27.8 action:  [-0.0158, -0.9939] n_targets:  1 reward:  80\n",
      "31.6 action:  [0.0056, -1.0] n_targets:  1 reward:  80\n",
      "39.4 action:  [0.0048, -0.9894] n_targets:  1 reward:  80\n",
      "40.9 action:  [0.0173, -0.9973] n_targets:  1 reward:  80\n",
      "58.7 action:  [-0.0121, -0.9909] n_targets:  1 reward:  80\n",
      "77.4 action:  [0.0055, -0.996] n_targets:  1 reward:  80\n",
      "92.2 action:  [0.0142, -0.9895] n_targets:  1 reward:  80\n",
      "97.5 action:  [0.0103, -0.9924] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  9 ff for 102.49999999999845 s: -------------------> 0.09\n",
      "Total reward for the episode:  720.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  173\n",
      "13.5 action:  [-0.0199, -0.9984] n_targets:  1 reward:  80\n",
      "39.2 action:  [0.0151, -0.9916] n_targets:  1 reward:  80\n",
      "52.5 action:  [-0.0024, -0.9915] n_targets:  1 reward:  80\n",
      "56.2 action:  [0.0034, -0.9919] n_targets:  1 reward:  80\n",
      "73.5 action:  [0.0061, -0.9986] n_targets:  1 reward:  80\n",
      "76.3 action:  [0.0177, -0.9929] n_targets:  1 reward:  80\n",
      "TIME before resetting: 82.49999999999959\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  174\n",
      "4.9 action:  [0.0178, -0.9983] n_targets:  2 reward:  160\n",
      "11.7 action:  [-0.0027, -0.9807] n_targets:  1 reward:  80\n",
      "32.6 action:  [-0.0029, -0.9825] n_targets:  1 reward:  80\n",
      "34.9 action:  [0.0127, -0.9884] n_targets:  1 reward:  80\n",
      "69.8 action:  [-0.0011, -0.9968] n_targets:  1 reward:  80\n",
      "94.5 action:  [-0.0142, -0.991] n_targets:  2 reward:  160\n",
      "100.1 action:  [0.0092, -0.9963] n_targets:  2 reward:  160\n",
      "Firely capture rate for the episode:  10 ff for 102.49999999999845 s: -------------------> 0.1\n",
      "Total reward for the episode:  800.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  175\n",
      "Eval num_timesteps=48000, episode_reward=800.00 +/- 0.00\n",
      "Episode length: 1025.00 +/- 0.00\n",
      "New best mean reward!\n",
      "5.5 action:  [-0.0194, -0.9969] n_targets:  1 reward:  80\n",
      "15.1 action:  [0.0119, -0.9842] n_targets:  1 reward:  80\n",
      "29.6 action:  [0.0083, -0.9872] n_targets:  1 reward:  80\n",
      "31.7 action:  [-0.0166, -0.9863] n_targets:  1 reward:  80\n",
      "39.8 action:  [-0.0061, -0.9965] n_targets:  1 reward:  80\n",
      "44.6 action:  [0.0076, -0.9974] n_targets:  1 reward:  80\n",
      "45.8 action:  [0.0028, -1.0] n_targets:  1 reward:  80\n",
      "49.2 action:  [-0.0015, -0.9976] n_targets:  1 reward:  80\n",
      "59.1 action:  [-0.0086, -0.9992] n_targets:  1 reward:  80\n",
      "61.3 action:  [-0.0087, -0.999] n_targets:  1 reward:  80\n",
      "75.4 action:  [-0.0147, -0.9992] n_targets:  2 reward:  160\n",
      "84.2 action:  [0.0121, -0.9913] n_targets:  1 reward:  80\n",
      "86.3 action:  [-0.0, -0.995] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  14 ff for 102.49999999999845 s: -------------------> 0.14\n",
      "Total reward for the episode:  1120.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  176\n",
      "6.7 action:  [0.0094, -0.9995] n_targets:  1 reward:  80\n",
      "11.8 action:  [-0.0165, -0.9874] n_targets:  1 reward:  80\n",
      "29.3 action:  [-0.0174, -0.988] n_targets:  1 reward:  80\n",
      "36.9 action:  [0.0148, -0.9998] n_targets:  1 reward:  80\n",
      "50.8 action:  [0.0058, -0.9954] n_targets:  1 reward:  80\n",
      "54.2 action:  [-0.0159, -0.998] n_targets:  1 reward:  80\n",
      "57.1 action:  [0.0078, -0.9921] n_targets:  2 reward:  160\n",
      "61.1 action:  [0.005, -0.9961] n_targets:  2 reward:  160\n",
      "63.4 action:  [-0.0165, -0.9849] n_targets:  1 reward:  80\n",
      "76.8 action:  [0.0066, -0.9985] n_targets:  1 reward:  80\n",
      "98.2 action:  [-0.0194, -0.9988] n_targets:  2 reward:  160\n",
      "Firely capture rate for the episode:  14 ff for 102.49999999999845 s: -------------------> 0.14\n",
      "Total reward for the episode:  1120.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  177\n",
      "17.1 action:  [-0.0025, -0.9993] n_targets:  1 reward:  80\n",
      "25.4 action:  [-0.0198, -0.997] n_targets:  1 reward:  80\n",
      "32.9 action:  [-0.0013, -0.9959] n_targets:  1 reward:  80\n",
      "48.4 action:  [-0.019, -0.9945] n_targets:  1 reward:  80\n",
      "49.7 action:  [-0.017, -0.9974] n_targets:  1 reward:  80\n",
      "84.2 action:  [0.0196, -0.9965] n_targets:  2 reward:  160\n",
      "92.9 action:  [0.0186, -0.9999] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  8 ff for 102.49999999999845 s: -------------------> 0.08\n",
      "Total reward for the episode:  640.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  178\n",
      "4.2 action:  [-0.0194, -0.9997] n_targets:  1 reward:  80\n",
      "13.5 action:  [0.0121, -0.9804] n_targets:  1 reward:  80\n",
      "21.2 action:  [0.0063, -0.9908] n_targets:  1 reward:  80\n",
      "22.7 action:  [-0.0095, -0.9979] n_targets:  1 reward:  80\n",
      "26.2 action:  [-0.0067, -0.997] n_targets:  1 reward:  80\n",
      "37.5 action:  [-0.0012, -0.9995] n_targets:  2 reward:  160\n",
      "47.1 action:  [0.0097, -0.996] n_targets:  1 reward:  80\n",
      "53.8 action:  [0.0004, -0.9982] n_targets:  1 reward:  80\n",
      "56.2 action:  [-0.0085, -0.9998] n_targets:  1 reward:  80\n",
      "63.8 action:  [-0.0038, -0.9878] n_targets:  1 reward:  80\n",
      "71.5 action:  [-0.012, -0.9966] n_targets:  1 reward:  80\n",
      "88.7 action:  [-0.0139, -0.9992] n_targets:  1 reward:  80\n",
      "91.1 action:  [-0.0074, -0.9971] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  14 ff for 102.49999999999845 s: -------------------> 0.14\n",
      "Total reward for the episode:  1120.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  179\n",
      "2.2 action:  [-0.0111, -0.996] n_targets:  1 reward:  80\n",
      "4.0 action:  [0.0153, -0.9939] n_targets:  1 reward:  80\n",
      "10.9 action:  [0.0054, -0.9987] n_targets:  1 reward:  80\n",
      "12.6 action:  [0.0061, -0.9927] n_targets:  1 reward:  80\n",
      "14.0 action:  [0.0106, -0.991] n_targets:  1 reward:  80\n",
      "43.8 action:  [-0.0059, -0.9844] n_targets:  1 reward:  80\n",
      "47.8 action:  [0.0175, -0.9881] n_targets:  1 reward:  80\n",
      "65.0 action:  [-0.0053, -0.9979] n_targets:  1 reward:  80\n",
      "77.8 action:  [0.0048, -0.9977] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  9 ff for 102.49999999999845 s: -------------------> 0.09\n",
      "Total reward for the episode:  720.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  180\n",
      "2.1 action:  [-0.0134, -0.9998] n_targets:  1 reward:  80\n",
      "42.7 action:  [-0.0059, -0.989] n_targets:  1 reward:  80\n",
      "47.6 action:  [0.0161, -0.9983] n_targets:  3 reward:  240\n",
      "52.6 action:  [0.006, -0.9963] n_targets:  1 reward:  80\n",
      "54.9 action:  [0.0175, -0.9942] n_targets:  1 reward:  80\n",
      "60.5 action:  [-0.015, -0.9999] n_targets:  1 reward:  80\n",
      "76.8 action:  [0.008, -0.9938] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  9 ff for 102.49999999999845 s: -------------------> 0.09\n",
      "Total reward for the episode:  720.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  181\n",
      "4.1 action:  [0.0072, -0.9975] n_targets:  1 reward:  80\n",
      "12.0 action:  [-0.0078, -0.9972] n_targets:  1 reward:  80\n",
      "36.9 action:  [-0.0154, -0.9999] n_targets:  1 reward:  80\n",
      "62.3 action:  [0.0148, -0.9918] n_targets:  1 reward:  80\n",
      "81.8 action:  [-0.0028, -0.9955] n_targets:  1 reward:  80\n",
      "89.7 action:  [-0.0027, -0.9969] n_targets:  1 reward:  80\n",
      "95.8 action:  [0.0035, -1.0] n_targets:  2 reward:  160\n",
      "Firely capture rate for the episode:  8 ff for 102.49999999999845 s: -------------------> 0.08\n",
      "Total reward for the episode:  640.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  182\n",
      "5.8 action:  [-0.0044, -0.9959] n_targets:  1 reward:  80\n",
      "30.5 action:  [0.0027, -0.9998] n_targets:  1 reward:  80\n",
      "35.1 action:  [-0.0045, -0.9975] n_targets:  1 reward:  80\n",
      "45.4 action:  [0.0173, -0.9803] n_targets:  2 reward:  160\n",
      "48.3 action:  [0.009, -0.9999] n_targets:  1 reward:  80\n",
      "50.5 action:  [-0.0153, -0.9981] n_targets:  1 reward:  80\n",
      "57.5 action:  [0.0198, -0.9941] n_targets:  1 reward:  80\n",
      "TIME before resetting: 82.49999999999959\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  183\n",
      "24.1 action:  [0.0003, -0.9874] n_targets:  1 reward:  80\n",
      "28.3 action:  [0.0133, -0.9921] n_targets:  1 reward:  80\n",
      "41.4 action:  [0.0123, -0.9986] n_targets:  1 reward:  80\n",
      "51.7 action:  [0.0122, -0.9946] n_targets:  1 reward:  80\n",
      "79.0 action:  [-0.0065, -0.9881] n_targets:  1 reward:  80\n",
      "86.6 action:  [0.0114, -0.9804] n_targets:  2 reward:  160\n",
      "92.2 action:  [0.0008, -0.999] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  8 ff for 102.49999999999845 s: -------------------> 0.08\n",
      "Total reward for the episode:  640.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  184\n",
      "Eval num_timesteps=56000, episode_reward=640.00 +/- 0.00\n",
      "Episode length: 1025.00 +/- 0.00\n",
      "29.4 action:  [-0.0123, -0.9965] n_targets:  1 reward:  80\n",
      "33.8 action:  [-0.0068, -0.9968] n_targets:  1 reward:  80\n",
      "56.3 action:  [-0.0066, -0.9998] n_targets:  1 reward:  80\n",
      "80.4 action:  [0.008, -0.9997] n_targets:  1 reward:  80\n",
      "92.7 action:  [-0.0037, -0.982] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  5 ff for 102.49999999999845 s: -------------------> 0.05\n",
      "Total reward for the episode:  400.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  185\n",
      "3.6 action:  [-0.0011, -0.9904] n_targets:  1 reward:  80\n",
      "8.7 action:  [-0.0021, -0.9954] n_targets:  1 reward:  80\n",
      "33.7 action:  [-0.0074, -0.9991] n_targets:  1 reward:  80\n",
      "35.4 action:  [-0.0191, -0.9886] n_targets:  1 reward:  80\n",
      "47.4 action:  [0.0001, -0.9848] n_targets:  1 reward:  80\n",
      "65.0 action:  [-0.0109, -0.9957] n_targets:  1 reward:  80\n",
      "66.5 action:  [-0.0112, -0.9843] n_targets:  1 reward:  80\n",
      "70.1 action:  [-0.0097, -0.9939] n_targets:  1 reward:  80\n",
      "74.0 action:  [-0.0139, -0.9991] n_targets:  1 reward:  80\n",
      "87.3 action:  [0.0043, -0.9962] n_targets:  1 reward:  80\n",
      "90.6 action:  [-0.0012, -0.9906] n_targets:  1 reward:  80\n",
      "93.7 action:  [-0.0155, -0.9976] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  12 ff for 102.49999999999845 s: -------------------> 0.12\n",
      "Total reward for the episode:  960.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  186\n",
      "9.5 action:  [-0.0014, -0.9991] n_targets:  1 reward:  80\n",
      "14.0 action:  [0.0031, -0.996] n_targets:  1 reward:  80\n",
      "21.6 action:  [-0.017, -0.9989] n_targets:  1 reward:  80\n",
      "27.1 action:  [-0.0026, -0.998] n_targets:  1 reward:  80\n",
      "32.0 action:  [0.0115, -0.9969] n_targets:  1 reward:  80\n",
      "39.0 action:  [0.0158, -0.9997] n_targets:  1 reward:  80\n",
      "40.9 action:  [-0.0059, -0.991] n_targets:  1 reward:  80\n",
      "47.3 action:  [0.0146, -0.9904] n_targets:  1 reward:  80\n",
      "55.4 action:  [-0.0107, -0.9963] n_targets:  1 reward:  80\n",
      "77.4 action:  [-0.0028, -0.9829] n_targets:  1 reward:  80\n",
      "97.1 action:  [0.0012, -0.999] n_targets:  1 reward:  80\n",
      "100.3 action:  [0.016, -0.998] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  12 ff for 102.49999999999845 s: -------------------> 0.12\n",
      "Total reward for the episode:  960.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  187\n",
      "4.1 action:  [-0.0111, -0.9986] n_targets:  1 reward:  80\n",
      "21.5 action:  [-0.0159, -0.9995] n_targets:  1 reward:  80\n",
      "38.3 action:  [-0.0001, -0.9973] n_targets:  1 reward:  80\n",
      "40.0 action:  [-0.0193, -0.9837] n_targets:  1 reward:  80\n",
      "42.6 action:  [-0.0092, -0.9886] n_targets:  1 reward:  80\n",
      "46.7 action:  [0.0014, -0.998] n_targets:  1 reward:  80\n",
      "49.1 action:  [0.0165, -0.9856] n_targets:  1 reward:  80\n",
      "56.2 action:  [-0.0067, -0.9989] n_targets:  1 reward:  80\n",
      "75.3 action:  [-0.0167, -0.9999] n_targets:  1 reward:  80\n",
      "76.1 action:  [-0.0194, -0.9828] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  10 ff for 102.49999999999845 s: -------------------> 0.1\n",
      "Total reward for the episode:  800.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  188\n",
      "9.3 action:  [-0.009, -0.9845] n_targets:  1 reward:  80\n",
      "11.6 action:  [-0.0107, -0.9998] n_targets:  1 reward:  80\n",
      "23.7 action:  [-0.0194, -0.9907] n_targets:  1 reward:  80\n",
      "28.4 action:  [0.0019, -0.9973] n_targets:  1 reward:  80\n",
      "30.6 action:  [-0.0134, -0.9986] n_targets:  2 reward:  160\n",
      "31.0 action:  [0.0052, -0.9944] n_targets:  1 reward:  80\n",
      "37.7 action:  [-0.0142, -0.9815] n_targets:  1 reward:  80\n",
      "50.9 action:  [-0.0003, -1.0] n_targets:  2 reward:  160\n",
      "52.1 action:  [-0.0042, -0.9891] n_targets:  1 reward:  80\n",
      "72.3 action:  [0.0183, -0.9988] n_targets:  1 reward:  80\n",
      "82.3 action:  [-0.0159, -0.9863] n_targets:  1 reward:  80\n",
      "91.9 action:  [0.0088, -0.9999] n_targets:  1 reward:  80\n",
      "95.8 action:  [0.0127, -0.9999] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  15 ff for 102.49999999999845 s: -------------------> 0.15\n",
      "Total reward for the episode:  1200.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  189\n",
      "2.2 action:  [0.0001, -0.9996] n_targets:  1 reward:  80\n",
      "3.1 action:  [-0.0038, -0.9931] n_targets:  1 reward:  80\n",
      "18.2 action:  [-0.0063, -0.9977] n_targets:  1 reward:  80\n",
      "23.6 action:  [-0.011, -0.9995] n_targets:  1 reward:  80\n",
      "32.2 action:  [-0.0103, -0.9821] n_targets:  1 reward:  80\n",
      "44.8 action:  [0.0007, -0.9856] n_targets:  1 reward:  80\n",
      "51.6 action:  [0.0091, -0.9988] n_targets:  1 reward:  80\n",
      "56.3 action:  [-0.0182, -0.9954] n_targets:  1 reward:  80\n",
      "67.1 action:  [-0.0019, -0.9988] n_targets:  2 reward:  160\n",
      "78.1 action:  [-0.0092, -0.996] n_targets:  1 reward:  80\n",
      "80.4 action:  [0.0164, -0.9999] n_targets:  2 reward:  160\n",
      "91.0 action:  [-0.0131, -0.9986] n_targets:  2 reward:  160\n",
      "99.0 action:  [0.0028, -0.9952] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  16 ff for 102.49999999999845 s: -------------------> 0.16\n",
      "Total reward for the episode:  1280.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  190\n",
      "11.5 action:  [-0.0179, -0.9991] n_targets:  1 reward:  80\n",
      "53.5 action:  [-0.0102, -0.9995] n_targets:  1 reward:  80\n",
      "89.4 action:  [0.0173, -0.9965] n_targets:  2 reward:  160\n",
      "93.0 action:  [-0.0109, -0.9859] n_targets:  2 reward:  160\n",
      "Firely capture rate for the episode:  6 ff for 102.49999999999845 s: -------------------> 0.06\n",
      "Total reward for the episode:  480.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  191\n",
      "2.5 action:  [-0.0123, -0.9938] n_targets:  1 reward:  80\n",
      "6.8 action:  [-0.003, -0.9998] n_targets:  2 reward:  160\n",
      "9.0 action:  [0.003, -0.9958] n_targets:  1 reward:  80\n",
      "13.5 action:  [-0.0009, -0.9962] n_targets:  1 reward:  80\n",
      "40.9 action:  [-0.0004, -0.9976] n_targets:  1 reward:  80\n",
      "45.2 action:  [0.015, -0.9999] n_targets:  2 reward:  160\n",
      "61.6 action:  [0.0034, -0.9991] n_targets:  1 reward:  80\n",
      "64.4 action:  [0.0008, -0.9894] n_targets:  1 reward:  80\n",
      "65.3 action:  [-0.0021, -0.9998] n_targets:  1 reward:  80\n",
      "66.1 action:  [0.0057, -0.9975] n_targets:  1 reward:  80\n",
      "77.3 action:  [-0.0085, -0.9959] n_targets:  1 reward:  80\n",
      "TIME before resetting: 82.49999999999959\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  192\n",
      "16.0 action:  [-0.0096, -0.9983] n_targets:  2 reward:  160\n",
      "26.8 action:  [0.0014, -0.9871] n_targets:  1 reward:  80\n",
      "31.6 action:  [0.0196, -0.9886] n_targets:  2 reward:  160\n",
      "40.2 action:  [-0.0137, -0.9842] n_targets:  1 reward:  80\n",
      "42.4 action:  [0.0133, -0.9888] n_targets:  1 reward:  80\n",
      "65.8 action:  [0.0184, -0.9967] n_targets:  1 reward:  80\n",
      "84.0 action:  [-0.0137, -0.9949] n_targets:  1 reward:  80\n",
      "87.8 action:  [0.003, -0.9929] n_targets:  1 reward:  80\n",
      "95.9 action:  [0.0059, -0.9802] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  11 ff for 102.49999999999845 s: -------------------> 0.11\n",
      "Total reward for the episode:  880.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.02\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  193\n",
      "Eval num_timesteps=64000, episode_reward=880.00 +/- 0.00\n",
      "Episode length: 1025.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Saved agent: RL_models/SB3_stored_models/all_agents/env1_relu/best_model_after_curriculum/best_model\n",
      "Saved replay buffer: RL_models/SB3_stored_models/all_agents/env1_relu/best_model_after_curriculum/buffer\n",
      "Current dt: 0.1\n",
      "Current gamma: 0.998\n",
      "Current angular_terminal_vel: 0.01\n",
      "Saving env params to RL_models/SB3_stored_models/all_agents/env1_relu/best_model_after_curriculum/env_params.txt\n",
      "TIME before resetting: 0\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  194\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  0.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  195\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  0.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  196\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  0.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  197\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  0.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  198\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  0.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  199\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  0.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  200\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  0.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  201\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  0.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  202\n",
      "Firely capture rate for the episode:  0 ff for 102.49999999999845 s: -------------------> 0.0\n",
      "Total reward for the episode:  0.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  203\n",
      "TIME before resetting: 77.49999999999987\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  204\n",
      "46.7 action:  [-0.0004, -0.9855] n_targets:  1 reward:  80\n",
      "68.4 action:  [-0.0058, -0.9818] n_targets:  1 reward:  80\n",
      "78.8 action:  [-0.0035, -0.9968] n_targets:  1 reward:  80\n",
      "91.7 action:  [-0.0039, -0.9907] n_targets:  1 reward:  80\n",
      "101.6 action:  [-0.003, -0.9841] n_targets:  2 reward:  160\n",
      "Firely capture rate for the episode:  6 ff for 102.49999999999845 s: -------------------> 0.06\n",
      "Total reward for the episode:  480.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  205\n",
      "6.3 action:  [0.0065, -0.9977] n_targets:  1 reward:  80\n",
      "8.0 action:  [0.0024, -0.9896] n_targets:  1 reward:  80\n",
      "14.5 action:  [0.0021, -0.999] n_targets:  1 reward:  80\n",
      "16.1 action:  [0.0001, -0.9828] n_targets:  2 reward:  160\n",
      "30.5 action:  [-0.0028, -0.9981] n_targets:  1 reward:  80\n",
      "36.4 action:  [-0.0006, -0.9919] n_targets:  1 reward:  80\n",
      "42.0 action:  [-0.0054, -0.9934] n_targets:  2 reward:  160\n",
      "51.7 action:  [0.0021, -0.9911] n_targets:  1 reward:  80\n",
      "60.5 action:  [0.0082, -0.9853] n_targets:  1 reward:  80\n",
      "74.9 action:  [-0.004, -0.9804] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  12 ff for 102.49999999999845 s: -------------------> 0.12\n",
      "Total reward for the episode:  960.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  206\n",
      "26.7 action:  [-0.0038, -0.9985] n_targets:  1 reward:  80\n",
      "43.5 action:  [0.0036, -0.9982] n_targets:  1 reward:  80\n",
      "57.0 action:  [0.0072, -0.9997] n_targets:  1 reward:  80\n",
      "90.1 action:  [-0.0016, -0.9977] n_targets:  1 reward:  80\n",
      "92.3 action:  [-0.0063, -0.998] n_targets:  1 reward:  80\n",
      "101.8 action:  [0.0018, -0.9979] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  6 ff for 102.49999999999845 s: -------------------> 0.06\n",
      "Total reward for the episode:  480.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  207\n",
      "Eval num_timesteps=10000, episode_reward=640.00 +/- 226.27\n",
      "Episode length: 1025.00 +/- 0.00\n",
      "New best mean reward!\n",
      "1.3 action:  [0.0078, -0.998] n_targets:  1 reward:  80\n",
      "17.5 action:  [0.0036, -0.9991] n_targets:  1 reward:  80\n",
      "22.2 action:  [-0.0093, -0.9966] n_targets:  1 reward:  80\n",
      "35.6 action:  [0.0083, -0.9837] n_targets:  2 reward:  160\n",
      "45.1 action:  [-0.0096, -0.9984] n_targets:  1 reward:  80\n",
      "46.7 action:  [0.0053, -0.9992] n_targets:  1 reward:  80\n",
      "78.5 action:  [0.0008, -0.9891] n_targets:  1 reward:  80\n",
      "82.3 action:  [0.006, -0.988] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  9 ff for 102.49999999999845 s: -------------------> 0.09\n",
      "Total reward for the episode:  720.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  208\n",
      "5.4 action:  [-0.0014, -0.9951] n_targets:  1 reward:  80\n",
      "30.6 action:  [-0.0008, -0.9961] n_targets:  1 reward:  80\n",
      "78.4 action:  [0.0046, -0.9963] n_targets:  1 reward:  80\n",
      "87.6 action:  [0.0036, -0.9957] n_targets:  2 reward:  160\n",
      "88.8 action:  [0.0056, -0.9989] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  6 ff for 102.49999999999845 s: -------------------> 0.06\n",
      "Total reward for the episode:  480.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  209\n",
      "7.7 action:  [0.0012, -0.9928] n_targets:  1 reward:  80\n",
      "16.9 action:  [0.0008, -0.9986] n_targets:  1 reward:  80\n",
      "26.4 action:  [-0.0098, -0.9967] n_targets:  1 reward:  80\n",
      "54.5 action:  [0.0035, -0.9988] n_targets:  2 reward:  160\n",
      "61.8 action:  [0.0009, -0.9996] n_targets:  1 reward:  80\n",
      "78.4 action:  [-0.004, -0.9996] n_targets:  2 reward:  160\n",
      "95.6 action:  [0.0015, -0.9999] n_targets:  2 reward:  160\n",
      "Firely capture rate for the episode:  10 ff for 102.49999999999845 s: -------------------> 0.1\n",
      "Total reward for the episode:  800.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  210\n",
      "2.2 action:  [0.0065, -0.9844] n_targets:  1 reward:  80\n",
      "27.5 action:  [0.0099, -0.9961] n_targets:  1 reward:  80\n",
      "34.2 action:  [-0.003, -0.9996] n_targets:  1 reward:  80\n",
      "46.6 action:  [-0.0005, -0.9918] n_targets:  3 reward:  240\n",
      "Firely capture rate for the episode:  6 ff for 102.49999999999845 s: -------------------> 0.06\n",
      "Total reward for the episode:  480.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  211\n",
      "9.1 action:  [-0.0088, -0.9963] n_targets:  1 reward:  80\n",
      "25.2 action:  [0.0002, -0.9987] n_targets:  1 reward:  80\n",
      "28.8 action:  [0.0029, -0.9875] n_targets:  1 reward:  80\n",
      "61.5 action:  [-0.0049, -0.9946] n_targets:  1 reward:  80\n",
      "72.6 action:  [-0.0076, -0.9993] n_targets:  1 reward:  80\n",
      "79.8 action:  [0.0057, -0.9998] n_targets:  1 reward:  80\n",
      "81.7 action:  [-0.0035, -0.9999] n_targets:  2 reward:  160\n",
      "84.5 action:  [0.0024, -0.9917] n_targets:  2 reward:  160\n",
      "Firely capture rate for the episode:  10 ff for 102.49999999999845 s: -------------------> 0.1\n",
      "Total reward for the episode:  800.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  212\n",
      "37.4 action:  [-0.0059, -0.9816] n_targets:  1 reward:  80\n",
      "53.1 action:  [0.0046, -0.9998] n_targets:  2 reward:  160\n",
      "62.2 action:  [-0.0021, -0.9984] n_targets:  1 reward:  80\n",
      "65.7 action:  [0.0074, -0.9955] n_targets:  2 reward:  160\n",
      "74.2 action:  [0.0086, -0.9821] n_targets:  1 reward:  80\n",
      "86.2 action:  [-0.0074, -0.9993] n_targets:  1 reward:  80\n",
      "89.8 action:  [-0.0001, -0.9967] n_targets:  1 reward:  80\n",
      "95.4 action:  [0.003, -0.9838] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  10 ff for 102.49999999999845 s: -------------------> 0.1\n",
      "Total reward for the episode:  800.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  213\n",
      "9.1 action:  [0.0038, -0.9905] n_targets:  1 reward:  80\n",
      "11.4 action:  [0.0073, -0.9961] n_targets:  1 reward:  80\n",
      "22.6 action:  [0.004, -0.9941] n_targets:  1 reward:  80\n",
      "27.1 action:  [0.0029, -0.9858] n_targets:  1 reward:  80\n",
      "29.0 action:  [0.0062, -0.9973] n_targets:  1 reward:  80\n",
      "55.1 action:  [-0.0049, -0.9814] n_targets:  1 reward:  80\n",
      "68.5 action:  [-0.0093, -0.9971] n_targets:  1 reward:  80\n",
      "77.3 action:  [-0.0026, -0.9977] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  8 ff for 102.49999999999845 s: -------------------> 0.08\n",
      "Total reward for the episode:  640.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  214\n",
      "16.7 action:  [-0.0014, -0.9997] n_targets:  1 reward:  80\n",
      "32.0 action:  [0.0026, -0.9984] n_targets:  2 reward:  160\n",
      "42.2 action:  [0.0071, -0.9907] n_targets:  1 reward:  80\n",
      "43.6 action:  [0.0031, -0.9994] n_targets:  1 reward:  80\n",
      "48.6 action:  [0.0068, -0.9997] n_targets:  1 reward:  80\n",
      "73.4 action:  [0.0033, -0.9996] n_targets:  1 reward:  80\n",
      "89.5 action:  [-0.0075, -0.999] n_targets:  1 reward:  80\n",
      "91.0 action:  [0.0052, -0.9996] n_targets:  1 reward:  80\n",
      "93.7 action:  [-0.0044, -0.9936] n_targets:  1 reward:  80\n",
      "100.1 action:  [-0.0046, -0.9995] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  11 ff for 102.49999999999845 s: -------------------> 0.11\n",
      "Total reward for the episode:  880.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  215\n",
      "3.9 action:  [-0.0074, -0.9975] n_targets:  1 reward:  80\n",
      "4.7 action:  [-0.0065, -0.9912] n_targets:  1 reward:  80\n",
      "19.9 action:  [0.0087, -0.9928] n_targets:  2 reward:  160\n",
      "28.7 action:  [0.0068, -0.9955] n_targets:  1 reward:  80\n",
      "32.8 action:  [-0.0008, -0.9985] n_targets:  2 reward:  160\n",
      "38.6 action:  [-0.0091, -0.9992] n_targets:  1 reward:  80\n",
      "44.3 action:  [-0.0074, -0.9978] n_targets:  1 reward:  80\n",
      "52.6 action:  [0.0018, -0.992] n_targets:  1 reward:  80\n",
      "85.8 action:  [0.004, -0.9963] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  11 ff for 102.49999999999845 s: -------------------> 0.11\n",
      "Total reward for the episode:  880.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  216\n",
      "3.9 action:  [-0.0081, -0.9942] n_targets:  1 reward:  80\n",
      "6.0 action:  [0.005, -0.9911] n_targets:  1 reward:  80\n",
      "38.1 action:  [0.0033, -0.9879] n_targets:  1 reward:  80\n",
      "43.8 action:  [-0.0089, -0.9821] n_targets:  1 reward:  80\n",
      "45.5 action:  [-0.006, -0.9925] n_targets:  1 reward:  80\n",
      "55.2 action:  [0.001, -0.9994] n_targets:  1 reward:  80\n",
      "63.4 action:  [0.0021, -0.9991] n_targets:  1 reward:  80\n",
      "75.3 action:  [0.0002, -0.9971] n_targets:  1 reward:  80\n",
      "TIME before resetting: 77.49999999999987\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  217\n",
      "8.4 action:  [0.0094, -0.9956] n_targets:  2 reward:  160\n",
      "32.1 action:  [-0.0095, -0.9988] n_targets:  2 reward:  160\n",
      "34.3 action:  [-0.0028, -0.9893] n_targets:  2 reward:  160\n",
      "42.3 action:  [-0.0025, -0.9993] n_targets:  1 reward:  80\n",
      "47.1 action:  [0.0072, -0.9909] n_targets:  1 reward:  80\n",
      "52.8 action:  [0.0017, -0.9955] n_targets:  2 reward:  160\n",
      "58.7 action:  [-0.0012, -0.9993] n_targets:  1 reward:  80\n",
      "76.6 action:  [-0.0092, -0.9908] n_targets:  1 reward:  80\n",
      "100.4 action:  [-0.0031, -0.9927] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  13 ff for 102.49999999999845 s: -------------------> 0.13\n",
      "Total reward for the episode:  1040.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  218\n",
      "8.7 action:  [0.0008, -0.9968] n_targets:  1 reward:  80\n",
      "20.3 action:  [-0.0054, -0.9868] n_targets:  1 reward:  80\n",
      "48.7 action:  [-0.0076, -0.9974] n_targets:  1 reward:  80\n",
      "59.6 action:  [0.0004, -0.9971] n_targets:  2 reward:  160\n",
      "73.5 action:  [-0.0073, -0.9966] n_targets:  1 reward:  80\n",
      "78.9 action:  [0.0062, -0.9948] n_targets:  1 reward:  80\n",
      "85.2 action:  [0.004, -0.9946] n_targets:  1 reward:  80\n",
      "92.5 action:  [-0.0092, -0.9998] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  9 ff for 102.49999999999845 s: -------------------> 0.09\n",
      "Total reward for the episode:  720.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  219\n",
      "14.5 action:  [0.0093, -0.9947] n_targets:  2 reward:  160\n",
      "19.5 action:  [0.009, -0.9965] n_targets:  1 reward:  80\n",
      "21.3 action:  [-0.0075, -0.999] n_targets:  2 reward:  160\n",
      "44.3 action:  [0.0071, -0.9981] n_targets:  1 reward:  80\n",
      "45.6 action:  [-0.0016, -0.9974] n_targets:  2 reward:  160\n",
      "51.4 action:  [0.0027, -0.9977] n_targets:  1 reward:  80\n",
      "66.9 action:  [0.0062, -0.998] n_targets:  1 reward:  80\n",
      "69.6 action:  [-0.0054, -0.9978] n_targets:  1 reward:  80\n",
      "70.4 action:  [0.0087, -0.9993] n_targets:  1 reward:  80\n",
      "78.4 action:  [0.0081, -0.9906] n_targets:  1 reward:  80\n",
      "90.8 action:  [0.0098, -0.9823] n_targets:  1 reward:  80\n",
      "101.8 action:  [-0.007, -0.9961] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  15 ff for 102.49999999999845 s: -------------------> 0.15\n",
      "Total reward for the episode:  1200.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  220\n",
      "Eval num_timesteps=20000, episode_reward=986.67 +/- 199.56\n",
      "Episode length: 1025.00 +/- 0.00\n",
      "New best mean reward!\n",
      "4.7 action:  [0.0019, -0.9984] n_targets:  1 reward:  80\n",
      "34.1 action:  [-0.0053, -0.9996] n_targets:  1 reward:  80\n",
      "38.1 action:  [0.0007, -0.9995] n_targets:  1 reward:  80\n",
      "45.0 action:  [-0.0036, -0.9908] n_targets:  1 reward:  80\n",
      "51.1 action:  [0.0068, -0.998] n_targets:  1 reward:  80\n",
      "62.3 action:  [0.0079, -0.9999] n_targets:  1 reward:  80\n",
      "69.0 action:  [-0.0092, -0.9862] n_targets:  1 reward:  80\n",
      "85.4 action:  [-0.0009, -0.9993] n_targets:  1 reward:  80\n",
      "86.7 action:  [0.0064, -0.9967] n_targets:  1 reward:  80\n",
      "94.9 action:  [-0.001, -0.983] n_targets:  1 reward:  80\n",
      "99.6 action:  [-0.0033, -0.9821] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  11 ff for 102.49999999999845 s: -------------------> 0.11\n",
      "Total reward for the episode:  880.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  221\n",
      "3.4 action:  [0.0015, -0.9995] n_targets:  1 reward:  80\n",
      "18.6 action:  [0.0062, -0.9926] n_targets:  1 reward:  80\n",
      "25.1 action:  [0.0002, -0.9939] n_targets:  2 reward:  160\n",
      "32.3 action:  [0.0029, -0.9963] n_targets:  1 reward:  80\n",
      "35.4 action:  [-0.0062, -0.9997] n_targets:  2 reward:  160\n",
      "50.3 action:  [-0.0021, -0.9944] n_targets:  1 reward:  80\n",
      "55.7 action:  [0.0093, -0.9955] n_targets:  1 reward:  80\n",
      "58.0 action:  [0.004, -0.9908] n_targets:  1 reward:  80\n",
      "64.0 action:  [0.0018, -0.9903] n_targets:  1 reward:  80\n",
      "68.6 action:  [-0.0056, -0.9986] n_targets:  1 reward:  80\n",
      "71.3 action:  [0.0025, -0.9866] n_targets:  1 reward:  80\n",
      "79.5 action:  [0.0071, -0.9825] n_targets:  1 reward:  80\n",
      "88.8 action:  [-0.0053, -0.9852] n_targets:  1 reward:  80\n",
      "93.7 action:  [0.0032, -0.9848] n_targets:  1 reward:  80\n",
      "102.2 action:  [0.0078, -0.9885] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  17 ff for 102.49999999999845 s: -------------------> 0.17\n",
      "Total reward for the episode:  1360.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  222\n",
      "10.0 action:  [0.0035, -0.9992] n_targets:  1 reward:  80\n",
      "16.8 action:  [0.0027, -0.9937] n_targets:  1 reward:  80\n",
      "24.3 action:  [-0.0061, -0.9849] n_targets:  2 reward:  160\n",
      "44.6 action:  [0.0015, -0.9991] n_targets:  1 reward:  80\n",
      "51.2 action:  [-0.0084, -0.9986] n_targets:  1 reward:  80\n",
      "53.5 action:  [0.0034, -0.9998] n_targets:  1 reward:  80\n",
      "78.8 action:  [-0.0014, -0.9992] n_targets:  1 reward:  80\n",
      "94.4 action:  [-0.0089, -0.9979] n_targets:  1 reward:  80\n",
      "98.1 action:  [-0.001, -0.9966] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  10 ff for 102.49999999999845 s: -------------------> 0.1\n",
      "Total reward for the episode:  800.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  223\n",
      "27.1 action:  [-0.0098, -0.9956] n_targets:  1 reward:  80\n",
      "38.8 action:  [0.0098, -0.9997] n_targets:  1 reward:  80\n",
      "53.3 action:  [-0.0065, -0.9857] n_targets:  1 reward:  80\n",
      "56.7 action:  [-0.0081, -0.9997] n_targets:  1 reward:  80\n",
      "59.6 action:  [0.0098, -0.9857] n_targets:  1 reward:  80\n",
      "71.7 action:  [-0.0035, -0.9837] n_targets:  1 reward:  80\n",
      "81.3 action:  [0.0011, -0.9988] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  7 ff for 102.49999999999845 s: -------------------> 0.07\n",
      "Total reward for the episode:  560.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  224\n",
      "13.2 action:  [-0.004, -0.9926] n_targets:  1 reward:  80\n",
      "40.3 action:  [0.0034, -0.9945] n_targets:  2 reward:  160\n",
      "79.4 action:  [-0.0016, -0.9895] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  4 ff for 102.49999999999845 s: -------------------> 0.04\n",
      "Total reward for the episode:  320.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  225\n",
      "10.7 action:  [-0.0098, -0.9993] n_targets:  1 reward:  80\n",
      "20.0 action:  [0.0084, -0.9858] n_targets:  1 reward:  80\n",
      "26.0 action:  [0.0011, -0.9987] n_targets:  1 reward:  80\n",
      "32.0 action:  [0.0074, -0.9905] n_targets:  1 reward:  80\n",
      "35.6 action:  [-0.0051, -0.9992] n_targets:  2 reward:  160\n",
      "54.1 action:  [-0.0017, -0.9948] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  7 ff for 102.49999999999845 s: -------------------> 0.07\n",
      "Total reward for the episode:  560.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  226\n",
      "9.6 action:  [0.0001, -0.9878] n_targets:  1 reward:  80\n",
      "17.8 action:  [0.0089, -0.994] n_targets:  1 reward:  80\n",
      "58.5 action:  [-0.0085, -0.9877] n_targets:  1 reward:  80\n",
      "64.0 action:  [0.0088, -0.9979] n_targets:  1 reward:  80\n",
      "86.6 action:  [-0.0082, -0.991] n_targets:  2 reward:  160\n",
      "Firely capture rate for the episode:  6 ff for 102.49999999999845 s: -------------------> 0.06\n",
      "Total reward for the episode:  480.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  227\n",
      "3.5 action:  [-0.0057, -0.9969] n_targets:  1 reward:  80\n",
      "6.5 action:  [0.0092, -0.9844] n_targets:  2 reward:  160\n",
      "28.2 action:  [-0.0057, -0.9984] n_targets:  1 reward:  80\n",
      "68.1 action:  [-0.0082, -0.9854] n_targets:  1 reward:  80\n",
      "71.1 action:  [-0.001, -0.9857] n_targets:  1 reward:  80\n",
      "76.5 action:  [0.0019, -0.9994] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  7 ff for 102.49999999999845 s: -------------------> 0.07\n",
      "Total reward for the episode:  560.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  228\n",
      "11.4 action:  [0.0005, -0.9947] n_targets:  2 reward:  160\n",
      "12.9 action:  [0.0093, -0.9897] n_targets:  2 reward:  160\n",
      "23.1 action:  [-0.0049, -0.991] n_targets:  1 reward:  80\n",
      "35.9 action:  [0.0016, -0.9973] n_targets:  1 reward:  80\n",
      "47.3 action:  [0.0078, -0.9997] n_targets:  1 reward:  80\n",
      "61.2 action:  [-0.0022, -0.9996] n_targets:  1 reward:  80\n",
      "73.2 action:  [0.0085, -0.9948] n_targets:  2 reward:  160\n",
      "76.8 action:  [-0.0034, -0.9962] n_targets:  1 reward:  80\n",
      "78.5 action:  [0.0003, -0.9993] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  12 ff for 102.49999999999845 s: -------------------> 0.12\n",
      "Total reward for the episode:  960.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  229\n",
      "6.3 action:  [-0.0019, -0.9832] n_targets:  1 reward:  80\n",
      "8.4 action:  [-0.0054, -0.9901] n_targets:  2 reward:  160\n",
      "32.9 action:  [-0.0052, -0.9951] n_targets:  1 reward:  80\n",
      "46.3 action:  [-0.0008, -0.9963] n_targets:  1 reward:  80\n",
      "60.3 action:  [0.0092, -0.9949] n_targets:  1 reward:  80\n",
      "65.0 action:  [0.0041, -0.9996] n_targets:  1 reward:  80\n",
      "67.2 action:  [-0.0004, -0.9965] n_targets:  1 reward:  80\n",
      "TIME before resetting: 77.49999999999987\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  230\n",
      "8.5 action:  [0.0047, -0.9878] n_targets:  1 reward:  80\n",
      "18.7 action:  [0.0089, -0.9958] n_targets:  1 reward:  80\n",
      "23.6 action:  [-0.0028, -0.9868] n_targets:  1 reward:  80\n",
      "29.5 action:  [-0.0041, -0.9948] n_targets:  1 reward:  80\n",
      "31.1 action:  [-0.0057, -0.9995] n_targets:  1 reward:  80\n",
      "88.6 action:  [0.0096, -0.995] n_targets:  1 reward:  80\n",
      "98.5 action:  [-0.0023, -0.9989] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  7 ff for 102.49999999999845 s: -------------------> 0.07\n",
      "Total reward for the episode:  560.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  231\n",
      "1.7 action:  [0.0011, -0.9907] n_targets:  1 reward:  80\n",
      "9.4 action:  [0.0093, -0.9912] n_targets:  2 reward:  160\n",
      "21.2 action:  [-0.0082, -0.9987] n_targets:  2 reward:  160\n",
      "33.0 action:  [0.0072, -0.9802] n_targets:  1 reward:  80\n",
      "41.7 action:  [0.0097, -0.9953] n_targets:  1 reward:  80\n",
      "43.5 action:  [0.0073, -0.9871] n_targets:  1 reward:  80\n",
      "46.0 action:  [0.0031, -0.9897] n_targets:  1 reward:  80\n",
      "47.2 action:  [-0.0016, -0.9857] n_targets:  1 reward:  80\n",
      "50.3 action:  [0.0049, -0.9978] n_targets:  2 reward:  160\n",
      "70.0 action:  [-0.0057, -0.9995] n_targets:  1 reward:  80\n",
      "79.5 action:  [0.0069, -0.9976] n_targets:  2 reward:  160\n",
      "86.6 action:  [-0.0076, -0.9881] n_targets:  1 reward:  80\n",
      "96.5 action:  [-0.003, -0.9986] n_targets:  1 reward:  80\n",
      "100.6 action:  [0.0087, -0.9944] n_targets:  2 reward:  160\n",
      "Firely capture rate for the episode:  19 ff for 102.49999999999845 s: -------------------> 0.19\n",
      "Total reward for the episode:  1520.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  232\n",
      "10.9 action:  [-0.001, -0.9924] n_targets:  1 reward:  80\n",
      "14.0 action:  [0.0064, -0.9943] n_targets:  1 reward:  80\n",
      "37.9 action:  [-0.0028, -0.9907] n_targets:  1 reward:  80\n",
      "38.8 action:  [0.0045, -0.9915] n_targets:  1 reward:  80\n",
      "43.1 action:  [-0.0066, -0.9952] n_targets:  1 reward:  80\n",
      "47.9 action:  [0.0055, -0.9984] n_targets:  1 reward:  80\n",
      "50.9 action:  [-0.0009, -0.9883] n_targets:  1 reward:  80\n",
      "52.6 action:  [-0.0007, -0.9895] n_targets:  1 reward:  80\n",
      "59.7 action:  [0.006, -0.9882] n_targets:  1 reward:  80\n",
      "61.8 action:  [0.0085, -0.9974] n_targets:  1 reward:  80\n",
      "91.4 action:  [0.0037, -0.998] n_targets:  2 reward:  160\n",
      "93.9 action:  [0.0018, -0.993] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  13 ff for 102.49999999999845 s: -------------------> 0.13\n",
      "Total reward for the episode:  1040.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  233\n",
      "Eval num_timesteps=30000, episode_reward=1040.00 +/- 391.92\n",
      "Episode length: 1025.00 +/- 0.00\n",
      "New best mean reward!\n",
      "11.6 action:  [-0.003, -0.9946] n_targets:  1 reward:  80\n",
      "14.7 action:  [0.0003, -0.9963] n_targets:  1 reward:  80\n",
      "34.7 action:  [0.0072, -0.9992] n_targets:  1 reward:  80\n",
      "56.8 action:  [-0.0023, -0.9911] n_targets:  1 reward:  80\n",
      "72.7 action:  [-0.0084, -1.0] n_targets:  1 reward:  80\n",
      "92.9 action:  [-0.0059, -0.9886] n_targets:  2 reward:  160\n",
      "95.2 action:  [0.0099, -0.997] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  8 ff for 102.49999999999845 s: -------------------> 0.08\n",
      "Total reward for the episode:  640.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  234\n",
      "9.5 action:  [-0.0034, -0.9948] n_targets:  1 reward:  80\n",
      "19.6 action:  [-0.0036, -0.9977] n_targets:  2 reward:  160\n",
      "24.0 action:  [0.0072, -0.9911] n_targets:  1 reward:  80\n",
      "34.2 action:  [-0.009, -0.9944] n_targets:  1 reward:  80\n",
      "40.0 action:  [0.0067, -0.9923] n_targets:  1 reward:  80\n",
      "43.8 action:  [-0.0069, -0.9994] n_targets:  1 reward:  80\n",
      "47.4 action:  [0.001, -0.9997] n_targets:  1 reward:  80\n",
      "76.4 action:  [-0.0013, -0.9959] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  9 ff for 102.49999999999845 s: -------------------> 0.09\n",
      "Total reward for the episode:  720.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  235\n",
      "2.6 action:  [-0.0066, -0.9846] n_targets:  1 reward:  80\n",
      "15.6 action:  [0.0006, -0.9847] n_targets:  1 reward:  80\n",
      "27.6 action:  [0.0092, -0.992] n_targets:  1 reward:  80\n",
      "36.3 action:  [-0.0077, -0.9883] n_targets:  1 reward:  80\n",
      "41.3 action:  [0.0063, -0.9804] n_targets:  1 reward:  80\n",
      "56.5 action:  [-0.0071, -0.9998] n_targets:  1 reward:  80\n",
      "66.3 action:  [-0.0038, -0.9988] n_targets:  1 reward:  80\n",
      "88.1 action:  [-0.0085, -0.9999] n_targets:  2 reward:  160\n",
      "Firely capture rate for the episode:  9 ff for 102.49999999999845 s: -------------------> 0.09\n",
      "Total reward for the episode:  720.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  236\n",
      "7.1 action:  [-0.0037, -0.9985] n_targets:  1 reward:  80\n",
      "13.1 action:  [0.0036, -0.9999] n_targets:  2 reward:  160\n",
      "39.6 action:  [-0.0025, -0.9935] n_targets:  1 reward:  80\n",
      "44.6 action:  [0.0063, -0.9982] n_targets:  2 reward:  160\n",
      "65.9 action:  [-0.0015, -0.9996] n_targets:  1 reward:  80\n",
      "66.7 action:  [0.009, -0.9891] n_targets:  2 reward:  160\n",
      "84.2 action:  [-0.002, -0.9938] n_targets:  1 reward:  80\n",
      "99.0 action:  [-0.002, -0.9828] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  11 ff for 102.49999999999845 s: -------------------> 0.11\n",
      "Total reward for the episode:  880.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  237\n",
      "4.9 action:  [-0.0039, -0.9918] n_targets:  1 reward:  80\n",
      "5.3 action:  [-0.0082, -0.9999] n_targets:  1 reward:  80\n",
      "9.9 action:  [0.0092, -0.9983] n_targets:  1 reward:  80\n",
      "42.2 action:  [0.0008, -0.9988] n_targets:  1 reward:  80\n",
      "56.2 action:  [-0.0041, -0.9998] n_targets:  1 reward:  80\n",
      "65.4 action:  [-0.0033, -0.9963] n_targets:  2 reward:  160\n",
      "73.4 action:  [0.0066, -0.9993] n_targets:  1 reward:  80\n",
      "78.2 action:  [0.0049, -0.9923] n_targets:  1 reward:  80\n",
      "88.1 action:  [-0.0006, -0.9958] n_targets:  2 reward:  160\n",
      "97.1 action:  [-0.0003, -0.9897] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  12 ff for 102.49999999999845 s: -------------------> 0.12\n",
      "Total reward for the episode:  960.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  238\n",
      "5.2 action:  [0.0092, -0.9993] n_targets:  1 reward:  80\n",
      "15.0 action:  [0.0045, -0.994] n_targets:  1 reward:  80\n",
      "26.8 action:  [-0.0099, -0.9924] n_targets:  1 reward:  80\n",
      "31.9 action:  [0.0028, -0.9851] n_targets:  1 reward:  80\n",
      "63.4 action:  [0.0016, -0.9971] n_targets:  1 reward:  80\n",
      "73.8 action:  [-0.0081, -0.9957] n_targets:  1 reward:  80\n",
      "84.0 action:  [-0.0065, -0.9995] n_targets:  1 reward:  80\n",
      "92.8 action:  [0.0015, -0.9864] n_targets:  1 reward:  80\n",
      "97.8 action:  [0.0069, -0.9978] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  9 ff for 102.49999999999845 s: -------------------> 0.09\n",
      "Total reward for the episode:  720.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  239\n",
      "11.1 action:  [-0.0021, -0.9936] n_targets:  1 reward:  80\n",
      "19.3 action:  [0.0066, -0.9993] n_targets:  1 reward:  80\n",
      "25.5 action:  [0.0048, -0.9979] n_targets:  1 reward:  80\n",
      "32.4 action:  [-0.0082, -0.9805] n_targets:  1 reward:  80\n",
      "49.1 action:  [-0.0022, -0.996] n_targets:  1 reward:  80\n",
      "78.6 action:  [-0.0016, -0.9936] n_targets:  1 reward:  80\n",
      "80.9 action:  [-0.0027, -0.9996] n_targets:  1 reward:  80\n",
      "86.7 action:  [0.0063, -0.9967] n_targets:  2 reward:  160\n",
      "90.0 action:  [0.0097, -0.982] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  10 ff for 102.49999999999845 s: -------------------> 0.1\n",
      "Total reward for the episode:  800.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  240\n",
      "20.4 action:  [0.0026, -0.992] n_targets:  2 reward:  160\n",
      "27.6 action:  [0.0081, -0.9985] n_targets:  2 reward:  160\n",
      "38.0 action:  [0.0048, -0.9985] n_targets:  1 reward:  80\n",
      "46.6 action:  [0.0019, -0.9831] n_targets:  2 reward:  160\n",
      "49.2 action:  [-0.0091, -0.9954] n_targets:  1 reward:  80\n",
      "52.2 action:  [-0.0031, -1.0] n_targets:  1 reward:  80\n",
      "77.5 action:  [0.0016, -0.9893] n_targets:  1 reward:  80\n",
      "82.3 action:  [-0.0017, -0.9989] n_targets:  1 reward:  80\n",
      "90.9 action:  [0.0013, -0.9918] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  12 ff for 102.49999999999845 s: -------------------> 0.12\n",
      "Total reward for the episode:  960.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  241\n",
      "10.3 action:  [-0.0083, -0.9976] n_targets:  2 reward:  160\n",
      "19.8 action:  [0.0079, -0.9984] n_targets:  1 reward:  80\n",
      "20.9 action:  [-0.0027, -0.9904] n_targets:  1 reward:  80\n",
      "45.4 action:  [0.0064, -0.9962] n_targets:  1 reward:  80\n",
      "53.9 action:  [-0.0011, -0.9871] n_targets:  2 reward:  160\n",
      "64.1 action:  [-0.0075, -0.9974] n_targets:  2 reward:  160\n",
      "76.2 action:  [-0.0044, -0.9942] n_targets:  2 reward:  160\n",
      "84.5 action:  [0.0051, -0.9971] n_targets:  1 reward:  80\n",
      "93.1 action:  [-0.0036, -0.9998] n_targets:  2 reward:  160\n",
      "97.4 action:  [-0.0057, -0.9978] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  15 ff for 102.49999999999845 s: -------------------> 0.15\n",
      "Total reward for the episode:  1200.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  242\n",
      "4.4 action:  [-0.0024, -0.9989] n_targets:  1 reward:  80\n",
      "14.0 action:  [0.0084, -0.9942] n_targets:  1 reward:  80\n",
      "21.3 action:  [0.0033, -0.9966] n_targets:  1 reward:  80\n",
      "26.6 action:  [-0.0058, -0.9971] n_targets:  1 reward:  80\n",
      "70.6 action:  [-0.0078, -0.9907] n_targets:  1 reward:  80\n",
      "71.6 action:  [0.0056, -0.9952] n_targets:  1 reward:  80\n",
      "75.8 action:  [0.0041, -0.9876] n_targets:  3 reward:  240\n",
      "TIME before resetting: 77.49999999999987\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  243\n",
      "1.7 action:  [0.0015, -0.9941] n_targets:  1 reward:  80\n",
      "3.5 action:  [0.0046, -0.9994] n_targets:  2 reward:  160\n",
      "22.3 action:  [0.0091, -0.9993] n_targets:  2 reward:  160\n",
      "30.2 action:  [0.005, -0.996] n_targets:  1 reward:  80\n",
      "43.5 action:  [0.0014, -0.9944] n_targets:  2 reward:  160\n",
      "50.6 action:  [0.0054, -0.9887] n_targets:  1 reward:  80\n",
      "76.7 action:  [0.0028, -0.9995] n_targets:  1 reward:  80\n",
      "88.2 action:  [0.0065, -0.9988] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  11 ff for 102.49999999999845 s: -------------------> 0.11\n",
      "Total reward for the episode:  880.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  244\n",
      "9.3 action:  [0.008, -0.9992] n_targets:  1 reward:  80\n",
      "44.2 action:  [0.0031, -0.9995] n_targets:  2 reward:  160\n",
      "51.3 action:  [0.0088, -0.9987] n_targets:  2 reward:  160\n",
      "65.6 action:  [0.0091, -0.9973] n_targets:  1 reward:  80\n",
      "74.9 action:  [0.0062, -0.9984] n_targets:  1 reward:  80\n",
      "79.4 action:  [0.0058, -0.9968] n_targets:  1 reward:  80\n",
      "83.0 action:  [0.0028, -0.9977] n_targets:  2 reward:  160\n",
      "86.2 action:  [0.0078, -0.9991] n_targets:  1 reward:  80\n",
      "100.7 action:  [-0.005, -0.9933] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  12 ff for 102.49999999999845 s: -------------------> 0.12\n",
      "Total reward for the episode:  960.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  245\n",
      "4.4 action:  [0.0017, -0.9961] n_targets:  1 reward:  80\n",
      "14.5 action:  [0.006, -0.9993] n_targets:  1 reward:  80\n",
      "22.7 action:  [0.0009, -0.9999] n_targets:  1 reward:  80\n",
      "25.8 action:  [-0.0057, -0.9931] n_targets:  1 reward:  80\n",
      "35.6 action:  [-0.0092, -0.9949] n_targets:  1 reward:  80\n",
      "41.1 action:  [0.0076, -0.9987] n_targets:  1 reward:  80\n",
      "65.5 action:  [0.0072, -0.9933] n_targets:  1 reward:  80\n",
      "71.0 action:  [0.0014, -0.9987] n_targets:  1 reward:  80\n",
      "83.5 action:  [-0.0089, -0.9906] n_targets:  1 reward:  80\n",
      "87.5 action:  [-0.0005, -0.9889] n_targets:  1 reward:  80\n",
      "92.2 action:  [0.0028, -0.9992] n_targets:  1 reward:  80\n",
      "94.2 action:  [0.0077, -0.9846] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  12 ff for 102.49999999999845 s: -------------------> 0.12\n",
      "Total reward for the episode:  960.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  246\n",
      "Eval num_timesteps=40000, episode_reward=933.33 +/- 37.71\n",
      "Episode length: 1025.00 +/- 0.00\n",
      "14.6 action:  [0.0082, -0.9999] n_targets:  1 reward:  80\n",
      "25.4 action:  [0.0095, -0.9808] n_targets:  1 reward:  80\n",
      "29.1 action:  [-0.0065, -0.9985] n_targets:  1 reward:  80\n",
      "46.8 action:  [0.0092, -0.9983] n_targets:  1 reward:  80\n",
      "66.1 action:  [0.0011, -0.9949] n_targets:  1 reward:  80\n",
      "67.8 action:  [-0.0035, -0.9945] n_targets:  1 reward:  80\n",
      "68.9 action:  [0.0044, -0.9972] n_targets:  1 reward:  80\n",
      "73.6 action:  [-0.0018, -0.9985] n_targets:  2 reward:  160\n",
      "91.5 action:  [-0.0089, -0.9957] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  10 ff for 102.49999999999845 s: -------------------> 0.1\n",
      "Total reward for the episode:  800.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  247\n",
      "2.4 action:  [-0.0027, -0.9866] n_targets:  1 reward:  80\n",
      "11.1 action:  [-0.0024, -0.9937] n_targets:  1 reward:  80\n",
      "40.0 action:  [0.0034, -0.9994] n_targets:  1 reward:  80\n",
      "44.3 action:  [0.0025, -0.9972] n_targets:  1 reward:  80\n",
      "52.0 action:  [0.0027, -0.9918] n_targets:  1 reward:  80\n",
      "53.4 action:  [0.0085, -0.9879] n_targets:  1 reward:  80\n",
      "65.5 action:  [-0.0046, -0.9952] n_targets:  1 reward:  80\n",
      "81.6 action:  [0.0094, -0.9989] n_targets:  2 reward:  160\n",
      "85.7 action:  [-0.01, -0.9952] n_targets:  1 reward:  80\n",
      "93.6 action:  [-0.0083, -0.9947] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  11 ff for 102.49999999999845 s: -------------------> 0.11\n",
      "Total reward for the episode:  880.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  248\n",
      "7.1 action:  [0.0045, -0.994] n_targets:  1 reward:  80\n",
      "37.6 action:  [0.0074, -0.9922] n_targets:  1 reward:  80\n",
      "59.0 action:  [0.0081, -0.9988] n_targets:  1 reward:  80\n",
      "66.1 action:  [0.0004, -0.9969] n_targets:  1 reward:  80\n",
      "74.8 action:  [-0.0057, -0.9943] n_targets:  1 reward:  80\n",
      "89.4 action:  [-0.0039, -0.9929] n_targets:  2 reward:  160\n",
      "92.4 action:  [-0.0004, -0.9961] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  8 ff for 102.49999999999845 s: -------------------> 0.08\n",
      "Total reward for the episode:  640.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  249\n",
      "16.3 action:  [0.0073, -0.9994] n_targets:  1 reward:  80\n",
      "20.3 action:  [0.0077, -0.9994] n_targets:  1 reward:  80\n",
      "22.7 action:  [0.0015, -0.9982] n_targets:  2 reward:  160\n",
      "26.7 action:  [0.0002, -0.9993] n_targets:  1 reward:  80\n",
      "57.4 action:  [0.0051, -0.9962] n_targets:  1 reward:  80\n",
      "75.7 action:  [-0.0098, -0.9967] n_targets:  1 reward:  80\n",
      "81.8 action:  [0.0005, -0.9904] n_targets:  1 reward:  80\n",
      "95.1 action:  [0.0042, -1.0] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  9 ff for 102.49999999999845 s: -------------------> 0.09\n",
      "Total reward for the episode:  720.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  250\n",
      "19.8 action:  [-0.0001, -0.9802] n_targets:  1 reward:  80\n",
      "21.3 action:  [0.0081, -0.9978] n_targets:  1 reward:  80\n",
      "32.2 action:  [0.0036, -0.997] n_targets:  1 reward:  80\n",
      "35.9 action:  [-0.0064, -0.997] n_targets:  1 reward:  80\n",
      "43.6 action:  [0.0009, -0.9986] n_targets:  1 reward:  80\n",
      "56.6 action:  [-0.0099, -0.9956] n_targets:  1 reward:  80\n",
      "63.6 action:  [0.0049, -0.9964] n_targets:  1 reward:  80\n",
      "68.5 action:  [0.0024, -0.9962] n_targets:  1 reward:  80\n",
      "80.8 action:  [-0.0078, -0.9926] n_targets:  1 reward:  80\n",
      "82.1 action:  [0.0027, -0.9998] n_targets:  1 reward:  80\n",
      "86.9 action:  [-0.0, -0.9998] n_targets:  1 reward:  80\n",
      "93.5 action:  [0.0071, -0.9965] n_targets:  1 reward:  80\n",
      "100.9 action:  [0.0064, -0.9891] n_targets:  1 reward:  80\n",
      "101.5 action:  [-0.0058, -0.9959] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  14 ff for 102.49999999999845 s: -------------------> 0.14\n",
      "Total reward for the episode:  1120.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  251\n",
      "5.0 action:  [0.0011, -0.9946] n_targets:  1 reward:  80\n",
      "7.5 action:  [0.0055, -0.9985] n_targets:  2 reward:  160\n",
      "19.5 action:  [0.0071, -0.9936] n_targets:  1 reward:  80\n",
      "26.1 action:  [0.0026, -0.9927] n_targets:  1 reward:  80\n",
      "36.7 action:  [-0.0053, -0.9967] n_targets:  1 reward:  80\n",
      "38.4 action:  [0.0029, -0.9979] n_targets:  1 reward:  80\n",
      "41.9 action:  [0.0083, -0.9999] n_targets:  2 reward:  160\n",
      "59.1 action:  [-0.0006, -0.9974] n_targets:  1 reward:  80\n",
      "63.4 action:  [0.0071, -0.9857] n_targets:  2 reward:  160\n",
      "77.8 action:  [0.0082, -0.9958] n_targets:  2 reward:  160\n",
      "80.3 action:  [-0.0053, -0.9965] n_targets:  1 reward:  80\n",
      "94.1 action:  [0.0084, -0.9961] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  16 ff for 102.49999999999845 s: -------------------> 0.16\n",
      "Total reward for the episode:  1280.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  252\n",
      "7.7 action:  [0.0045, -0.9977] n_targets:  1 reward:  80\n",
      "9.2 action:  [-0.0034, -0.9988] n_targets:  1 reward:  80\n",
      "10.0 action:  [0.0026, -0.997] n_targets:  1 reward:  80\n",
      "14.2 action:  [0.0059, -0.9991] n_targets:  1 reward:  80\n",
      "21.3 action:  [-0.006, -0.9827] n_targets:  1 reward:  80\n",
      "28.6 action:  [-0.0001, -0.9997] n_targets:  1 reward:  80\n",
      "32.0 action:  [0.0084, -0.9974] n_targets:  1 reward:  80\n",
      "33.8 action:  [-0.0099, -0.9926] n_targets:  2 reward:  160\n",
      "36.2 action:  [-0.0055, -0.9913] n_targets:  1 reward:  80\n",
      "41.2 action:  [-0.0005, -0.9972] n_targets:  1 reward:  80\n",
      "46.4 action:  [-0.0013, -0.9999] n_targets:  1 reward:  80\n",
      "64.7 action:  [0.001, -0.9987] n_targets:  2 reward:  160\n",
      "87.0 action:  [0.005, -0.9983] n_targets:  1 reward:  80\n",
      "96.4 action:  [-0.0013, -0.9831] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  16 ff for 102.49999999999845 s: -------------------> 0.16\n",
      "Total reward for the episode:  1280.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  253\n",
      "6.0 action:  [-0.0076, -0.9874] n_targets:  1 reward:  80\n",
      "11.4 action:  [0.0011, -0.9955] n_targets:  1 reward:  80\n",
      "18.1 action:  [0.0044, -0.9961] n_targets:  1 reward:  80\n",
      "22.9 action:  [-0.0031, -0.9995] n_targets:  1 reward:  80\n",
      "33.1 action:  [-0.0052, -0.9946] n_targets:  1 reward:  80\n",
      "56.5 action:  [0.007, -0.9999] n_targets:  2 reward:  160\n",
      "60.7 action:  [-0.0007, -0.9982] n_targets:  1 reward:  80\n",
      "72.1 action:  [-0.0062, -0.9955] n_targets:  1 reward:  80\n",
      "79.1 action:  [0.0074, -0.9983] n_targets:  1 reward:  80\n",
      "80.4 action:  [-0.0009, -0.9841] n_targets:  1 reward:  80\n",
      "82.3 action:  [-0.0096, -0.9954] n_targets:  1 reward:  80\n",
      "87.2 action:  [-0.0074, -0.9984] n_targets:  1 reward:  80\n",
      "90.2 action:  [-0.0048, -0.9931] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  14 ff for 102.49999999999845 s: -------------------> 0.14\n",
      "Total reward for the episode:  1120.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  254\n",
      "3.8 action:  [0.0056, -0.9989] n_targets:  1 reward:  80\n",
      "12.1 action:  [0.0018, -0.9963] n_targets:  1 reward:  80\n",
      "14.3 action:  [0.0062, -0.9945] n_targets:  1 reward:  80\n",
      "20.9 action:  [-0.0047, -0.9997] n_targets:  2 reward:  160\n",
      "27.7 action:  [-0.0022, -0.9943] n_targets:  1 reward:  80\n",
      "39.1 action:  [-0.0083, -0.9949] n_targets:  2 reward:  160\n",
      "48.2 action:  [-0.0029, -0.9919] n_targets:  1 reward:  80\n",
      "55.5 action:  [0.002, -0.9833] n_targets:  1 reward:  80\n",
      "64.9 action:  [0.0049, -0.9815] n_targets:  1 reward:  80\n",
      "70.2 action:  [0.0005, -0.9983] n_targets:  1 reward:  80\n",
      "78.8 action:  [-0.0091, -0.9992] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  13 ff for 102.49999999999845 s: -------------------> 0.13\n",
      "Total reward for the episode:  1040.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  255\n",
      "6.8 action:  [-0.0061, -0.9918] n_targets:  1 reward:  80\n",
      "13.8 action:  [0.0065, -0.9964] n_targets:  1 reward:  80\n",
      "21.8 action:  [-0.0012, -0.9955] n_targets:  1 reward:  80\n",
      "25.1 action:  [-0.0069, -0.9996] n_targets:  2 reward:  160\n",
      "42.4 action:  [0.0078, -0.998] n_targets:  1 reward:  80\n",
      "44.1 action:  [-0.0097, -0.9979] n_targets:  1 reward:  80\n",
      "51.1 action:  [-0.0, -0.9948] n_targets:  1 reward:  80\n",
      "52.6 action:  [0.0055, -0.9838] n_targets:  1 reward:  80\n",
      "54.7 action:  [0.0032, -0.9969] n_targets:  2 reward:  160\n",
      "68.3 action:  [0.002, -0.9998] n_targets:  1 reward:  80\n",
      "68.9 action:  [-0.0087, -0.9943] n_targets:  1 reward:  80\n",
      "69.4 action:  [0.0004, -0.9907] n_targets:  1 reward:  80\n",
      "72.8 action:  [0.0045, -0.9907] n_targets:  1 reward:  80\n",
      "75.7 action:  [0.0002, -0.9997] n_targets:  1 reward:  80\n",
      "TIME before resetting: 77.49999999999987\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  256\n",
      "11.7 action:  [0.0016, -0.9982] n_targets:  1 reward:  80\n",
      "42.1 action:  [-0.0066, -0.9827] n_targets:  1 reward:  80\n",
      "42.6 action:  [0.0004, -0.9866] n_targets:  1 reward:  80\n",
      "49.5 action:  [0.009, -0.995] n_targets:  1 reward:  80\n",
      "50.1 action:  [0.0005, -0.9971] n_targets:  1 reward:  80\n",
      "63.0 action:  [0.0014, -0.9814] n_targets:  1 reward:  80\n",
      "89.1 action:  [-0.0051, -0.9947] n_targets:  1 reward:  80\n",
      "91.2 action:  [-0.0028, -0.9974] n_targets:  1 reward:  80\n",
      "96.5 action:  [0.0066, -0.9848] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  9 ff for 102.49999999999845 s: -------------------> 0.09\n",
      "Total reward for the episode:  720.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  257\n",
      "20.9 action:  [0.0057, -0.998] n_targets:  2 reward:  160\n",
      "28.2 action:  [-0.0084, -0.9957] n_targets:  1 reward:  80\n",
      "46.3 action:  [0.0005, -0.9984] n_targets:  1 reward:  80\n",
      "55.3 action:  [-0.0004, -0.9847] n_targets:  1 reward:  80\n",
      "65.2 action:  [-0.0019, -0.9996] n_targets:  1 reward:  80\n",
      "68.8 action:  [-0.0063, -0.998] n_targets:  1 reward:  80\n",
      "88.5 action:  [0.0027, -0.9881] n_targets:  1 reward:  80\n",
      "97.9 action:  [-0.0084, -0.9983] n_targets:  2 reward:  160\n",
      "Firely capture rate for the episode:  10 ff for 102.49999999999845 s: -------------------> 0.1\n",
      "Total reward for the episode:  800.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  258\n",
      "16.7 action:  [0.0034, -0.9992] n_targets:  2 reward:  160\n",
      "22.5 action:  [-0.0048, -0.9813] n_targets:  1 reward:  80\n",
      "46.0 action:  [-0.0054, -0.982] n_targets:  2 reward:  160\n",
      "86.1 action:  [0.0088, -0.9992] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  6 ff for 102.49999999999845 s: -------------------> 0.06\n",
      "Total reward for the episode:  480.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  259\n",
      "Eval num_timesteps=50000, episode_reward=666.67 +/- 135.97\n",
      "Episode length: 1025.00 +/- 0.00\n",
      "28.8 action:  [-0.0047, -0.999] n_targets:  1 reward:  80\n",
      "31.2 action:  [-0.0059, -0.9973] n_targets:  1 reward:  80\n",
      "33.3 action:  [0.0043, -0.9934] n_targets:  1 reward:  80\n",
      "45.7 action:  [-0.0072, -0.9862] n_targets:  1 reward:  80\n",
      "48.6 action:  [0.0005, -0.9991] n_targets:  2 reward:  160\n",
      "62.5 action:  [-0.0012, -0.9982] n_targets:  1 reward:  80\n",
      "80.8 action:  [0.0058, -0.9802] n_targets:  1 reward:  80\n",
      "88.0 action:  [0.0036, -0.995] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  9 ff for 102.49999999999845 s: -------------------> 0.09\n",
      "Total reward for the episode:  720.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  260\n",
      "7.0 action:  [-0.0036, -0.9986] n_targets:  1 reward:  80\n",
      "34.8 action:  [-0.0077, -0.9952] n_targets:  1 reward:  80\n",
      "41.5 action:  [0.0004, -0.9959] n_targets:  2 reward:  160\n",
      "63.8 action:  [-0.006, -0.999] n_targets:  1 reward:  80\n",
      "85.9 action:  [-0.0056, -0.9953] n_targets:  1 reward:  80\n",
      "89.2 action:  [-0.0012, -0.9999] n_targets:  2 reward:  160\n",
      "99.0 action:  [-0.0022, -0.9925] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  9 ff for 102.49999999999845 s: -------------------> 0.09\n",
      "Total reward for the episode:  720.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  261\n",
      "2.7 action:  [-0.0082, -0.9921] n_targets:  1 reward:  80\n",
      "5.6 action:  [-0.0052, -0.9937] n_targets:  1 reward:  80\n",
      "6.2 action:  [0.002, -0.9989] n_targets:  1 reward:  80\n",
      "10.7 action:  [0.0001, -0.9981] n_targets:  2 reward:  160\n",
      "17.4 action:  [0.0081, -0.9959] n_targets:  1 reward:  80\n",
      "23.0 action:  [0.0052, -0.9996] n_targets:  2 reward:  160\n",
      "28.5 action:  [0.0065, -0.9978] n_targets:  1 reward:  80\n",
      "41.6 action:  [-0.0029, -0.9979] n_targets:  1 reward:  80\n",
      "51.9 action:  [0.0085, -0.9847] n_targets:  1 reward:  80\n",
      "54.4 action:  [0.0002, -0.9997] n_targets:  1 reward:  80\n",
      "62.9 action:  [0.0045, -0.9811] n_targets:  1 reward:  80\n",
      "69.7 action:  [0.0032, -0.9999] n_targets:  1 reward:  80\n",
      "74.7 action:  [-0.0056, -0.9993] n_targets:  1 reward:  80\n",
      "76.2 action:  [-0.0018, -0.9992] n_targets:  1 reward:  80\n",
      "78.9 action:  [0.0066, -0.9903] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  17 ff for 102.49999999999845 s: -------------------> 0.17\n",
      "Total reward for the episode:  1360.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  262\n",
      "8.6 action:  [0.0018, -0.9953] n_targets:  1 reward:  80\n",
      "30.1 action:  [-0.006, -0.9995] n_targets:  1 reward:  80\n",
      "32.3 action:  [0.0055, -0.999] n_targets:  1 reward:  80\n",
      "39.6 action:  [0.002, -0.9998] n_targets:  1 reward:  80\n",
      "47.5 action:  [-0.0036, -0.9926] n_targets:  2 reward:  160\n",
      "50.9 action:  [-0.0012, -0.9974] n_targets:  1 reward:  80\n",
      "54.2 action:  [0.0097, -0.9822] n_targets:  1 reward:  80\n",
      "59.1 action:  [-0.0026, -0.9996] n_targets:  1 reward:  80\n",
      "61.1 action:  [0.0005, -0.9992] n_targets:  1 reward:  80\n",
      "62.9 action:  [-0.0034, -0.9994] n_targets:  1 reward:  80\n",
      "70.1 action:  [0.007, -0.9984] n_targets:  1 reward:  80\n",
      "73.6 action:  [-0.0096, -0.9814] n_targets:  3 reward:  240\n",
      "88.4 action:  [0.0046, -0.9923] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  16 ff for 102.49999999999845 s: -------------------> 0.16\n",
      "Total reward for the episode:  1280.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  263\n",
      "10.9 action:  [-0.0025, -0.9803] n_targets:  1 reward:  80\n",
      "21.3 action:  [-0.007, -0.9885] n_targets:  1 reward:  80\n",
      "24.8 action:  [0.0009, -0.9994] n_targets:  1 reward:  80\n",
      "27.9 action:  [-0.0011, -0.9902] n_targets:  1 reward:  80\n",
      "74.6 action:  [-0.0099, -0.9989] n_targets:  1 reward:  80\n",
      "89.2 action:  [-0.0049, -0.9864] n_targets:  1 reward:  80\n",
      "99.6 action:  [-0.0091, -0.9875] n_targets:  1 reward:  80\n",
      "102.1 action:  [-0.0026, -0.9977] n_targets:  2 reward:  160\n",
      "Firely capture rate for the episode:  9 ff for 102.49999999999845 s: -------------------> 0.09\n",
      "Total reward for the episode:  720.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  264\n",
      "5.3 action:  [-0.0002, -0.9988] n_targets:  1 reward:  80\n",
      "7.8 action:  [0.0034, -0.9978] n_targets:  1 reward:  80\n",
      "9.0 action:  [-0.0059, -0.9984] n_targets:  1 reward:  80\n",
      "19.3 action:  [-0.0071, -0.9868] n_targets:  1 reward:  80\n",
      "29.3 action:  [0.0014, -0.9959] n_targets:  2 reward:  160\n",
      "31.0 action:  [-0.0099, -0.9984] n_targets:  1 reward:  80\n",
      "53.6 action:  [0.004, -0.9997] n_targets:  1 reward:  80\n",
      "58.6 action:  [0.0012, -0.9974] n_targets:  1 reward:  80\n",
      "62.7 action:  [0.0012, -0.9842] n_targets:  1 reward:  80\n",
      "72.2 action:  [0.0021, -0.9843] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  11 ff for 102.49999999999845 s: -------------------> 0.11\n",
      "Total reward for the episode:  880.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  265\n",
      "9.1 action:  [-0.0045, -0.9958] n_targets:  1 reward:  80\n",
      "12.6 action:  [0.0001, -0.9824] n_targets:  2 reward:  160\n",
      "17.8 action:  [0.0015, -0.9991] n_targets:  2 reward:  160\n",
      "31.3 action:  [0.0048, -0.9906] n_targets:  1 reward:  80\n",
      "41.1 action:  [-0.0073, -0.9989] n_targets:  1 reward:  80\n",
      "53.5 action:  [-0.0081, -0.9999] n_targets:  1 reward:  80\n",
      "64.2 action:  [0.0081, -0.9992] n_targets:  1 reward:  80\n",
      "70.2 action:  [0.0093, -0.9963] n_targets:  1 reward:  80\n",
      "78.4 action:  [-0.0088, -0.9997] n_targets:  1 reward:  80\n",
      "89.8 action:  [-0.0068, -0.9887] n_targets:  1 reward:  80\n",
      "96.7 action:  [-0.0076, -0.9914] n_targets:  1 reward:  80\n",
      "99.1 action:  [-0.0019, -0.986] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  14 ff for 102.49999999999845 s: -------------------> 0.14\n",
      "Total reward for the episode:  1120.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  266\n",
      "4.1 action:  [0.0063, -0.989] n_targets:  1 reward:  80\n",
      "9.0 action:  [0.0005, -0.9906] n_targets:  1 reward:  80\n",
      "19.5 action:  [0.0045, -0.9931] n_targets:  1 reward:  80\n",
      "27.3 action:  [0.0083, -0.9848] n_targets:  1 reward:  80\n",
      "36.3 action:  [0.0044, -0.9883] n_targets:  1 reward:  80\n",
      "39.3 action:  [-0.003, -0.985] n_targets:  2 reward:  160\n",
      "42.2 action:  [-0.0056, -0.9985] n_targets:  2 reward:  160\n",
      "51.4 action:  [-0.0052, -0.9969] n_targets:  1 reward:  80\n",
      "59.2 action:  [0.0011, -0.994] n_targets:  1 reward:  80\n",
      "77.5 action:  [0.0004, -0.9843] n_targets:  1 reward:  80\n",
      "83.5 action:  [-0.0074, -0.993] n_targets:  1 reward:  80\n",
      "87.9 action:  [0.0093, -0.9821] n_targets:  1 reward:  80\n",
      "96.3 action:  [-0.0089, -0.994] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  15 ff for 102.49999999999845 s: -------------------> 0.15\n",
      "Total reward for the episode:  1200.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  267\n",
      "8.3 action:  [0.0043, -0.9975] n_targets:  1 reward:  80\n",
      "32.5 action:  [0.0005, -0.9987] n_targets:  1 reward:  80\n",
      "32.9 action:  [0.0017, -0.9941] n_targets:  1 reward:  80\n",
      "37.0 action:  [0.009, -0.9991] n_targets:  1 reward:  80\n",
      "41.4 action:  [0.0089, -0.9982] n_targets:  1 reward:  80\n",
      "46.9 action:  [-0.0055, -0.997] n_targets:  1 reward:  80\n",
      "60.0 action:  [-0.0061, -0.9997] n_targets:  1 reward:  80\n",
      "61.4 action:  [-0.0047, -0.9987] n_targets:  1 reward:  80\n",
      "82.5 action:  [0.0062, -0.9875] n_targets:  1 reward:  80\n",
      "88.4 action:  [-0.0087, -0.9876] n_targets:  1 reward:  80\n",
      "90.6 action:  [0.0016, -0.9947] n_targets:  1 reward:  80\n",
      "98.0 action:  [0.0026, -0.9895] n_targets:  2 reward:  160\n",
      "101.5 action:  [0.0058, -0.9987] n_targets:  2 reward:  160\n",
      "Firely capture rate for the episode:  15 ff for 102.49999999999845 s: -------------------> 0.15\n",
      "Total reward for the episode:  1200.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  268\n",
      "4.6 action:  [-0.0059, -0.9834] n_targets:  1 reward:  80\n",
      "9.9 action:  [-0.0, -0.9988] n_targets:  1 reward:  80\n",
      "18.4 action:  [0.0014, -0.9932] n_targets:  1 reward:  80\n",
      "21.9 action:  [0.0027, -0.9932] n_targets:  1 reward:  80\n",
      "23.0 action:  [0.0037, -0.9958] n_targets:  1 reward:  80\n",
      "32.9 action:  [0.0007, -0.9923] n_targets:  1 reward:  80\n",
      "35.5 action:  [0.0004, -0.9981] n_targets:  1 reward:  80\n",
      "40.6 action:  [0.0094, -0.9814] n_targets:  1 reward:  80\n",
      "42.5 action:  [0.0022, -0.9985] n_targets:  1 reward:  80\n",
      "48.3 action:  [0.0016, -0.9997] n_targets:  1 reward:  80\n",
      "52.3 action:  [0.0089, -0.9989] n_targets:  1 reward:  80\n",
      "56.2 action:  [0.0055, -0.9854] n_targets:  1 reward:  80\n",
      "68.3 action:  [0.0091, -0.9994] n_targets:  1 reward:  80\n",
      "74.3 action:  [0.0018, -0.9989] n_targets:  1 reward:  80\n",
      "TIME before resetting: 77.49999999999987\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  269\n",
      "13.6 action:  [-0.0074, -0.9986] n_targets:  1 reward:  80\n",
      "16.2 action:  [0.0075, -0.9831] n_targets:  1 reward:  80\n",
      "18.0 action:  [0.006, -0.9944] n_targets:  1 reward:  80\n",
      "37.7 action:  [0.0087, -0.9871] n_targets:  1 reward:  80\n",
      "44.9 action:  [-0.0093, -0.994] n_targets:  2 reward:  160\n",
      "52.3 action:  [-0.0063, -0.9938] n_targets:  1 reward:  80\n",
      "55.9 action:  [0.0067, -0.9841] n_targets:  1 reward:  80\n",
      "57.6 action:  [0.0092, -0.9987] n_targets:  1 reward:  80\n",
      "62.3 action:  [0.0095, -0.9993] n_targets:  2 reward:  160\n",
      "75.3 action:  [0.0034, -0.9939] n_targets:  1 reward:  80\n",
      "87.9 action:  [-0.0018, -0.9957] n_targets:  1 reward:  80\n",
      "93.8 action:  [0.0015, -0.9986] n_targets:  1 reward:  80\n",
      "98.6 action:  [-0.0033, -0.9838] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  15 ff for 102.49999999999845 s: -------------------> 0.15\n",
      "Total reward for the episode:  1200.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  270\n",
      "5.2 action:  [0.0004, -0.9821] n_targets:  1 reward:  80\n",
      "13.1 action:  [-0.0084, -0.9941] n_targets:  1 reward:  80\n",
      "16.3 action:  [0.0084, -0.9963] n_targets:  2 reward:  160\n",
      "20.6 action:  [0.0006, -0.9984] n_targets:  1 reward:  80\n",
      "23.9 action:  [0.0082, -0.998] n_targets:  1 reward:  80\n",
      "34.3 action:  [-0.0098, -0.9804] n_targets:  2 reward:  160\n",
      "47.1 action:  [-0.0013, -0.9953] n_targets:  1 reward:  80\n",
      "59.4 action:  [-0.0079, -0.997] n_targets:  1 reward:  80\n",
      "74.1 action:  [-0.0013, -0.9965] n_targets:  1 reward:  80\n",
      "98.4 action:  [0.0096, -0.9953] n_targets:  2 reward:  160\n",
      "Firely capture rate for the episode:  13 ff for 102.49999999999845 s: -------------------> 0.13\n",
      "Total reward for the episode:  1040.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  271\n",
      "9.4 action:  [-0.0066, -0.9974] n_targets:  1 reward:  80\n",
      "12.0 action:  [0.0041, -0.9983] n_targets:  1 reward:  80\n",
      "22.5 action:  [-0.0087, -0.9992] n_targets:  1 reward:  80\n",
      "36.0 action:  [-0.0015, -0.9988] n_targets:  2 reward:  160\n",
      "38.7 action:  [0.0064, -0.9964] n_targets:  1 reward:  80\n",
      "39.7 action:  [0.0041, -0.9815] n_targets:  1 reward:  80\n",
      "49.1 action:  [-0.0086, -0.995] n_targets:  1 reward:  80\n",
      "52.2 action:  [0.0039, -0.9947] n_targets:  1 reward:  80\n",
      "64.1 action:  [0.0055, -0.9986] n_targets:  1 reward:  80\n",
      "70.2 action:  [0.0093, -0.984] n_targets:  1 reward:  80\n",
      "71.7 action:  [-0.0011, -0.9964] n_targets:  1 reward:  80\n",
      "84.9 action:  [-0.0077, -0.9948] n_targets:  2 reward:  160\n",
      "97.5 action:  [0.0007, -0.9845] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  15 ff for 102.49999999999845 s: -------------------> 0.15\n",
      "Total reward for the episode:  1200.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  272\n",
      "Eval num_timesteps=60000, episode_reward=1146.67 +/- 75.42\n",
      "Episode length: 1025.00 +/- 0.00\n",
      "New best mean reward!\n",
      "9.0 action:  [0.0002, -0.9805] n_targets:  1 reward:  80\n",
      "22.5 action:  [-0.0025, -0.9802] n_targets:  1 reward:  80\n",
      "32.5 action:  [0.007, -0.9872] n_targets:  1 reward:  80\n",
      "36.4 action:  [0.0087, -0.9925] n_targets:  1 reward:  80\n",
      "42.6 action:  [-0.0011, -0.9991] n_targets:  1 reward:  80\n",
      "45.8 action:  [0.0016, -0.9913] n_targets:  1 reward:  80\n",
      "53.4 action:  [-0.0039, -0.99] n_targets:  1 reward:  80\n",
      "57.0 action:  [0.01, -0.9925] n_targets:  1 reward:  80\n",
      "60.4 action:  [-0.0057, -0.9899] n_targets:  1 reward:  80\n",
      "66.5 action:  [0.003, -0.9994] n_targets:  1 reward:  80\n",
      "67.8 action:  [-0.0006, -0.9998] n_targets:  1 reward:  80\n",
      "68.5 action:  [0.0016, -0.9992] n_targets:  1 reward:  80\n",
      "74.7 action:  [0.0035, -0.9982] n_targets:  1 reward:  80\n",
      "77.9 action:  [-0.0065, -0.9841] n_targets:  1 reward:  80\n",
      "78.7 action:  [-0.006, -0.9918] n_targets:  1 reward:  80\n",
      "86.9 action:  [0.0079, -1.0] n_targets:  2 reward:  160\n",
      "93.1 action:  [-0.0086, -0.9975] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  18 ff for 102.49999999999845 s: -------------------> 0.18\n",
      "Total reward for the episode:  1440.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  273\n",
      "9.2 action:  [0.0085, -0.9895] n_targets:  1 reward:  80\n",
      "10.3 action:  [0.0033, -0.9985] n_targets:  1 reward:  80\n",
      "12.6 action:  [0.005, -0.9899] n_targets:  1 reward:  80\n",
      "21.4 action:  [-0.0025, -0.99] n_targets:  1 reward:  80\n",
      "28.5 action:  [0.0087, -0.9995] n_targets:  1 reward:  80\n",
      "31.5 action:  [0.0025, -0.9873] n_targets:  1 reward:  80\n",
      "32.2 action:  [-0.0028, -0.988] n_targets:  1 reward:  80\n",
      "33.6 action:  [0.0098, -0.9998] n_targets:  1 reward:  80\n",
      "42.6 action:  [0.0051, -0.991] n_targets:  2 reward:  160\n",
      "73.1 action:  [-0.0072, -0.9944] n_targets:  1 reward:  80\n",
      "78.4 action:  [0.0062, -0.9811] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  12 ff for 102.49999999999845 s: -------------------> 0.12\n",
      "Total reward for the episode:  960.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  274\n",
      "7.2 action:  [-0.0028, -0.9941] n_targets:  1 reward:  80\n",
      "9.1 action:  [0.0005, -0.9888] n_targets:  1 reward:  80\n",
      "13.4 action:  [0.004, -0.9975] n_targets:  1 reward:  80\n",
      "14.6 action:  [0.0057, -0.9969] n_targets:  1 reward:  80\n",
      "20.5 action:  [0.0012, -0.9927] n_targets:  1 reward:  80\n",
      "29.3 action:  [0.0031, -0.9887] n_targets:  1 reward:  80\n",
      "32.8 action:  [-0.0052, -0.9954] n_targets:  1 reward:  80\n",
      "34.0 action:  [-0.0083, -0.9997] n_targets:  1 reward:  80\n",
      "36.3 action:  [-0.0014, -0.9956] n_targets:  1 reward:  80\n",
      "40.1 action:  [-0.0083, -0.9864] n_targets:  2 reward:  160\n",
      "46.0 action:  [-0.0014, -0.9848] n_targets:  1 reward:  80\n",
      "51.0 action:  [-0.0046, -0.9989] n_targets:  1 reward:  80\n",
      "55.5 action:  [0.0009, -0.9971] n_targets:  1 reward:  80\n",
      "57.8 action:  [-0.0008, -0.997] n_targets:  2 reward:  160\n",
      "58.9 action:  [0.0069, -0.9863] n_targets:  1 reward:  80\n",
      "61.5 action:  [0.0026, -0.9971] n_targets:  1 reward:  80\n",
      "64.1 action:  [-0.0096, -0.9894] n_targets:  1 reward:  80\n",
      "72.5 action:  [0.0078, -0.9973] n_targets:  1 reward:  80\n",
      "83.6 action:  [0.0, -0.9911] n_targets:  1 reward:  80\n",
      "84.5 action:  [0.0036, -0.9987] n_targets:  2 reward:  160\n",
      "87.0 action:  [0.0056, -0.9814] n_targets:  1 reward:  80\n",
      "95.3 action:  [-0.0034, -0.9978] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  25 ff for 102.49999999999845 s: -------------------> 0.24\n",
      "Total reward for the episode:  2000.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  275\n",
      "5.3 action:  [-0.0005, -0.9932] n_targets:  1 reward:  80\n",
      "8.2 action:  [-0.006, -0.9994] n_targets:  1 reward:  80\n",
      "12.0 action:  [-0.0091, -0.9938] n_targets:  1 reward:  80\n",
      "17.6 action:  [0.0007, -0.9927] n_targets:  1 reward:  80\n",
      "24.2 action:  [0.0044, -0.9981] n_targets:  1 reward:  80\n",
      "51.2 action:  [0.004, -0.9993] n_targets:  1 reward:  80\n",
      "60.0 action:  [-0.002, -0.9956] n_targets:  1 reward:  80\n",
      "74.1 action:  [0.0055, -0.9973] n_targets:  1 reward:  80\n",
      "86.4 action:  [0.0046, -0.9881] n_targets:  1 reward:  80\n",
      "92.4 action:  [0.0033, -0.9918] n_targets:  1 reward:  80\n",
      "94.1 action:  [0.004, -0.9963] n_targets:  1 reward:  80\n",
      "96.4 action:  [-0.0011, -0.999] n_targets:  1 reward:  80\n",
      "100.4 action:  [0.0087, -0.9977] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  13 ff for 102.49999999999845 s: -------------------> 0.13\n",
      "Total reward for the episode:  1040.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  276\n",
      "7.7 action:  [0.0074, -0.996] n_targets:  1 reward:  80\n",
      "10.8 action:  [-0.01, -0.9986] n_targets:  1 reward:  80\n",
      "22.8 action:  [0.0098, -0.9989] n_targets:  2 reward:  160\n",
      "28.3 action:  [0.0012, -0.9879] n_targets:  1 reward:  80\n",
      "30.9 action:  [0.0012, -0.993] n_targets:  2 reward:  160\n",
      "33.1 action:  [-0.0066, -0.989] n_targets:  2 reward:  160\n",
      "39.9 action:  [-0.007, -0.9982] n_targets:  1 reward:  80\n",
      "52.1 action:  [0.0031, -0.9866] n_targets:  1 reward:  80\n",
      "55.4 action:  [0.0082, -0.9998] n_targets:  2 reward:  160\n",
      "62.8 action:  [0.0023, -0.999] n_targets:  1 reward:  80\n",
      "69.3 action:  [-0.0098, -0.9984] n_targets:  1 reward:  80\n",
      "71.4 action:  [0.0082, -0.9858] n_targets:  1 reward:  80\n",
      "80.6 action:  [-0.005, -0.9809] n_targets:  1 reward:  80\n",
      "89.6 action:  [-0.0081, -0.9916] n_targets:  1 reward:  80\n",
      "98.3 action:  [-0.009, -0.9995] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  19 ff for 102.49999999999845 s: -------------------> 0.19\n",
      "Total reward for the episode:  1520.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  277\n",
      "6.1 action:  [0.0053, -0.9977] n_targets:  1 reward:  80\n",
      "19.1 action:  [0.0092, -0.9992] n_targets:  1 reward:  80\n",
      "26.0 action:  [-0.0002, -0.9956] n_targets:  1 reward:  80\n",
      "33.8 action:  [-0.003, -0.9925] n_targets:  1 reward:  80\n",
      "38.3 action:  [-0.0009, -0.998] n_targets:  1 reward:  80\n",
      "44.7 action:  [0.005, -0.9895] n_targets:  1 reward:  80\n",
      "48.7 action:  [0.001, -0.9973] n_targets:  1 reward:  80\n",
      "50.0 action:  [-0.0035, -0.9984] n_targets:  1 reward:  80\n",
      "54.3 action:  [0.0091, -0.9927] n_targets:  1 reward:  80\n",
      "58.2 action:  [-0.0064, -0.9813] n_targets:  1 reward:  80\n",
      "75.3 action:  [0.0096, -0.99] n_targets:  1 reward:  80\n",
      "83.5 action:  [0.0056, -0.9992] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  12 ff for 102.49999999999845 s: -------------------> 0.12\n",
      "Total reward for the episode:  960.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  278\n",
      "4.1 action:  [0.0062, -0.9911] n_targets:  2 reward:  160\n",
      "6.8 action:  [0.0093, -0.9992] n_targets:  1 reward:  80\n",
      "24.1 action:  [0.0005, -0.9975] n_targets:  2 reward:  160\n",
      "26.8 action:  [0.01, -0.9939] n_targets:  1 reward:  80\n",
      "27.6 action:  [0.0067, -0.9939] n_targets:  1 reward:  80\n",
      "30.1 action:  [0.0081, -0.983] n_targets:  1 reward:  80\n",
      "36.2 action:  [0.0059, -0.9963] n_targets:  1 reward:  80\n",
      "37.0 action:  [0.0036, -0.998] n_targets:  1 reward:  80\n",
      "45.2 action:  [0.009, -0.9991] n_targets:  2 reward:  160\n",
      "51.0 action:  [0.0086, -0.9973] n_targets:  1 reward:  80\n",
      "55.4 action:  [-0.0092, -0.9877] n_targets:  2 reward:  160\n",
      "60.5 action:  [0.0034, -0.9859] n_targets:  2 reward:  160\n",
      "74.4 action:  [-0.0096, -0.992] n_targets:  1 reward:  80\n",
      "75.6 action:  [0.0068, -0.999] n_targets:  1 reward:  80\n",
      "79.4 action:  [-0.0039, -0.9861] n_targets:  1 reward:  80\n",
      "82.1 action:  [0.0067, -0.9941] n_targets:  1 reward:  80\n",
      "89.0 action:  [0.0024, -0.9989] n_targets:  2 reward:  160\n",
      "93.4 action:  [-0.0032, -0.9948] n_targets:  1 reward:  80\n",
      "97.4 action:  [-0.0077, -0.9974] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  25 ff for 102.49999999999845 s: -------------------> 0.24\n",
      "Total reward for the episode:  2000.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  279\n",
      "8.5 action:  [0.0033, -0.9956] n_targets:  1 reward:  80\n",
      "35.1 action:  [0.0085, -0.9897] n_targets:  1 reward:  80\n",
      "37.3 action:  [0.0014, -0.9961] n_targets:  1 reward:  80\n",
      "40.4 action:  [0.0022, -0.9942] n_targets:  1 reward:  80\n",
      "42.9 action:  [-0.0067, -0.9896] n_targets:  1 reward:  80\n",
      "58.2 action:  [0.0067, -0.9988] n_targets:  1 reward:  80\n",
      "65.5 action:  [-0.0099, -0.9915] n_targets:  1 reward:  80\n",
      "78.5 action:  [0.0019, -0.9939] n_targets:  2 reward:  160\n",
      "83.7 action:  [0.0073, -0.9974] n_targets:  1 reward:  80\n",
      "85.2 action:  [-0.0002, -0.9978] n_targets:  1 reward:  80\n",
      "90.5 action:  [0.0019, -0.9833] n_targets:  1 reward:  80\n",
      "93.7 action:  [-0.0073, -0.9995] n_targets:  1 reward:  80\n",
      "95.1 action:  [0.0038, -0.9986] n_targets:  1 reward:  80\n",
      "96.5 action:  [0.0061, -0.9844] n_targets:  1 reward:  80\n",
      "99.3 action:  [0.0012, -0.9941] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  16 ff for 102.49999999999845 s: -------------------> 0.16\n",
      "Total reward for the episode:  1280.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  280\n",
      "4.6 action:  [-0.0078, -0.9983] n_targets:  1 reward:  80\n",
      "12.0 action:  [-0.0084, -0.9995] n_targets:  1 reward:  80\n",
      "28.5 action:  [-0.0015, -0.9992] n_targets:  1 reward:  80\n",
      "36.5 action:  [-0.0061, -0.9968] n_targets:  1 reward:  80\n",
      "52.3 action:  [-0.0046, -0.9988] n_targets:  1 reward:  80\n",
      "55.6 action:  [-0.0009, -0.9984] n_targets:  1 reward:  80\n",
      "58.3 action:  [0.0007, -0.9993] n_targets:  1 reward:  80\n",
      "67.6 action:  [0.0067, -0.9918] n_targets:  1 reward:  80\n",
      "73.9 action:  [0.0057, -0.9872] n_targets:  2 reward:  160\n",
      "76.0 action:  [-0.0075, -0.9946] n_targets:  1 reward:  80\n",
      "80.2 action:  [0.0095, -0.9989] n_targets:  1 reward:  80\n",
      "84.0 action:  [0.0033, -0.9952] n_targets:  1 reward:  80\n",
      "91.4 action:  [0.0046, -0.9944] n_targets:  1 reward:  80\n",
      "96.1 action:  [-0.0024, -0.9924] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  15 ff for 102.49999999999845 s: -------------------> 0.15\n",
      "Total reward for the episode:  1200.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  281\n",
      "6.9 action:  [0.0058, -0.9946] n_targets:  1 reward:  80\n",
      "9.0 action:  [0.0068, -0.9934] n_targets:  1 reward:  80\n",
      "12.7 action:  [-0.0012, -0.9919] n_targets:  1 reward:  80\n",
      "23.1 action:  [-0.0038, -1.0] n_targets:  1 reward:  80\n",
      "38.0 action:  [-0.0021, -0.9962] n_targets:  1 reward:  80\n",
      "44.7 action:  [0.0041, -0.9976] n_targets:  2 reward:  160\n",
      "46.1 action:  [-0.0024, -0.999] n_targets:  1 reward:  80\n",
      "53.9 action:  [-0.0017, -0.9946] n_targets:  1 reward:  80\n",
      "60.3 action:  [-0.0045, -0.9989] n_targets:  2 reward:  160\n",
      "66.8 action:  [-0.0066, -0.9994] n_targets:  1 reward:  80\n",
      "69.3 action:  [-0.0001, -0.9991] n_targets:  1 reward:  80\n",
      "70.3 action:  [-0.0092, -0.9983] n_targets:  1 reward:  80\n",
      "72.0 action:  [0.001, -0.9984] n_targets:  1 reward:  80\n",
      "TIME before resetting: 77.49999999999987\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  282\n",
      "8.2 action:  [0.0033, -0.9916] n_targets:  1 reward:  80\n",
      "13.1 action:  [-0.0042, -0.9977] n_targets:  1 reward:  80\n",
      "19.8 action:  [-0.0085, -0.999] n_targets:  1 reward:  80\n",
      "29.5 action:  [-0.0014, -0.9986] n_targets:  1 reward:  80\n",
      "32.2 action:  [-0.0049, -0.9985] n_targets:  1 reward:  80\n",
      "33.7 action:  [-0.0017, -0.9824] n_targets:  1 reward:  80\n",
      "41.4 action:  [-0.0055, -0.9986] n_targets:  1 reward:  80\n",
      "47.5 action:  [0.0016, -0.9962] n_targets:  1 reward:  80\n",
      "52.4 action:  [0.0071, -0.9977] n_targets:  1 reward:  80\n",
      "56.3 action:  [-0.0021, -0.9883] n_targets:  1 reward:  80\n",
      "60.6 action:  [-0.0076, -0.9834] n_targets:  1 reward:  80\n",
      "63.5 action:  [-0.0002, -0.9958] n_targets:  1 reward:  80\n",
      "74.7 action:  [-0.0016, -0.9996] n_targets:  1 reward:  80\n",
      "88.4 action:  [-0.0054, -0.9876] n_targets:  1 reward:  80\n",
      "102.4 action:  [-0.0055, -0.9844] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  15 ff for 102.49999999999845 s: -------------------> 0.15\n",
      "Total reward for the episode:  1200.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  283\n",
      "7.7 action:  [-0.0009, -0.9904] n_targets:  1 reward:  80\n",
      "12.0 action:  [-0.008, -0.9961] n_targets:  2 reward:  160\n",
      "13.7 action:  [0.0092, -0.9959] n_targets:  1 reward:  80\n",
      "27.5 action:  [0.0011, -0.9945] n_targets:  1 reward:  80\n",
      "34.2 action:  [-0.0034, -0.9992] n_targets:  1 reward:  80\n",
      "43.5 action:  [-0.0003, -0.9994] n_targets:  1 reward:  80\n",
      "57.5 action:  [-0.0001, -0.9966] n_targets:  1 reward:  80\n",
      "80.4 action:  [0.0036, -0.9958] n_targets:  1 reward:  80\n",
      "84.3 action:  [-0.0024, -0.98] n_targets:  1 reward:  80\n",
      "97.1 action:  [0.0089, -0.9893] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  11 ff for 102.49999999999845 s: -------------------> 0.11\n",
      "Total reward for the episode:  880.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  284\n",
      "7.3 action:  [0.008, -0.9957] n_targets:  1 reward:  80\n",
      "9.1 action:  [-0.0049, -0.9946] n_targets:  2 reward:  160\n",
      "14.7 action:  [-0.0087, -0.9982] n_targets:  2 reward:  160\n",
      "18.6 action:  [0.0097, -0.9947] n_targets:  1 reward:  80\n",
      "23.8 action:  [0.0081, -0.9939] n_targets:  2 reward:  160\n",
      "37.2 action:  [0.0004, -0.9946] n_targets:  1 reward:  80\n",
      "42.8 action:  [-0.0018, -0.9821] n_targets:  1 reward:  80\n",
      "51.4 action:  [-0.008, -0.9858] n_targets:  1 reward:  80\n",
      "55.1 action:  [0.0081, -0.9976] n_targets:  1 reward:  80\n",
      "67.1 action:  [-0.0077, -0.9945] n_targets:  1 reward:  80\n",
      "69.2 action:  [-0.0036, -0.9923] n_targets:  1 reward:  80\n",
      "83.2 action:  [0.0088, -0.9968] n_targets:  1 reward:  80\n",
      "89.5 action:  [0.0077, -0.9833] n_targets:  1 reward:  80\n",
      "94.1 action:  [-0.0009, -0.9859] n_targets:  1 reward:  80\n",
      "100.7 action:  [-0.0094, -0.994] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  18 ff for 102.49999999999845 s: -------------------> 0.18\n",
      "Total reward for the episode:  1440.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  285\n",
      "Eval num_timesteps=70000, episode_reward=1173.33 +/- 229.40\n",
      "Episode length: 1025.00 +/- 0.00\n",
      "New best mean reward!\n",
      "2.7 action:  [-0.0091, -0.996] n_targets:  1 reward:  80\n",
      "5.4 action:  [-0.0076, -0.9978] n_targets:  2 reward:  160\n",
      "7.4 action:  [-0.0023, -0.9949] n_targets:  1 reward:  80\n",
      "9.6 action:  [-0.008, -0.9914] n_targets:  1 reward:  80\n",
      "16.0 action:  [-0.0014, -0.9964] n_targets:  1 reward:  80\n",
      "22.9 action:  [0.0079, -0.9808] n_targets:  1 reward:  80\n",
      "25.3 action:  [-0.0031, -0.9978] n_targets:  1 reward:  80\n",
      "26.9 action:  [-0.006, -0.998] n_targets:  1 reward:  80\n",
      "32.6 action:  [0.0009, -0.9998] n_targets:  1 reward:  80\n",
      "43.1 action:  [0.0056, -0.9905] n_targets:  1 reward:  80\n",
      "48.2 action:  [-0.0079, -0.9978] n_targets:  1 reward:  80\n",
      "54.1 action:  [0.0001, -0.9874] n_targets:  1 reward:  80\n",
      "58.5 action:  [-0.008, -0.9995] n_targets:  1 reward:  80\n",
      "66.1 action:  [-0.0008, -0.998] n_targets:  1 reward:  80\n",
      "68.6 action:  [0.0058, -0.9925] n_targets:  1 reward:  80\n",
      "70.0 action:  [-0.0004, -0.9852] n_targets:  2 reward:  160\n",
      "71.7 action:  [0.0038, -0.9997] n_targets:  3 reward:  240\n",
      "76.6 action:  [-0.0086, -0.9958] n_targets:  1 reward:  80\n",
      "80.5 action:  [0.0019, -0.9905] n_targets:  1 reward:  80\n",
      "81.9 action:  [-0.0068, -0.9994] n_targets:  1 reward:  80\n",
      "84.5 action:  [-0.0057, -0.9852] n_targets:  1 reward:  80\n",
      "97.7 action:  [0.0051, -0.9978] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  26 ff for 102.49999999999845 s: -------------------> 0.25\n",
      "Total reward for the episode:  2080.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  286\n",
      "15.5 action:  [0.0055, -0.9966] n_targets:  1 reward:  80\n",
      "39.9 action:  [0.0043, -0.9982] n_targets:  1 reward:  80\n",
      "52.3 action:  [0.0033, -0.9931] n_targets:  1 reward:  80\n",
      "68.7 action:  [-0.0093, -0.9982] n_targets:  1 reward:  80\n",
      "78.3 action:  [-0.0061, -0.993] n_targets:  1 reward:  80\n",
      "81.0 action:  [0.0098, -0.9981] n_targets:  1 reward:  80\n",
      "85.6 action:  [-0.0071, -0.9993] n_targets:  1 reward:  80\n",
      "88.6 action:  [-0.0052, -0.985] n_targets:  1 reward:  80\n",
      "91.3 action:  [0.0085, -0.9925] n_targets:  1 reward:  80\n",
      "96.6 action:  [-0.0022, -0.9995] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  10 ff for 102.49999999999845 s: -------------------> 0.1\n",
      "Total reward for the episode:  800.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  287\n",
      "11.6 action:  [-0.0045, -0.9893] n_targets:  1 reward:  80\n",
      "23.7 action:  [-0.0062, -0.9918] n_targets:  1 reward:  80\n",
      "33.8 action:  [0.0, -0.9994] n_targets:  1 reward:  80\n",
      "40.1 action:  [-0.0049, -0.9882] n_targets:  1 reward:  80\n",
      "44.0 action:  [-0.002, -0.9981] n_targets:  1 reward:  80\n",
      "47.2 action:  [-0.0027, -0.9854] n_targets:  1 reward:  80\n",
      "49.9 action:  [-0.0048, -0.997] n_targets:  2 reward:  160\n",
      "52.1 action:  [0.0059, -0.9916] n_targets:  1 reward:  80\n",
      "53.9 action:  [0.003, -0.9956] n_targets:  1 reward:  80\n",
      "60.2 action:  [-0.0035, -0.9999] n_targets:  1 reward:  80\n",
      "63.7 action:  [-0.0041, -0.9994] n_targets:  1 reward:  80\n",
      "71.6 action:  [-0.005, -0.998] n_targets:  1 reward:  80\n",
      "87.2 action:  [-0.0001, -0.9906] n_targets:  1 reward:  80\n",
      "88.0 action:  [-0.0088, -0.9885] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  15 ff for 102.49999999999845 s: -------------------> 0.15\n",
      "Total reward for the episode:  1200.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  288\n",
      "5.6 action:  [-0.0072, -0.999] n_targets:  1 reward:  80\n",
      "12.0 action:  [-0.0035, -0.9951] n_targets:  2 reward:  160\n",
      "24.5 action:  [-0.0058, -1.0] n_targets:  1 reward:  80\n",
      "37.0 action:  [0.0048, -0.9928] n_targets:  1 reward:  80\n",
      "42.5 action:  [0.0045, -0.9996] n_targets:  1 reward:  80\n",
      "50.6 action:  [0.0033, -0.9826] n_targets:  2 reward:  160\n",
      "55.1 action:  [-0.0044, -0.9901] n_targets:  1 reward:  80\n",
      "58.6 action:  [-0.0013, -0.9817] n_targets:  1 reward:  80\n",
      "62.0 action:  [-0.0037, -0.997] n_targets:  1 reward:  80\n",
      "68.3 action:  [-0.0006, -0.9985] n_targets:  1 reward:  80\n",
      "79.9 action:  [-0.0082, -0.9962] n_targets:  1 reward:  80\n",
      "91.0 action:  [0.0042, -0.9878] n_targets:  1 reward:  80\n",
      "97.2 action:  [0.0087, -0.9867] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  15 ff for 102.49999999999845 s: -------------------> 0.15\n",
      "Total reward for the episode:  1200.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  289\n",
      "4.6 action:  [0.0022, -0.9983] n_targets:  1 reward:  80\n",
      "5.6 action:  [-0.0018, -0.9991] n_targets:  1 reward:  80\n",
      "11.1 action:  [-0.0078, -0.9955] n_targets:  1 reward:  80\n",
      "15.5 action:  [0.0058, -0.9801] n_targets:  1 reward:  80\n",
      "18.2 action:  [-0.0031, -0.9979] n_targets:  1 reward:  80\n",
      "26.2 action:  [-0.006, -0.996] n_targets:  1 reward:  80\n",
      "27.8 action:  [-0.0098, -0.9977] n_targets:  1 reward:  80\n",
      "34.9 action:  [-0.0048, -0.9993] n_targets:  1 reward:  80\n",
      "44.6 action:  [0.0012, -0.9974] n_targets:  1 reward:  80\n",
      "46.3 action:  [0.0014, -0.9852] n_targets:  1 reward:  80\n",
      "49.7 action:  [-0.0005, -0.9858] n_targets:  1 reward:  80\n",
      "54.0 action:  [-0.0092, -0.9992] n_targets:  2 reward:  160\n",
      "67.5 action:  [0.0004, -0.9926] n_targets:  1 reward:  80\n",
      "77.3 action:  [0.0006, -0.9964] n_targets:  1 reward:  80\n",
      "80.1 action:  [-0.0073, -0.9812] n_targets:  1 reward:  80\n",
      "92.0 action:  [-0.0047, -0.993] n_targets:  1 reward:  80\n",
      "96.3 action:  [0.0027, -0.9998] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  18 ff for 102.49999999999845 s: -------------------> 0.18\n",
      "Total reward for the episode:  1440.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  290\n",
      "9.5 action:  [-0.0069, -0.9995] n_targets:  1 reward:  80\n",
      "11.0 action:  [0.0019, -0.9927] n_targets:  1 reward:  80\n",
      "25.7 action:  [0.0034, -0.98] n_targets:  1 reward:  80\n",
      "42.6 action:  [-0.0008, -0.9942] n_targets:  1 reward:  80\n",
      "43.7 action:  [-0.0089, -0.9927] n_targets:  1 reward:  80\n",
      "45.3 action:  [0.0072, -0.9908] n_targets:  1 reward:  80\n",
      "50.3 action:  [0.0056, -0.9836] n_targets:  1 reward:  80\n",
      "54.2 action:  [-0.0037, -0.9844] n_targets:  1 reward:  80\n",
      "57.5 action:  [0.0028, -0.9904] n_targets:  1 reward:  80\n",
      "58.6 action:  [-0.006, -0.9915] n_targets:  1 reward:  80\n",
      "59.9 action:  [0.0068, -0.9966] n_targets:  1 reward:  80\n",
      "66.5 action:  [0.0058, -0.9969] n_targets:  1 reward:  80\n",
      "70.6 action:  [-0.0028, -0.985] n_targets:  1 reward:  80\n",
      "75.6 action:  [0.0064, -0.9801] n_targets:  1 reward:  80\n",
      "79.1 action:  [0.0025, -0.9923] n_targets:  1 reward:  80\n",
      "93.7 action:  [-0.0004, -0.9863] n_targets:  1 reward:  80\n",
      "98.1 action:  [0.0066, -0.9898] n_targets:  1 reward:  80\n",
      "100.3 action:  [0.0021, -0.9959] n_targets:  1 reward:  80\n",
      "101.0 action:  [0.003, -0.9984] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  19 ff for 102.49999999999845 s: -------------------> 0.19\n",
      "Total reward for the episode:  1520.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  291\n",
      "17.3 action:  [0.0092, -0.9973] n_targets:  1 reward:  80\n",
      "33.0 action:  [0.0077, -0.9992] n_targets:  1 reward:  80\n",
      "47.0 action:  [-0.007, -0.9968] n_targets:  1 reward:  80\n",
      "56.5 action:  [-0.006, -0.9982] n_targets:  1 reward:  80\n",
      "57.3 action:  [-0.0041, -0.9978] n_targets:  1 reward:  80\n",
      "62.3 action:  [-0.0012, -0.998] n_targets:  1 reward:  80\n",
      "64.4 action:  [-0.0056, -0.9928] n_targets:  1 reward:  80\n",
      "66.8 action:  [0.0068, -0.9809] n_targets:  1 reward:  80\n",
      "68.9 action:  [-0.0068, -0.9953] n_targets:  1 reward:  80\n",
      "73.7 action:  [0.0052, -0.9991] n_targets:  1 reward:  80\n",
      "78.5 action:  [-0.0002, -0.9987] n_targets:  1 reward:  80\n",
      "80.4 action:  [-0.0067, -0.9806] n_targets:  1 reward:  80\n",
      "81.6 action:  [0.0071, -0.9972] n_targets:  2 reward:  160\n",
      "89.8 action:  [0.0058, -0.9974] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  15 ff for 102.49999999999845 s: -------------------> 0.15\n",
      "Total reward for the episode:  1200.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  292\n",
      "9.2 action:  [0.0086, -0.9953] n_targets:  1 reward:  80\n",
      "14.1 action:  [-0.0099, -0.9995] n_targets:  1 reward:  80\n",
      "18.7 action:  [0.0066, -0.9871] n_targets:  1 reward:  80\n",
      "22.2 action:  [-0.0025, -0.9992] n_targets:  1 reward:  80\n",
      "28.3 action:  [0.0028, -0.9973] n_targets:  1 reward:  80\n",
      "31.0 action:  [0.0072, -0.9969] n_targets:  1 reward:  80\n",
      "44.1 action:  [0.0079, -0.9991] n_targets:  1 reward:  80\n",
      "46.2 action:  [-0.0079, -0.9821] n_targets:  1 reward:  80\n",
      "50.6 action:  [0.0084, -0.9989] n_targets:  1 reward:  80\n",
      "50.9 action:  [-0.0012, -0.9975] n_targets:  1 reward:  80\n",
      "55.7 action:  [0.0079, -0.9857] n_targets:  1 reward:  80\n",
      "59.4 action:  [-0.0034, -0.994] n_targets:  1 reward:  80\n",
      "68.1 action:  [-0.009, -0.9993] n_targets:  1 reward:  80\n",
      "75.3 action:  [0.0037, -0.9908] n_targets:  1 reward:  80\n",
      "81.2 action:  [-0.0008, -0.9929] n_targets:  1 reward:  80\n",
      "84.6 action:  [0.001, -0.999] n_targets:  1 reward:  80\n",
      "94.1 action:  [-0.0003, -0.9991] n_targets:  2 reward:  160\n",
      "98.7 action:  [-0.0024, -0.9996] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  19 ff for 102.49999999999845 s: -------------------> 0.19\n",
      "Total reward for the episode:  1520.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  293\n",
      "4.1 action:  [-0.0011, -0.9925] n_targets:  1 reward:  80\n",
      "8.0 action:  [0.0099, -0.9979] n_targets:  2 reward:  160\n",
      "10.2 action:  [0.0006, -0.9958] n_targets:  1 reward:  80\n",
      "13.8 action:  [0.0037, -0.9982] n_targets:  1 reward:  80\n",
      "15.1 action:  [0.0011, -0.9986] n_targets:  1 reward:  80\n",
      "20.2 action:  [0.0055, -0.9989] n_targets:  1 reward:  80\n",
      "24.1 action:  [0.005, -0.9955] n_targets:  1 reward:  80\n",
      "37.0 action:  [0.0088, -0.9863] n_targets:  1 reward:  80\n",
      "42.3 action:  [-0.0098, -0.9971] n_targets:  1 reward:  80\n",
      "44.7 action:  [-0.0061, -0.9999] n_targets:  1 reward:  80\n",
      "51.0 action:  [0.0068, -0.9996] n_targets:  1 reward:  80\n",
      "61.9 action:  [0.0007, -0.996] n_targets:  1 reward:  80\n",
      "63.9 action:  [-0.0065, -0.9891] n_targets:  2 reward:  160\n",
      "66.1 action:  [0.0039, -0.9994] n_targets:  2 reward:  160\n",
      "69.8 action:  [-0.0081, -0.9974] n_targets:  1 reward:  80\n",
      "71.5 action:  [0.0029, -0.9964] n_targets:  1 reward:  80\n",
      "73.5 action:  [0.0034, -0.9916] n_targets:  1 reward:  80\n",
      "76.0 action:  [0.0038, -0.9974] n_targets:  2 reward:  160\n",
      "77.2 action:  [-0.0048, -0.9869] n_targets:  1 reward:  80\n",
      "79.2 action:  [0.0095, -0.9908] n_targets:  1 reward:  80\n",
      "90.1 action:  [-0.0094, -0.9963] n_targets:  2 reward:  160\n",
      "91.0 action:  [0.0074, -0.9999] n_targets:  1 reward:  80\n",
      "97.7 action:  [0.0008, -0.9999] n_targets:  1 reward:  80\n",
      "98.6 action:  [0.0062, -0.993] n_targets:  2 reward:  160\n",
      "99.9 action:  [0.0065, -0.9972] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  31 ff for 102.49999999999845 s: -------------------> 0.3\n",
      "Total reward for the episode:  2480.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  294\n",
      "8.8 action:  [0.0078, -0.9994] n_targets:  1 reward:  80\n",
      "29.9 action:  [-0.0079, -0.9892] n_targets:  1 reward:  80\n",
      "33.6 action:  [-0.0005, -0.9988] n_targets:  2 reward:  160\n",
      "39.4 action:  [-0.0041, -0.9997] n_targets:  1 reward:  80\n",
      "40.5 action:  [0.0017, -0.9838] n_targets:  1 reward:  80\n",
      "45.0 action:  [-0.0035, -0.9854] n_targets:  1 reward:  80\n",
      "49.4 action:  [-0.0082, -0.9982] n_targets:  1 reward:  80\n",
      "52.7 action:  [0.0046, -0.9997] n_targets:  1 reward:  80\n",
      "54.8 action:  [0.0003, -0.985] n_targets:  2 reward:  160\n",
      "61.2 action:  [0.0074, -0.9996] n_targets:  1 reward:  80\n",
      "65.2 action:  [-0.0065, -0.9824] n_targets:  1 reward:  80\n",
      "72.9 action:  [-0.0065, -0.9989] n_targets:  1 reward:  80\n",
      "76.6 action:  [-0.0097, -0.9897] n_targets:  1 reward:  80\n",
      "TIME before resetting: 77.49999999999987\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  295\n",
      "9.8 action:  [0.0047, -0.9972] n_targets:  1 reward:  80\n",
      "25.4 action:  [0.0074, -0.995] n_targets:  1 reward:  80\n",
      "31.6 action:  [0.002, -0.9938] n_targets:  1 reward:  80\n",
      "34.4 action:  [-0.0092, -0.9948] n_targets:  1 reward:  80\n",
      "47.4 action:  [-0.0039, -0.9994] n_targets:  2 reward:  160\n",
      "56.2 action:  [0.0058, -0.9863] n_targets:  2 reward:  160\n",
      "63.8 action:  [0.0024, -0.9958] n_targets:  1 reward:  80\n",
      "67.4 action:  [0.0036, -0.9996] n_targets:  1 reward:  80\n",
      "71.9 action:  [0.0075, -0.9883] n_targets:  1 reward:  80\n",
      "76.3 action:  [0.0099, -0.9957] n_targets:  2 reward:  160\n",
      "80.3 action:  [-0.0011, -0.9965] n_targets:  1 reward:  80\n",
      "87.6 action:  [-0.0066, -0.9891] n_targets:  2 reward:  160\n",
      "99.7 action:  [0.0097, -0.999] n_targets:  2 reward:  160\n",
      "Firely capture rate for the episode:  18 ff for 102.49999999999845 s: -------------------> 0.18\n",
      "Total reward for the episode:  1440.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  296\n",
      "2.9 action:  [0.0064, -0.9937] n_targets:  1 reward:  80\n",
      "7.5 action:  [0.0083, -0.9974] n_targets:  1 reward:  80\n",
      "16.0 action:  [0.0099, -0.9969] n_targets:  1 reward:  80\n",
      "21.7 action:  [-0.0078, -0.9955] n_targets:  1 reward:  80\n",
      "27.9 action:  [0.0094, -0.9895] n_targets:  1 reward:  80\n",
      "43.2 action:  [0.0094, -0.9899] n_targets:  1 reward:  80\n",
      "46.0 action:  [0.0063, -0.9878] n_targets:  1 reward:  80\n",
      "48.3 action:  [-0.0047, -0.9935] n_targets:  1 reward:  80\n",
      "52.7 action:  [0.0039, -0.9913] n_targets:  1 reward:  80\n",
      "56.5 action:  [0.0039, -0.9984] n_targets:  1 reward:  80\n",
      "61.3 action:  [0.0065, -0.999] n_targets:  1 reward:  80\n",
      "64.4 action:  [-0.0058, -0.998] n_targets:  1 reward:  80\n",
      "66.5 action:  [0.0062, -0.9988] n_targets:  1 reward:  80\n",
      "72.8 action:  [0.0037, -0.9984] n_targets:  1 reward:  80\n",
      "82.3 action:  [-0.0026, -0.9966] n_targets:  1 reward:  80\n",
      "85.4 action:  [0.0068, -0.9923] n_targets:  1 reward:  80\n",
      "93.0 action:  [0.0072, -0.9978] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  17 ff for 102.49999999999845 s: -------------------> 0.17\n",
      "Total reward for the episode:  1360.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  297\n",
      "20.9 action:  [0.0089, -0.9987] n_targets:  1 reward:  80\n",
      "24.5 action:  [0.0091, -0.9885] n_targets:  2 reward:  160\n",
      "25.8 action:  [0.0003, -0.9979] n_targets:  2 reward:  160\n",
      "30.4 action:  [0.0083, -0.9896] n_targets:  1 reward:  80\n",
      "34.0 action:  [0.0011, -0.9951] n_targets:  1 reward:  80\n",
      "35.0 action:  [0.0095, -0.9966] n_targets:  2 reward:  160\n",
      "47.0 action:  [0.0064, -0.9986] n_targets:  1 reward:  80\n",
      "52.2 action:  [0.0034, -0.9987] n_targets:  2 reward:  160\n",
      "56.6 action:  [0.008, -0.9958] n_targets:  1 reward:  80\n",
      "60.5 action:  [0.0036, -0.9941] n_targets:  1 reward:  80\n",
      "61.9 action:  [0.0013, -0.9945] n_targets:  1 reward:  80\n",
      "63.5 action:  [0.0041, -0.9908] n_targets:  2 reward:  160\n",
      "64.6 action:  [-0.0043, -0.9951] n_targets:  1 reward:  80\n",
      "66.0 action:  [-0.0009, -0.9831] n_targets:  1 reward:  80\n",
      "67.9 action:  [0.0092, -0.9986] n_targets:  1 reward:  80\n",
      "72.5 action:  [-0.0096, -0.9984] n_targets:  1 reward:  80\n",
      "75.0 action:  [0.0072, -0.9953] n_targets:  1 reward:  80\n",
      "77.4 action:  [0.0063, -0.992] n_targets:  1 reward:  80\n",
      "85.8 action:  [0.0054, -0.9981] n_targets:  1 reward:  80\n",
      "87.4 action:  [0.0077, -0.9975] n_targets:  1 reward:  80\n",
      "94.6 action:  [-0.0011, -0.9981] n_targets:  1 reward:  80\n",
      "98.4 action:  [-0.0046, -0.9899] n_targets:  2 reward:  160\n",
      "101.5 action:  [0.0032, -0.9908] n_targets:  1 reward:  80\n",
      "102.4 action:  [0.0045, -0.9963] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  30 ff for 102.49999999999845 s: -------------------> 0.29\n",
      "Total reward for the episode:  2400.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  298\n",
      "Eval num_timesteps=80000, episode_reward=1733.33 +/- 472.53\n",
      "Episode length: 1025.00 +/- 0.00\n",
      "New best mean reward!\n",
      "6.8 action:  [0.0082, -0.9976] n_targets:  2 reward:  160\n",
      "13.8 action:  [-0.0046, -0.9998] n_targets:  2 reward:  160\n",
      "19.3 action:  [-0.0003, -0.9984] n_targets:  1 reward:  80\n",
      "32.9 action:  [-0.0057, -0.997] n_targets:  1 reward:  80\n",
      "37.1 action:  [0.0067, -0.9859] n_targets:  1 reward:  80\n",
      "38.8 action:  [-0.0013, -0.999] n_targets:  1 reward:  80\n",
      "43.9 action:  [0.0032, -0.9976] n_targets:  1 reward:  80\n",
      "49.2 action:  [0.0001, -0.9951] n_targets:  1 reward:  80\n",
      "51.4 action:  [-0.0089, -0.9988] n_targets:  1 reward:  80\n",
      "52.6 action:  [-0.0055, -0.9992] n_targets:  2 reward:  160\n",
      "59.8 action:  [-0.0078, -0.9968] n_targets:  1 reward:  80\n",
      "63.7 action:  [-0.0053, -0.994] n_targets:  1 reward:  80\n",
      "68.8 action:  [0.008, -0.9918] n_targets:  1 reward:  80\n",
      "79.4 action:  [0.0064, -0.9939] n_targets:  1 reward:  80\n",
      "87.8 action:  [-0.0035, -0.9935] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  18 ff for 102.49999999999845 s: -------------------> 0.18\n",
      "Total reward for the episode:  1440.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  299\n",
      "6.8 action:  [0.0015, -0.9826] n_targets:  1 reward:  80\n",
      "8.9 action:  [0.007, -0.9991] n_targets:  1 reward:  80\n",
      "16.1 action:  [0.0099, -0.999] n_targets:  1 reward:  80\n",
      "23.0 action:  [-0.0094, -0.999] n_targets:  1 reward:  80\n",
      "26.0 action:  [0.0008, -0.9933] n_targets:  1 reward:  80\n",
      "38.1 action:  [0.0053, -0.9905] n_targets:  1 reward:  80\n",
      "41.3 action:  [0.0051, -0.9953] n_targets:  2 reward:  160\n",
      "51.8 action:  [-0.0089, -0.9987] n_targets:  1 reward:  80\n",
      "57.5 action:  [0.0026, -0.9838] n_targets:  2 reward:  160\n",
      "63.6 action:  [-0.0047, -0.9989] n_targets:  1 reward:  80\n",
      "70.6 action:  [-0.0083, -0.9967] n_targets:  1 reward:  80\n",
      "74.6 action:  [0.0005, -0.9978] n_targets:  2 reward:  160\n",
      "78.0 action:  [0.0045, -0.9914] n_targets:  1 reward:  80\n",
      "82.0 action:  [0.0011, -0.998] n_targets:  2 reward:  160\n",
      "85.3 action:  [0.0026, -0.9899] n_targets:  1 reward:  80\n",
      "94.3 action:  [0.0058, -0.9992] n_targets:  1 reward:  80\n",
      "96.6 action:  [0.0015, -0.9926] n_targets:  1 reward:  80\n",
      "101.1 action:  [0.0086, -0.9826] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  22 ff for 102.49999999999845 s: -------------------> 0.21\n",
      "Total reward for the episode:  1760.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  300\n",
      "3.8 action:  [0.0013, -0.9995] n_targets:  1 reward:  80\n",
      "4.4 action:  [0.0029, -0.9981] n_targets:  1 reward:  80\n",
      "17.0 action:  [0.0044, -0.9985] n_targets:  1 reward:  80\n",
      "24.1 action:  [0.0055, -1.0] n_targets:  1 reward:  80\n",
      "27.6 action:  [-0.0041, -0.9965] n_targets:  1 reward:  80\n",
      "36.5 action:  [0.0054, -0.9988] n_targets:  1 reward:  80\n",
      "45.9 action:  [-0.0079, -0.9989] n_targets:  1 reward:  80\n",
      "53.1 action:  [0.0072, -0.9967] n_targets:  1 reward:  80\n",
      "55.1 action:  [-0.0043, -0.9969] n_targets:  2 reward:  160\n",
      "63.4 action:  [-0.0044, -0.9818] n_targets:  1 reward:  80\n",
      "64.5 action:  [0.0082, -0.9999] n_targets:  1 reward:  80\n",
      "65.5 action:  [0.0048, -0.995] n_targets:  1 reward:  80\n",
      "66.6 action:  [-0.0065, -0.9983] n_targets:  1 reward:  80\n",
      "69.4 action:  [0.0087, -0.9861] n_targets:  1 reward:  80\n",
      "75.8 action:  [-0.0015, -0.9956] n_targets:  1 reward:  80\n",
      "82.4 action:  [-0.0007, -0.9854] n_targets:  1 reward:  80\n",
      "85.1 action:  [-0.0003, -0.9975] n_targets:  1 reward:  80\n",
      "87.5 action:  [0.0, -0.9999] n_targets:  2 reward:  160\n",
      "89.2 action:  [-0.0011, -0.9988] n_targets:  1 reward:  80\n",
      "92.9 action:  [0.0067, -0.9954] n_targets:  1 reward:  80\n",
      "102.1 action:  [-0.0054, -1.0] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  23 ff for 102.49999999999845 s: -------------------> 0.22\n",
      "Total reward for the episode:  1840.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  301\n",
      "5.3 action:  [0.0043, -0.9987] n_targets:  1 reward:  80\n",
      "7.3 action:  [0.0024, -0.9839] n_targets:  1 reward:  80\n",
      "10.2 action:  [-0.0042, -0.9937] n_targets:  1 reward:  80\n",
      "17.3 action:  [-0.0078, -0.9971] n_targets:  1 reward:  80\n",
      "21.2 action:  [-0.0077, -0.9999] n_targets:  1 reward:  80\n",
      "27.9 action:  [0.0092, -0.9932] n_targets:  1 reward:  80\n",
      "29.0 action:  [-0.0097, -0.9898] n_targets:  2 reward:  160\n",
      "32.1 action:  [-0.0084, -0.9971] n_targets:  1 reward:  80\n",
      "36.4 action:  [-0.0061, -0.9867] n_targets:  1 reward:  80\n",
      "45.5 action:  [-0.0049, -0.9891] n_targets:  1 reward:  80\n",
      "52.5 action:  [0.0038, -0.9976] n_targets:  1 reward:  80\n",
      "57.3 action:  [0.0074, -0.9986] n_targets:  1 reward:  80\n",
      "63.8 action:  [0.0015, -0.9996] n_targets:  2 reward:  160\n",
      "67.3 action:  [0.0029, -0.9967] n_targets:  1 reward:  80\n",
      "71.6 action:  [0.0096, -0.9947] n_targets:  1 reward:  80\n",
      "75.1 action:  [0.0026, -0.9862] n_targets:  1 reward:  80\n",
      "87.0 action:  [-0.0044, -0.9955] n_targets:  1 reward:  80\n",
      "93.2 action:  [-0.0051, -0.9903] n_targets:  1 reward:  80\n",
      "95.1 action:  [-0.003, -0.9976] n_targets:  2 reward:  160\n",
      "101.6 action:  [-0.008, -0.9827] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  23 ff for 102.49999999999845 s: -------------------> 0.22\n",
      "Total reward for the episode:  1840.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  302\n",
      "4.6 action:  [-0.0058, -0.9954] n_targets:  1 reward:  80\n",
      "7.7 action:  [-0.0064, -0.9973] n_targets:  1 reward:  80\n",
      "11.6 action:  [-0.0083, -0.992] n_targets:  1 reward:  80\n",
      "13.7 action:  [0.0064, -0.992] n_targets:  1 reward:  80\n",
      "17.0 action:  [-0.0057, -0.9998] n_targets:  2 reward:  160\n",
      "23.6 action:  [-0.0054, -0.9953] n_targets:  1 reward:  80\n",
      "28.8 action:  [-0.0014, -0.9984] n_targets:  1 reward:  80\n",
      "31.7 action:  [0.0059, -0.9943] n_targets:  2 reward:  160\n",
      "35.7 action:  [-0.0059, -0.9993] n_targets:  1 reward:  80\n",
      "37.8 action:  [-0.0033, -0.9994] n_targets:  1 reward:  80\n",
      "43.9 action:  [0.0053, -0.9994] n_targets:  2 reward:  160\n",
      "45.4 action:  [0.0018, -0.9993] n_targets:  1 reward:  80\n",
      "51.3 action:  [-0.0044, -0.9999] n_targets:  1 reward:  80\n",
      "56.7 action:  [0.0013, -0.9986] n_targets:  1 reward:  80\n",
      "58.6 action:  [0.0045, -0.9982] n_targets:  2 reward:  160\n",
      "60.9 action:  [-0.0014, -0.9969] n_targets:  1 reward:  80\n",
      "66.7 action:  [0.0047, -0.995] n_targets:  1 reward:  80\n",
      "71.3 action:  [0.0005, -0.9888] n_targets:  2 reward:  160\n",
      "77.3 action:  [0.0068, -0.991] n_targets:  1 reward:  80\n",
      "85.6 action:  [0.0055, -0.9975] n_targets:  1 reward:  80\n",
      "93.4 action:  [-0.0055, -0.9885] n_targets:  1 reward:  80\n",
      "102.1 action:  [0.0014, -0.9987] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  27 ff for 102.49999999999845 s: -------------------> 0.26\n",
      "Total reward for the episode:  2160.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  303\n",
      "1.1 action:  [0.0028, -0.982] n_targets:  1 reward:  80\n",
      "19.1 action:  [-0.0037, -0.9952] n_targets:  1 reward:  80\n",
      "22.7 action:  [-0.0031, -0.9998] n_targets:  2 reward:  160\n",
      "26.6 action:  [-0.0038, -0.9814] n_targets:  1 reward:  80\n",
      "27.8 action:  [0.0033, -0.9984] n_targets:  1 reward:  80\n",
      "30.8 action:  [-0.0002, -0.9973] n_targets:  2 reward:  160\n",
      "32.1 action:  [-0.008, -0.9998] n_targets:  2 reward:  160\n",
      "49.5 action:  [-0.0097, -0.9977] n_targets:  1 reward:  80\n",
      "53.4 action:  [-0.0059, -0.9913] n_targets:  2 reward:  160\n",
      "57.8 action:  [0.0052, -0.997] n_targets:  1 reward:  80\n",
      "63.5 action:  [-0.0043, -0.9893] n_targets:  1 reward:  80\n",
      "66.2 action:  [0.0019, -0.9997] n_targets:  3 reward:  240\n",
      "70.5 action:  [-0.0, -0.9976] n_targets:  1 reward:  80\n",
      "74.9 action:  [-0.0053, -0.9895] n_targets:  1 reward:  80\n",
      "85.9 action:  [0.0083, -0.9959] n_targets:  1 reward:  80\n",
      "89.5 action:  [-0.0069, -0.9975] n_targets:  1 reward:  80\n",
      "89.8 action:  [-0.0083, -0.9904] n_targets:  1 reward:  80\n",
      "96.7 action:  [-0.0031, -0.9987] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  24 ff for 102.49999999999845 s: -------------------> 0.23\n",
      "Total reward for the episode:  1920.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  304\n",
      "3.8 action:  [0.0028, -0.994] n_targets:  1 reward:  80\n",
      "6.0 action:  [0.0022, -0.9917] n_targets:  1 reward:  80\n",
      "7.6 action:  [0.0049, -0.9926] n_targets:  2 reward:  160\n",
      "10.8 action:  [-0.0059, -0.996] n_targets:  1 reward:  80\n",
      "13.8 action:  [-0.0055, -0.9972] n_targets:  1 reward:  80\n",
      "18.0 action:  [-0.0073, -0.992] n_targets:  2 reward:  160\n",
      "24.0 action:  [-0.0069, -0.9989] n_targets:  2 reward:  160\n",
      "28.2 action:  [0.003, -0.9999] n_targets:  1 reward:  80\n",
      "34.8 action:  [0.005, -0.9995] n_targets:  2 reward:  160\n",
      "36.5 action:  [-0.0027, -0.9976] n_targets:  1 reward:  80\n",
      "38.7 action:  [0.0084, -0.9953] n_targets:  1 reward:  80\n",
      "50.2 action:  [-0.0012, -0.9968] n_targets:  1 reward:  80\n",
      "54.1 action:  [0.0039, -0.998] n_targets:  1 reward:  80\n",
      "58.8 action:  [0.0014, -0.9948] n_targets:  1 reward:  80\n",
      "63.0 action:  [-0.0002, -0.9955] n_targets:  1 reward:  80\n",
      "68.7 action:  [-0.0041, -0.9982] n_targets:  2 reward:  160\n",
      "71.2 action:  [-0.0002, -0.9998] n_targets:  1 reward:  80\n",
      "75.3 action:  [0.0041, -0.9973] n_targets:  2 reward:  160\n",
      "84.7 action:  [-0.0, -0.9996] n_targets:  1 reward:  80\n",
      "98.9 action:  [-0.0065, -0.9889] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  26 ff for 102.49999999999845 s: -------------------> 0.25\n",
      "Total reward for the episode:  2080.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  305\n",
      "1.6 action:  [0.0014, -0.9996] n_targets:  1 reward:  80\n",
      "16.3 action:  [0.0076, -0.9867] n_targets:  1 reward:  80\n",
      "19.7 action:  [-0.0051, -0.9824] n_targets:  1 reward:  80\n",
      "27.9 action:  [0.0062, -0.9946] n_targets:  1 reward:  80\n",
      "29.6 action:  [-0.0067, -0.9982] n_targets:  2 reward:  160\n",
      "33.3 action:  [-0.0051, -0.9843] n_targets:  1 reward:  80\n",
      "35.7 action:  [0.0006, -0.9918] n_targets:  1 reward:  80\n",
      "37.6 action:  [0.0048, -0.9975] n_targets:  1 reward:  80\n",
      "42.1 action:  [-0.0007, -0.9957] n_targets:  1 reward:  80\n",
      "47.0 action:  [-0.0069, -0.9993] n_targets:  1 reward:  80\n",
      "49.3 action:  [0.007, -0.9946] n_targets:  1 reward:  80\n",
      "51.2 action:  [-0.0086, -0.9995] n_targets:  1 reward:  80\n",
      "52.5 action:  [0.0069, -0.9988] n_targets:  1 reward:  80\n",
      "57.9 action:  [-0.0069, -0.9866] n_targets:  1 reward:  80\n",
      "76.5 action:  [0.0054, -0.9953] n_targets:  1 reward:  80\n",
      "77.7 action:  [-0.0038, -0.9982] n_targets:  2 reward:  160\n",
      "80.5 action:  [0.0021, -0.9852] n_targets:  1 reward:  80\n",
      "83.7 action:  [-0.005, -0.9983] n_targets:  1 reward:  80\n",
      "88.3 action:  [0.0076, -0.9996] n_targets:  2 reward:  160\n",
      "94.1 action:  [0.0067, -0.9983] n_targets:  1 reward:  80\n",
      "97.1 action:  [0.0074, -0.9992] n_targets:  2 reward:  160\n",
      "100.9 action:  [-0.008, -0.9966] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  26 ff for 102.49999999999845 s: -------------------> 0.25\n",
      "Total reward for the episode:  2080.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  306\n",
      "8.3 action:  [0.0067, -0.9987] n_targets:  1 reward:  80\n",
      "9.2 action:  [0.0071, -0.9973] n_targets:  1 reward:  80\n",
      "13.5 action:  [0.01, -1.0] n_targets:  3 reward:  240\n",
      "21.4 action:  [0.0079, -0.9996] n_targets:  1 reward:  80\n",
      "25.5 action:  [0.0028, -0.9957] n_targets:  1 reward:  80\n",
      "27.3 action:  [-0.0039, -0.9847] n_targets:  1 reward:  80\n",
      "32.8 action:  [-0.0082, -0.9996] n_targets:  2 reward:  160\n",
      "35.3 action:  [-0.0094, -0.9998] n_targets:  1 reward:  80\n",
      "41.9 action:  [0.0061, -0.9993] n_targets:  1 reward:  80\n",
      "42.8 action:  [0.0087, -0.9975] n_targets:  1 reward:  80\n",
      "45.2 action:  [-0.0048, -0.9912] n_targets:  1 reward:  80\n",
      "49.3 action:  [0.0096, -0.9864] n_targets:  1 reward:  80\n",
      "51.4 action:  [-0.0015, -0.9988] n_targets:  1 reward:  80\n",
      "51.8 action:  [-0.0072, -0.9972] n_targets:  1 reward:  80\n",
      "57.1 action:  [-0.0005, -0.9987] n_targets:  2 reward:  160\n",
      "62.5 action:  [0.0001, -0.9992] n_targets:  1 reward:  80\n",
      "64.6 action:  [0.004, -0.9984] n_targets:  1 reward:  80\n",
      "67.1 action:  [0.005, -0.9984] n_targets:  1 reward:  80\n",
      "69.9 action:  [0.0045, -0.9903] n_targets:  1 reward:  80\n",
      "79.6 action:  [0.0021, -0.9993] n_targets:  1 reward:  80\n",
      "82.6 action:  [0.0061, -0.9914] n_targets:  2 reward:  160\n",
      "84.9 action:  [-0.0061, -0.999] n_targets:  1 reward:  80\n",
      "94.5 action:  [-0.0036, -0.9919] n_targets:  1 reward:  80\n",
      "98.8 action:  [-0.0056, -0.9999] n_targets:  2 reward:  160\n",
      "102.2 action:  [-0.0099, -0.9985] n_targets:  2 reward:  160\n",
      "Firely capture rate for the episode:  32 ff for 102.49999999999845 s: -------------------> 0.31\n",
      "Total reward for the episode:  2560.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  307\n",
      "3.5 action:  [0.0065, -0.9978] n_targets:  1 reward:  80\n",
      "6.6 action:  [-0.003, -0.9946] n_targets:  1 reward:  80\n",
      "8.7 action:  [0.0097, -0.9956] n_targets:  1 reward:  80\n",
      "11.2 action:  [0.0008, -0.9965] n_targets:  1 reward:  80\n",
      "13.6 action:  [0.0011, -0.9812] n_targets:  1 reward:  80\n",
      "15.7 action:  [-0.0054, -0.9932] n_targets:  1 reward:  80\n",
      "24.3 action:  [0.0075, -0.9951] n_targets:  1 reward:  80\n",
      "33.4 action:  [-0.0058, -0.9993] n_targets:  1 reward:  80\n",
      "47.0 action:  [0.0088, -0.9932] n_targets:  1 reward:  80\n",
      "54.7 action:  [0.0038, -0.9938] n_targets:  1 reward:  80\n",
      "56.7 action:  [0.0091, -0.9999] n_targets:  1 reward:  80\n",
      "62.4 action:  [-0.0029, -0.9992] n_targets:  1 reward:  80\n",
      "77.1 action:  [-0.0039, -0.9979] n_targets:  1 reward:  80\n",
      "TIME before resetting: 77.49999999999987\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  308\n",
      "2.4 action:  [-0.0016, -0.9896] n_targets:  1 reward:  80\n",
      "7.8 action:  [0.0095, -0.9824] n_targets:  1 reward:  80\n",
      "15.3 action:  [-0.002, -0.9978] n_targets:  1 reward:  80\n",
      "32.6 action:  [-0.0054, -0.9936] n_targets:  2 reward:  160\n",
      "43.4 action:  [0.0029, -0.9917] n_targets:  1 reward:  80\n",
      "50.5 action:  [0.0079, -0.9912] n_targets:  2 reward:  160\n",
      "55.5 action:  [-0.001, -0.9992] n_targets:  1 reward:  80\n",
      "58.7 action:  [0.0031, -0.9966] n_targets:  1 reward:  80\n",
      "60.7 action:  [-0.007, -0.9973] n_targets:  1 reward:  80\n",
      "66.9 action:  [0.0084, -0.9899] n_targets:  1 reward:  80\n",
      "71.1 action:  [-0.0008, -0.9928] n_targets:  1 reward:  80\n",
      "73.9 action:  [0.008, -0.9995] n_targets:  1 reward:  80\n",
      "76.1 action:  [0.0097, -0.9853] n_targets:  1 reward:  80\n",
      "86.4 action:  [-0.0076, -0.9969] n_targets:  1 reward:  80\n",
      "88.2 action:  [0.0064, -0.9951] n_targets:  1 reward:  80\n",
      "94.4 action:  [-0.0002, -0.999] n_targets:  1 reward:  80\n",
      "99.0 action:  [-0.0065, -0.9891] n_targets:  1 reward:  80\n",
      "102.1 action:  [0.0061, -0.9845] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  20 ff for 102.49999999999845 s: -------------------> 0.2\n",
      "Total reward for the episode:  1600.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  309\n",
      "3.9 action:  [0.0073, -0.9959] n_targets:  1 reward:  80\n",
      "6.7 action:  [-0.008, -0.9984] n_targets:  1 reward:  80\n",
      "10.3 action:  [-0.0051, -0.9949] n_targets:  1 reward:  80\n",
      "13.5 action:  [0.004, -0.9993] n_targets:  1 reward:  80\n",
      "23.4 action:  [-0.0033, -0.9876] n_targets:  1 reward:  80\n",
      "25.3 action:  [-0.0091, -0.9911] n_targets:  2 reward:  160\n",
      "29.1 action:  [-0.0069, -0.9995] n_targets:  1 reward:  80\n",
      "30.7 action:  [-0.0089, -0.9875] n_targets:  1 reward:  80\n",
      "33.7 action:  [-0.0029, -0.9967] n_targets:  1 reward:  80\n",
      "37.9 action:  [0.0028, -0.9972] n_targets:  1 reward:  80\n",
      "41.7 action:  [0.0049, -0.9947] n_targets:  1 reward:  80\n",
      "42.9 action:  [-0.0042, -0.9849] n_targets:  1 reward:  80\n",
      "52.8 action:  [-0.0046, -0.9965] n_targets:  1 reward:  80\n",
      "57.9 action:  [-0.0066, -0.9843] n_targets:  1 reward:  80\n",
      "66.7 action:  [-0.0049, -0.9958] n_targets:  1 reward:  80\n",
      "67.9 action:  [-0.0012, -0.9949] n_targets:  1 reward:  80\n",
      "68.8 action:  [-0.0056, -0.9951] n_targets:  1 reward:  80\n",
      "75.8 action:  [0.0041, -0.9803] n_targets:  1 reward:  80\n",
      "88.5 action:  [-0.0085, -0.9981] n_targets:  1 reward:  80\n",
      "92.3 action:  [-0.0047, -0.9932] n_targets:  1 reward:  80\n",
      "93.7 action:  [0.0008, -0.9957] n_targets:  1 reward:  80\n",
      "94.2 action:  [-0.0048, -0.9899] n_targets:  1 reward:  80\n",
      "98.8 action:  [-0.0027, -0.9878] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  24 ff for 102.49999999999845 s: -------------------> 0.23\n",
      "Total reward for the episode:  1920.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  310\n",
      "4.1 action:  [-0.0006, -0.9951] n_targets:  1 reward:  80\n",
      "6.0 action:  [0.0015, -0.9985] n_targets:  1 reward:  80\n",
      "17.8 action:  [-0.0051, -0.9859] n_targets:  1 reward:  80\n",
      "21.2 action:  [-0.0061, -0.9976] n_targets:  1 reward:  80\n",
      "24.7 action:  [0.0083, -0.9853] n_targets:  1 reward:  80\n",
      "34.4 action:  [-0.006, -0.9982] n_targets:  1 reward:  80\n",
      "36.4 action:  [0.0094, -0.9855] n_targets:  1 reward:  80\n",
      "38.8 action:  [-0.0051, -0.9879] n_targets:  1 reward:  80\n",
      "39.5 action:  [-0.0024, -0.9861] n_targets:  1 reward:  80\n",
      "44.3 action:  [0.0054, -0.9866] n_targets:  2 reward:  160\n",
      "49.2 action:  [-0.0063, -0.9977] n_targets:  1 reward:  80\n",
      "71.2 action:  [-0.0075, -0.9967] n_targets:  1 reward:  80\n",
      "74.3 action:  [-0.0031, -0.9904] n_targets:  1 reward:  80\n",
      "80.1 action:  [-0.0087, -0.9991] n_targets:  4 reward:  320\n",
      "82.5 action:  [0.0074, -0.9824] n_targets:  1 reward:  80\n",
      "84.6 action:  [0.0047, -0.9988] n_targets:  1 reward:  80\n",
      "90.9 action:  [-0.0015, -0.9982] n_targets:  1 reward:  80\n",
      "94.1 action:  [-0.0012, -0.9879] n_targets:  2 reward:  160\n",
      "95.6 action:  [-0.0022, -0.9984] n_targets:  1 reward:  80\n",
      "96.4 action:  [0.0014, -0.9815] n_targets:  2 reward:  160\n",
      "100.3 action:  [0.008, -0.9985] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  27 ff for 102.49999999999845 s: -------------------> 0.26\n",
      "Total reward for the episode:  2160.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  311\n",
      "Eval num_timesteps=90000, episode_reward=1893.33 +/- 229.40\n",
      "Episode length: 1025.00 +/- 0.00\n",
      "New best mean reward!\n",
      "3.4 action:  [0.0055, -0.9997] n_targets:  1 reward:  80\n",
      "5.2 action:  [0.0041, -0.9995] n_targets:  1 reward:  80\n",
      "8.3 action:  [0.0016, -0.9992] n_targets:  1 reward:  80\n",
      "14.1 action:  [-0.0077, -0.9957] n_targets:  2 reward:  160\n",
      "20.7 action:  [0.0066, -0.9975] n_targets:  1 reward:  80\n",
      "27.6 action:  [-0.0016, -0.9986] n_targets:  1 reward:  80\n",
      "34.0 action:  [-0.0076, -0.9907] n_targets:  1 reward:  80\n",
      "36.6 action:  [-0.0007, -0.989] n_targets:  2 reward:  160\n",
      "43.3 action:  [0.0017, -0.9998] n_targets:  1 reward:  80\n",
      "47.9 action:  [0.0063, -0.9992] n_targets:  1 reward:  80\n",
      "52.9 action:  [0.0029, -0.9988] n_targets:  2 reward:  160\n",
      "54.4 action:  [0.0017, -0.9998] n_targets:  1 reward:  80\n",
      "71.5 action:  [-0.0045, -0.9972] n_targets:  1 reward:  80\n",
      "76.6 action:  [-0.0022, -0.9978] n_targets:  2 reward:  160\n",
      "93.2 action:  [-0.0009, -0.9909] n_targets:  1 reward:  80\n",
      "101.6 action:  [0.01, -0.9946] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  20 ff for 102.49999999999845 s: -------------------> 0.2\n",
      "Total reward for the episode:  1600.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  312\n",
      "6.5 action:  [0.0032, -0.9962] n_targets:  1 reward:  80\n",
      "9.0 action:  [-0.0089, -0.9968] n_targets:  1 reward:  80\n",
      "14.9 action:  [0.0017, -0.99] n_targets:  1 reward:  80\n",
      "20.3 action:  [-0.0006, -0.9852] n_targets:  1 reward:  80\n",
      "24.3 action:  [0.0001, -0.9985] n_targets:  1 reward:  80\n",
      "36.7 action:  [-0.0047, -0.9997] n_targets:  2 reward:  160\n",
      "40.9 action:  [-0.0015, -0.9924] n_targets:  1 reward:  80\n",
      "43.4 action:  [0.0051, -0.9988] n_targets:  1 reward:  80\n",
      "45.3 action:  [0.0035, -0.989] n_targets:  1 reward:  80\n",
      "47.9 action:  [0.0048, -0.999] n_targets:  1 reward:  80\n",
      "54.1 action:  [-0.0028, -0.9828] n_targets:  1 reward:  80\n",
      "56.1 action:  [-0.0081, -0.9924] n_targets:  1 reward:  80\n",
      "56.7 action:  [0.0074, -0.9981] n_targets:  1 reward:  80\n",
      "62.5 action:  [-0.0025, -0.999] n_targets:  1 reward:  80\n",
      "63.6 action:  [0.005, -0.9994] n_targets:  1 reward:  80\n",
      "71.7 action:  [0.002, -0.9972] n_targets:  1 reward:  80\n",
      "77.1 action:  [-0.0005, -0.9878] n_targets:  1 reward:  80\n",
      "80.2 action:  [0.0044, -0.9997] n_targets:  1 reward:  80\n",
      "82.6 action:  [0.0022, -0.998] n_targets:  1 reward:  80\n",
      "83.7 action:  [0.008, -0.9984] n_targets:  1 reward:  80\n",
      "89.8 action:  [0.0081, -0.9909] n_targets:  1 reward:  80\n",
      "92.1 action:  [0.0077, -0.9881] n_targets:  1 reward:  80\n",
      "94.3 action:  [0.0008, -0.9992] n_targets:  2 reward:  160\n",
      "Firely capture rate for the episode:  25 ff for 102.49999999999845 s: -------------------> 0.24\n",
      "Total reward for the episode:  2000.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  313\n",
      "5.6 action:  [0.0053, -0.9961] n_targets:  1 reward:  80\n",
      "16.4 action:  [0.0028, -0.9987] n_targets:  1 reward:  80\n",
      "25.3 action:  [0.0048, -0.9858] n_targets:  1 reward:  80\n",
      "27.9 action:  [-0.0067, -0.9955] n_targets:  1 reward:  80\n",
      "32.7 action:  [-0.0005, -0.9978] n_targets:  1 reward:  80\n",
      "33.4 action:  [0.0087, -0.9972] n_targets:  1 reward:  80\n",
      "40.0 action:  [0.0076, -0.9865] n_targets:  1 reward:  80\n",
      "51.9 action:  [0.0034, -0.9928] n_targets:  1 reward:  80\n",
      "55.2 action:  [0.0086, -0.9958] n_targets:  1 reward:  80\n",
      "75.3 action:  [0.0068, -0.9959] n_targets:  1 reward:  80\n",
      "76.7 action:  [-0.0097, -0.9971] n_targets:  1 reward:  80\n",
      "77.0 action:  [0.0071, -0.9882] n_targets:  1 reward:  80\n",
      "88.0 action:  [-0.0017, -0.9953] n_targets:  1 reward:  80\n",
      "89.9 action:  [-0.0089, -0.9993] n_targets:  2 reward:  160\n",
      "92.8 action:  [-0.0063, -0.9992] n_targets:  1 reward:  80\n",
      "95.0 action:  [0.007, -0.9829] n_targets:  1 reward:  80\n",
      "99.4 action:  [0.0092, -0.9832] n_targets:  1 reward:  80\n",
      "101.1 action:  [0.0002, -0.9951] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  19 ff for 102.49999999999845 s: -------------------> 0.19\n",
      "Total reward for the episode:  1520.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  314\n",
      "8.5 action:  [-0.0097, -0.9993] n_targets:  1 reward:  80\n",
      "10.4 action:  [0.0061, -0.9834] n_targets:  1 reward:  80\n",
      "14.0 action:  [-0.0083, -0.9973] n_targets:  1 reward:  80\n",
      "16.8 action:  [-0.0011, -0.9914] n_targets:  1 reward:  80\n",
      "26.3 action:  [0.0028, -0.9985] n_targets:  1 reward:  80\n",
      "31.8 action:  [0.0059, -0.998] n_targets:  1 reward:  80\n",
      "37.0 action:  [-0.0008, -0.9881] n_targets:  1 reward:  80\n",
      "40.6 action:  [0.0056, -0.9903] n_targets:  1 reward:  80\n",
      "41.7 action:  [-0.0031, -0.9973] n_targets:  2 reward:  160\n",
      "45.1 action:  [-0.0012, -0.9966] n_targets:  1 reward:  80\n",
      "47.9 action:  [0.0009, -0.9965] n_targets:  1 reward:  80\n",
      "49.2 action:  [0.0018, -0.9808] n_targets:  1 reward:  80\n",
      "51.9 action:  [-0.0053, -0.9972] n_targets:  2 reward:  160\n",
      "60.3 action:  [0.0097, -0.9956] n_targets:  1 reward:  80\n",
      "63.1 action:  [-0.0015, -0.9987] n_targets:  1 reward:  80\n",
      "69.3 action:  [-0.0024, -0.9954] n_targets:  1 reward:  80\n",
      "79.1 action:  [-0.0073, -0.9993] n_targets:  1 reward:  80\n",
      "84.2 action:  [0.0055, -0.9884] n_targets:  1 reward:  80\n",
      "89.4 action:  [0.0043, -0.9881] n_targets:  1 reward:  80\n",
      "94.0 action:  [0.0006, -0.9956] n_targets:  1 reward:  80\n",
      "96.0 action:  [-0.0071, -0.9915] n_targets:  1 reward:  80\n",
      "99.5 action:  [-0.0037, -0.9923] n_targets:  1 reward:  80\n",
      "100.6 action:  [0.0059, -0.9933] n_targets:  1 reward:  80\n",
      "102.2 action:  [0.0047, -0.9887] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  26 ff for 102.49999999999845 s: -------------------> 0.25\n",
      "Total reward for the episode:  2080.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  315\n",
      "2.0 action:  [-0.0005, -0.999] n_targets:  1 reward:  80\n",
      "11.7 action:  [-0.0011, -0.9955] n_targets:  1 reward:  80\n",
      "13.6 action:  [-0.0044, -0.9899] n_targets:  1 reward:  80\n",
      "15.2 action:  [-0.0072, -0.9993] n_targets:  1 reward:  80\n",
      "18.1 action:  [-0.0054, -0.996] n_targets:  1 reward:  80\n",
      "19.0 action:  [-0.0084, -0.997] n_targets:  1 reward:  80\n",
      "21.1 action:  [-0.0011, -0.9987] n_targets:  1 reward:  80\n",
      "28.5 action:  [-0.0012, -0.9888] n_targets:  1 reward:  80\n",
      "33.6 action:  [-0.0023, -0.9925] n_targets:  1 reward:  80\n",
      "39.6 action:  [0.0059, -0.9931] n_targets:  1 reward:  80\n",
      "42.2 action:  [-0.0004, -0.9913] n_targets:  1 reward:  80\n",
      "48.8 action:  [0.0045, -0.9954] n_targets:  1 reward:  80\n",
      "61.1 action:  [-0.0032, -0.9929] n_targets:  2 reward:  160\n",
      "63.9 action:  [0.0062, -0.9847] n_targets:  2 reward:  160\n",
      "71.7 action:  [0.0043, -0.9963] n_targets:  1 reward:  80\n",
      "75.1 action:  [0.0068, -0.9993] n_targets:  1 reward:  80\n",
      "76.0 action:  [-0.0018, -0.9995] n_targets:  1 reward:  80\n",
      "80.4 action:  [0.0084, -0.9959] n_targets:  1 reward:  80\n",
      "91.1 action:  [0.0076, -0.9956] n_targets:  1 reward:  80\n",
      "94.1 action:  [-0.001, -0.9963] n_targets:  1 reward:  80\n",
      "101.0 action:  [0.0056, -0.994] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  23 ff for 102.49999999999845 s: -------------------> 0.22\n",
      "Total reward for the episode:  1840.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  316\n",
      "3.8 action:  [-0.0066, -0.9968] n_targets:  2 reward:  160\n",
      "6.3 action:  [-0.0047, -0.995] n_targets:  1 reward:  80\n",
      "12.2 action:  [-0.005, -0.991] n_targets:  1 reward:  80\n",
      "15.2 action:  [0.0078, -0.9988] n_targets:  1 reward:  80\n",
      "19.4 action:  [0.0058, -0.9982] n_targets:  1 reward:  80\n",
      "42.6 action:  [-0.0024, -0.9986] n_targets:  1 reward:  80\n",
      "44.9 action:  [0.0088, -0.9992] n_targets:  1 reward:  80\n",
      "55.6 action:  [0.0049, -0.9934] n_targets:  1 reward:  80\n",
      "58.3 action:  [-0.0043, -0.9803] n_targets:  1 reward:  80\n",
      "62.6 action:  [-0.0062, -0.9974] n_targets:  1 reward:  80\n",
      "65.2 action:  [-0.0058, -0.9996] n_targets:  1 reward:  80\n",
      "68.3 action:  [0.0014, -0.9991] n_targets:  1 reward:  80\n",
      "72.4 action:  [-0.0002, -0.9989] n_targets:  1 reward:  80\n",
      "82.8 action:  [0.0061, -0.9926] n_targets:  1 reward:  80\n",
      "87.3 action:  [-0.0017, -0.9889] n_targets:  1 reward:  80\n",
      "89.6 action:  [-0.0061, -0.996] n_targets:  1 reward:  80\n",
      "96.4 action:  [0.0019, -0.9994] n_targets:  1 reward:  80\n",
      "98.5 action:  [0.0054, -0.9955] n_targets:  1 reward:  80\n",
      "100.9 action:  [0.0012, -0.9968] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  20 ff for 102.49999999999845 s: -------------------> 0.2\n",
      "Total reward for the episode:  1600.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  317\n",
      "3.6 action:  [-0.0089, -0.9879] n_targets:  1 reward:  80\n",
      "7.4 action:  [-0.0069, -0.9979] n_targets:  1 reward:  80\n",
      "12.3 action:  [0.0064, -0.9897] n_targets:  1 reward:  80\n",
      "16.4 action:  [0.0046, -0.9932] n_targets:  1 reward:  80\n",
      "23.2 action:  [-0.0096, -0.9995] n_targets:  1 reward:  80\n",
      "41.0 action:  [0.0092, -0.9995] n_targets:  1 reward:  80\n",
      "43.2 action:  [0.0002, -0.9993] n_targets:  1 reward:  80\n",
      "55.0 action:  [-0.0069, -0.9997] n_targets:  2 reward:  160\n",
      "59.3 action:  [-0.0029, -0.9984] n_targets:  1 reward:  80\n",
      "64.2 action:  [-0.0056, -0.9988] n_targets:  1 reward:  80\n",
      "72.3 action:  [0.0002, -0.9962] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  12 ff for 102.49999999999845 s: -------------------> 0.12\n",
      "Total reward for the episode:  960.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  318\n",
      "5.3 action:  [-0.004, -0.9949] n_targets:  1 reward:  80\n",
      "5.6 action:  [-0.0067, -0.9955] n_targets:  1 reward:  80\n",
      "15.7 action:  [0.0011, -0.9986] n_targets:  2 reward:  160\n",
      "17.1 action:  [0.0075, -0.9934] n_targets:  1 reward:  80\n",
      "22.0 action:  [-0.0081, -0.9996] n_targets:  1 reward:  80\n",
      "26.3 action:  [0.0094, -0.9826] n_targets:  1 reward:  80\n",
      "29.3 action:  [0.0099, -0.9976] n_targets:  1 reward:  80\n",
      "32.5 action:  [0.0038, -0.9976] n_targets:  1 reward:  80\n",
      "34.3 action:  [-0.0016, -0.995] n_targets:  1 reward:  80\n",
      "39.3 action:  [0.0052, -0.9955] n_targets:  1 reward:  80\n",
      "41.8 action:  [0.0046, -0.9858] n_targets:  1 reward:  80\n",
      "45.4 action:  [-0.0091, -0.9987] n_targets:  1 reward:  80\n",
      "50.7 action:  [-0.0031, -0.9961] n_targets:  1 reward:  80\n",
      "62.0 action:  [0.0003, -0.9926] n_targets:  1 reward:  80\n",
      "64.3 action:  [0.0092, -0.9989] n_targets:  1 reward:  80\n",
      "65.6 action:  [0.0089, -0.9902] n_targets:  1 reward:  80\n",
      "68.4 action:  [0.0082, -0.997] n_targets:  1 reward:  80\n",
      "73.3 action:  [-0.007, -0.9979] n_targets:  1 reward:  80\n",
      "75.0 action:  [-0.0065, -0.999] n_targets:  1 reward:  80\n",
      "77.8 action:  [-0.0039, -0.9958] n_targets:  1 reward:  80\n",
      "80.6 action:  [-0.0021, -0.9984] n_targets:  1 reward:  80\n",
      "81.8 action:  [-0.0029, -0.9972] n_targets:  1 reward:  80\n",
      "100.6 action:  [-0.0024, -0.9985] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  24 ff for 102.49999999999845 s: -------------------> 0.23\n",
      "Total reward for the episode:  1920.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  319\n",
      "10.9 action:  [0.0037, -0.9996] n_targets:  1 reward:  80\n",
      "14.2 action:  [0.0009, -0.9951] n_targets:  1 reward:  80\n",
      "16.3 action:  [0.0032, -0.9927] n_targets:  1 reward:  80\n",
      "17.1 action:  [-0.0028, -0.9963] n_targets:  1 reward:  80\n",
      "20.7 action:  [0.0065, -0.9945] n_targets:  1 reward:  80\n",
      "22.2 action:  [0.0062, -0.988] n_targets:  1 reward:  80\n",
      "28.2 action:  [0.0092, -0.9922] n_targets:  1 reward:  80\n",
      "29.3 action:  [0.0095, -0.9955] n_targets:  1 reward:  80\n",
      "38.8 action:  [-0.0076, -0.9996] n_targets:  1 reward:  80\n",
      "42.4 action:  [-0.0086, -0.999] n_targets:  1 reward:  80\n",
      "48.0 action:  [0.0066, -0.9971] n_targets:  2 reward:  160\n",
      "52.1 action:  [-0.006, -0.9955] n_targets:  1 reward:  80\n",
      "66.8 action:  [0.0089, -0.9965] n_targets:  1 reward:  80\n",
      "68.9 action:  [-0.0091, -0.9992] n_targets:  2 reward:  160\n",
      "70.2 action:  [0.0002, -1.0] n_targets:  1 reward:  80\n",
      "76.9 action:  [-0.004, -0.9847] n_targets:  1 reward:  80\n",
      "77.8 action:  [-0.0069, -0.9898] n_targets:  1 reward:  80\n",
      "100.7 action:  [-0.0087, -0.9973] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  20 ff for 102.49999999999845 s: -------------------> 0.2\n",
      "Total reward for the episode:  1600.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  320\n",
      "3.1 action:  [-0.0044, -0.9947] n_targets:  2 reward:  160\n",
      "11.0 action:  [-0.0019, -0.9978] n_targets:  1 reward:  80\n",
      "15.6 action:  [-0.0006, -0.9964] n_targets:  1 reward:  80\n",
      "23.0 action:  [0.0076, -0.9906] n_targets:  2 reward:  160\n",
      "24.9 action:  [-0.0048, -0.9994] n_targets:  1 reward:  80\n",
      "36.2 action:  [-0.0023, -0.9923] n_targets:  1 reward:  80\n",
      "45.1 action:  [0.0032, -0.9904] n_targets:  1 reward:  80\n",
      "46.6 action:  [0.0026, -0.9895] n_targets:  2 reward:  160\n",
      "58.9 action:  [0.0066, -0.9979] n_targets:  1 reward:  80\n",
      "61.4 action:  [-0.0029, -0.9996] n_targets:  1 reward:  80\n",
      "63.9 action:  [-0.0094, -0.9999] n_targets:  1 reward:  80\n",
      "65.2 action:  [-0.0062, -0.9952] n_targets:  1 reward:  80\n",
      "68.2 action:  [0.0074, -0.9987] n_targets:  1 reward:  80\n",
      "71.9 action:  [-0.0009, -0.9818] n_targets:  1 reward:  80\n",
      "75.6 action:  [-0.0031, -0.9966] n_targets:  1 reward:  80\n",
      "76.6 action:  [0.0011, -0.9891] n_targets:  1 reward:  80\n",
      "77.1 action:  [0.0032, -0.9899] n_targets:  2 reward:  160\n",
      "TIME before resetting: 77.49999999999987\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  321\n",
      "5.4 action:  [0.01, -0.998] n_targets:  1 reward:  80\n",
      "13.1 action:  [0.0026, -0.991] n_targets:  2 reward:  160\n",
      "18.0 action:  [-0.0028, -0.9993] n_targets:  2 reward:  160\n",
      "23.8 action:  [0.0007, -0.9986] n_targets:  1 reward:  80\n",
      "26.7 action:  [-0.0028, -0.9986] n_targets:  1 reward:  80\n",
      "36.9 action:  [0.008, -0.9828] n_targets:  1 reward:  80\n",
      "44.0 action:  [-0.0031, -0.9982] n_targets:  1 reward:  80\n",
      "49.2 action:  [-0.0088, -0.9936] n_targets:  1 reward:  80\n",
      "51.1 action:  [-0.0017, -0.9908] n_targets:  1 reward:  80\n",
      "52.7 action:  [-0.0019, -0.9926] n_targets:  1 reward:  80\n",
      "57.2 action:  [-0.003, -0.9947] n_targets:  1 reward:  80\n",
      "58.2 action:  [0.0055, -0.9958] n_targets:  1 reward:  80\n",
      "61.4 action:  [-0.009, -0.9864] n_targets:  1 reward:  80\n",
      "65.7 action:  [-0.0021, -0.996] n_targets:  1 reward:  80\n",
      "67.9 action:  [-0.0043, -0.9967] n_targets:  2 reward:  160\n",
      "75.2 action:  [0.0091, -0.994] n_targets:  1 reward:  80\n",
      "77.5 action:  [0.0067, -0.9967] n_targets:  1 reward:  80\n",
      "77.8 action:  [0.0031, -0.991] n_targets:  1 reward:  80\n",
      "80.9 action:  [-0.0052, -0.9907] n_targets:  1 reward:  80\n",
      "83.7 action:  [0.0049, -0.9859] n_targets:  1 reward:  80\n",
      "87.4 action:  [0.0055, -0.9948] n_targets:  1 reward:  80\n",
      "88.6 action:  [0.0092, -0.9994] n_targets:  1 reward:  80\n",
      "89.4 action:  [0.0054, -0.9854] n_targets:  1 reward:  80\n",
      "90.3 action:  [-0.0099, -0.9908] n_targets:  1 reward:  80\n",
      "91.6 action:  [0.0046, -0.9932] n_targets:  1 reward:  80\n",
      "100.7 action:  [0.0076, -0.9936] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  29 ff for 102.49999999999845 s: -------------------> 0.28\n",
      "Total reward for the episode:  2320.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  322\n",
      "2.5 action:  [0.0093, -0.9972] n_targets:  1 reward:  80\n",
      "6.2 action:  [0.0004, -0.9946] n_targets:  2 reward:  160\n",
      "20.2 action:  [-0.0076, -0.9843] n_targets:  1 reward:  80\n",
      "40.4 action:  [0.0046, -0.9848] n_targets:  2 reward:  160\n",
      "47.3 action:  [-0.0073, -0.9986] n_targets:  1 reward:  80\n",
      "51.9 action:  [-0.0, -0.9865] n_targets:  1 reward:  80\n",
      "53.0 action:  [0.0005, -0.9816] n_targets:  1 reward:  80\n",
      "57.2 action:  [0.0023, -0.9968] n_targets:  2 reward:  160\n",
      "61.9 action:  [0.0072, -0.9895] n_targets:  1 reward:  80\n",
      "67.7 action:  [0.0041, -0.9939] n_targets:  2 reward:  160\n",
      "78.1 action:  [-0.0057, -0.9803] n_targets:  1 reward:  80\n",
      "79.8 action:  [-0.0033, -0.9896] n_targets:  1 reward:  80\n",
      "81.0 action:  [-0.0018, -0.9938] n_targets:  1 reward:  80\n",
      "82.8 action:  [0.0002, -0.9807] n_targets:  1 reward:  80\n",
      "98.4 action:  [0.0029, -0.9956] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  19 ff for 102.49999999999845 s: -------------------> 0.19\n",
      "Total reward for the episode:  1520.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  323\n",
      "4.7 action:  [-0.0074, -0.9981] n_targets:  1 reward:  80\n",
      "5.4 action:  [0.0077, -0.9808] n_targets:  1 reward:  80\n",
      "8.5 action:  [0.0038, -0.9956] n_targets:  1 reward:  80\n",
      "11.1 action:  [-0.0009, -0.9826] n_targets:  1 reward:  80\n",
      "13.3 action:  [0.0001, -0.9972] n_targets:  1 reward:  80\n",
      "16.0 action:  [-0.0086, -0.9997] n_targets:  1 reward:  80\n",
      "26.0 action:  [-0.0046, -0.9918] n_targets:  1 reward:  80\n",
      "28.3 action:  [0.0043, -0.9991] n_targets:  1 reward:  80\n",
      "34.1 action:  [-0.0064, -0.9982] n_targets:  1 reward:  80\n",
      "36.7 action:  [0.009, -0.9911] n_targets:  1 reward:  80\n",
      "38.2 action:  [-0.009, -0.9981] n_targets:  1 reward:  80\n",
      "42.7 action:  [-0.0083, -0.9965] n_targets:  1 reward:  80\n",
      "46.3 action:  [0.0056, -0.9988] n_targets:  1 reward:  80\n",
      "52.0 action:  [-0.0088, -0.9977] n_targets:  1 reward:  80\n",
      "57.3 action:  [-0.0093, -0.9907] n_targets:  1 reward:  80\n",
      "64.6 action:  [0.0095, -0.9871] n_targets:  1 reward:  80\n",
      "76.3 action:  [0.0093, -0.986] n_targets:  1 reward:  80\n",
      "78.0 action:  [0.0064, -0.9965] n_targets:  1 reward:  80\n",
      "81.8 action:  [-0.0022, -0.9984] n_targets:  1 reward:  80\n",
      "82.8 action:  [0.01, -0.9941] n_targets:  1 reward:  80\n",
      "84.6 action:  [-0.0025, -0.998] n_targets:  1 reward:  80\n",
      "84.9 action:  [-0.004, -0.9947] n_targets:  1 reward:  80\n",
      "87.2 action:  [0.0018, -0.9942] n_targets:  1 reward:  80\n",
      "91.3 action:  [-0.002, -0.9996] n_targets:  1 reward:  80\n",
      "93.2 action:  [0.0082, -0.9866] n_targets:  2 reward:  160\n",
      "94.9 action:  [0.0066, -0.9916] n_targets:  1 reward:  80\n",
      "100.8 action:  [0.0041, -0.9961] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  28 ff for 102.49999999999845 s: -------------------> 0.27\n",
      "Total reward for the episode:  2240.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  324\n",
      "Eval num_timesteps=100000, episode_reward=2026.67 +/- 359.75\n",
      "Episode length: 1025.00 +/- 0.00\n",
      "New best mean reward!\n",
      "9.0 action:  [-0.0067, -0.9986] n_targets:  2 reward:  160\n",
      "17.1 action:  [-0.0095, -0.9839] n_targets:  1 reward:  80\n",
      "20.5 action:  [0.0098, -0.9822] n_targets:  1 reward:  80\n",
      "21.5 action:  [0.0085, -0.9958] n_targets:  1 reward:  80\n",
      "34.6 action:  [-0.0023, -0.9992] n_targets:  1 reward:  80\n",
      "43.3 action:  [0.0001, -0.997] n_targets:  1 reward:  80\n",
      "47.7 action:  [0.0061, -0.9996] n_targets:  1 reward:  80\n",
      "50.4 action:  [-0.0036, -0.9987] n_targets:  2 reward:  160\n",
      "54.5 action:  [0.0085, -0.9941] n_targets:  1 reward:  80\n",
      "56.7 action:  [0.0072, -0.9937] n_targets:  1 reward:  80\n",
      "59.8 action:  [-0.0051, -0.9985] n_targets:  1 reward:  80\n",
      "61.9 action:  [-0.0071, -1.0] n_targets:  2 reward:  160\n",
      "70.4 action:  [0.0027, -0.9998] n_targets:  1 reward:  80\n",
      "71.5 action:  [-0.0005, -0.9989] n_targets:  1 reward:  80\n",
      "74.7 action:  [0.0072, -0.9995] n_targets:  1 reward:  80\n",
      "80.0 action:  [-0.0057, -0.9991] n_targets:  1 reward:  80\n",
      "85.2 action:  [0.0036, -0.9865] n_targets:  1 reward:  80\n",
      "91.1 action:  [-0.0003, -0.9956] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  21 ff for 102.49999999999845 s: -------------------> 0.2\n",
      "Total reward for the episode:  1680.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  325\n",
      "6.0 action:  [-0.0029, -0.9891] n_targets:  1 reward:  80\n",
      "7.2 action:  [0.005, -0.9971] n_targets:  1 reward:  80\n",
      "11.8 action:  [-0.0097, -0.9948] n_targets:  1 reward:  80\n",
      "13.8 action:  [0.0044, -0.9862] n_targets:  1 reward:  80\n",
      "15.2 action:  [0.01, -0.9987] n_targets:  2 reward:  160\n",
      "24.0 action:  [-0.0027, -0.9964] n_targets:  1 reward:  80\n",
      "27.3 action:  [0.0035, -0.9999] n_targets:  1 reward:  80\n",
      "31.2 action:  [0.0038, -0.9989] n_targets:  1 reward:  80\n",
      "33.7 action:  [-0.0085, -0.9997] n_targets:  1 reward:  80\n",
      "36.5 action:  [-0.0001, -0.9957] n_targets:  1 reward:  80\n",
      "44.9 action:  [-0.0026, -0.9926] n_targets:  1 reward:  80\n",
      "51.3 action:  [-0.0054, -0.9978] n_targets:  1 reward:  80\n",
      "52.0 action:  [0.0066, -0.9977] n_targets:  1 reward:  80\n",
      "53.9 action:  [-0.003, -0.9837] n_targets:  1 reward:  80\n",
      "60.5 action:  [-0.0061, -0.9963] n_targets:  1 reward:  80\n",
      "73.8 action:  [0.0069, -0.9809] n_targets:  1 reward:  80\n",
      "82.9 action:  [-0.0061, -0.9984] n_targets:  1 reward:  80\n",
      "84.9 action:  [-0.0002, -0.985] n_targets:  1 reward:  80\n",
      "86.0 action:  [-0.001, -0.9959] n_targets:  1 reward:  80\n",
      "89.7 action:  [-0.0098, -0.9904] n_targets:  2 reward:  160\n",
      "92.4 action:  [0.0051, -0.9993] n_targets:  1 reward:  80\n",
      "93.7 action:  [-0.007, -0.9995] n_targets:  1 reward:  80\n",
      "95.7 action:  [0.0057, -0.9965] n_targets:  1 reward:  80\n",
      "100.1 action:  [0.0042, -0.995] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  26 ff for 102.49999999999845 s: -------------------> 0.25\n",
      "Total reward for the episode:  2080.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  326\n",
      "3.3 action:  [-0.0031, -0.9997] n_targets:  1 reward:  80\n",
      "5.9 action:  [-0.0095, -0.9988] n_targets:  1 reward:  80\n",
      "11.2 action:  [-0.0038, -0.9881] n_targets:  1 reward:  80\n",
      "15.8 action:  [-0.0003, -0.9971] n_targets:  1 reward:  80\n",
      "30.1 action:  [-0.0094, -0.9932] n_targets:  1 reward:  80\n",
      "31.5 action:  [-0.0062, -0.9957] n_targets:  1 reward:  80\n",
      "32.7 action:  [-0.0082, -0.9947] n_targets:  1 reward:  80\n",
      "35.2 action:  [0.0011, -0.9858] n_targets:  1 reward:  80\n",
      "39.5 action:  [0.0095, -0.9924] n_targets:  1 reward:  80\n",
      "43.2 action:  [0.0009, -0.9841] n_targets:  1 reward:  80\n",
      "46.7 action:  [-0.0054, -0.9879] n_targets:  1 reward:  80\n",
      "48.7 action:  [0.0057, -0.9963] n_targets:  1 reward:  80\n",
      "50.9 action:  [0.0077, -0.9969] n_targets:  1 reward:  80\n",
      "52.6 action:  [-0.0033, -0.9994] n_targets:  2 reward:  160\n",
      "58.0 action:  [-0.0083, -0.9834] n_targets:  1 reward:  80\n",
      "73.7 action:  [-0.0037, -0.999] n_targets:  1 reward:  80\n",
      "79.8 action:  [0.0097, -0.9935] n_targets:  2 reward:  160\n",
      "81.6 action:  [-0.0041, -0.9957] n_targets:  1 reward:  80\n",
      "85.0 action:  [0.0051, -0.9993] n_targets:  1 reward:  80\n",
      "89.8 action:  [-0.0058, -0.9984] n_targets:  1 reward:  80\n",
      "96.2 action:  [0.0079, -0.9999] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  23 ff for 102.49999999999845 s: -------------------> 0.22\n",
      "Total reward for the episode:  1840.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  327\n",
      "6.6 action:  [-0.0006, -0.9886] n_targets:  1 reward:  80\n",
      "9.3 action:  [0.0055, -0.9983] n_targets:  1 reward:  80\n",
      "10.7 action:  [-0.0012, -0.9964] n_targets:  2 reward:  160\n",
      "12.2 action:  [0.0031, -0.9908] n_targets:  1 reward:  80\n",
      "22.6 action:  [0.004, -0.9967] n_targets:  1 reward:  80\n",
      "27.9 action:  [-0.0092, -0.9999] n_targets:  1 reward:  80\n",
      "32.9 action:  [-0.0083, -0.9989] n_targets:  1 reward:  80\n",
      "47.7 action:  [0.0064, -0.9974] n_targets:  1 reward:  80\n",
      "51.2 action:  [0.0022, -0.9912] n_targets:  1 reward:  80\n",
      "55.5 action:  [-0.0038, -0.9987] n_targets:  1 reward:  80\n",
      "61.9 action:  [-0.0048, -0.9993] n_targets:  1 reward:  80\n",
      "64.2 action:  [0.0046, -0.9964] n_targets:  1 reward:  80\n",
      "66.4 action:  [0.0094, -0.9971] n_targets:  1 reward:  80\n",
      "67.9 action:  [0.0037, -0.9887] n_targets:  1 reward:  80\n",
      "68.7 action:  [0.006, -0.9932] n_targets:  1 reward:  80\n",
      "73.0 action:  [-0.0085, -0.9892] n_targets:  1 reward:  80\n",
      "75.4 action:  [0.0031, -0.9966] n_targets:  1 reward:  80\n",
      "76.5 action:  [-0.0087, -0.9889] n_targets:  1 reward:  80\n",
      "80.3 action:  [0.002, -0.9943] n_targets:  1 reward:  80\n",
      "80.8 action:  [0.0048, -0.9927] n_targets:  2 reward:  160\n",
      "85.0 action:  [0.0057, -0.9952] n_targets:  1 reward:  80\n",
      "87.9 action:  [0.0097, -0.9968] n_targets:  1 reward:  80\n",
      "89.4 action:  [-0.0021, -0.999] n_targets:  1 reward:  80\n",
      "93.1 action:  [-0.0016, -0.9977] n_targets:  1 reward:  80\n",
      "94.0 action:  [-0.0081, -0.9934] n_targets:  1 reward:  80\n",
      "96.7 action:  [0.0012, -0.9999] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  28 ff for 102.49999999999845 s: -------------------> 0.27\n",
      "Total reward for the episode:  2240.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  328\n",
      "4.0 action:  [0.0086, -0.981] n_targets:  1 reward:  80\n",
      "6.0 action:  [-0.0073, -0.9966] n_targets:  1 reward:  80\n",
      "12.0 action:  [0.0073, -0.9979] n_targets:  1 reward:  80\n",
      "12.2 action:  [-0.0047, -0.9898] n_targets:  1 reward:  80\n",
      "15.0 action:  [0.0089, -0.9986] n_targets:  1 reward:  80\n",
      "18.8 action:  [-0.0016, -0.9804] n_targets:  1 reward:  80\n",
      "21.3 action:  [-0.0072, -0.998] n_targets:  1 reward:  80\n",
      "24.0 action:  [0.0078, -0.9978] n_targets:  1 reward:  80\n",
      "25.9 action:  [-0.0049, -0.9947] n_targets:  1 reward:  80\n",
      "28.2 action:  [0.0027, -0.9909] n_targets:  1 reward:  80\n",
      "29.3 action:  [-0.0075, -0.9924] n_targets:  1 reward:  80\n",
      "31.9 action:  [0.0006, -0.9978] n_targets:  1 reward:  80\n",
      "34.9 action:  [-0.0009, -0.997] n_targets:  1 reward:  80\n",
      "39.3 action:  [-0.0004, -0.9957] n_targets:  1 reward:  80\n",
      "44.7 action:  [0.0044, -0.9975] n_targets:  1 reward:  80\n",
      "52.7 action:  [0.0042, -0.9875] n_targets:  1 reward:  80\n",
      "57.9 action:  [0.0009, -0.9947] n_targets:  1 reward:  80\n",
      "62.9 action:  [0.0056, -0.9976] n_targets:  1 reward:  80\n",
      "65.1 action:  [-0.0066, -0.9993] n_targets:  1 reward:  80\n",
      "68.6 action:  [-0.0078, -0.9917] n_targets:  1 reward:  80\n",
      "73.6 action:  [-0.0081, -0.9904] n_targets:  1 reward:  80\n",
      "76.6 action:  [-0.0021, -0.9848] n_targets:  1 reward:  80\n",
      "79.1 action:  [0.0023, -0.9998] n_targets:  1 reward:  80\n",
      "80.6 action:  [0.0021, -0.9954] n_targets:  1 reward:  80\n",
      "81.4 action:  [0.0015, -0.9869] n_targets:  2 reward:  160\n",
      "84.0 action:  [0.0002, -0.9978] n_targets:  2 reward:  160\n",
      "90.4 action:  [-0.0076, -0.9915] n_targets:  1 reward:  80\n",
      "94.2 action:  [0.0096, -0.9952] n_targets:  1 reward:  80\n",
      "99.8 action:  [0.0066, -0.9997] n_targets:  1 reward:  80\n",
      "101.7 action:  [0.0024, -0.9922] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  32 ff for 102.49999999999845 s: -------------------> 0.31\n",
      "Total reward for the episode:  2560.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  329\n",
      "7.8 action:  [0.0009, -0.9987] n_targets:  1 reward:  80\n",
      "15.3 action:  [-0.0092, -0.9961] n_targets:  1 reward:  80\n",
      "20.0 action:  [-0.0001, -0.9944] n_targets:  1 reward:  80\n",
      "21.9 action:  [-0.0099, -0.9973] n_targets:  1 reward:  80\n",
      "25.3 action:  [-0.0027, -0.9988] n_targets:  1 reward:  80\n",
      "27.6 action:  [-0.0045, -0.9989] n_targets:  2 reward:  160\n",
      "32.0 action:  [0.0013, -0.9952] n_targets:  1 reward:  80\n",
      "33.7 action:  [0.0065, -0.9988] n_targets:  1 reward:  80\n",
      "44.6 action:  [-0.0006, -0.9968] n_targets:  1 reward:  80\n",
      "50.6 action:  [0.0067, -0.9989] n_targets:  1 reward:  80\n",
      "55.5 action:  [0.0091, -0.9984] n_targets:  1 reward:  80\n",
      "58.4 action:  [0.0059, -0.999] n_targets:  2 reward:  160\n",
      "59.3 action:  [0.0071, -0.9986] n_targets:  1 reward:  80\n",
      "60.5 action:  [0.0051, -0.9989] n_targets:  1 reward:  80\n",
      "62.9 action:  [-0.0067, -0.9962] n_targets:  1 reward:  80\n",
      "66.4 action:  [-0.008, -0.996] n_targets:  1 reward:  80\n",
      "71.8 action:  [-0.0047, -0.9931] n_targets:  1 reward:  80\n",
      "75.1 action:  [-0.0048, -0.994] n_targets:  1 reward:  80\n",
      "77.9 action:  [0.0021, -0.9882] n_targets:  1 reward:  80\n",
      "85.9 action:  [0.0022, -0.9956] n_targets:  2 reward:  160\n",
      "87.8 action:  [-0.0048, -0.9946] n_targets:  1 reward:  80\n",
      "93.6 action:  [-0.001, -0.9983] n_targets:  1 reward:  80\n",
      "98.3 action:  [-0.0095, -0.9987] n_targets:  1 reward:  80\n",
      "100.8 action:  [-0.0079, -0.9896] n_targets:  2 reward:  160\n",
      "Firely capture rate for the episode:  28 ff for 102.49999999999845 s: -------------------> 0.27\n",
      "Total reward for the episode:  2240.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  330\n",
      "3.1 action:  [-0.0095, -0.9954] n_targets:  1 reward:  80\n",
      "7.5 action:  [-0.0047, -0.993] n_targets:  1 reward:  80\n",
      "9.9 action:  [-0.0079, -0.9991] n_targets:  2 reward:  160\n",
      "12.4 action:  [0.0016, -0.9986] n_targets:  1 reward:  80\n",
      "16.4 action:  [-0.0011, -0.9987] n_targets:  1 reward:  80\n",
      "23.8 action:  [0.0068, -0.9983] n_targets:  1 reward:  80\n",
      "25.1 action:  [0.0067, -0.9991] n_targets:  1 reward:  80\n",
      "28.0 action:  [0.0049, -0.9982] n_targets:  2 reward:  160\n",
      "29.7 action:  [0.0023, -0.9979] n_targets:  1 reward:  80\n",
      "34.1 action:  [0.0063, -0.9903] n_targets:  1 reward:  80\n",
      "44.9 action:  [-0.0073, -0.999] n_targets:  1 reward:  80\n",
      "45.9 action:  [-0.0022, -0.9844] n_targets:  1 reward:  80\n",
      "48.5 action:  [-0.0079, -0.997] n_targets:  1 reward:  80\n",
      "59.6 action:  [-0.0042, -0.9929] n_targets:  1 reward:  80\n",
      "64.4 action:  [-0.0057, -0.9905] n_targets:  2 reward:  160\n",
      "67.4 action:  [0.0062, -0.9986] n_targets:  1 reward:  80\n",
      "69.0 action:  [-0.0018, -0.9933] n_targets:  1 reward:  80\n",
      "84.2 action:  [0.0073, -0.9987] n_targets:  1 reward:  80\n",
      "90.0 action:  [0.0037, -0.9986] n_targets:  1 reward:  80\n",
      "90.7 action:  [0.0041, -0.9886] n_targets:  1 reward:  80\n",
      "92.0 action:  [-0.0021, -0.9982] n_targets:  1 reward:  80\n",
      "94.4 action:  [0.001, -0.9952] n_targets:  1 reward:  80\n",
      "98.9 action:  [0.0088, -0.9999] n_targets:  1 reward:  80\n",
      "99.7 action:  [0.0038, -0.9987] n_targets:  1 reward:  80\n",
      "101.3 action:  [0.0082, -0.9916] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  28 ff for 102.49999999999845 s: -------------------> 0.27\n",
      "Total reward for the episode:  2240.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  331\n",
      "7.2 action:  [0.0069, -0.9961] n_targets:  1 reward:  80\n",
      "8.8 action:  [-0.0027, -0.9996] n_targets:  1 reward:  80\n",
      "17.9 action:  [0.0033, -0.9879] n_targets:  1 reward:  80\n",
      "19.3 action:  [0.001, -0.998] n_targets:  1 reward:  80\n",
      "19.9 action:  [0.0087, -0.998] n_targets:  1 reward:  80\n",
      "20.3 action:  [0.0033, -0.9952] n_targets:  1 reward:  80\n",
      "23.0 action:  [-0.0095, -0.997] n_targets:  1 reward:  80\n",
      "25.0 action:  [-0.0048, -0.9986] n_targets:  1 reward:  80\n",
      "25.7 action:  [-0.0035, -0.9968] n_targets:  1 reward:  80\n",
      "31.0 action:  [-0.0043, -0.995] n_targets:  1 reward:  80\n",
      "35.3 action:  [-0.0084, -0.9941] n_targets:  3 reward:  240\n",
      "38.0 action:  [-0.0047, -0.9992] n_targets:  1 reward:  80\n",
      "41.1 action:  [0.0064, -0.9919] n_targets:  1 reward:  80\n",
      "44.0 action:  [0.001, -0.9971] n_targets:  1 reward:  80\n",
      "56.5 action:  [0.0097, -0.999] n_targets:  1 reward:  80\n",
      "63.6 action:  [0.0003, -0.9987] n_targets:  1 reward:  80\n",
      "66.8 action:  [-0.0093, -0.9959] n_targets:  1 reward:  80\n",
      "73.2 action:  [0.0058, -0.9977] n_targets:  1 reward:  80\n",
      "83.8 action:  [-0.0042, -0.9936] n_targets:  1 reward:  80\n",
      "85.7 action:  [0.002, -0.9891] n_targets:  1 reward:  80\n",
      "87.9 action:  [-0.003, -0.9993] n_targets:  1 reward:  80\n",
      "91.2 action:  [0.0082, -0.9911] n_targets:  1 reward:  80\n",
      "95.7 action:  [-0.0093, -0.9955] n_targets:  1 reward:  80\n",
      "96.8 action:  [-0.0014, -0.9885] n_targets:  1 reward:  80\n",
      "100.9 action:  [0.0016, -0.9976] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  27 ff for 102.49999999999845 s: -------------------> 0.26\n",
      "Total reward for the episode:  2160.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  332\n",
      "3.1 action:  [0.0077, -0.9991] n_targets:  2 reward:  160\n",
      "4.2 action:  [-0.0058, -0.9995] n_targets:  1 reward:  80\n",
      "29.9 action:  [0.0088, -0.9851] n_targets:  1 reward:  80\n",
      "38.2 action:  [-0.0016, -0.9946] n_targets:  1 reward:  80\n",
      "39.6 action:  [0.0016, -0.9973] n_targets:  1 reward:  80\n",
      "48.5 action:  [-0.0094, -0.9931] n_targets:  1 reward:  80\n",
      "50.1 action:  [-0.0007, -0.9974] n_targets:  1 reward:  80\n",
      "53.4 action:  [0.0035, -0.9903] n_targets:  1 reward:  80\n",
      "58.4 action:  [0.0084, -0.9953] n_targets:  1 reward:  80\n",
      "62.5 action:  [0.0027, -0.9892] n_targets:  2 reward:  160\n",
      "66.2 action:  [0.0088, -0.9998] n_targets:  1 reward:  80\n",
      "71.2 action:  [-0.0024, -0.9998] n_targets:  1 reward:  80\n",
      "72.5 action:  [-0.0086, -0.9987] n_targets:  1 reward:  80\n",
      "75.4 action:  [-0.0088, -0.9985] n_targets:  3 reward:  240\n",
      "78.9 action:  [-0.0002, -0.9921] n_targets:  1 reward:  80\n",
      "85.2 action:  [0.0098, -0.9998] n_targets:  1 reward:  80\n",
      "94.3 action:  [-0.0045, -0.9963] n_targets:  1 reward:  80\n",
      "98.6 action:  [-0.0059, -0.9932] n_targets:  1 reward:  80\n",
      "100.1 action:  [0.0014, -0.984] n_targets:  2 reward:  160\n",
      "Firely capture rate for the episode:  24 ff for 102.49999999999845 s: -------------------> 0.23\n",
      "Total reward for the episode:  1920.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  333\n",
      "5.9 action:  [-0.0022, -0.9966] n_targets:  2 reward:  160\n",
      "9.6 action:  [-0.0051, -0.9933] n_targets:  1 reward:  80\n",
      "19.4 action:  [-0.009, -0.9951] n_targets:  1 reward:  80\n",
      "23.9 action:  [-0.0066, -0.9801] n_targets:  1 reward:  80\n",
      "25.1 action:  [-0.0045, -0.999] n_targets:  1 reward:  80\n",
      "26.8 action:  [0.0054, -0.99] n_targets:  2 reward:  160\n",
      "30.0 action:  [-0.0056, -0.9895] n_targets:  1 reward:  80\n",
      "31.7 action:  [-0.0, -0.9963] n_targets:  1 reward:  80\n",
      "32.3 action:  [-0.005, -0.9926] n_targets:  1 reward:  80\n",
      "36.8 action:  [-0.0097, -0.999] n_targets:  1 reward:  80\n",
      "38.1 action:  [-0.0001, -0.9985] n_targets:  1 reward:  80\n",
      "39.5 action:  [0.0091, -0.983] n_targets:  1 reward:  80\n",
      "40.2 action:  [-0.0064, -0.9985] n_targets:  1 reward:  80\n",
      "45.2 action:  [0.0042, -0.9965] n_targets:  1 reward:  80\n",
      "49.8 action:  [0.0051, -0.9982] n_targets:  1 reward:  80\n",
      "53.7 action:  [0.0037, -0.9893] n_targets:  1 reward:  80\n",
      "56.7 action:  [-0.01, -0.9995] n_targets:  1 reward:  80\n",
      "58.8 action:  [-0.0024, -0.9989] n_targets:  1 reward:  80\n",
      "70.7 action:  [0.0011, -0.9881] n_targets:  1 reward:  80\n",
      "76.2 action:  [-0.0013, -0.9967] n_targets:  1 reward:  80\n",
      "TIME before resetting: 77.49999999999987\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  334\n",
      "6.7 action:  [-0.0087, -0.9908] n_targets:  1 reward:  80\n",
      "9.3 action:  [-0.0082, -0.992] n_targets:  1 reward:  80\n",
      "52.3 action:  [-0.0053, -0.9974] n_targets:  2 reward:  160\n",
      "54.8 action:  [-0.0039, -0.9891] n_targets:  2 reward:  160\n",
      "65.2 action:  [0.006, -0.9936] n_targets:  1 reward:  80\n",
      "93.9 action:  [-0.0076, -0.9816] n_targets:  1 reward:  80\n",
      "97.6 action:  [-0.008, -0.9918] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  9 ff for 102.49999999999845 s: -------------------> 0.09\n",
      "Total reward for the episode:  720.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  335\n",
      "12.9 action:  [-0.0098, -0.986] n_targets:  1 reward:  80\n",
      "26.3 action:  [0.0029, -0.9839] n_targets:  1 reward:  80\n",
      "38.0 action:  [-0.0057, -0.9868] n_targets:  1 reward:  80\n",
      "73.9 action:  [-0.0035, -0.9873] n_targets:  1 reward:  80\n",
      "86.3 action:  [-0.0082, -0.9837] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  5 ff for 102.49999999999845 s: -------------------> 0.05\n",
      "Total reward for the episode:  400.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  336\n",
      "5.6 action:  [-0.01, -0.9939] n_targets:  1 reward:  80\n",
      "12.8 action:  [0.0042, -0.9933] n_targets:  1 reward:  80\n",
      "22.4 action:  [-0.008, -0.9841] n_targets:  1 reward:  80\n",
      "39.3 action:  [-0.0012, -0.999] n_targets:  1 reward:  80\n",
      "53.3 action:  [-0.0099, -0.9942] n_targets:  1 reward:  80\n",
      "62.9 action:  [-0.0099, -0.9867] n_targets:  1 reward:  80\n",
      "75.0 action:  [0.0057, -0.9983] n_targets:  1 reward:  80\n",
      "80.5 action:  [-0.008, -0.9848] n_targets:  1 reward:  80\n",
      "101.7 action:  [0.0074, -0.992] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  9 ff for 102.49999999999845 s: -------------------> 0.09\n",
      "Total reward for the episode:  720.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  337\n",
      "Eval num_timesteps=110000, episode_reward=613.33 +/- 150.85\n",
      "Episode length: 1025.00 +/- 0.00\n",
      "8.2 action:  [-0.009, -0.9986] n_targets:  1 reward:  80\n",
      "16.7 action:  [-0.008, -0.9932] n_targets:  2 reward:  160\n",
      "18.5 action:  [0.0077, -0.9963] n_targets:  1 reward:  80\n",
      "20.0 action:  [-0.0045, -0.9958] n_targets:  2 reward:  160\n",
      "22.6 action:  [0.0019, -0.9992] n_targets:  1 reward:  80\n",
      "26.8 action:  [0.0021, -0.9953] n_targets:  1 reward:  80\n",
      "32.0 action:  [-0.0008, -0.9957] n_targets:  1 reward:  80\n",
      "36.1 action:  [0.0073, -0.9982] n_targets:  1 reward:  80\n",
      "48.0 action:  [0.0033, -0.9993] n_targets:  1 reward:  80\n",
      "50.8 action:  [0.0062, -0.9956] n_targets:  2 reward:  160\n",
      "52.6 action:  [0.0076, -0.9997] n_targets:  1 reward:  80\n",
      "56.2 action:  [0.0072, -0.9945] n_targets:  1 reward:  80\n",
      "58.4 action:  [-0.0074, -0.9992] n_targets:  1 reward:  80\n",
      "71.3 action:  [0.0052, -0.9992] n_targets:  1 reward:  80\n",
      "74.4 action:  [-0.0049, -0.9978] n_targets:  2 reward:  160\n",
      "90.6 action:  [-0.0077, -0.9962] n_targets:  1 reward:  80\n",
      "93.4 action:  [0.0075, -0.9879] n_targets:  1 reward:  80\n",
      "95.2 action:  [-0.0064, -0.9904] n_targets:  2 reward:  160\n",
      "101.7 action:  [-0.01, -0.9909] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  24 ff for 102.49999999999845 s: -------------------> 0.23\n",
      "Total reward for the episode:  1920.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  338\n",
      "4.0 action:  [0.0011, -0.995] n_targets:  1 reward:  80\n",
      "7.1 action:  [-0.0048, -0.9991] n_targets:  1 reward:  80\n",
      "8.8 action:  [0.0029, -0.9992] n_targets:  1 reward:  80\n",
      "9.4 action:  [0.0023, -0.9805] n_targets:  1 reward:  80\n",
      "10.1 action:  [0.0095, -0.9979] n_targets:  1 reward:  80\n",
      "13.9 action:  [0.0094, -0.997] n_targets:  1 reward:  80\n",
      "15.5 action:  [0.0093, -0.9983] n_targets:  1 reward:  80\n",
      "19.3 action:  [-0.0028, -0.988] n_targets:  1 reward:  80\n",
      "24.3 action:  [0.0015, -0.9912] n_targets:  1 reward:  80\n",
      "41.5 action:  [-0.0031, -0.9966] n_targets:  2 reward:  160\n",
      "42.1 action:  [0.0047, -0.9996] n_targets:  1 reward:  80\n",
      "44.1 action:  [-0.0091, -0.9998] n_targets:  2 reward:  160\n",
      "51.5 action:  [-0.005, -0.9979] n_targets:  1 reward:  80\n",
      "60.5 action:  [0.0065, -0.9977] n_targets:  1 reward:  80\n",
      "62.4 action:  [0.0028, -0.9898] n_targets:  1 reward:  80\n",
      "66.2 action:  [0.0027, -0.9874] n_targets:  1 reward:  80\n",
      "78.1 action:  [0.0045, -0.9994] n_targets:  1 reward:  80\n",
      "81.0 action:  [0.0058, -0.9838] n_targets:  1 reward:  80\n",
      "86.7 action:  [-0.0066, -0.9935] n_targets:  2 reward:  160\n",
      "88.1 action:  [-0.0006, -0.9839] n_targets:  1 reward:  80\n",
      "90.9 action:  [-0.0036, -0.9947] n_targets:  1 reward:  80\n",
      "93.3 action:  [-0.0005, -0.9994] n_targets:  1 reward:  80\n",
      "95.3 action:  [0.0037, -0.995] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  26 ff for 102.49999999999845 s: -------------------> 0.25\n",
      "Total reward for the episode:  2080.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  339\n",
      "5.5 action:  [-0.0069, -0.9983] n_targets:  2 reward:  160\n",
      "8.1 action:  [-0.0085, -0.9984] n_targets:  1 reward:  80\n",
      "13.4 action:  [0.0034, -0.9962] n_targets:  1 reward:  80\n",
      "23.3 action:  [-0.0059, -0.9995] n_targets:  1 reward:  80\n",
      "27.3 action:  [0.0092, -0.9952] n_targets:  1 reward:  80\n",
      "41.5 action:  [-0.0087, -0.9979] n_targets:  2 reward:  160\n",
      "43.8 action:  [0.0032, -0.9885] n_targets:  1 reward:  80\n",
      "49.7 action:  [-0.0, -0.9902] n_targets:  1 reward:  80\n",
      "55.1 action:  [-0.0027, -0.9923] n_targets:  1 reward:  80\n",
      "70.4 action:  [0.009, -0.9969] n_targets:  2 reward:  160\n",
      "82.1 action:  [-0.0086, -0.9986] n_targets:  1 reward:  80\n",
      "89.0 action:  [-0.0045, -0.9936] n_targets:  1 reward:  80\n",
      "95.7 action:  [-0.0063, -0.9918] n_targets:  1 reward:  80\n",
      "99.0 action:  [-0.0088, -0.9923] n_targets:  1 reward:  80\n",
      "99.8 action:  [0.009, -0.9918] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  18 ff for 102.49999999999845 s: -------------------> 0.18\n",
      "Total reward for the episode:  1440.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  340\n",
      "1.9 action:  [0.0056, -0.9974] n_targets:  1 reward:  80\n",
      "9.2 action:  [-0.0077, -0.9997] n_targets:  1 reward:  80\n",
      "15.4 action:  [0.0014, -0.993] n_targets:  1 reward:  80\n",
      "17.8 action:  [0.008, -0.9988] n_targets:  1 reward:  80\n",
      "22.4 action:  [-0.0015, -0.9998] n_targets:  1 reward:  80\n",
      "23.7 action:  [-0.009, -0.9972] n_targets:  1 reward:  80\n",
      "28.3 action:  [0.0042, -0.9996] n_targets:  1 reward:  80\n",
      "30.6 action:  [-0.0071, -0.9987] n_targets:  2 reward:  160\n",
      "33.5 action:  [-0.0005, -0.9989] n_targets:  1 reward:  80\n",
      "38.3 action:  [0.0056, -0.9958] n_targets:  1 reward:  80\n",
      "45.6 action:  [-0.0054, -0.9994] n_targets:  1 reward:  80\n",
      "47.4 action:  [0.0015, -0.997] n_targets:  1 reward:  80\n",
      "50.4 action:  [-0.002, -0.9978] n_targets:  1 reward:  80\n",
      "53.7 action:  [0.0036, -0.9993] n_targets:  1 reward:  80\n",
      "56.0 action:  [0.0067, -0.9975] n_targets:  1 reward:  80\n",
      "64.5 action:  [0.0074, -0.9998] n_targets:  1 reward:  80\n",
      "75.2 action:  [0.0044, -0.9873] n_targets:  1 reward:  80\n",
      "81.4 action:  [-0.0087, -0.9837] n_targets:  1 reward:  80\n",
      "82.4 action:  [-0.0025, -0.9929] n_targets:  1 reward:  80\n",
      "85.5 action:  [-0.0099, -0.9987] n_targets:  1 reward:  80\n",
      "94.5 action:  [-0.0068, -0.9998] n_targets:  2 reward:  160\n",
      "96.0 action:  [0.0096, -0.9806] n_targets:  1 reward:  80\n",
      "97.4 action:  [0.0042, -0.9979] n_targets:  1 reward:  80\n",
      "99.1 action:  [0.0015, -0.9849] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  26 ff for 102.49999999999845 s: -------------------> 0.25\n",
      "Total reward for the episode:  2080.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  341\n",
      "6.2 action:  [-0.0046, -0.996] n_targets:  2 reward:  160\n",
      "17.4 action:  [-0.0078, -0.9935] n_targets:  1 reward:  80\n",
      "21.3 action:  [-0.0041, -0.9941] n_targets:  1 reward:  80\n",
      "27.9 action:  [-0.0092, -0.9915] n_targets:  1 reward:  80\n",
      "30.6 action:  [-0.007, -0.9958] n_targets:  1 reward:  80\n",
      "33.1 action:  [-0.0036, -0.999] n_targets:  2 reward:  160\n",
      "35.7 action:  [0.0053, -0.9826] n_targets:  3 reward:  240\n",
      "40.6 action:  [-0.0047, -0.9961] n_targets:  1 reward:  80\n",
      "49.6 action:  [0.0061, -0.997] n_targets:  1 reward:  80\n",
      "55.8 action:  [0.0062, -0.9888] n_targets:  1 reward:  80\n",
      "57.7 action:  [0.0029, -0.981] n_targets:  2 reward:  160\n",
      "64.0 action:  [0.0095, -0.9979] n_targets:  1 reward:  80\n",
      "65.3 action:  [-0.0053, -0.9999] n_targets:  1 reward:  80\n",
      "70.0 action:  [-0.0024, -0.999] n_targets:  1 reward:  80\n",
      "73.1 action:  [-0.0012, -0.9995] n_targets:  2 reward:  160\n",
      "75.7 action:  [0.0068, -0.9999] n_targets:  1 reward:  80\n",
      "80.1 action:  [0.0037, -0.9865] n_targets:  1 reward:  80\n",
      "81.8 action:  [0.0056, -0.9997] n_targets:  1 reward:  80\n",
      "84.4 action:  [-0.0067, -0.9983] n_targets:  1 reward:  80\n",
      "91.3 action:  [0.0001, -0.9817] n_targets:  1 reward:  80\n",
      "102.4 action:  [0.0072, -0.9978] n_targets:  2 reward:  160\n",
      "Firely capture rate for the episode:  28 ff for 102.49999999999845 s: -------------------> 0.27\n",
      "Total reward for the episode:  2240.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  342\n",
      "3.9 action:  [-0.0044, -0.996] n_targets:  1 reward:  80\n",
      "9.4 action:  [-0.0068, -0.9987] n_targets:  1 reward:  80\n",
      "10.4 action:  [-0.0067, -0.9978] n_targets:  1 reward:  80\n",
      "12.7 action:  [0.0042, -0.9894] n_targets:  1 reward:  80\n",
      "15.9 action:  [0.0068, -0.9963] n_targets:  1 reward:  80\n",
      "26.5 action:  [0.0024, -0.9837] n_targets:  1 reward:  80\n",
      "29.1 action:  [0.0002, -0.9898] n_targets:  1 reward:  80\n",
      "32.3 action:  [-0.0001, -0.9995] n_targets:  1 reward:  80\n",
      "35.1 action:  [0.0091, -0.9962] n_targets:  1 reward:  80\n",
      "37.2 action:  [-0.0015, -0.9926] n_targets:  1 reward:  80\n",
      "51.3 action:  [-0.0075, -0.9989] n_targets:  1 reward:  80\n",
      "54.6 action:  [-0.0006, -0.9975] n_targets:  2 reward:  160\n",
      "56.7 action:  [-0.0008, -0.9804] n_targets:  1 reward:  80\n",
      "62.2 action:  [-0.0058, -0.9999] n_targets:  1 reward:  80\n",
      "66.5 action:  [-0.0011, -0.9961] n_targets:  1 reward:  80\n",
      "70.8 action:  [-0.0076, -0.9989] n_targets:  1 reward:  80\n",
      "79.4 action:  [0.0087, -0.9883] n_targets:  1 reward:  80\n",
      "81.8 action:  [-0.0058, -0.9962] n_targets:  1 reward:  80\n",
      "87.9 action:  [0.0083, -0.9968] n_targets:  1 reward:  80\n",
      "93.9 action:  [-0.0051, -0.9954] n_targets:  1 reward:  80\n",
      "96.7 action:  [0.0063, -0.9973] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  22 ff for 102.49999999999845 s: -------------------> 0.21\n",
      "Total reward for the episode:  1760.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  343\n",
      "1.9 action:  [0.0056, -0.991] n_targets:  1 reward:  80\n",
      "5.0 action:  [-0.0089, -0.9943] n_targets:  1 reward:  80\n",
      "7.0 action:  [-0.0046, -0.9924] n_targets:  1 reward:  80\n",
      "11.0 action:  [0.0029, -0.9998] n_targets:  1 reward:  80\n",
      "14.8 action:  [-0.0013, -0.9819] n_targets:  1 reward:  80\n",
      "21.0 action:  [0.0058, -0.9947] n_targets:  1 reward:  80\n",
      "23.3 action:  [-0.0065, -0.9944] n_targets:  1 reward:  80\n",
      "29.0 action:  [-0.0076, -0.9927] n_targets:  1 reward:  80\n",
      "31.4 action:  [-0.006, -0.9988] n_targets:  1 reward:  80\n",
      "33.5 action:  [-0.009, -0.999] n_targets:  1 reward:  80\n",
      "37.1 action:  [-0.0024, -0.9956] n_targets:  1 reward:  80\n",
      "44.1 action:  [-0.0013, -0.9937] n_targets:  2 reward:  160\n",
      "47.6 action:  [-0.0083, -0.9984] n_targets:  1 reward:  80\n",
      "49.2 action:  [-0.008, -0.9852] n_targets:  1 reward:  80\n",
      "59.7 action:  [-0.0035, -0.9999] n_targets:  1 reward:  80\n",
      "63.7 action:  [-0.0085, -0.999] n_targets:  1 reward:  80\n",
      "67.7 action:  [0.0094, -0.9858] n_targets:  1 reward:  80\n",
      "76.6 action:  [-0.0053, -0.9929] n_targets:  1 reward:  80\n",
      "80.3 action:  [0.0077, -0.9994] n_targets:  1 reward:  80\n",
      "84.8 action:  [-0.007, -0.9965] n_targets:  3 reward:  240\n",
      "87.5 action:  [0.0098, -0.997] n_targets:  1 reward:  80\n",
      "90.4 action:  [0.0051, -0.9991] n_targets:  1 reward:  80\n",
      "93.5 action:  [0.0018, -0.9884] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  26 ff for 102.49999999999845 s: -------------------> 0.25\n",
      "Total reward for the episode:  2080.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  344\n",
      "2.0 action:  [0.0046, -0.9922] n_targets:  1 reward:  80\n",
      "4.2 action:  [-0.0078, -0.9999] n_targets:  1 reward:  80\n",
      "7.2 action:  [-0.0054, -0.9994] n_targets:  2 reward:  160\n",
      "9.5 action:  [-0.0013, -0.9952] n_targets:  1 reward:  80\n",
      "12.4 action:  [-0.0059, -0.9949] n_targets:  1 reward:  80\n",
      "14.4 action:  [0.0041, -0.9801] n_targets:  1 reward:  80\n",
      "19.9 action:  [-0.0092, -0.9991] n_targets:  1 reward:  80\n",
      "23.3 action:  [0.0087, -0.9833] n_targets:  1 reward:  80\n",
      "30.8 action:  [0.0087, -0.9986] n_targets:  1 reward:  80\n",
      "35.1 action:  [0.0029, -0.9994] n_targets:  1 reward:  80\n",
      "37.2 action:  [-0.0025, -0.9999] n_targets:  1 reward:  80\n",
      "44.3 action:  [-0.004, -0.9983] n_targets:  1 reward:  80\n",
      "50.4 action:  [0.0064, -0.9989] n_targets:  1 reward:  80\n",
      "58.9 action:  [-0.0033, -0.9986] n_targets:  1 reward:  80\n",
      "63.1 action:  [0.0017, -0.9882] n_targets:  1 reward:  80\n",
      "64.0 action:  [-0.008, -0.999] n_targets:  1 reward:  80\n",
      "67.5 action:  [0.0074, -0.9993] n_targets:  1 reward:  80\n",
      "69.7 action:  [0.0016, -0.9975] n_targets:  1 reward:  80\n",
      "76.5 action:  [-0.0008, -0.9974] n_targets:  1 reward:  80\n",
      "77.0 action:  [-0.0023, -0.9887] n_targets:  1 reward:  80\n",
      "79.7 action:  [-0.0033, -0.9963] n_targets:  1 reward:  80\n",
      "81.8 action:  [-0.0084, -0.9945] n_targets:  1 reward:  80\n",
      "83.6 action:  [-0.006, -0.9993] n_targets:  1 reward:  80\n",
      "85.7 action:  [0.0042, -0.9973] n_targets:  1 reward:  80\n",
      "96.4 action:  [0.0081, -0.9969] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  26 ff for 102.49999999999845 s: -------------------> 0.25\n",
      "Total reward for the episode:  2080.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  345\n",
      "4.9 action:  [-0.0067, -0.9968] n_targets:  1 reward:  80\n",
      "7.5 action:  [0.0052, -0.9875] n_targets:  2 reward:  160\n",
      "15.0 action:  [0.0005, -0.9925] n_targets:  1 reward:  80\n",
      "18.0 action:  [-0.0088, -0.998] n_targets:  1 reward:  80\n",
      "21.4 action:  [0.009, -0.9883] n_targets:  1 reward:  80\n",
      "24.7 action:  [-0.0006, -0.9985] n_targets:  1 reward:  80\n",
      "29.0 action:  [0.0088, -0.9902] n_targets:  2 reward:  160\n",
      "33.2 action:  [-0.0076, -0.9879] n_targets:  1 reward:  80\n",
      "37.8 action:  [-0.0029, -0.9969] n_targets:  1 reward:  80\n",
      "40.3 action:  [0.0014, -0.9997] n_targets:  1 reward:  80\n",
      "46.5 action:  [-0.0045, -0.9988] n_targets:  1 reward:  80\n",
      "50.8 action:  [0.0066, -0.9988] n_targets:  1 reward:  80\n",
      "55.0 action:  [-0.0027, -0.9996] n_targets:  1 reward:  80\n",
      "56.2 action:  [0.0092, -0.9997] n_targets:  1 reward:  80\n",
      "61.1 action:  [0.0028, -0.9936] n_targets:  1 reward:  80\n",
      "63.8 action:  [0.0073, -0.9998] n_targets:  1 reward:  80\n",
      "66.7 action:  [-0.0031, -0.9998] n_targets:  1 reward:  80\n",
      "70.3 action:  [0.008, -0.9817] n_targets:  1 reward:  80\n",
      "82.5 action:  [-0.0027, -0.9984] n_targets:  1 reward:  80\n",
      "85.5 action:  [-0.0032, -0.9965] n_targets:  2 reward:  160\n",
      "87.0 action:  [-0.0025, -0.9873] n_targets:  1 reward:  80\n",
      "88.1 action:  [-0.0069, -0.9948] n_targets:  1 reward:  80\n",
      "88.4 action:  [0.0037, -0.9957] n_targets:  1 reward:  80\n",
      "91.5 action:  [0.0006, -0.9992] n_targets:  1 reward:  80\n",
      "93.2 action:  [-0.0043, -0.9992] n_targets:  1 reward:  80\n",
      "94.1 action:  [0.0086, -0.9982] n_targets:  2 reward:  160\n",
      "96.0 action:  [0.0056, -0.9979] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  31 ff for 102.49999999999845 s: -------------------> 0.3\n",
      "Total reward for the episode:  2480.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  346\n",
      "3.5 action:  [-0.0045, -0.9963] n_targets:  1 reward:  80\n",
      "9.8 action:  [-0.0079, -0.9852] n_targets:  1 reward:  80\n",
      "14.4 action:  [0.0087, -0.9994] n_targets:  1 reward:  80\n",
      "16.8 action:  [-0.0014, -0.9912] n_targets:  1 reward:  80\n",
      "21.1 action:  [-0.0075, -0.9997] n_targets:  1 reward:  80\n",
      "24.4 action:  [0.002, -0.9884] n_targets:  1 reward:  80\n",
      "40.8 action:  [0.0052, -0.9869] n_targets:  1 reward:  80\n",
      "44.5 action:  [-0.0007, -0.9973] n_targets:  1 reward:  80\n",
      "53.4 action:  [-0.0062, -0.9924] n_targets:  1 reward:  80\n",
      "TIME before resetting: 77.49999999999987\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  347\n",
      "4.3 action:  [-0.0042, -0.9961] n_targets:  1 reward:  80\n",
      "7.8 action:  [-0.0076, -0.9832] n_targets:  1 reward:  80\n",
      "27.8 action:  [-0.01, -0.9965] n_targets:  1 reward:  80\n",
      "32.2 action:  [-0.0071, -0.9874] n_targets:  1 reward:  80\n",
      "44.6 action:  [-0.0085, -0.9882] n_targets:  1 reward:  80\n",
      "63.5 action:  [-0.0083, -0.9863] n_targets:  1 reward:  80\n",
      "69.2 action:  [-0.0063, -0.9925] n_targets:  1 reward:  80\n",
      "78.0 action:  [0.0013, -0.9971] n_targets:  1 reward:  80\n",
      "96.4 action:  [-0.0022, -0.9958] n_targets:  1 reward:  80\n",
      "97.5 action:  [-0.0031, -0.9892] n_targets:  2 reward:  160\n",
      "Firely capture rate for the episode:  11 ff for 102.49999999999845 s: -------------------> 0.11\n",
      "Total reward for the episode:  880.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  348\n",
      "14.4 action:  [0.0015, -0.9825] n_targets:  1 reward:  80\n",
      "18.0 action:  [-0.0084, -0.9953] n_targets:  1 reward:  80\n",
      "31.5 action:  [0.002, -0.9992] n_targets:  2 reward:  160\n",
      "40.0 action:  [-0.0089, -0.9809] n_targets:  2 reward:  160\n",
      "44.5 action:  [-0.0084, -0.986] n_targets:  1 reward:  80\n",
      "51.5 action:  [-0.0068, -0.9946] n_targets:  1 reward:  80\n",
      "56.2 action:  [-0.0091, -0.9922] n_targets:  1 reward:  80\n",
      "65.0 action:  [-0.0097, -0.9968] n_targets:  1 reward:  80\n",
      "72.1 action:  [-0.0079, -0.9874] n_targets:  1 reward:  80\n",
      "79.3 action:  [-0.0094, -0.9958] n_targets:  1 reward:  80\n",
      "84.5 action:  [-0.0097, -0.9939] n_targets:  2 reward:  160\n",
      "Firely capture rate for the episode:  14 ff for 102.49999999999845 s: -------------------> 0.14\n",
      "Total reward for the episode:  1120.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  349\n",
      "15.9 action:  [-0.0088, -0.9907] n_targets:  2 reward:  160\n",
      "20.6 action:  [-0.0093, -0.998] n_targets:  1 reward:  80\n",
      "23.9 action:  [0.001, -0.9882] n_targets:  1 reward:  80\n",
      "27.9 action:  [-0.0056, -0.9824] n_targets:  1 reward:  80\n",
      "33.7 action:  [-0.0085, -0.9811] n_targets:  1 reward:  80\n",
      "36.7 action:  [0.0082, -0.9975] n_targets:  1 reward:  80\n",
      "41.1 action:  [-0.0023, -0.9911] n_targets:  1 reward:  80\n",
      "50.3 action:  [-0.0096, -0.9962] n_targets:  1 reward:  80\n",
      "55.7 action:  [-0.0016, -0.995] n_targets:  2 reward:  160\n",
      "59.6 action:  [-0.0098, -0.9891] n_targets:  1 reward:  80\n",
      "78.8 action:  [-0.0048, -0.9817] n_targets:  1 reward:  80\n",
      "90.2 action:  [-0.0022, -0.9955] n_targets:  1 reward:  80\n",
      "95.2 action:  [-0.0089, -0.9836] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  15 ff for 102.49999999999845 s: -------------------> 0.15\n",
      "Total reward for the episode:  1200.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  350\n",
      "Eval num_timesteps=120000, episode_reward=1066.67 +/- 135.97\n",
      "Episode length: 1025.00 +/- 0.00\n",
      "6.3 action:  [0.003, -0.9975] n_targets:  1 reward:  80\n",
      "12.6 action:  [-0.0041, -0.9907] n_targets:  1 reward:  80\n",
      "15.9 action:  [-0.0035, -0.9963] n_targets:  1 reward:  80\n",
      "19.1 action:  [0.0064, -0.983] n_targets:  1 reward:  80\n",
      "21.6 action:  [-0.0083, -0.9913] n_targets:  1 reward:  80\n",
      "24.0 action:  [-0.004, -0.9941] n_targets:  1 reward:  80\n",
      "30.4 action:  [-0.0072, -0.9997] n_targets:  1 reward:  80\n",
      "31.9 action:  [-0.0085, -0.9818] n_targets:  1 reward:  80\n",
      "37.5 action:  [-0.0001, -0.9972] n_targets:  2 reward:  160\n",
      "46.4 action:  [0.0047, -0.999] n_targets:  1 reward:  80\n",
      "50.6 action:  [0.0033, -0.9884] n_targets:  1 reward:  80\n",
      "52.9 action:  [0.0096, -0.9999] n_targets:  2 reward:  160\n",
      "54.2 action:  [0.0099, -0.991] n_targets:  2 reward:  160\n",
      "55.3 action:  [0.0096, -0.9974] n_targets:  1 reward:  80\n",
      "57.2 action:  [0.0065, -0.998] n_targets:  1 reward:  80\n",
      "61.7 action:  [-0.0008, -0.9956] n_targets:  1 reward:  80\n",
      "62.5 action:  [-0.0083, -0.9876] n_targets:  1 reward:  80\n",
      "65.9 action:  [0.0076, -0.9988] n_targets:  1 reward:  80\n",
      "66.8 action:  [-0.0076, -0.9999] n_targets:  1 reward:  80\n",
      "70.2 action:  [-0.0015, -0.997] n_targets:  1 reward:  80\n",
      "72.9 action:  [0.0077, -0.9994] n_targets:  2 reward:  160\n",
      "78.3 action:  [0.0027, -0.9937] n_targets:  1 reward:  80\n",
      "81.7 action:  [-0.0026, -0.9981] n_targets:  1 reward:  80\n",
      "85.2 action:  [-0.0077, -0.9913] n_targets:  1 reward:  80\n",
      "89.5 action:  [0.0081, -0.9932] n_targets:  1 reward:  80\n",
      "91.8 action:  [0.0091, -0.9923] n_targets:  1 reward:  80\n",
      "101.3 action:  [-0.0072, -0.9902] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  31 ff for 102.49999999999845 s: -------------------> 0.3\n",
      "Total reward for the episode:  2480.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  351\n",
      "1.2 action:  [-0.0042, -0.9998] n_targets:  1 reward:  80\n",
      "15.9 action:  [-0.0059, -0.9898] n_targets:  1 reward:  80\n",
      "20.9 action:  [-0.0004, -0.9938] n_targets:  1 reward:  80\n",
      "22.5 action:  [0.0029, -0.9973] n_targets:  2 reward:  160\n",
      "25.0 action:  [-0.0025, -0.9947] n_targets:  1 reward:  80\n",
      "26.5 action:  [-0.0015, -0.9811] n_targets:  2 reward:  160\n",
      "28.4 action:  [0.0036, -0.999] n_targets:  1 reward:  80\n",
      "33.8 action:  [-0.0032, -0.999] n_targets:  1 reward:  80\n",
      "36.8 action:  [0.0059, -0.9908] n_targets:  1 reward:  80\n",
      "38.0 action:  [0.0041, -0.9848] n_targets:  1 reward:  80\n",
      "40.8 action:  [-0.009, -0.9919] n_targets:  1 reward:  80\n",
      "46.7 action:  [-0.0083, -0.9935] n_targets:  1 reward:  80\n",
      "48.2 action:  [0.0003, -0.9862] n_targets:  1 reward:  80\n",
      "50.8 action:  [0.0012, -0.9932] n_targets:  1 reward:  80\n",
      "56.7 action:  [-0.003, -0.9983] n_targets:  2 reward:  160\n",
      "60.5 action:  [0.0092, -0.9991] n_targets:  1 reward:  80\n",
      "62.6 action:  [-0.0046, -0.9989] n_targets:  1 reward:  80\n",
      "69.2 action:  [-0.0068, -0.9948] n_targets:  1 reward:  80\n",
      "91.3 action:  [0.0056, -0.9983] n_targets:  1 reward:  80\n",
      "95.7 action:  [0.0051, -0.9903] n_targets:  1 reward:  80\n",
      "97.0 action:  [-0.0068, -0.9979] n_targets:  1 reward:  80\n",
      "98.4 action:  [0.007, -0.9995] n_targets:  2 reward:  160\n",
      "102.0 action:  [0.0, -0.9892] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  27 ff for 102.49999999999845 s: -------------------> 0.26\n",
      "Total reward for the episode:  2160.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  352\n",
      "3.8 action:  [0.0064, -0.9991] n_targets:  1 reward:  80\n",
      "7.4 action:  [-0.0054, -0.9828] n_targets:  1 reward:  80\n",
      "10.5 action:  [0.0077, -0.9991] n_targets:  1 reward:  80\n",
      "18.1 action:  [-0.006, -0.9907] n_targets:  1 reward:  80\n",
      "20.6 action:  [-0.0095, -0.9895] n_targets:  1 reward:  80\n",
      "32.9 action:  [-0.0034, -0.9994] n_targets:  1 reward:  80\n",
      "35.6 action:  [-0.009, -0.9974] n_targets:  1 reward:  80\n",
      "42.7 action:  [-0.0037, -0.9948] n_targets:  1 reward:  80\n",
      "47.4 action:  [0.0095, -0.999] n_targets:  1 reward:  80\n",
      "49.1 action:  [-0.0034, -0.9994] n_targets:  1 reward:  80\n",
      "52.4 action:  [0.0086, -0.9985] n_targets:  1 reward:  80\n",
      "56.7 action:  [0.0013, -0.9996] n_targets:  1 reward:  80\n",
      "58.8 action:  [-0.0091, -0.9984] n_targets:  2 reward:  160\n",
      "59.9 action:  [-0.0065, -0.9973] n_targets:  1 reward:  80\n",
      "61.5 action:  [-0.0091, -0.9937] n_targets:  1 reward:  80\n",
      "62.5 action:  [0.0087, -0.9867] n_targets:  1 reward:  80\n",
      "68.1 action:  [-0.0051, -0.9968] n_targets:  1 reward:  80\n",
      "71.2 action:  [0.0035, -0.9984] n_targets:  1 reward:  80\n",
      "77.8 action:  [-0.0045, -0.9986] n_targets:  1 reward:  80\n",
      "82.7 action:  [0.0084, -0.9923] n_targets:  1 reward:  80\n",
      "87.5 action:  [0.0057, -0.9998] n_targets:  2 reward:  160\n",
      "92.1 action:  [-0.0049, -0.998] n_targets:  1 reward:  80\n",
      "95.0 action:  [0.0085, -0.9963] n_targets:  1 reward:  80\n",
      "98.5 action:  [-0.0084, -0.9992] n_targets:  1 reward:  80\n",
      "99.9 action:  [-0.0057, -0.9981] n_targets:  1 reward:  80\n",
      "101.8 action:  [0.0095, -0.9871] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  28 ff for 102.49999999999845 s: -------------------> 0.27\n",
      "Total reward for the episode:  2240.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  353\n",
      "5.2 action:  [-0.0052, -0.9873] n_targets:  1 reward:  80\n",
      "8.2 action:  [0.0079, -0.9927] n_targets:  1 reward:  80\n",
      "14.9 action:  [0.0021, -0.9998] n_targets:  1 reward:  80\n",
      "25.6 action:  [-0.0027, -0.9962] n_targets:  1 reward:  80\n",
      "33.8 action:  [0.0059, -0.9999] n_targets:  2 reward:  160\n",
      "38.3 action:  [0.0066, -0.9999] n_targets:  2 reward:  160\n",
      "43.4 action:  [0.0071, -0.9999] n_targets:  2 reward:  160\n",
      "45.1 action:  [0.0045, -0.9998] n_targets:  1 reward:  80\n",
      "49.8 action:  [0.007, -0.9829] n_targets:  1 reward:  80\n",
      "50.9 action:  [-0.006, -0.9922] n_targets:  1 reward:  80\n",
      "52.6 action:  [-0.0025, -0.9985] n_targets:  1 reward:  80\n",
      "54.1 action:  [0.0021, -0.9934] n_targets:  1 reward:  80\n",
      "55.6 action:  [0.0095, -0.9904] n_targets:  2 reward:  160\n",
      "60.4 action:  [0.0049, -0.9916] n_targets:  1 reward:  80\n",
      "61.6 action:  [0.007, -0.9997] n_targets:  1 reward:  80\n",
      "67.4 action:  [-0.0048, -0.9912] n_targets:  1 reward:  80\n",
      "70.0 action:  [-0.0061, -0.9987] n_targets:  2 reward:  160\n",
      "75.1 action:  [-0.0024, -0.9959] n_targets:  1 reward:  80\n",
      "76.1 action:  [0.0062, -0.9955] n_targets:  1 reward:  80\n",
      "78.5 action:  [-0.0038, -0.9958] n_targets:  1 reward:  80\n",
      "83.9 action:  [-0.0092, -0.993] n_targets:  2 reward:  160\n",
      "90.0 action:  [0.0079, -0.9874] n_targets:  1 reward:  80\n",
      "94.6 action:  [-0.0027, -0.9988] n_targets:  1 reward:  80\n",
      "97.7 action:  [-0.0072, -0.9993] n_targets:  1 reward:  80\n",
      "101.0 action:  [0.0, -0.9894] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  31 ff for 102.49999999999845 s: -------------------> 0.3\n",
      "Total reward for the episode:  2480.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  354\n",
      "5.0 action:  [-0.0097, -0.9895] n_targets:  1 reward:  80\n",
      "6.0 action:  [0.0085, -0.9902] n_targets:  1 reward:  80\n",
      "9.3 action:  [-0.0069, -0.9951] n_targets:  1 reward:  80\n",
      "9.7 action:  [-0.0008, -0.9928] n_targets:  2 reward:  160\n",
      "10.9 action:  [0.0022, -0.9993] n_targets:  1 reward:  80\n",
      "24.5 action:  [-0.0078, -0.9997] n_targets:  1 reward:  80\n",
      "28.0 action:  [0.0024, -0.9955] n_targets:  1 reward:  80\n",
      "34.7 action:  [0.0095, -0.9955] n_targets:  1 reward:  80\n",
      "36.8 action:  [-0.009, -0.9966] n_targets:  2 reward:  160\n",
      "38.9 action:  [-0.0009, -0.9839] n_targets:  1 reward:  80\n",
      "39.5 action:  [-0.0058, -0.9829] n_targets:  1 reward:  80\n",
      "40.5 action:  [0.0062, -0.9912] n_targets:  1 reward:  80\n",
      "45.2 action:  [-0.0073, -0.9936] n_targets:  1 reward:  80\n",
      "51.5 action:  [-0.0034, -0.9974] n_targets:  1 reward:  80\n",
      "53.8 action:  [-0.0002, -0.9967] n_targets:  1 reward:  80\n",
      "58.4 action:  [-0.0, -0.9995] n_targets:  1 reward:  80\n",
      "63.3 action:  [-0.0008, -0.9955] n_targets:  1 reward:  80\n",
      "66.1 action:  [-0.0014, -0.9893] n_targets:  1 reward:  80\n",
      "68.3 action:  [0.0055, -0.9859] n_targets:  1 reward:  80\n",
      "71.5 action:  [0.0042, -0.9838] n_targets:  1 reward:  80\n",
      "74.5 action:  [-0.0077, -0.9878] n_targets:  1 reward:  80\n",
      "77.8 action:  [-0.0032, -0.9946] n_targets:  1 reward:  80\n",
      "84.9 action:  [-0.0036, -0.9983] n_targets:  1 reward:  80\n",
      "86.3 action:  [0.0095, -0.9966] n_targets:  1 reward:  80\n",
      "90.5 action:  [0.009, -0.9909] n_targets:  1 reward:  80\n",
      "92.2 action:  [-0.0001, -0.9995] n_targets:  1 reward:  80\n",
      "97.4 action:  [0.0039, -0.9974] n_targets:  1 reward:  80\n",
      "99.9 action:  [0.0084, -0.9915] n_targets:  1 reward:  80\n",
      "102.2 action:  [-0.0034, -0.99] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  31 ff for 102.49999999999845 s: -------------------> 0.3\n",
      "Total reward for the episode:  2480.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  355\n",
      "5.0 action:  [0.0017, -0.9979] n_targets:  1 reward:  80\n",
      "7.0 action:  [0.0023, -0.9878] n_targets:  2 reward:  160\n",
      "10.0 action:  [-0.0011, -0.9976] n_targets:  1 reward:  80\n",
      "13.7 action:  [-0.0062, -0.981] n_targets:  1 reward:  80\n",
      "16.5 action:  [-0.0038, -0.9975] n_targets:  1 reward:  80\n",
      "19.1 action:  [0.0021, -0.9926] n_targets:  1 reward:  80\n",
      "23.2 action:  [0.0055, -0.9979] n_targets:  1 reward:  80\n",
      "29.9 action:  [0.0069, -0.9973] n_targets:  1 reward:  80\n",
      "38.2 action:  [-0.0024, -0.999] n_targets:  1 reward:  80\n",
      "38.7 action:  [0.0096, -0.9966] n_targets:  1 reward:  80\n",
      "39.5 action:  [-0.0002, -0.9953] n_targets:  1 reward:  80\n",
      "40.9 action:  [-0.0012, -0.9872] n_targets:  1 reward:  80\n",
      "42.8 action:  [-0.0037, -0.9946] n_targets:  1 reward:  80\n",
      "45.5 action:  [-0.0082, -0.982] n_targets:  1 reward:  80\n",
      "49.3 action:  [0.0041, -0.9895] n_targets:  1 reward:  80\n",
      "52.6 action:  [0.0092, -0.9979] n_targets:  1 reward:  80\n",
      "56.5 action:  [-0.0009, -0.9903] n_targets:  1 reward:  80\n",
      "58.7 action:  [-0.0073, -0.9858] n_targets:  1 reward:  80\n",
      "62.1 action:  [0.0072, -0.997] n_targets:  1 reward:  80\n",
      "69.5 action:  [0.0082, -0.9987] n_targets:  1 reward:  80\n",
      "72.7 action:  [0.0095, -0.9935] n_targets:  1 reward:  80\n",
      "75.4 action:  [0.008, -0.9978] n_targets:  1 reward:  80\n",
      "89.7 action:  [0.0016, -0.9971] n_targets:  1 reward:  80\n",
      "96.6 action:  [0.0025, -0.9929] n_targets:  1 reward:  80\n",
      "101.2 action:  [0.0092, -0.9832] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  26 ff for 102.49999999999845 s: -------------------> 0.25\n",
      "Total reward for the episode:  2080.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  356\n",
      "2.7 action:  [-0.0079, -0.9968] n_targets:  1 reward:  80\n",
      "5.1 action:  [-0.0099, -0.9993] n_targets:  2 reward:  160\n",
      "9.9 action:  [-0.0013, -0.9956] n_targets:  1 reward:  80\n",
      "11.3 action:  [-0.003, -0.9949] n_targets:  1 reward:  80\n",
      "17.6 action:  [-0.0009, -0.9974] n_targets:  1 reward:  80\n",
      "24.9 action:  [-0.0084, -0.9956] n_targets:  2 reward:  160\n",
      "32.1 action:  [0.0049, -0.9991] n_targets:  1 reward:  80\n",
      "37.8 action:  [-0.0019, -0.9944] n_targets:  1 reward:  80\n",
      "47.9 action:  [0.0026, -0.9913] n_targets:  1 reward:  80\n",
      "48.9 action:  [0.004, -0.9963] n_targets:  2 reward:  160\n",
      "50.2 action:  [-0.0048, -0.9824] n_targets:  1 reward:  80\n",
      "53.8 action:  [0.0058, -0.9995] n_targets:  1 reward:  80\n",
      "59.3 action:  [-0.0057, -0.9877] n_targets:  1 reward:  80\n",
      "63.6 action:  [0.0029, -0.9949] n_targets:  1 reward:  80\n",
      "65.7 action:  [0.0047, -0.9975] n_targets:  1 reward:  80\n",
      "68.4 action:  [0.0026, -0.9946] n_targets:  1 reward:  80\n",
      "73.0 action:  [-0.0034, -0.9963] n_targets:  1 reward:  80\n",
      "74.7 action:  [-0.0052, -0.9996] n_targets:  1 reward:  80\n",
      "77.8 action:  [-0.0038, -0.993] n_targets:  1 reward:  80\n",
      "80.7 action:  [-0.0044, -0.9956] n_targets:  1 reward:  80\n",
      "88.0 action:  [-0.0006, -0.9957] n_targets:  1 reward:  80\n",
      "91.5 action:  [0.0036, -0.9956] n_targets:  1 reward:  80\n",
      "94.8 action:  [-0.0004, -0.9996] n_targets:  1 reward:  80\n",
      "98.0 action:  [-0.0087, -0.9996] n_targets:  2 reward:  160\n",
      "100.1 action:  [-0.0008, -0.9909] n_targets:  1 reward:  80\n",
      "100.7 action:  [-0.0087, -0.9998] n_targets:  2 reward:  160\n",
      "Firely capture rate for the episode:  31 ff for 102.49999999999845 s: -------------------> 0.3\n",
      "Total reward for the episode:  2480.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  357\n",
      "3.7 action:  [-0.0088, -0.9952] n_targets:  2 reward:  160\n",
      "6.5 action:  [-0.0056, -0.9981] n_targets:  1 reward:  80\n",
      "9.6 action:  [-0.0029, -0.997] n_targets:  2 reward:  160\n",
      "28.4 action:  [0.0087, -0.9955] n_targets:  1 reward:  80\n",
      "32.4 action:  [-0.0003, -0.9982] n_targets:  1 reward:  80\n",
      "33.2 action:  [-0.0095, -0.9935] n_targets:  1 reward:  80\n",
      "42.4 action:  [0.0018, -0.9923] n_targets:  1 reward:  80\n",
      "46.1 action:  [0.0057, -0.9938] n_targets:  2 reward:  160\n",
      "47.7 action:  [-0.0018, -0.9988] n_targets:  1 reward:  80\n",
      "57.9 action:  [0.0066, -0.9992] n_targets:  1 reward:  80\n",
      "61.1 action:  [-0.0086, -0.989] n_targets:  1 reward:  80\n",
      "68.1 action:  [-0.007, -0.9892] n_targets:  1 reward:  80\n",
      "71.4 action:  [0.0085, -0.9984] n_targets:  2 reward:  160\n",
      "77.9 action:  [0.008, -0.9972] n_targets:  1 reward:  80\n",
      "81.0 action:  [-0.0075, -0.9833] n_targets:  1 reward:  80\n",
      "84.2 action:  [-0.0032, -0.9976] n_targets:  1 reward:  80\n",
      "90.3 action:  [0.004, -0.9999] n_targets:  1 reward:  80\n",
      "93.6 action:  [-0.006, -0.9975] n_targets:  1 reward:  80\n",
      "95.6 action:  [0.003, -0.9987] n_targets:  1 reward:  80\n",
      "101.7 action:  [0.0036, -0.997] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  24 ff for 102.49999999999845 s: -------------------> 0.23\n",
      "Total reward for the episode:  1920.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  358\n",
      "2.7 action:  [-0.0062, -0.9912] n_targets:  1 reward:  80\n",
      "4.5 action:  [0.0081, -0.9975] n_targets:  1 reward:  80\n",
      "5.9 action:  [0.0095, -0.9973] n_targets:  1 reward:  80\n",
      "10.2 action:  [-0.0068, -0.9904] n_targets:  1 reward:  80\n",
      "11.7 action:  [0.0004, -0.992] n_targets:  1 reward:  80\n",
      "13.5 action:  [0.0088, -0.9929] n_targets:  1 reward:  80\n",
      "16.1 action:  [0.0027, -0.9999] n_targets:  1 reward:  80\n",
      "17.4 action:  [-0.0013, -0.9901] n_targets:  1 reward:  80\n",
      "21.4 action:  [-0.0005, -0.9865] n_targets:  1 reward:  80\n",
      "24.1 action:  [-0.0033, -0.9987] n_targets:  1 reward:  80\n",
      "25.6 action:  [0.005, -0.9806] n_targets:  1 reward:  80\n",
      "27.8 action:  [-0.0057, -0.9996] n_targets:  1 reward:  80\n",
      "37.9 action:  [-0.0038, -0.9948] n_targets:  2 reward:  160\n",
      "46.9 action:  [-0.0025, -0.9869] n_targets:  1 reward:  80\n",
      "52.7 action:  [0.0024, -0.9924] n_targets:  1 reward:  80\n",
      "55.1 action:  [-0.0021, -0.9935] n_targets:  1 reward:  80\n",
      "61.3 action:  [-0.0045, -0.9986] n_targets:  1 reward:  80\n",
      "63.1 action:  [-0.0057, -0.9906] n_targets:  1 reward:  80\n",
      "64.4 action:  [-0.0051, -0.9849] n_targets:  1 reward:  80\n",
      "67.1 action:  [-0.0057, -0.9931] n_targets:  1 reward:  80\n",
      "67.9 action:  [-0.0012, -0.9958] n_targets:  1 reward:  80\n",
      "73.8 action:  [0.004, -0.9903] n_targets:  1 reward:  80\n",
      "80.6 action:  [-0.0031, -0.9964] n_targets:  1 reward:  80\n",
      "87.7 action:  [-0.0081, -0.9991] n_targets:  2 reward:  160\n",
      "99.4 action:  [0.0075, -0.9948] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  27 ff for 102.49999999999845 s: -------------------> 0.26\n",
      "Total reward for the episode:  2160.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  359\n",
      "4.3 action:  [-0.0029, -0.9852] n_targets:  1 reward:  80\n",
      "6.7 action:  [-0.0058, -0.9836] n_targets:  1 reward:  80\n",
      "21.4 action:  [-0.0011, -0.9991] n_targets:  1 reward:  80\n",
      "24.0 action:  [0.0084, -0.9979] n_targets:  2 reward:  160\n",
      "30.3 action:  [0.0039, -0.9967] n_targets:  1 reward:  80\n",
      "34.6 action:  [0.0052, -0.9946] n_targets:  1 reward:  80\n",
      "36.3 action:  [-0.0064, -0.9997] n_targets:  1 reward:  80\n",
      "38.5 action:  [0.0022, -0.994] n_targets:  1 reward:  80\n",
      "39.7 action:  [-0.0074, -0.9944] n_targets:  1 reward:  80\n",
      "42.7 action:  [-0.0041, -0.9982] n_targets:  1 reward:  80\n",
      "50.3 action:  [0.0004, -0.9878] n_targets:  1 reward:  80\n",
      "52.1 action:  [-0.0073, -0.9972] n_targets:  1 reward:  80\n",
      "54.4 action:  [-0.0009, -0.9804] n_targets:  1 reward:  80\n",
      "56.0 action:  [-0.0099, -0.9993] n_targets:  1 reward:  80\n",
      "66.6 action:  [-0.0011, -0.999] n_targets:  1 reward:  80\n",
      "69.9 action:  [0.0091, -0.9985] n_targets:  1 reward:  80\n",
      "72.9 action:  [0.0074, -0.9963] n_targets:  2 reward:  160\n",
      "77.5 action:  [0.0023, -0.9994] n_targets:  1 reward:  80\n",
      "TIME before resetting: 77.49999999999987\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  360\n",
      "2.6 action:  [0.0071, -0.9932] n_targets:  2 reward:  160\n",
      "5.9 action:  [-0.0019, -0.9805] n_targets:  2 reward:  160\n",
      "8.9 action:  [-0.0032, -0.9981] n_targets:  1 reward:  80\n",
      "10.3 action:  [0.0048, -0.9948] n_targets:  1 reward:  80\n",
      "14.6 action:  [-0.0084, -0.9948] n_targets:  1 reward:  80\n",
      "16.9 action:  [-0.0003, -0.9868] n_targets:  1 reward:  80\n",
      "18.7 action:  [-0.0034, -0.9995] n_targets:  1 reward:  80\n",
      "23.5 action:  [-0.009, -0.9987] n_targets:  1 reward:  80\n",
      "26.2 action:  [-0.0033, -0.9815] n_targets:  1 reward:  80\n",
      "28.2 action:  [-0.0061, -0.9984] n_targets:  1 reward:  80\n",
      "30.9 action:  [-0.0088, -0.9976] n_targets:  1 reward:  80\n",
      "43.4 action:  [-0.0059, -0.9964] n_targets:  1 reward:  80\n",
      "45.4 action:  [0.0052, -0.9811] n_targets:  1 reward:  80\n",
      "47.6 action:  [0.0041, -0.9832] n_targets:  1 reward:  80\n",
      "49.8 action:  [0.0051, -0.9988] n_targets:  1 reward:  80\n",
      "54.7 action:  [0.0058, -0.9952] n_targets:  1 reward:  80\n",
      "71.9 action:  [-0.003, -0.9981] n_targets:  1 reward:  80\n",
      "73.4 action:  [-0.0065, -0.9924] n_targets:  1 reward:  80\n",
      "76.6 action:  [-0.0071, -0.9966] n_targets:  1 reward:  80\n",
      "80.9 action:  [-0.0058, -0.9978] n_targets:  1 reward:  80\n",
      "84.9 action:  [-0.0036, -0.9875] n_targets:  1 reward:  80\n",
      "94.0 action:  [-0.01, -0.9967] n_targets:  1 reward:  80\n",
      "98.1 action:  [-0.0028, -0.9962] n_targets:  1 reward:  80\n",
      "101.0 action:  [0.0064, -0.9926] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  26 ff for 102.49999999999845 s: -------------------> 0.25\n",
      "Total reward for the episode:  2080.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  361\n",
      "3.0 action:  [-0.0019, -0.9986] n_targets:  1 reward:  80\n",
      "5.1 action:  [0.0024, -0.9883] n_targets:  1 reward:  80\n",
      "5.8 action:  [-0.0071, -0.9979] n_targets:  1 reward:  80\n",
      "12.9 action:  [0.0077, -0.9925] n_targets:  2 reward:  160\n",
      "17.0 action:  [-0.0034, -0.9813] n_targets:  1 reward:  80\n",
      "21.0 action:  [0.004, -0.9906] n_targets:  2 reward:  160\n",
      "22.0 action:  [0.0022, -0.9879] n_targets:  1 reward:  80\n",
      "24.8 action:  [-0.0026, -0.9969] n_targets:  1 reward:  80\n",
      "27.1 action:  [-0.0049, -0.9834] n_targets:  1 reward:  80\n",
      "29.2 action:  [0.0037, -0.9992] n_targets:  2 reward:  160\n",
      "33.1 action:  [0.0051, -0.9979] n_targets:  1 reward:  80\n",
      "33.9 action:  [-0.0016, -0.9883] n_targets:  1 reward:  80\n",
      "36.1 action:  [0.0008, -0.9977] n_targets:  1 reward:  80\n",
      "36.7 action:  [-0.0095, -0.9968] n_targets:  1 reward:  80\n",
      "37.6 action:  [0.0071, -0.9977] n_targets:  1 reward:  80\n",
      "49.4 action:  [-0.0069, -0.9951] n_targets:  1 reward:  80\n",
      "52.3 action:  [-0.0098, -0.9978] n_targets:  1 reward:  80\n",
      "52.6 action:  [-0.0045, -0.9988] n_targets:  1 reward:  80\n",
      "55.3 action:  [-0.0043, -0.9964] n_targets:  2 reward:  160\n",
      "58.0 action:  [-0.0025, -0.9885] n_targets:  1 reward:  80\n",
      "61.8 action:  [-0.0019, -0.9948] n_targets:  2 reward:  160\n",
      "65.0 action:  [0.004, -0.9832] n_targets:  2 reward:  160\n",
      "66.2 action:  [-0.0034, -0.9807] n_targets:  1 reward:  80\n",
      "68.7 action:  [-0.0062, -0.999] n_targets:  1 reward:  80\n",
      "73.3 action:  [-0.0004, -0.9988] n_targets:  1 reward:  80\n",
      "80.9 action:  [-0.0047, -0.983] n_targets:  1 reward:  80\n",
      "87.4 action:  [0.0027, -0.9984] n_targets:  1 reward:  80\n",
      "92.4 action:  [-0.0099, -0.9852] n_targets:  1 reward:  80\n",
      "96.2 action:  [0.0037, -0.9902] n_targets:  1 reward:  80\n",
      "99.6 action:  [-0.0001, -0.9911] n_targets:  1 reward:  80\n",
      "101.9 action:  [-0.0055, -0.9886] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  37 ff for 102.49999999999845 s: -------------------> 0.36\n",
      "Total reward for the episode:  2960.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  362\n",
      "3.9 action:  [0.0026, -0.9979] n_targets:  1 reward:  80\n",
      "6.3 action:  [0.0045, -0.9815] n_targets:  1 reward:  80\n",
      "10.6 action:  [0.0009, -0.9982] n_targets:  1 reward:  80\n",
      "11.9 action:  [-0.0068, -0.9937] n_targets:  2 reward:  160\n",
      "14.7 action:  [-0.0079, -0.9942] n_targets:  1 reward:  80\n",
      "17.3 action:  [0.0091, -0.9921] n_targets:  1 reward:  80\n",
      "25.8 action:  [-0.0065, -0.9968] n_targets:  1 reward:  80\n",
      "32.3 action:  [-0.0051, -0.9921] n_targets:  2 reward:  160\n",
      "35.3 action:  [0.0036, -0.992] n_targets:  1 reward:  80\n",
      "36.8 action:  [-0.0088, -0.9911] n_targets:  2 reward:  160\n",
      "46.5 action:  [-0.0044, -0.9954] n_targets:  1 reward:  80\n",
      "51.3 action:  [0.007, -0.9967] n_targets:  1 reward:  80\n",
      "53.1 action:  [-0.0034, -0.998] n_targets:  1 reward:  80\n",
      "58.9 action:  [-0.0048, -0.9854] n_targets:  1 reward:  80\n",
      "60.0 action:  [0.0072, -0.9942] n_targets:  1 reward:  80\n",
      "63.9 action:  [-0.0056, -0.9814] n_targets:  2 reward:  160\n",
      "67.8 action:  [-0.0086, -0.9997] n_targets:  1 reward:  80\n",
      "70.8 action:  [-0.0075, -0.9831] n_targets:  1 reward:  80\n",
      "75.5 action:  [-0.0009, -0.9951] n_targets:  1 reward:  80\n",
      "76.9 action:  [-0.0094, -0.9967] n_targets:  1 reward:  80\n",
      "85.2 action:  [-0.0097, -0.9832] n_targets:  1 reward:  80\n",
      "86.6 action:  [-0.0061, -0.9838] n_targets:  2 reward:  160\n",
      "90.5 action:  [-0.006, -0.9902] n_targets:  1 reward:  80\n",
      "93.5 action:  [-0.0045, -0.9975] n_targets:  2 reward:  160\n",
      "96.4 action:  [0.0056, -0.9985] n_targets:  1 reward:  80\n",
      "98.3 action:  [-0.0006, -0.9951] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  32 ff for 102.49999999999845 s: -------------------> 0.31\n",
      "Total reward for the episode:  2560.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  363\n",
      "Eval num_timesteps=130000, episode_reward=2533.33 +/- 359.75\n",
      "Episode length: 1025.00 +/- 0.00\n",
      "New best mean reward!\n",
      "7.0 action:  [0.002, -0.9988] n_targets:  2 reward:  160\n",
      "11.4 action:  [-0.0088, -0.9987] n_targets:  2 reward:  160\n",
      "14.6 action:  [0.0007, -0.9977] n_targets:  2 reward:  160\n",
      "25.8 action:  [0.0056, -0.9928] n_targets:  1 reward:  80\n",
      "26.8 action:  [0.0009, -0.9802] n_targets:  2 reward:  160\n",
      "28.6 action:  [0.0014, -0.9954] n_targets:  1 reward:  80\n",
      "33.9 action:  [-0.0076, -0.999] n_targets:  2 reward:  160\n",
      "38.0 action:  [0.0002, -0.983] n_targets:  1 reward:  80\n",
      "40.8 action:  [-0.0094, -0.9997] n_targets:  1 reward:  80\n",
      "43.0 action:  [-0.0089, -0.9972] n_targets:  1 reward:  80\n",
      "49.6 action:  [-0.005, -0.9873] n_targets:  1 reward:  80\n",
      "50.8 action:  [0.0092, -0.9986] n_targets:  1 reward:  80\n",
      "52.3 action:  [-0.0003, -0.9997] n_targets:  1 reward:  80\n",
      "55.7 action:  [0.0094, -0.9972] n_targets:  1 reward:  80\n",
      "57.0 action:  [0.002, -0.9984] n_targets:  1 reward:  80\n",
      "58.3 action:  [-0.0062, -0.9875] n_targets:  1 reward:  80\n",
      "58.9 action:  [0.0085, -0.9866] n_targets:  1 reward:  80\n",
      "64.5 action:  [-0.0041, -0.9979] n_targets:  1 reward:  80\n",
      "67.2 action:  [0.0075, -0.9892] n_targets:  1 reward:  80\n",
      "73.1 action:  [-0.0091, -0.9992] n_targets:  2 reward:  160\n",
      "78.0 action:  [0.0003, -0.9906] n_targets:  2 reward:  160\n",
      "84.7 action:  [0.0068, -0.9977] n_targets:  1 reward:  80\n",
      "89.9 action:  [-0.0025, -0.9801] n_targets:  1 reward:  80\n",
      "92.7 action:  [0.0058, -0.9893] n_targets:  1 reward:  80\n",
      "94.9 action:  [-0.008, -0.9855] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  32 ff for 102.49999999999845 s: -------------------> 0.31\n",
      "Total reward for the episode:  2560.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  364\n",
      "6.3 action:  [-0.0078, -0.9916] n_targets:  1 reward:  80\n",
      "10.5 action:  [-0.0074, -0.9961] n_targets:  1 reward:  80\n",
      "13.7 action:  [0.0085, -0.9856] n_targets:  1 reward:  80\n",
      "16.6 action:  [-0.0068, -0.9834] n_targets:  1 reward:  80\n",
      "18.4 action:  [0.0051, -0.9957] n_targets:  2 reward:  160\n",
      "20.7 action:  [-0.0034, -0.9954] n_targets:  1 reward:  80\n",
      "24.7 action:  [0.0061, -0.9997] n_targets:  1 reward:  80\n",
      "26.4 action:  [-0.0087, -0.9935] n_targets:  1 reward:  80\n",
      "29.7 action:  [0.0027, -0.9961] n_targets:  1 reward:  80\n",
      "31.0 action:  [0.0004, -0.9972] n_targets:  1 reward:  80\n",
      "35.8 action:  [0.0064, -0.983] n_targets:  1 reward:  80\n",
      "39.2 action:  [-0.0044, -0.9825] n_targets:  1 reward:  80\n",
      "42.2 action:  [0.0084, -0.9974] n_targets:  1 reward:  80\n",
      "44.7 action:  [0.0015, -0.9887] n_targets:  1 reward:  80\n",
      "46.9 action:  [0.0036, -0.996] n_targets:  1 reward:  80\n",
      "49.8 action:  [-0.0076, -0.9883] n_targets:  1 reward:  80\n",
      "56.0 action:  [0.0094, -0.9966] n_targets:  1 reward:  80\n",
      "66.8 action:  [0.0086, -0.9983] n_targets:  1 reward:  80\n",
      "70.4 action:  [-0.0059, -0.9998] n_targets:  1 reward:  80\n",
      "73.1 action:  [-0.0, -0.997] n_targets:  1 reward:  80\n",
      "75.1 action:  [0.0093, -0.9976] n_targets:  1 reward:  80\n",
      "86.6 action:  [-0.0058, -0.9992] n_targets:  1 reward:  80\n",
      "91.9 action:  [-0.0003, -0.9948] n_targets:  1 reward:  80\n",
      "96.9 action:  [0.0048, -0.9873] n_targets:  1 reward:  80\n",
      "100.4 action:  [-0.0001, -0.9961] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  26 ff for 102.49999999999845 s: -------------------> 0.25\n",
      "Total reward for the episode:  2080.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  365\n",
      "4.5 action:  [0.0024, -0.9997] n_targets:  1 reward:  80\n",
      "13.6 action:  [0.0081, -0.9983] n_targets:  1 reward:  80\n",
      "15.7 action:  [-0.0097, -0.9982] n_targets:  1 reward:  80\n",
      "19.9 action:  [-0.0061, -0.9918] n_targets:  1 reward:  80\n",
      "21.3 action:  [-0.0054, -0.981] n_targets:  1 reward:  80\n",
      "22.9 action:  [0.0077, -0.9998] n_targets:  1 reward:  80\n",
      "24.8 action:  [0.0095, -0.9921] n_targets:  1 reward:  80\n",
      "26.3 action:  [-0.0022, -0.9961] n_targets:  1 reward:  80\n",
      "30.1 action:  [0.0021, -0.9915] n_targets:  1 reward:  80\n",
      "34.3 action:  [-0.0018, -0.9965] n_targets:  1 reward:  80\n",
      "38.2 action:  [-0.0013, -0.9963] n_targets:  1 reward:  80\n",
      "41.0 action:  [0.0026, -0.9922] n_targets:  1 reward:  80\n",
      "45.8 action:  [-0.009, -0.9997] n_targets:  2 reward:  160\n",
      "47.8 action:  [-0.0057, -0.9907] n_targets:  1 reward:  80\n",
      "49.2 action:  [0.0089, -0.9923] n_targets:  2 reward:  160\n",
      "52.0 action:  [0.0035, -0.9994] n_targets:  1 reward:  80\n",
      "56.2 action:  [-0.0032, -0.9959] n_targets:  1 reward:  80\n",
      "59.0 action:  [-0.0094, -0.9931] n_targets:  1 reward:  80\n",
      "60.2 action:  [-0.0028, -0.9962] n_targets:  1 reward:  80\n",
      "64.5 action:  [0.0048, -0.9811] n_targets:  1 reward:  80\n",
      "67.5 action:  [0.0002, -0.9878] n_targets:  1 reward:  80\n",
      "71.3 action:  [0.0082, -0.998] n_targets:  1 reward:  80\n",
      "74.6 action:  [0.0028, -0.9994] n_targets:  1 reward:  80\n",
      "76.0 action:  [0.0022, -0.9999] n_targets:  1 reward:  80\n",
      "76.7 action:  [-0.0056, -0.9953] n_targets:  2 reward:  160\n",
      "78.3 action:  [-0.0089, -0.9996] n_targets:  1 reward:  80\n",
      "82.1 action:  [-0.008, -0.9871] n_targets:  1 reward:  80\n",
      "86.3 action:  [-0.002, -0.9988] n_targets:  1 reward:  80\n",
      "96.4 action:  [-0.0095, -0.9854] n_targets:  1 reward:  80\n",
      "98.3 action:  [-0.0074, -0.9953] n_targets:  1 reward:  80\n",
      "99.9 action:  [-0.009, -0.9948] n_targets:  2 reward:  160\n",
      "Firely capture rate for the episode:  35 ff for 102.49999999999845 s: -------------------> 0.34\n",
      "Total reward for the episode:  2800.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  366\n",
      "9.3 action:  [-0.0018, -0.9932] n_targets:  1 reward:  80\n",
      "15.9 action:  [0.006, -0.9989] n_targets:  1 reward:  80\n",
      "18.9 action:  [-0.0034, -0.9965] n_targets:  1 reward:  80\n",
      "22.1 action:  [-0.0014, -0.9972] n_targets:  1 reward:  80\n",
      "23.1 action:  [0.0045, -0.9991] n_targets:  2 reward:  160\n",
      "34.8 action:  [0.0092, -0.9993] n_targets:  1 reward:  80\n",
      "36.8 action:  [0.0067, -0.999] n_targets:  1 reward:  80\n",
      "40.8 action:  [-0.0056, -0.983] n_targets:  1 reward:  80\n",
      "46.3 action:  [-0.009, -0.9995] n_targets:  1 reward:  80\n",
      "48.0 action:  [-0.0097, -0.9973] n_targets:  1 reward:  80\n",
      "51.1 action:  [-0.0041, -0.9936] n_targets:  1 reward:  80\n",
      "54.9 action:  [-0.0021, -0.9982] n_targets:  1 reward:  80\n",
      "62.1 action:  [-0.0086, -0.9881] n_targets:  1 reward:  80\n",
      "68.9 action:  [0.0096, -0.9879] n_targets:  1 reward:  80\n",
      "70.0 action:  [0.0002, -0.9847] n_targets:  1 reward:  80\n",
      "72.9 action:  [-0.0053, -0.9924] n_targets:  1 reward:  80\n",
      "75.7 action:  [-0.0075, -0.9959] n_targets:  1 reward:  80\n",
      "79.9 action:  [0.001, -0.9966] n_targets:  1 reward:  80\n",
      "82.5 action:  [-0.0023, -0.98] n_targets:  1 reward:  80\n",
      "86.3 action:  [0.0016, -0.994] n_targets:  1 reward:  80\n",
      "89.3 action:  [-0.0022, -0.9864] n_targets:  1 reward:  80\n",
      "96.5 action:  [0.0063, -0.9992] n_targets:  2 reward:  160\n",
      "100.3 action:  [0.0094, -0.9959] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  25 ff for 102.49999999999845 s: -------------------> 0.24\n",
      "Total reward for the episode:  2000.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  367\n",
      "6.5 action:  [-0.0059, -0.9982] n_targets:  1 reward:  80\n",
      "13.1 action:  [0.0006, -0.9992] n_targets:  1 reward:  80\n",
      "22.9 action:  [-0.0081, -0.999] n_targets:  1 reward:  80\n",
      "26.3 action:  [0.0041, -0.998] n_targets:  1 reward:  80\n",
      "27.3 action:  [-0.0092, -0.9965] n_targets:  1 reward:  80\n",
      "29.1 action:  [-0.009, -0.9939] n_targets:  1 reward:  80\n",
      "32.8 action:  [-0.0056, -0.9979] n_targets:  1 reward:  80\n",
      "34.9 action:  [-0.0075, -0.9992] n_targets:  1 reward:  80\n",
      "37.0 action:  [0.0087, -0.9994] n_targets:  1 reward:  80\n",
      "39.9 action:  [-0.0096, -0.9952] n_targets:  1 reward:  80\n",
      "42.8 action:  [-0.0037, -0.9974] n_targets:  1 reward:  80\n",
      "46.4 action:  [-0.0056, -0.9909] n_targets:  2 reward:  160\n",
      "54.3 action:  [-0.0022, -0.9838] n_targets:  1 reward:  80\n",
      "61.5 action:  [-0.0007, -0.9971] n_targets:  1 reward:  80\n",
      "69.2 action:  [0.0049, -0.9893] n_targets:  1 reward:  80\n",
      "71.9 action:  [0.0025, -0.9987] n_targets:  1 reward:  80\n",
      "83.9 action:  [0.0066, -0.9972] n_targets:  1 reward:  80\n",
      "87.8 action:  [0.0089, -0.9917] n_targets:  1 reward:  80\n",
      "98.0 action:  [-0.0041, -0.9955] n_targets:  1 reward:  80\n",
      "102.1 action:  [-0.0059, -0.9925] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  21 ff for 102.49999999999845 s: -------------------> 0.2\n",
      "Total reward for the episode:  1680.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  368\n",
      "1.8 action:  [-0.0003, -0.9925] n_targets:  1 reward:  80\n",
      "2.5 action:  [0.0071, -0.9995] n_targets:  1 reward:  80\n",
      "4.4 action:  [0.0054, -0.9819] n_targets:  1 reward:  80\n",
      "8.6 action:  [-0.0006, -0.9974] n_targets:  1 reward:  80\n",
      "16.0 action:  [0.0014, -0.9982] n_targets:  1 reward:  80\n",
      "17.7 action:  [-0.0042, -0.9997] n_targets:  3 reward:  240\n",
      "22.8 action:  [0.0098, -0.993] n_targets:  1 reward:  80\n",
      "25.7 action:  [-0.0046, -0.9952] n_targets:  1 reward:  80\n",
      "26.8 action:  [-0.006, -0.9962] n_targets:  1 reward:  80\n",
      "29.7 action:  [-0.004, -0.9839] n_targets:  1 reward:  80\n",
      "31.1 action:  [-0.0038, -0.9818] n_targets:  2 reward:  160\n",
      "33.6 action:  [-0.0049, -0.9989] n_targets:  1 reward:  80\n",
      "40.8 action:  [-0.0023, -0.9992] n_targets:  1 reward:  80\n",
      "48.6 action:  [-0.0014, -0.9997] n_targets:  1 reward:  80\n",
      "50.4 action:  [-0.0044, -0.998] n_targets:  1 reward:  80\n",
      "52.9 action:  [-0.0058, -0.9945] n_targets:  1 reward:  80\n",
      "55.6 action:  [0.0083, -0.9991] n_targets:  1 reward:  80\n",
      "58.2 action:  [-0.0069, -0.9977] n_targets:  1 reward:  80\n",
      "69.3 action:  [0.0031, -0.9854] n_targets:  1 reward:  80\n",
      "71.5 action:  [-0.0099, -0.9834] n_targets:  1 reward:  80\n",
      "73.7 action:  [-0.0012, -0.9967] n_targets:  1 reward:  80\n",
      "79.1 action:  [0.0001, -0.9907] n_targets:  1 reward:  80\n",
      "83.3 action:  [0.0043, -0.9859] n_targets:  1 reward:  80\n",
      "85.2 action:  [0.0038, -0.9973] n_targets:  1 reward:  80\n",
      "91.5 action:  [0.0093, -0.9843] n_targets:  1 reward:  80\n",
      "96.7 action:  [-0.0083, -0.9987] n_targets:  2 reward:  160\n",
      "98.1 action:  [0.0023, -0.9873] n_targets:  1 reward:  80\n",
      "101.4 action:  [0.0025, -0.9927] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  32 ff for 102.49999999999845 s: -------------------> 0.31\n",
      "Total reward for the episode:  2560.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  369\n",
      "2.4 action:  [0.008, -0.9942] n_targets:  1 reward:  80\n",
      "3.7 action:  [0.0028, -0.9927] n_targets:  1 reward:  80\n",
      "8.1 action:  [-0.0058, -0.9912] n_targets:  1 reward:  80\n",
      "20.4 action:  [-0.0076, -0.9997] n_targets:  1 reward:  80\n",
      "26.3 action:  [-0.0074, -0.9821] n_targets:  1 reward:  80\n",
      "28.5 action:  [0.005, -0.9926] n_targets:  1 reward:  80\n",
      "31.2 action:  [0.0066, -0.9847] n_targets:  2 reward:  160\n",
      "35.0 action:  [-0.009, -0.9957] n_targets:  1 reward:  80\n",
      "46.8 action:  [0.0042, -0.9966] n_targets:  1 reward:  80\n",
      "48.1 action:  [0.0048, -0.9881] n_targets:  1 reward:  80\n",
      "51.7 action:  [-0.0064, -0.9955] n_targets:  1 reward:  80\n",
      "58.4 action:  [-0.0055, -0.9834] n_targets:  1 reward:  80\n",
      "64.6 action:  [0.0009, -0.996] n_targets:  1 reward:  80\n",
      "66.2 action:  [-0.0061, -0.9967] n_targets:  1 reward:  80\n",
      "75.5 action:  [0.0032, -0.9922] n_targets:  2 reward:  160\n",
      "77.0 action:  [0.005, -0.9888] n_targets:  1 reward:  80\n",
      "78.6 action:  [0.008, -0.9983] n_targets:  1 reward:  80\n",
      "80.5 action:  [0.0074, -0.9997] n_targets:  1 reward:  80\n",
      "82.0 action:  [0.0054, -0.9896] n_targets:  1 reward:  80\n",
      "90.3 action:  [-0.0087, -0.9961] n_targets:  1 reward:  80\n",
      "93.0 action:  [0.002, -0.9994] n_targets:  1 reward:  80\n",
      "96.5 action:  [-0.0027, -0.9923] n_targets:  1 reward:  80\n",
      "99.2 action:  [-0.0032, -0.9993] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  25 ff for 102.49999999999845 s: -------------------> 0.24\n",
      "Total reward for the episode:  2000.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  370\n",
      "3.0 action:  [0.0019, -0.9809] n_targets:  1 reward:  80\n",
      "8.4 action:  [0.0075, -0.9968] n_targets:  1 reward:  80\n",
      "12.7 action:  [0.0053, -0.9986] n_targets:  1 reward:  80\n",
      "17.7 action:  [0.0081, -0.9989] n_targets:  1 reward:  80\n",
      "31.6 action:  [0.0049, -0.9957] n_targets:  1 reward:  80\n",
      "35.0 action:  [-0.007, -0.9928] n_targets:  1 reward:  80\n",
      "37.1 action:  [0.0024, -0.9989] n_targets:  2 reward:  160\n",
      "46.2 action:  [-0.0092, -0.9904] n_targets:  1 reward:  80\n",
      "51.0 action:  [-0.008, -0.9813] n_targets:  2 reward:  160\n",
      "53.7 action:  [-0.0015, -0.9978] n_targets:  1 reward:  80\n",
      "55.5 action:  [-0.0035, -0.997] n_targets:  1 reward:  80\n",
      "65.1 action:  [0.0034, -0.9997] n_targets:  1 reward:  80\n",
      "67.9 action:  [-0.0091, -0.9989] n_targets:  1 reward:  80\n",
      "70.9 action:  [0.0008, -0.9971] n_targets:  1 reward:  80\n",
      "71.5 action:  [0.0075, -0.999] n_targets:  1 reward:  80\n",
      "73.6 action:  [-0.0059, -0.9806] n_targets:  1 reward:  80\n",
      "75.0 action:  [-0.0051, -0.9992] n_targets:  1 reward:  80\n",
      "77.0 action:  [-0.0004, -0.9938] n_targets:  1 reward:  80\n",
      "78.9 action:  [0.0041, -0.9973] n_targets:  1 reward:  80\n",
      "81.3 action:  [-0.0038, -0.998] n_targets:  1 reward:  80\n",
      "82.6 action:  [0.0027, -0.9986] n_targets:  1 reward:  80\n",
      "85.3 action:  [0.0049, -0.9867] n_targets:  1 reward:  80\n",
      "85.7 action:  [0.0012, -0.9985] n_targets:  1 reward:  80\n",
      "89.2 action:  [0.0017, -0.9926] n_targets:  1 reward:  80\n",
      "93.7 action:  [-0.0095, -0.9829] n_targets:  1 reward:  80\n",
      "93.8 action:  [-0.0012, -0.9814] n_targets:  1 reward:  80\n",
      "95.8 action:  [-0.0007, -0.9946] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  29 ff for 102.49999999999845 s: -------------------> 0.28\n",
      "Total reward for the episode:  2320.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  371\n",
      "4.4 action:  [-0.0077, -0.9993] n_targets:  2 reward:  160\n",
      "7.5 action:  [-0.0019, -0.9906] n_targets:  1 reward:  80\n",
      "14.0 action:  [0.0002, -0.9988] n_targets:  1 reward:  80\n",
      "15.1 action:  [-0.0022, -0.996] n_targets:  1 reward:  80\n",
      "17.0 action:  [-0.0086, -0.9985] n_targets:  1 reward:  80\n",
      "18.8 action:  [-0.0081, -0.9911] n_targets:  1 reward:  80\n",
      "19.9 action:  [-0.007, -0.9978] n_targets:  1 reward:  80\n",
      "21.8 action:  [0.0083, -0.9985] n_targets:  1 reward:  80\n",
      "24.6 action:  [-0.0071, -0.9977] n_targets:  1 reward:  80\n",
      "26.6 action:  [0.0025, -0.9848] n_targets:  1 reward:  80\n",
      "40.6 action:  [-0.0078, -0.999] n_targets:  1 reward:  80\n",
      "41.8 action:  [0.0082, -0.995] n_targets:  1 reward:  80\n",
      "45.9 action:  [-0.01, -0.9835] n_targets:  1 reward:  80\n",
      "48.6 action:  [0.003, -0.9922] n_targets:  1 reward:  80\n",
      "53.8 action:  [-0.0079, -0.9857] n_targets:  1 reward:  80\n",
      "56.6 action:  [-0.008, -0.9905] n_targets:  1 reward:  80\n",
      "59.2 action:  [0.0066, -0.9958] n_targets:  1 reward:  80\n",
      "60.6 action:  [-0.0081, -0.9941] n_targets:  1 reward:  80\n",
      "63.0 action:  [-0.0009, -0.9998] n_targets:  1 reward:  80\n",
      "68.3 action:  [-0.0073, -0.9961] n_targets:  2 reward:  160\n",
      "70.4 action:  [0.008, -0.9989] n_targets:  1 reward:  80\n",
      "72.6 action:  [-0.0002, -0.9981] n_targets:  1 reward:  80\n",
      "76.3 action:  [-0.0018, -0.9981] n_targets:  1 reward:  80\n",
      "77.7 action:  [-0.0066, -0.9986] n_targets:  2 reward:  160\n",
      "80.4 action:  [0.0042, -0.9877] n_targets:  1 reward:  80\n",
      "82.0 action:  [-0.003, -0.9993] n_targets:  1 reward:  80\n",
      "85.2 action:  [0.0098, -0.9892] n_targets:  2 reward:  160\n",
      "93.7 action:  [0.0026, -0.9869] n_targets:  1 reward:  80\n",
      "96.7 action:  [-0.0008, -0.9861] n_targets:  1 reward:  80\n",
      "97.6 action:  [0.0014, -0.9985] n_targets:  1 reward:  80\n",
      "101.2 action:  [-0.0021, -0.9987] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  35 ff for 102.49999999999845 s: -------------------> 0.34\n",
      "Total reward for the episode:  2800.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  372\n",
      "3.3 action:  [0.0093, -0.9973] n_targets:  1 reward:  80\n",
      "7.3 action:  [-0.0086, -0.9998] n_targets:  1 reward:  80\n",
      "13.1 action:  [-0.0079, -0.9977] n_targets:  1 reward:  80\n",
      "16.8 action:  [0.0047, -0.9961] n_targets:  1 reward:  80\n",
      "21.8 action:  [0.0069, -0.9999] n_targets:  1 reward:  80\n",
      "24.6 action:  [0.0078, -0.9947] n_targets:  2 reward:  160\n",
      "26.9 action:  [0.009, -0.997] n_targets:  1 reward:  80\n",
      "35.7 action:  [0.0015, -0.9979] n_targets:  1 reward:  80\n",
      "38.3 action:  [0.0069, -0.9862] n_targets:  1 reward:  80\n",
      "40.2 action:  [-0.0044, -0.998] n_targets:  2 reward:  160\n",
      "40.6 action:  [0.0069, -0.9997] n_targets:  1 reward:  80\n",
      "55.0 action:  [-0.006, -0.9972] n_targets:  1 reward:  80\n",
      "57.9 action:  [-0.0035, -0.9934] n_targets:  1 reward:  80\n",
      "60.3 action:  [-0.0093, -0.9924] n_targets:  1 reward:  80\n",
      "66.0 action:  [0.0052, -0.9929] n_targets:  1 reward:  80\n",
      "70.0 action:  [-0.0011, -0.986] n_targets:  1 reward:  80\n",
      "75.0 action:  [-0.0083, -0.9921] n_targets:  1 reward:  80\n",
      "TIME before resetting: 77.49999999999987\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  373\n",
      "2.0 action:  [-0.0067, -0.9956] n_targets:  1 reward:  80\n",
      "22.9 action:  [-0.0097, -0.9978] n_targets:  1 reward:  80\n",
      "37.6 action:  [-0.0084, -0.9959] n_targets:  1 reward:  80\n",
      "39.7 action:  [-0.0099, -0.9937] n_targets:  1 reward:  80\n",
      "40.5 action:  [-0.0098, -0.9938] n_targets:  1 reward:  80\n",
      "43.8 action:  [-0.0082, -0.9948] n_targets:  1 reward:  80\n",
      "46.3 action:  [0.0091, -0.9892] n_targets:  1 reward:  80\n",
      "53.4 action:  [-0.0058, -0.9969] n_targets:  2 reward:  160\n",
      "57.8 action:  [-0.0097, -0.9943] n_targets:  1 reward:  80\n",
      "59.1 action:  [0.0005, -0.99] n_targets:  1 reward:  80\n",
      "62.9 action:  [-0.0078, -0.9987] n_targets:  1 reward:  80\n",
      "66.8 action:  [-0.0084, -0.9939] n_targets:  1 reward:  80\n",
      "71.7 action:  [-0.0096, -0.9838] n_targets:  1 reward:  80\n",
      "76.6 action:  [-0.0058, -0.9864] n_targets:  1 reward:  80\n",
      "84.4 action:  [-0.0099, -0.9994] n_targets:  1 reward:  80\n",
      "87.7 action:  [-0.005, -0.9957] n_targets:  1 reward:  80\n",
      "100.4 action:  [-0.0099, -0.9987] n_targets:  1 reward:  80\n",
      "102.2 action:  [0.0036, -0.9884] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  19 ff for 102.49999999999845 s: -------------------> 0.19\n",
      "Total reward for the episode:  1520.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  374\n",
      "14.9 action:  [-0.01, -0.9965] n_targets:  1 reward:  80\n",
      "21.0 action:  [-0.009, -0.9805] n_targets:  1 reward:  80\n",
      "36.0 action:  [-0.0087, -0.9931] n_targets:  1 reward:  80\n",
      "46.2 action:  [-0.0088, -0.9976] n_targets:  1 reward:  80\n",
      "61.8 action:  [-0.0098, -0.9933] n_targets:  1 reward:  80\n",
      "79.7 action:  [-0.0099, -0.9966] n_targets:  1 reward:  80\n",
      "85.0 action:  [-0.0099, -0.9914] n_targets:  1 reward:  80\n",
      "86.8 action:  [-0.009, -0.9843] n_targets:  1 reward:  80\n",
      "98.5 action:  [-0.0099, -0.991] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  9 ff for 102.49999999999845 s: -------------------> 0.09\n",
      "Total reward for the episode:  720.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  375\n",
      "4.3 action:  [-0.0079, -0.9973] n_targets:  1 reward:  80\n",
      "7.6 action:  [-0.007, -0.996] n_targets:  1 reward:  80\n",
      "13.0 action:  [-0.0056, -0.9848] n_targets:  1 reward:  80\n",
      "17.9 action:  [-0.0093, -0.9868] n_targets:  1 reward:  80\n",
      "19.0 action:  [-0.0058, -0.991] n_targets:  1 reward:  80\n",
      "19.6 action:  [-0.0086, -0.9972] n_targets:  1 reward:  80\n",
      "20.9 action:  [-0.0097, -0.9921] n_targets:  1 reward:  80\n",
      "23.3 action:  [-0.0093, -0.9924] n_targets:  1 reward:  80\n",
      "28.3 action:  [-0.0066, -0.9959] n_targets:  2 reward:  160\n",
      "33.1 action:  [-0.0099, -0.9965] n_targets:  1 reward:  80\n",
      "34.7 action:  [-0.0058, -0.9888] n_targets:  1 reward:  80\n",
      "52.8 action:  [-0.0099, -0.9976] n_targets:  1 reward:  80\n",
      "61.3 action:  [-0.0094, -0.9975] n_targets:  1 reward:  80\n",
      "64.7 action:  [-0.0081, -0.9871] n_targets:  1 reward:  80\n",
      "89.3 action:  [-0.0062, -0.9968] n_targets:  1 reward:  80\n",
      "95.4 action:  [-0.007, -0.9996] n_targets:  2 reward:  160\n",
      "97.6 action:  [-0.0091, -0.9962] n_targets:  1 reward:  80\n",
      "99.0 action:  [-0.0072, -0.9967] n_targets:  1 reward:  80\n",
      "101.9 action:  [-0.0082, -0.9903] n_targets:  2 reward:  160\n",
      "Firely capture rate for the episode:  22 ff for 102.49999999999845 s: -------------------> 0.21\n",
      "Total reward for the episode:  1760.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  376\n",
      "Eval num_timesteps=140000, episode_reward=1333.33 +/- 444.62\n",
      "Episode length: 1025.00 +/- 0.00\n",
      "3.9 action:  [0.0043, -0.994] n_targets:  2 reward:  160\n",
      "7.2 action:  [0.0058, -0.9979] n_targets:  1 reward:  80\n",
      "9.6 action:  [0.0018, -0.9914] n_targets:  1 reward:  80\n",
      "18.1 action:  [0.0045, -0.9983] n_targets:  1 reward:  80\n",
      "19.0 action:  [0.0038, -0.9966] n_targets:  1 reward:  80\n",
      "23.0 action:  [0.0052, -0.9981] n_targets:  1 reward:  80\n",
      "24.2 action:  [0.0008, -0.9989] n_targets:  1 reward:  80\n",
      "27.3 action:  [0.0026, -0.9998] n_targets:  1 reward:  80\n",
      "29.2 action:  [-0.0099, -0.9988] n_targets:  1 reward:  80\n",
      "35.3 action:  [-0.0043, -0.9909] n_targets:  1 reward:  80\n",
      "38.2 action:  [-0.0054, -0.988] n_targets:  1 reward:  80\n",
      "42.4 action:  [0.006, -0.9983] n_targets:  1 reward:  80\n",
      "45.6 action:  [0.005, -0.998] n_targets:  1 reward:  80\n",
      "48.0 action:  [0.0035, -0.995] n_targets:  1 reward:  80\n",
      "54.0 action:  [0.0079, -0.9939] n_targets:  1 reward:  80\n",
      "58.7 action:  [-0.004, -0.9956] n_targets:  2 reward:  160\n",
      "62.4 action:  [-0.008, -0.9976] n_targets:  4 reward:  320\n",
      "63.3 action:  [0.0043, -0.9956] n_targets:  1 reward:  80\n",
      "66.8 action:  [-0.0025, -0.9975] n_targets:  1 reward:  80\n",
      "68.2 action:  [0.0056, -0.9916] n_targets:  1 reward:  80\n",
      "71.6 action:  [0.0036, -0.9984] n_targets:  1 reward:  80\n",
      "78.5 action:  [0.0007, -0.9807] n_targets:  1 reward:  80\n",
      "87.9 action:  [0.0049, -0.9981] n_targets:  1 reward:  80\n",
      "91.3 action:  [0.0067, -0.9963] n_targets:  1 reward:  80\n",
      "93.9 action:  [-0.0011, -0.9979] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  30 ff for 102.49999999999845 s: -------------------> 0.29\n",
      "Total reward for the episode:  2400.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  377\n",
      "6.3 action:  [-0.0082, -0.9953] n_targets:  1 reward:  80\n",
      "6.8 action:  [0.0021, -0.9957] n_targets:  1 reward:  80\n",
      "12.0 action:  [-0.0066, -0.9802] n_targets:  1 reward:  80\n",
      "19.4 action:  [0.0028, -0.9864] n_targets:  1 reward:  80\n",
      "21.0 action:  [0.0011, -0.9998] n_targets:  1 reward:  80\n",
      "24.7 action:  [-0.0084, -0.9989] n_targets:  1 reward:  80\n",
      "28.3 action:  [0.0038, -0.9985] n_targets:  1 reward:  80\n",
      "32.0 action:  [0.0091, -0.9994] n_targets:  1 reward:  80\n",
      "34.5 action:  [-0.0007, -0.9978] n_targets:  1 reward:  80\n",
      "37.3 action:  [-0.0079, -0.9983] n_targets:  1 reward:  80\n",
      "39.9 action:  [0.0031, -0.9925] n_targets:  2 reward:  160\n",
      "44.3 action:  [0.0037, -0.984] n_targets:  1 reward:  80\n",
      "46.2 action:  [0.0061, -0.9964] n_targets:  1 reward:  80\n",
      "50.6 action:  [-0.0078, -0.9962] n_targets:  1 reward:  80\n",
      "51.3 action:  [0.0058, -0.9928] n_targets:  1 reward:  80\n",
      "53.1 action:  [-0.0024, -0.9976] n_targets:  1 reward:  80\n",
      "54.7 action:  [-0.0005, -0.9909] n_targets:  1 reward:  80\n",
      "58.3 action:  [0.0096, -0.9989] n_targets:  2 reward:  160\n",
      "60.6 action:  [-0.0056, -0.9807] n_targets:  1 reward:  80\n",
      "62.7 action:  [0.0007, -0.9994] n_targets:  1 reward:  80\n",
      "64.2 action:  [-0.0017, -0.9989] n_targets:  1 reward:  80\n",
      "65.4 action:  [-0.0033, -0.9992] n_targets:  1 reward:  80\n",
      "72.5 action:  [0.0046, -0.9978] n_targets:  1 reward:  80\n",
      "74.7 action:  [0.0079, -0.9957] n_targets:  2 reward:  160\n",
      "76.8 action:  [-0.0081, -0.9978] n_targets:  1 reward:  80\n",
      "79.9 action:  [-0.0088, -0.9985] n_targets:  1 reward:  80\n",
      "84.7 action:  [-0.0059, -0.9972] n_targets:  1 reward:  80\n",
      "87.9 action:  [0.0088, -0.9936] n_targets:  1 reward:  80\n",
      "96.6 action:  [-0.0006, -0.9947] n_targets:  1 reward:  80\n",
      "99.9 action:  [-0.0069, -0.9935] n_targets:  1 reward:  80\n",
      "102.0 action:  [-0.0037, -0.9817] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  34 ff for 102.49999999999845 s: -------------------> 0.33\n",
      "Total reward for the episode:  2720.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  378\n",
      "3.5 action:  [0.003, -0.9909] n_targets:  2 reward:  160\n",
      "6.8 action:  [-0.0075, -0.9924] n_targets:  1 reward:  80\n",
      "7.9 action:  [-0.0053, -0.9992] n_targets:  1 reward:  80\n",
      "20.8 action:  [0.0031, -0.9996] n_targets:  1 reward:  80\n",
      "24.9 action:  [0.0084, -0.9831] n_targets:  1 reward:  80\n",
      "32.2 action:  [-0.0064, -0.9976] n_targets:  1 reward:  80\n",
      "34.8 action:  [0.0088, -0.997] n_targets:  1 reward:  80\n",
      "35.4 action:  [0.0008, -0.9968] n_targets:  1 reward:  80\n",
      "35.9 action:  [0.0066, -0.9802] n_targets:  1 reward:  80\n",
      "39.3 action:  [-0.0098, -0.9815] n_targets:  1 reward:  80\n",
      "41.6 action:  [-0.0076, -0.9926] n_targets:  1 reward:  80\n",
      "46.3 action:  [-0.0018, -0.9974] n_targets:  2 reward:  160\n",
      "51.1 action:  [-0.0041, -0.9836] n_targets:  1 reward:  80\n",
      "53.3 action:  [0.003, -0.9893] n_targets:  1 reward:  80\n",
      "55.3 action:  [0.003, -0.9994] n_targets:  2 reward:  160\n",
      "63.6 action:  [0.008, -0.9997] n_targets:  1 reward:  80\n",
      "66.0 action:  [-0.0026, -0.9968] n_targets:  1 reward:  80\n",
      "72.3 action:  [-0.0018, -0.9991] n_targets:  2 reward:  160\n",
      "74.2 action:  [0.0057, -0.9975] n_targets:  1 reward:  80\n",
      "81.1 action:  [-0.0046, -0.9955] n_targets:  1 reward:  80\n",
      "82.6 action:  [-0.0065, -0.9967] n_targets:  1 reward:  80\n",
      "84.1 action:  [0.0007, -0.9977] n_targets:  1 reward:  80\n",
      "85.7 action:  [-0.0096, -0.9955] n_targets:  1 reward:  80\n",
      "94.6 action:  [-0.0014, -0.9945] n_targets:  1 reward:  80\n",
      "97.2 action:  [-0.0056, -0.9982] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  29 ff for 102.49999999999845 s: -------------------> 0.28\n",
      "Total reward for the episode:  2320.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  379\n",
      "2.2 action:  [0.0001, -0.9981] n_targets:  1 reward:  80\n",
      "3.0 action:  [0.0055, -0.9983] n_targets:  1 reward:  80\n",
      "5.3 action:  [0.0053, -0.9964] n_targets:  1 reward:  80\n",
      "11.5 action:  [-0.0003, -0.9986] n_targets:  1 reward:  80\n",
      "13.2 action:  [-0.0002, -0.9944] n_targets:  1 reward:  80\n",
      "15.8 action:  [0.003, -0.9993] n_targets:  1 reward:  80\n",
      "20.3 action:  [0.0078, -0.9991] n_targets:  1 reward:  80\n",
      "26.1 action:  [-0.0028, -0.9956] n_targets:  1 reward:  80\n",
      "28.2 action:  [-0.007, -0.9992] n_targets:  1 reward:  80\n",
      "31.3 action:  [-0.0035, -0.9987] n_targets:  1 reward:  80\n",
      "36.6 action:  [0.0058, -0.9959] n_targets:  1 reward:  80\n",
      "38.9 action:  [0.0091, -0.9994] n_targets:  1 reward:  80\n",
      "41.2 action:  [0.0071, -0.9994] n_targets:  1 reward:  80\n",
      "46.0 action:  [-0.0053, -0.987] n_targets:  2 reward:  160\n",
      "47.0 action:  [0.0026, -0.9993] n_targets:  1 reward:  80\n",
      "48.4 action:  [-0.0054, -0.997] n_targets:  1 reward:  80\n",
      "51.6 action:  [0.0062, -0.9954] n_targets:  1 reward:  80\n",
      "52.0 action:  [-0.0039, -0.9921] n_targets:  1 reward:  80\n",
      "53.6 action:  [0.0056, -0.9976] n_targets:  1 reward:  80\n",
      "57.2 action:  [-0.0045, -0.9986] n_targets:  1 reward:  80\n",
      "60.9 action:  [-0.0014, -0.9994] n_targets:  1 reward:  80\n",
      "64.2 action:  [-0.0019, -0.9984] n_targets:  1 reward:  80\n",
      "66.1 action:  [0.0026, -0.9997] n_targets:  1 reward:  80\n",
      "74.2 action:  [-0.0062, -0.9903] n_targets:  1 reward:  80\n",
      "74.8 action:  [-0.0097, -0.9969] n_targets:  1 reward:  80\n",
      "78.5 action:  [0.0038, -0.998] n_targets:  1 reward:  80\n",
      "80.4 action:  [-0.0021, -0.9982] n_targets:  1 reward:  80\n",
      "84.1 action:  [-0.01, -0.9904] n_targets:  1 reward:  80\n",
      "85.3 action:  [-0.0022, -0.9997] n_targets:  1 reward:  80\n",
      "88.4 action:  [0.0026, -0.988] n_targets:  1 reward:  80\n",
      "92.1 action:  [-0.01, -0.9961] n_targets:  1 reward:  80\n",
      "94.0 action:  [-0.0049, -0.9998] n_targets:  1 reward:  80\n",
      "100.2 action:  [-0.0033, -0.9842] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  34 ff for 102.49999999999845 s: -------------------> 0.33\n",
      "Total reward for the episode:  2720.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  380\n",
      "2.4 action:  [-0.0082, -0.9995] n_targets:  1 reward:  80\n",
      "5.1 action:  [-0.0076, -0.9982] n_targets:  1 reward:  80\n",
      "7.5 action:  [-0.0085, -0.9966] n_targets:  1 reward:  80\n",
      "8.6 action:  [-0.0022, -0.9992] n_targets:  1 reward:  80\n",
      "10.5 action:  [-0.0018, -0.9948] n_targets:  1 reward:  80\n",
      "13.7 action:  [-0.0028, -0.9971] n_targets:  2 reward:  160\n",
      "17.2 action:  [-0.007, -0.9965] n_targets:  2 reward:  160\n",
      "19.7 action:  [-0.0098, -0.9934] n_targets:  1 reward:  80\n",
      "22.7 action:  [0.0072, -0.9994] n_targets:  1 reward:  80\n",
      "24.7 action:  [-0.0024, -0.996] n_targets:  1 reward:  80\n",
      "27.6 action:  [-0.0005, -0.9998] n_targets:  2 reward:  160\n",
      "31.4 action:  [0.0082, -0.9968] n_targets:  2 reward:  160\n",
      "33.9 action:  [-0.0056, -0.9929] n_targets:  2 reward:  160\n",
      "36.4 action:  [-0.0052, -0.9983] n_targets:  1 reward:  80\n",
      "42.2 action:  [-0.0049, -0.9972] n_targets:  1 reward:  80\n",
      "46.2 action:  [-0.0041, -0.9992] n_targets:  1 reward:  80\n",
      "47.4 action:  [0.0034, -0.9945] n_targets:  1 reward:  80\n",
      "49.5 action:  [0.0081, -0.9996] n_targets:  1 reward:  80\n",
      "53.1 action:  [0.0076, -0.9852] n_targets:  1 reward:  80\n",
      "62.4 action:  [-0.0071, -0.9863] n_targets:  2 reward:  160\n",
      "65.4 action:  [0.0043, -0.9937] n_targets:  1 reward:  80\n",
      "66.0 action:  [0.0088, -0.9914] n_targets:  1 reward:  80\n",
      "73.9 action:  [-0.0035, -0.9952] n_targets:  2 reward:  160\n",
      "78.7 action:  [-0.0073, -0.9983] n_targets:  1 reward:  80\n",
      "79.4 action:  [-0.0072, -0.9952] n_targets:  1 reward:  80\n",
      "82.9 action:  [0.005, -0.999] n_targets:  1 reward:  80\n",
      "85.3 action:  [0.0071, -0.9989] n_targets:  1 reward:  80\n",
      "92.4 action:  [-0.003, -0.9986] n_targets:  1 reward:  80\n",
      "98.8 action:  [-0.0036, -0.9995] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  36 ff for 102.49999999999845 s: -------------------> 0.35\n",
      "Total reward for the episode:  2880.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  381\n",
      "4.5 action:  [0.0067, -0.9975] n_targets:  1 reward:  80\n",
      "8.3 action:  [0.0016, -0.9868] n_targets:  1 reward:  80\n",
      "18.5 action:  [-0.0091, -0.9995] n_targets:  1 reward:  80\n",
      "21.9 action:  [-0.0029, -0.994] n_targets:  1 reward:  80\n",
      "24.5 action:  [-0.0006, -0.9905] n_targets:  1 reward:  80\n",
      "26.0 action:  [-0.0086, -0.9924] n_targets:  1 reward:  80\n",
      "28.8 action:  [0.0085, -0.9999] n_targets:  1 reward:  80\n",
      "35.7 action:  [-0.0004, -0.993] n_targets:  1 reward:  80\n",
      "40.3 action:  [0.007, -0.9848] n_targets:  1 reward:  80\n",
      "42.3 action:  [-0.0047, -0.9831] n_targets:  1 reward:  80\n",
      "54.7 action:  [0.001, -1.0] n_targets:  1 reward:  80\n",
      "56.6 action:  [-0.008, -0.9978] n_targets:  1 reward:  80\n",
      "58.0 action:  [-0.0048, -0.9999] n_targets:  1 reward:  80\n",
      "66.9 action:  [-0.0037, -0.9826] n_targets:  1 reward:  80\n",
      "68.8 action:  [0.001, -0.9957] n_targets:  1 reward:  80\n",
      "78.1 action:  [-0.0006, -0.9982] n_targets:  1 reward:  80\n",
      "80.9 action:  [-0.0056, -0.9982] n_targets:  1 reward:  80\n",
      "83.0 action:  [-0.0001, -0.9912] n_targets:  1 reward:  80\n",
      "96.9 action:  [0.0089, -0.9997] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  19 ff for 102.49999999999845 s: -------------------> 0.19\n",
      "Total reward for the episode:  1520.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  382\n",
      "1.9 action:  [-0.0065, -0.9893] n_targets:  2 reward:  160\n",
      "4.0 action:  [0.0053, -0.9838] n_targets:  1 reward:  80\n",
      "5.8 action:  [0.0045, -0.9988] n_targets:  1 reward:  80\n",
      "7.0 action:  [-0.0002, -0.9985] n_targets:  1 reward:  80\n",
      "8.8 action:  [-0.0047, -0.9965] n_targets:  1 reward:  80\n",
      "11.1 action:  [-0.001, -0.9994] n_targets:  1 reward:  80\n",
      "16.2 action:  [0.0082, -0.9904] n_targets:  2 reward:  160\n",
      "18.3 action:  [0.0021, -0.9989] n_targets:  1 reward:  80\n",
      "25.8 action:  [-0.0, -0.9993] n_targets:  1 reward:  80\n",
      "29.4 action:  [0.0059, -0.9859] n_targets:  1 reward:  80\n",
      "30.1 action:  [-0.0074, -0.9995] n_targets:  1 reward:  80\n",
      "31.6 action:  [0.0013, -0.9974] n_targets:  1 reward:  80\n",
      "33.5 action:  [0.0007, -0.991] n_targets:  1 reward:  80\n",
      "37.0 action:  [-0.0037, -0.9991] n_targets:  1 reward:  80\n",
      "43.6 action:  [0.0073, -0.9826] n_targets:  1 reward:  80\n",
      "46.0 action:  [-0.0063, -0.9816] n_targets:  1 reward:  80\n",
      "50.5 action:  [0.0017, -0.9905] n_targets:  1 reward:  80\n",
      "58.1 action:  [-0.0008, -0.9969] n_targets:  1 reward:  80\n",
      "64.7 action:  [-0.0021, -0.9917] n_targets:  1 reward:  80\n",
      "66.0 action:  [-0.0023, -0.9941] n_targets:  1 reward:  80\n",
      "69.6 action:  [-0.0063, -0.9992] n_targets:  1 reward:  80\n",
      "72.7 action:  [-0.0092, -0.999] n_targets:  1 reward:  80\n",
      "74.7 action:  [-0.0072, -0.9996] n_targets:  1 reward:  80\n",
      "76.2 action:  [-0.0046, -0.989] n_targets:  1 reward:  80\n",
      "77.9 action:  [0.0073, -0.9988] n_targets:  1 reward:  80\n",
      "78.9 action:  [-0.008, -0.995] n_targets:  1 reward:  80\n",
      "81.7 action:  [0.0093, -0.9957] n_targets:  1 reward:  80\n",
      "88.6 action:  [0.0085, -0.9932] n_targets:  1 reward:  80\n",
      "90.4 action:  [-0.0, -0.9991] n_targets:  1 reward:  80\n",
      "96.2 action:  [-0.0099, -0.9994] n_targets:  2 reward:  160\n",
      "98.6 action:  [0.0081, -0.9955] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  34 ff for 102.49999999999845 s: -------------------> 0.33\n",
      "Total reward for the episode:  2720.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  383\n",
      "5.3 action:  [-0.0009, -0.9959] n_targets:  1 reward:  80\n",
      "7.8 action:  [-0.0032, -0.9931] n_targets:  1 reward:  80\n",
      "16.0 action:  [0.0031, -0.9902] n_targets:  1 reward:  80\n",
      "19.2 action:  [-0.0081, -0.9987] n_targets:  1 reward:  80\n",
      "22.5 action:  [0.0029, -0.984] n_targets:  1 reward:  80\n",
      "26.5 action:  [0.0056, -0.9933] n_targets:  2 reward:  160\n",
      "33.3 action:  [0.0003, -0.9984] n_targets:  1 reward:  80\n",
      "34.9 action:  [-0.006, -0.9992] n_targets:  1 reward:  80\n",
      "39.6 action:  [0.0071, -0.9945] n_targets:  1 reward:  80\n",
      "42.0 action:  [0.0059, -0.9811] n_targets:  1 reward:  80\n",
      "43.5 action:  [-0.0076, -0.9869] n_targets:  1 reward:  80\n",
      "48.4 action:  [0.0098, -0.9993] n_targets:  1 reward:  80\n",
      "51.2 action:  [-0.0049, -0.9939] n_targets:  1 reward:  80\n",
      "53.4 action:  [0.0063, -0.9891] n_targets:  1 reward:  80\n",
      "61.1 action:  [0.0079, -0.9843] n_targets:  1 reward:  80\n",
      "65.2 action:  [0.0056, -0.9921] n_targets:  1 reward:  80\n",
      "67.3 action:  [-0.0012, -0.9983] n_targets:  1 reward:  80\n",
      "72.4 action:  [-0.0032, -0.9881] n_targets:  1 reward:  80\n",
      "76.5 action:  [-0.001, -0.9816] n_targets:  1 reward:  80\n",
      "78.2 action:  [-0.0045, -0.9978] n_targets:  1 reward:  80\n",
      "83.4 action:  [0.0005, -0.9923] n_targets:  1 reward:  80\n",
      "88.8 action:  [0.0085, -0.9986] n_targets:  3 reward:  240\n",
      "91.4 action:  [-0.002, -0.9968] n_targets:  2 reward:  160\n",
      "99.3 action:  [0.0095, -0.9914] n_targets:  1 reward:  80\n",
      "101.0 action:  [0.0033, -0.9967] n_targets:  2 reward:  160\n",
      "102.1 action:  [0.004, -0.9925] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  31 ff for 102.49999999999845 s: -------------------> 0.3\n",
      "Total reward for the episode:  2480.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  384\n",
      "4.8 action:  [-0.0004, -0.9989] n_targets:  1 reward:  80\n",
      "6.0 action:  [0.0038, -0.9808] n_targets:  2 reward:  160\n",
      "9.7 action:  [0.0052, -0.9993] n_targets:  1 reward:  80\n",
      "14.7 action:  [-0.0041, -0.9979] n_targets:  2 reward:  160\n",
      "22.1 action:  [0.0034, -0.9985] n_targets:  1 reward:  80\n",
      "23.3 action:  [-0.0063, -0.9996] n_targets:  2 reward:  160\n",
      "27.4 action:  [0.0009, -0.9892] n_targets:  1 reward:  80\n",
      "33.9 action:  [0.0051, -0.9994] n_targets:  1 reward:  80\n",
      "37.8 action:  [-0.0066, -0.9878] n_targets:  1 reward:  80\n",
      "39.3 action:  [-0.009, -0.9941] n_targets:  1 reward:  80\n",
      "43.2 action:  [-0.0058, -0.9917] n_targets:  2 reward:  160\n",
      "46.8 action:  [-0.0098, -0.9992] n_targets:  1 reward:  80\n",
      "47.6 action:  [-0.002, -0.9894] n_targets:  1 reward:  80\n",
      "48.9 action:  [-0.0032, -0.9889] n_targets:  1 reward:  80\n",
      "54.1 action:  [0.006, -0.995] n_targets:  1 reward:  80\n",
      "58.8 action:  [0.0021, -0.9932] n_targets:  1 reward:  80\n",
      "64.9 action:  [0.0093, -0.9986] n_targets:  1 reward:  80\n",
      "67.4 action:  [-0.0009, -0.9988] n_targets:  1 reward:  80\n",
      "72.3 action:  [0.0003, -0.9846] n_targets:  1 reward:  80\n",
      "74.7 action:  [-0.0072, -0.9959] n_targets:  1 reward:  80\n",
      "77.7 action:  [-0.0058, -0.9993] n_targets:  1 reward:  80\n",
      "79.6 action:  [-0.0064, -0.9998] n_targets:  1 reward:  80\n",
      "81.1 action:  [-0.0039, -0.9968] n_targets:  1 reward:  80\n",
      "85.4 action:  [-0.0034, -0.999] n_targets:  1 reward:  80\n",
      "86.9 action:  [-0.0003, -0.9901] n_targets:  1 reward:  80\n",
      "89.0 action:  [-0.0018, -0.9964] n_targets:  1 reward:  80\n",
      "92.0 action:  [0.0081, -0.9938] n_targets:  1 reward:  80\n",
      "93.1 action:  [0.0034, -0.9958] n_targets:  1 reward:  80\n",
      "94.9 action:  [0.0036, -0.9956] n_targets:  1 reward:  80\n",
      "101.4 action:  [-0.0085, -0.9984] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  34 ff for 102.49999999999845 s: -------------------> 0.33\n",
      "Total reward for the episode:  2720.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  385\n",
      "6.9 action:  [-0.0094, -0.9871] n_targets:  2 reward:  160\n",
      "13.9 action:  [0.0043, -0.9899] n_targets:  2 reward:  160\n",
      "17.7 action:  [-0.0034, -0.9814] n_targets:  1 reward:  80\n",
      "18.8 action:  [0.0017, -0.9888] n_targets:  1 reward:  80\n",
      "24.2 action:  [0.0082, -0.9896] n_targets:  1 reward:  80\n",
      "26.0 action:  [-0.0026, -0.9984] n_targets:  1 reward:  80\n",
      "27.6 action:  [-0.0021, -0.9981] n_targets:  1 reward:  80\n",
      "28.4 action:  [-0.0075, -0.9902] n_targets:  1 reward:  80\n",
      "29.2 action:  [-0.0046, -0.994] n_targets:  1 reward:  80\n",
      "29.9 action:  [0.0052, -0.9996] n_targets:  1 reward:  80\n",
      "30.5 action:  [-0.0085, -0.9818] n_targets:  1 reward:  80\n",
      "30.8 action:  [-0.0059, -0.9977] n_targets:  1 reward:  80\n",
      "33.3 action:  [0.0054, -0.9975] n_targets:  1 reward:  80\n",
      "35.0 action:  [-0.0021, -0.9945] n_targets:  1 reward:  80\n",
      "38.9 action:  [-0.0019, -0.9999] n_targets:  2 reward:  160\n",
      "43.5 action:  [0.0017, -0.9944] n_targets:  2 reward:  160\n",
      "44.2 action:  [0.0013, -0.9993] n_targets:  1 reward:  80\n",
      "46.5 action:  [-0.0081, -0.9853] n_targets:  1 reward:  80\n",
      "49.3 action:  [-0.0076, -0.9899] n_targets:  1 reward:  80\n",
      "50.1 action:  [-0.0064, -0.9978] n_targets:  1 reward:  80\n",
      "53.4 action:  [0.0009, -0.9854] n_targets:  1 reward:  80\n",
      "55.4 action:  [-0.0082, -0.9857] n_targets:  1 reward:  80\n",
      "68.1 action:  [0.0059, -0.9975] n_targets:  2 reward:  160\n",
      "69.9 action:  [0.0074, -0.9967] n_targets:  2 reward:  160\n",
      "72.6 action:  [0.01, -0.9959] n_targets:  1 reward:  80\n",
      "TIME before resetting: 77.49999999999987\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  386\n",
      "3.4 action:  [0.0069, -0.9884] n_targets:  2 reward:  160\n",
      "10.5 action:  [-0.005, -0.9991] n_targets:  1 reward:  80\n",
      "12.1 action:  [0.0006, -0.9968] n_targets:  1 reward:  80\n",
      "14.1 action:  [0.0031, -0.9987] n_targets:  1 reward:  80\n",
      "17.9 action:  [-0.005, -0.9804] n_targets:  1 reward:  80\n",
      "25.1 action:  [0.0034, -0.9918] n_targets:  1 reward:  80\n",
      "29.6 action:  [-0.0009, -0.9857] n_targets:  1 reward:  80\n",
      "30.8 action:  [-0.0086, -0.9953] n_targets:  2 reward:  160\n",
      "35.7 action:  [0.008, -0.9931] n_targets:  2 reward:  160\n",
      "40.0 action:  [0.0007, -0.9837] n_targets:  1 reward:  80\n",
      "49.2 action:  [-0.0025, -0.9987] n_targets:  2 reward:  160\n",
      "54.0 action:  [0.0041, -0.9916] n_targets:  1 reward:  80\n",
      "56.9 action:  [-0.0058, -0.9982] n_targets:  1 reward:  80\n",
      "62.0 action:  [0.0037, -0.9998] n_targets:  1 reward:  80\n",
      "63.7 action:  [-0.0093, -0.9984] n_targets:  1 reward:  80\n",
      "66.3 action:  [-0.0087, -0.9955] n_targets:  1 reward:  80\n",
      "68.3 action:  [-0.0033, -0.997] n_targets:  1 reward:  80\n",
      "73.6 action:  [0.009, -0.995] n_targets:  1 reward:  80\n",
      "75.8 action:  [0.0017, -0.9849] n_targets:  1 reward:  80\n",
      "77.1 action:  [0.0008, -0.999] n_targets:  1 reward:  80\n",
      "80.3 action:  [-0.0009, -0.9948] n_targets:  2 reward:  160\n",
      "81.3 action:  [-0.0055, -0.9986] n_targets:  1 reward:  80\n",
      "82.1 action:  [-0.0064, -0.9987] n_targets:  2 reward:  160\n",
      "83.3 action:  [-0.0067, -0.9962] n_targets:  1 reward:  80\n",
      "89.5 action:  [-0.0029, -0.9913] n_targets:  2 reward:  160\n",
      "93.6 action:  [-0.0043, -0.9968] n_targets:  1 reward:  80\n",
      "97.2 action:  [-0.0094, -0.9894] n_targets:  1 reward:  80\n",
      "98.8 action:  [-0.0064, -0.9985] n_targets:  1 reward:  80\n",
      "101.6 action:  [-0.006, -0.9977] n_targets:  2 reward:  160\n",
      "Firely capture rate for the episode:  37 ff for 102.49999999999845 s: -------------------> 0.36\n",
      "Total reward for the episode:  2960.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  387\n",
      "5.5 action:  [0.0063, -0.9979] n_targets:  1 reward:  80\n",
      "6.1 action:  [0.0081, -0.9963] n_targets:  1 reward:  80\n",
      "9.1 action:  [-0.0004, -0.9966] n_targets:  1 reward:  80\n",
      "14.8 action:  [-0.0079, -0.9921] n_targets:  1 reward:  80\n",
      "17.3 action:  [-0.0016, -0.9811] n_targets:  1 reward:  80\n",
      "18.0 action:  [-0.0013, -0.9876] n_targets:  1 reward:  80\n",
      "23.4 action:  [-0.0079, -0.9961] n_targets:  1 reward:  80\n",
      "24.0 action:  [0.0099, -0.9817] n_targets:  1 reward:  80\n",
      "25.1 action:  [0.003, -0.9804] n_targets:  1 reward:  80\n",
      "35.7 action:  [-0.004, -0.9937] n_targets:  1 reward:  80\n",
      "37.7 action:  [-0.0074, -0.9984] n_targets:  2 reward:  160\n",
      "41.3 action:  [0.0036, -0.9837] n_targets:  1 reward:  80\n",
      "42.6 action:  [-0.0077, -0.9932] n_targets:  1 reward:  80\n",
      "45.5 action:  [-0.0071, -0.9975] n_targets:  1 reward:  80\n",
      "50.3 action:  [0.0036, -0.9966] n_targets:  1 reward:  80\n",
      "51.8 action:  [-0.0029, -0.9948] n_targets:  1 reward:  80\n",
      "54.1 action:  [-0.0001, -0.9918] n_targets:  1 reward:  80\n",
      "57.7 action:  [-0.0064, -0.9985] n_targets:  1 reward:  80\n",
      "63.7 action:  [0.0097, -0.9929] n_targets:  1 reward:  80\n",
      "67.7 action:  [-0.0019, -0.9895] n_targets:  2 reward:  160\n",
      "71.4 action:  [-0.0093, -0.9967] n_targets:  1 reward:  80\n",
      "73.6 action:  [0.0022, -0.9974] n_targets:  1 reward:  80\n",
      "79.4 action:  [-0.0061, -0.9931] n_targets:  1 reward:  80\n",
      "80.3 action:  [-0.005, -0.999] n_targets:  1 reward:  80\n",
      "83.6 action:  [0.006, -0.9925] n_targets:  1 reward:  80\n",
      "84.4 action:  [0.0048, -0.9924] n_targets:  1 reward:  80\n",
      "89.8 action:  [-0.0098, -0.9912] n_targets:  1 reward:  80\n",
      "92.1 action:  [0.0022, -0.9915] n_targets:  1 reward:  80\n",
      "101.1 action:  [0.0028, -0.9916] n_targets:  1 reward:  80\n",
      "102.5 action:  [-0.0026, -0.9992] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  32 ff for 102.49999999999845 s: -------------------> 0.31\n",
      "Total reward for the episode:  2560.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  388\n",
      "1.7 action:  [0.0044, -0.9989] n_targets:  1 reward:  80\n",
      "4.5 action:  [-0.0092, -0.9962] n_targets:  1 reward:  80\n",
      "7.3 action:  [0.0087, -0.9968] n_targets:  1 reward:  80\n",
      "10.7 action:  [-0.01, -0.9854] n_targets:  1 reward:  80\n",
      "12.4 action:  [-0.0043, -0.9918] n_targets:  1 reward:  80\n",
      "17.4 action:  [-0.0098, -0.9997] n_targets:  1 reward:  80\n",
      "19.0 action:  [-0.0002, -0.9958] n_targets:  1 reward:  80\n",
      "21.2 action:  [-0.0084, -0.9963] n_targets:  1 reward:  80\n",
      "23.0 action:  [0.0097, -0.9829] n_targets:  1 reward:  80\n",
      "23.3 action:  [0.003, -0.9907] n_targets:  1 reward:  80\n",
      "23.7 action:  [-0.0005, -0.9939] n_targets:  1 reward:  80\n",
      "26.7 action:  [-0.0096, -0.9994] n_targets:  1 reward:  80\n",
      "32.6 action:  [-0.0024, -0.9895] n_targets:  1 reward:  80\n",
      "39.9 action:  [-0.0054, -0.9992] n_targets:  1 reward:  80\n",
      "43.5 action:  [0.0062, -0.9934] n_targets:  1 reward:  80\n",
      "45.4 action:  [0.0023, -0.9967] n_targets:  1 reward:  80\n",
      "47.1 action:  [0.0071, -0.9946] n_targets:  1 reward:  80\n",
      "55.5 action:  [-0.0039, -0.9989] n_targets:  1 reward:  80\n",
      "58.6 action:  [0.008, -0.9971] n_targets:  1 reward:  80\n",
      "61.1 action:  [-0.0084, -0.9954] n_targets:  1 reward:  80\n",
      "66.6 action:  [0.0048, -0.9865] n_targets:  1 reward:  80\n",
      "69.9 action:  [0.007, -0.9945] n_targets:  1 reward:  80\n",
      "71.0 action:  [-0.0031, -0.9914] n_targets:  1 reward:  80\n",
      "77.8 action:  [-0.0046, -0.9906] n_targets:  2 reward:  160\n",
      "84.1 action:  [0.0016, -0.9856] n_targets:  2 reward:  160\n",
      "84.7 action:  [0.0083, -0.9842] n_targets:  1 reward:  80\n",
      "84.9 action:  [-0.0017, -0.9873] n_targets:  1 reward:  80\n",
      "86.7 action:  [0.0026, -0.9914] n_targets:  1 reward:  80\n",
      "89.4 action:  [-0.0085, -0.9913] n_targets:  1 reward:  80\n",
      "90.4 action:  [-0.0008, -0.9955] n_targets:  1 reward:  80\n",
      "92.0 action:  [0.0018, -0.9903] n_targets:  1 reward:  80\n",
      "100.1 action:  [-0.0044, -0.9872] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  34 ff for 102.49999999999845 s: -------------------> 0.33\n",
      "Total reward for the episode:  2720.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  389\n",
      "Eval num_timesteps=150000, episode_reward=2746.67 +/- 164.38\n",
      "Episode length: 1025.00 +/- 0.00\n",
      "New best mean reward!\n",
      "3.4 action:  [0.0042, -0.9821] n_targets:  1 reward:  80\n",
      "10.6 action:  [0.0012, -0.9985] n_targets:  1 reward:  80\n",
      "12.6 action:  [-0.0003, -0.9987] n_targets:  1 reward:  80\n",
      "16.0 action:  [0.0087, -0.9971] n_targets:  2 reward:  160\n",
      "31.3 action:  [-0.0099, -0.9961] n_targets:  1 reward:  80\n",
      "35.3 action:  [-0.0097, -0.9965] n_targets:  1 reward:  80\n",
      "37.9 action:  [0.0047, -0.9819] n_targets:  1 reward:  80\n",
      "40.4 action:  [0.0046, -0.9837] n_targets:  1 reward:  80\n",
      "41.8 action:  [-0.0053, -0.9967] n_targets:  1 reward:  80\n",
      "46.1 action:  [0.0047, -0.9974] n_targets:  1 reward:  80\n",
      "60.1 action:  [-0.0029, -0.9961] n_targets:  1 reward:  80\n",
      "65.6 action:  [-0.0028, -0.9982] n_targets:  2 reward:  160\n",
      "71.1 action:  [-0.0017, -0.9958] n_targets:  1 reward:  80\n",
      "71.8 action:  [0.0082, -0.9987] n_targets:  1 reward:  80\n",
      "74.1 action:  [0.0024, -0.9821] n_targets:  1 reward:  80\n",
      "79.6 action:  [-0.0008, -0.9929] n_targets:  1 reward:  80\n",
      "83.5 action:  [0.0066, -0.9801] n_targets:  1 reward:  80\n",
      "89.5 action:  [-0.0018, -0.9993] n_targets:  1 reward:  80\n",
      "95.6 action:  [-0.0021, -0.9812] n_targets:  1 reward:  80\n",
      "96.3 action:  [0.0094, -0.998] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  22 ff for 102.49999999999845 s: -------------------> 0.21\n",
      "Total reward for the episode:  1760.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  390\n",
      "3.9 action:  [0.0027, -0.9988] n_targets:  2 reward:  160\n",
      "6.2 action:  [0.0037, -0.9962] n_targets:  1 reward:  80\n",
      "10.3 action:  [-0.0058, -0.9978] n_targets:  1 reward:  80\n",
      "15.2 action:  [0.0016, -0.9939] n_targets:  1 reward:  80\n",
      "16.9 action:  [-0.0018, -0.9924] n_targets:  1 reward:  80\n",
      "21.5 action:  [-0.0039, -0.9821] n_targets:  1 reward:  80\n",
      "23.1 action:  [0.0024, -0.9965] n_targets:  1 reward:  80\n",
      "23.9 action:  [-0.007, -0.9989] n_targets:  1 reward:  80\n",
      "27.9 action:  [0.006, -0.9928] n_targets:  1 reward:  80\n",
      "33.1 action:  [0.0056, -0.9977] n_targets:  1 reward:  80\n",
      "38.7 action:  [0.0066, -0.9894] n_targets:  1 reward:  80\n",
      "42.6 action:  [-0.0085, -0.9975] n_targets:  1 reward:  80\n",
      "48.8 action:  [-0.0041, -0.9988] n_targets:  1 reward:  80\n",
      "49.9 action:  [0.0023, -0.9942] n_targets:  1 reward:  80\n",
      "57.4 action:  [-0.0044, -0.9874] n_targets:  1 reward:  80\n",
      "57.9 action:  [-0.0029, -0.9886] n_targets:  1 reward:  80\n",
      "63.8 action:  [-0.0089, -0.994] n_targets:  1 reward:  80\n",
      "66.1 action:  [-0.0022, -0.9976] n_targets:  1 reward:  80\n",
      "72.5 action:  [-0.0066, -0.9864] n_targets:  1 reward:  80\n",
      "74.4 action:  [-0.0016, -0.9906] n_targets:  1 reward:  80\n",
      "77.9 action:  [0.0056, -0.998] n_targets:  1 reward:  80\n",
      "78.8 action:  [0.0014, -0.9996] n_targets:  1 reward:  80\n",
      "79.7 action:  [-0.0048, -0.9841] n_targets:  1 reward:  80\n",
      "86.3 action:  [-0.0081, -0.9989] n_targets:  1 reward:  80\n",
      "92.6 action:  [-0.0059, -0.9992] n_targets:  1 reward:  80\n",
      "94.0 action:  [0.0064, -0.9989] n_targets:  1 reward:  80\n",
      "97.6 action:  [-0.0005, -0.9989] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  28 ff for 102.49999999999845 s: -------------------> 0.27\n",
      "Total reward for the episode:  2240.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  391\n",
      "1.4 action:  [-0.0046, -0.9884] n_targets:  1 reward:  80\n",
      "17.2 action:  [-0.0077, -0.9883] n_targets:  1 reward:  80\n",
      "19.8 action:  [-0.0033, -0.9979] n_targets:  1 reward:  80\n",
      "21.2 action:  [-0.0007, -0.9986] n_targets:  2 reward:  160\n",
      "23.0 action:  [0.0094, -0.9863] n_targets:  1 reward:  80\n",
      "26.2 action:  [0.0096, -0.9966] n_targets:  1 reward:  80\n",
      "31.1 action:  [0.0017, -0.998] n_targets:  1 reward:  80\n",
      "33.7 action:  [-0.0039, -0.9992] n_targets:  1 reward:  80\n",
      "34.9 action:  [-0.0089, -0.9883] n_targets:  1 reward:  80\n",
      "36.2 action:  [-0.0044, -0.9976] n_targets:  2 reward:  160\n",
      "38.1 action:  [-0.0075, -0.9997] n_targets:  1 reward:  80\n",
      "45.9 action:  [-0.0019, -0.9987] n_targets:  1 reward:  80\n",
      "48.8 action:  [-0.0038, -0.9919] n_targets:  1 reward:  80\n",
      "55.2 action:  [-0.0066, -0.9996] n_targets:  1 reward:  80\n",
      "58.9 action:  [0.002, -0.9989] n_targets:  3 reward:  240\n",
      "63.2 action:  [-0.0037, -0.9991] n_targets:  1 reward:  80\n",
      "65.8 action:  [0.001, -0.9823] n_targets:  1 reward:  80\n",
      "68.1 action:  [-0.0088, -0.9984] n_targets:  1 reward:  80\n",
      "73.3 action:  [-0.0054, -0.9925] n_targets:  1 reward:  80\n",
      "79.6 action:  [0.0019, -0.9943] n_targets:  1 reward:  80\n",
      "87.3 action:  [-0.0033, -0.9993] n_targets:  1 reward:  80\n",
      "89.6 action:  [-0.0061, -0.9988] n_targets:  1 reward:  80\n",
      "92.0 action:  [-0.0057, -0.9897] n_targets:  1 reward:  80\n",
      "95.2 action:  [0.006, -0.9996] n_targets:  1 reward:  80\n",
      "97.4 action:  [-0.0061, -0.9839] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  29 ff for 102.49999999999845 s: -------------------> 0.28\n",
      "Total reward for the episode:  2320.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  392\n",
      "3.4 action:  [0.0036, -0.9853] n_targets:  1 reward:  80\n",
      "3.8 action:  [-0.0088, -0.9998] n_targets:  1 reward:  80\n",
      "5.4 action:  [-0.0021, -0.9985] n_targets:  1 reward:  80\n",
      "8.8 action:  [0.002, -0.9965] n_targets:  2 reward:  160\n",
      "10.6 action:  [-0.0099, -0.9947] n_targets:  2 reward:  160\n",
      "12.7 action:  [-0.0036, -0.9989] n_targets:  2 reward:  160\n",
      "16.0 action:  [-0.0092, -0.9939] n_targets:  1 reward:  80\n",
      "19.2 action:  [-0.0013, -0.9876] n_targets:  1 reward:  80\n",
      "21.1 action:  [0.0023, -0.9927] n_targets:  1 reward:  80\n",
      "22.5 action:  [0.0037, -0.9986] n_targets:  1 reward:  80\n",
      "24.4 action:  [0.0065, -0.9968] n_targets:  1 reward:  80\n",
      "28.5 action:  [-0.007, -0.9868] n_targets:  1 reward:  80\n",
      "28.8 action:  [0.0048, -0.9962] n_targets:  1 reward:  80\n",
      "35.5 action:  [-0.0037, -0.9923] n_targets:  1 reward:  80\n",
      "40.0 action:  [-0.0082, -0.99] n_targets:  1 reward:  80\n",
      "43.6 action:  [-0.0067, -0.9984] n_targets:  2 reward:  160\n",
      "45.6 action:  [0.0069, -0.9831] n_targets:  1 reward:  80\n",
      "53.0 action:  [-0.0065, -0.9961] n_targets:  1 reward:  80\n",
      "55.3 action:  [0.0021, -0.9949] n_targets:  2 reward:  160\n",
      "57.8 action:  [0.0084, -0.996] n_targets:  1 reward:  80\n",
      "58.9 action:  [-0.0005, -0.9995] n_targets:  1 reward:  80\n",
      "61.3 action:  [-0.0021, -0.9914] n_targets:  1 reward:  80\n",
      "63.9 action:  [0.002, -0.9961] n_targets:  2 reward:  160\n",
      "68.9 action:  [0.0022, -0.9939] n_targets:  1 reward:  80\n",
      "74.8 action:  [-0.0033, -0.9989] n_targets:  1 reward:  80\n",
      "76.6 action:  [-0.0088, -0.998] n_targets:  1 reward:  80\n",
      "78.3 action:  [0.0024, -0.9926] n_targets:  1 reward:  80\n",
      "79.7 action:  [0.0007, -0.9984] n_targets:  1 reward:  80\n",
      "90.6 action:  [0.0016, -0.9994] n_targets:  1 reward:  80\n",
      "94.9 action:  [-0.0026, -0.9997] n_targets:  1 reward:  80\n",
      "97.1 action:  [-0.009, -0.9994] n_targets:  1 reward:  80\n",
      "97.9 action:  [-0.008, -0.9928] n_targets:  1 reward:  80\n",
      "100.7 action:  [-0.0018, -0.9998] n_targets:  1 reward:  80\n",
      "101.4 action:  [0.0081, -0.9987] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  40 ff for 102.49999999999845 s: -------------------> 0.39\n",
      "Total reward for the episode:  3200.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  393\n",
      "2.4 action:  [-0.0008, -0.9934] n_targets:  1 reward:  80\n",
      "5.0 action:  [0.0012, -0.9819] n_targets:  1 reward:  80\n",
      "8.6 action:  [0.0088, -0.9987] n_targets:  1 reward:  80\n",
      "10.1 action:  [-0.0044, -0.9856] n_targets:  1 reward:  80\n",
      "17.2 action:  [-0.0078, -0.9853] n_targets:  1 reward:  80\n",
      "24.0 action:  [-0.0007, -0.9939] n_targets:  1 reward:  80\n",
      "32.0 action:  [-0.0014, -0.9877] n_targets:  1 reward:  80\n",
      "35.7 action:  [0.0011, -0.9904] n_targets:  1 reward:  80\n",
      "36.7 action:  [0.0022, -0.9814] n_targets:  1 reward:  80\n",
      "38.1 action:  [-0.0036, -0.9966] n_targets:  1 reward:  80\n",
      "42.2 action:  [-0.0074, -0.9991] n_targets:  1 reward:  80\n",
      "44.7 action:  [-0.0026, -0.9975] n_targets:  2 reward:  160\n",
      "47.6 action:  [-0.0052, -0.9852] n_targets:  1 reward:  80\n",
      "49.0 action:  [-0.0053, -0.9987] n_targets:  1 reward:  80\n",
      "50.3 action:  [-0.003, -0.9983] n_targets:  1 reward:  80\n",
      "52.8 action:  [-0.0094, -0.9975] n_targets:  1 reward:  80\n",
      "60.8 action:  [-0.0037, -0.9887] n_targets:  1 reward:  80\n",
      "61.6 action:  [-0.0056, -0.9983] n_targets:  2 reward:  160\n",
      "65.5 action:  [0.0014, -0.997] n_targets:  1 reward:  80\n",
      "68.5 action:  [0.0019, -0.9974] n_targets:  1 reward:  80\n",
      "70.0 action:  [-0.0092, -0.9825] n_targets:  1 reward:  80\n",
      "75.7 action:  [-0.0065, -0.9997] n_targets:  1 reward:  80\n",
      "81.4 action:  [-0.0035, -0.9935] n_targets:  1 reward:  80\n",
      "89.8 action:  [0.005, -0.9972] n_targets:  1 reward:  80\n",
      "94.9 action:  [-0.0056, -0.9862] n_targets:  2 reward:  160\n",
      "95.7 action:  [-0.0079, -0.9961] n_targets:  2 reward:  160\n",
      "99.1 action:  [-0.0082, -0.9941] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  31 ff for 102.49999999999845 s: -------------------> 0.3\n",
      "Total reward for the episode:  2480.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  394\n",
      "3.5 action:  [-0.0077, -0.9921] n_targets:  1 reward:  80\n",
      "6.3 action:  [0.0086, -0.9976] n_targets:  1 reward:  80\n",
      "14.2 action:  [0.002, -0.9817] n_targets:  1 reward:  80\n",
      "19.1 action:  [0.0088, -0.9842] n_targets:  1 reward:  80\n",
      "22.0 action:  [0.0042, -0.9912] n_targets:  1 reward:  80\n",
      "25.1 action:  [-0.0086, -0.9968] n_targets:  1 reward:  80\n",
      "31.4 action:  [0.0005, -0.9912] n_targets:  1 reward:  80\n",
      "32.8 action:  [-0.0045, -0.9851] n_targets:  2 reward:  160\n",
      "36.9 action:  [-0.0047, -0.998] n_targets:  1 reward:  80\n",
      "39.2 action:  [0.0055, -0.9856] n_targets:  1 reward:  80\n",
      "42.2 action:  [0.0084, -0.9982] n_targets:  1 reward:  80\n",
      "49.5 action:  [-0.0095, -0.9861] n_targets:  1 reward:  80\n",
      "63.4 action:  [0.0068, -0.9987] n_targets:  1 reward:  80\n",
      "66.4 action:  [0.0053, -0.9951] n_targets:  2 reward:  160\n",
      "67.4 action:  [0.0038, -0.9979] n_targets:  1 reward:  80\n",
      "76.5 action:  [-0.0064, -0.9974] n_targets:  1 reward:  80\n",
      "96.0 action:  [0.0014, -0.9959] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  19 ff for 102.49999999999845 s: -------------------> 0.19\n",
      "Total reward for the episode:  1520.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  395\n",
      "3.2 action:  [0.0002, -0.9808] n_targets:  2 reward:  160\n",
      "4.3 action:  [0.0005, -0.9961] n_targets:  1 reward:  80\n",
      "5.8 action:  [0.0054, -0.9956] n_targets:  1 reward:  80\n",
      "6.8 action:  [0.0089, -0.9993] n_targets:  1 reward:  80\n",
      "7.8 action:  [0.0043, -0.999] n_targets:  1 reward:  80\n",
      "14.4 action:  [0.0039, -0.9972] n_targets:  1 reward:  80\n",
      "16.7 action:  [-0.0088, -0.989] n_targets:  1 reward:  80\n",
      "18.2 action:  [0.0095, -0.995] n_targets:  1 reward:  80\n",
      "19.9 action:  [-0.0078, -0.987] n_targets:  1 reward:  80\n",
      "22.4 action:  [-0.0085, -0.9925] n_targets:  1 reward:  80\n",
      "23.4 action:  [0.006, -0.9981] n_targets:  1 reward:  80\n",
      "27.6 action:  [-0.0015, -0.9994] n_targets:  1 reward:  80\n",
      "30.1 action:  [0.0014, -0.9801] n_targets:  2 reward:  160\n",
      "34.9 action:  [-0.0043, -0.9946] n_targets:  1 reward:  80\n",
      "39.4 action:  [-0.0057, -0.9965] n_targets:  1 reward:  80\n",
      "42.5 action:  [-0.0088, -0.9985] n_targets:  1 reward:  80\n",
      "48.2 action:  [-0.0053, -0.9866] n_targets:  1 reward:  80\n",
      "49.2 action:  [0.0095, -0.9956] n_targets:  1 reward:  80\n",
      "51.1 action:  [0.0098, -0.997] n_targets:  1 reward:  80\n",
      "53.5 action:  [0.003, -0.9851] n_targets:  1 reward:  80\n",
      "56.3 action:  [-0.008, -0.9876] n_targets:  1 reward:  80\n",
      "58.6 action:  [0.0092, -0.9994] n_targets:  1 reward:  80\n",
      "64.4 action:  [-0.0068, -0.9987] n_targets:  1 reward:  80\n",
      "65.3 action:  [-0.0014, -0.9833] n_targets:  1 reward:  80\n",
      "67.9 action:  [-0.0065, -0.9953] n_targets:  1 reward:  80\n",
      "74.4 action:  [-0.0053, -0.9989] n_targets:  1 reward:  80\n",
      "79.6 action:  [0.0059, -0.994] n_targets:  1 reward:  80\n",
      "80.8 action:  [-0.0015, -0.9999] n_targets:  1 reward:  80\n",
      "86.0 action:  [-0.0006, -0.9873] n_targets:  1 reward:  80\n",
      "87.2 action:  [0.0075, -0.9895] n_targets:  1 reward:  80\n",
      "92.7 action:  [0.0049, -0.9919] n_targets:  2 reward:  160\n",
      "97.6 action:  [0.0039, -0.9942] n_targets:  1 reward:  80\n",
      "101.0 action:  [-0.0008, -0.9991] n_targets:  2 reward:  160\n",
      "Firely capture rate for the episode:  37 ff for 102.49999999999845 s: -------------------> 0.36\n",
      "Total reward for the episode:  2960.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  396\n",
      "3.4 action:  [-0.003, -0.9837] n_targets:  2 reward:  160\n",
      "4.7 action:  [-0.0072, -0.9933] n_targets:  1 reward:  80\n",
      "7.0 action:  [-0.0088, -0.9939] n_targets:  1 reward:  80\n",
      "11.6 action:  [0.0011, -0.9979] n_targets:  1 reward:  80\n",
      "12.8 action:  [0.009, -0.9959] n_targets:  1 reward:  80\n",
      "16.9 action:  [-0.0087, -0.9934] n_targets:  1 reward:  80\n",
      "20.7 action:  [-0.0025, -0.9903] n_targets:  1 reward:  80\n",
      "22.5 action:  [0.0087, -0.9934] n_targets:  1 reward:  80\n",
      "32.7 action:  [-0.0046, -0.9976] n_targets:  1 reward:  80\n",
      "34.8 action:  [0.009, -0.9977] n_targets:  1 reward:  80\n",
      "36.6 action:  [0.0081, -0.9958] n_targets:  1 reward:  80\n",
      "41.7 action:  [0.0095, -0.9988] n_targets:  2 reward:  160\n",
      "43.4 action:  [-0.0073, -0.9977] n_targets:  1 reward:  80\n",
      "45.1 action:  [-0.0059, -0.991] n_targets:  1 reward:  80\n",
      "47.2 action:  [0.0038, -0.9928] n_targets:  1 reward:  80\n",
      "52.0 action:  [0.0098, -0.9997] n_targets:  1 reward:  80\n",
      "58.3 action:  [0.006, -0.9932] n_targets:  1 reward:  80\n",
      "63.5 action:  [0.0094, -0.9985] n_targets:  1 reward:  80\n",
      "64.5 action:  [-0.0007, -0.9992] n_targets:  1 reward:  80\n",
      "65.1 action:  [-0.0066, -0.984] n_targets:  2 reward:  160\n",
      "67.9 action:  [-0.0079, -0.9902] n_targets:  1 reward:  80\n",
      "78.2 action:  [0.0007, -0.9815] n_targets:  1 reward:  80\n",
      "80.7 action:  [-0.0029, -0.9832] n_targets:  1 reward:  80\n",
      "82.0 action:  [0.0023, -0.9983] n_targets:  1 reward:  80\n",
      "84.7 action:  [0.0083, -0.9958] n_targets:  1 reward:  80\n",
      "87.4 action:  [-0.0089, -0.9991] n_targets:  1 reward:  80\n",
      "90.9 action:  [-0.0034, -0.9986] n_targets:  1 reward:  80\n",
      "95.0 action:  [-0.0082, -0.9829] n_targets:  2 reward:  160\n",
      "Firely capture rate for the episode:  32 ff for 102.49999999999845 s: -------------------> 0.31\n",
      "Total reward for the episode:  2560.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  397\n",
      "4.6 action:  [-0.01, -0.9993] n_targets:  1 reward:  80\n",
      "7.6 action:  [-0.0047, -0.9978] n_targets:  1 reward:  80\n",
      "11.9 action:  [-0.0023, -0.9912] n_targets:  1 reward:  80\n",
      "24.0 action:  [0.0052, -0.9994] n_targets:  2 reward:  160\n",
      "27.3 action:  [-0.0078, -0.9969] n_targets:  1 reward:  80\n",
      "28.1 action:  [0.0038, -0.9957] n_targets:  1 reward:  80\n",
      "28.9 action:  [-0.0004, -0.9887] n_targets:  1 reward:  80\n",
      "30.3 action:  [-0.0006, -0.9964] n_targets:  1 reward:  80\n",
      "32.2 action:  [0.0015, -0.9889] n_targets:  1 reward:  80\n",
      "36.0 action:  [-0.0016, -0.9983] n_targets:  1 reward:  80\n",
      "37.9 action:  [-0.001, -0.99] n_targets:  1 reward:  80\n",
      "39.3 action:  [0.0092, -0.9994] n_targets:  1 reward:  80\n",
      "42.0 action:  [-0.0016, -0.9996] n_targets:  1 reward:  80\n",
      "46.7 action:  [-0.0036, -0.9907] n_targets:  1 reward:  80\n",
      "52.8 action:  [0.0021, -0.9838] n_targets:  1 reward:  80\n",
      "62.4 action:  [0.0016, -0.9982] n_targets:  2 reward:  160\n",
      "70.3 action:  [-0.0097, -0.9962] n_targets:  1 reward:  80\n",
      "72.6 action:  [-0.0016, -0.989] n_targets:  1 reward:  80\n",
      "74.1 action:  [-0.0055, -0.9994] n_targets:  1 reward:  80\n",
      "76.2 action:  [-0.0063, -0.9848] n_targets:  1 reward:  80\n",
      "79.8 action:  [-0.0024, -0.9942] n_targets:  1 reward:  80\n",
      "90.7 action:  [0.0093, -0.994] n_targets:  2 reward:  160\n",
      "93.7 action:  [0.0068, -0.9991] n_targets:  1 reward:  80\n",
      "101.9 action:  [-0.0032, -0.9972] n_targets:  2 reward:  160\n",
      "Firely capture rate for the episode:  28 ff for 102.49999999999845 s: -------------------> 0.27\n",
      "Total reward for the episode:  2240.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  398\n",
      "3.2 action:  [0.0069, -0.9983] n_targets:  1 reward:  80\n",
      "4.4 action:  [0.0093, -0.9863] n_targets:  1 reward:  80\n",
      "7.7 action:  [-0.0046, -0.9981] n_targets:  1 reward:  80\n",
      "9.6 action:  [-0.0073, -0.9979] n_targets:  1 reward:  80\n",
      "12.1 action:  [-0.0017, -0.9963] n_targets:  2 reward:  160\n",
      "15.3 action:  [0.0013, -0.9824] n_targets:  1 reward:  80\n",
      "19.1 action:  [0.0023, -0.9963] n_targets:  1 reward:  80\n",
      "26.0 action:  [0.0007, -0.9972] n_targets:  1 reward:  80\n",
      "29.0 action:  [0.0062, -0.9994] n_targets:  1 reward:  80\n",
      "32.0 action:  [-0.0033, -0.9881] n_targets:  2 reward:  160\n",
      "35.3 action:  [0.0086, -0.9989] n_targets:  1 reward:  80\n",
      "39.2 action:  [0.0041, -0.9821] n_targets:  1 reward:  80\n",
      "40.9 action:  [-0.0068, -0.9982] n_targets:  1 reward:  80\n",
      "43.3 action:  [-0.0053, -0.9998] n_targets:  1 reward:  80\n",
      "44.5 action:  [0.0007, -0.9986] n_targets:  1 reward:  80\n",
      "45.6 action:  [0.0013, -0.9897] n_targets:  2 reward:  160\n",
      "46.5 action:  [0.007, -0.9985] n_targets:  1 reward:  80\n",
      "47.9 action:  [0.0032, -0.9891] n_targets:  1 reward:  80\n",
      "52.2 action:  [-0.0041, -0.9987] n_targets:  1 reward:  80\n",
      "54.5 action:  [0.0012, -0.9985] n_targets:  2 reward:  160\n",
      "57.8 action:  [-0.0063, -0.9869] n_targets:  2 reward:  160\n",
      "63.4 action:  [-0.0063, -0.9963] n_targets:  1 reward:  80\n",
      "70.6 action:  [0.0098, -0.9982] n_targets:  1 reward:  80\n",
      "74.0 action:  [0.0097, -0.997] n_targets:  1 reward:  80\n",
      "TIME before resetting: 77.49999999999987\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  399\n",
      "21.3 action:  [-0.0053, -0.9864] n_targets:  1 reward:  80\n",
      "37.5 action:  [-0.0097, -0.9846] n_targets:  1 reward:  80\n",
      "40.4 action:  [-0.0064, -0.9861] n_targets:  1 reward:  80\n",
      "50.1 action:  [-0.001, -0.9835] n_targets:  1 reward:  80\n",
      "71.9 action:  [-0.0075, -0.9958] n_targets:  1 reward:  80\n",
      "82.5 action:  [-0.0073, -0.9878] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  6 ff for 102.49999999999845 s: -------------------> 0.06\n",
      "Total reward for the episode:  480.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  400\n",
      "6.5 action:  [0.0045, -0.987] n_targets:  1 reward:  80\n",
      "25.2 action:  [-0.0072, -0.9862] n_targets:  1 reward:  80\n",
      "26.3 action:  [-0.0075, -0.9855] n_targets:  1 reward:  80\n",
      "32.3 action:  [-0.0081, -0.9812] n_targets:  1 reward:  80\n",
      "54.2 action:  [0.0047, -0.9984] n_targets:  1 reward:  80\n",
      "58.2 action:  [-0.0099, -0.9974] n_targets:  2 reward:  160\n",
      "88.3 action:  [-0.01, -0.9905] n_targets:  1 reward:  80\n",
      "98.2 action:  [-0.0099, -0.9934] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  9 ff for 102.49999999999845 s: -------------------> 0.09\n",
      "Total reward for the episode:  720.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  401\n",
      "2.2 action:  [-0.0098, -0.9892] n_targets:  1 reward:  80\n",
      "5.5 action:  [-0.0055, -0.9942] n_targets:  1 reward:  80\n",
      "12.6 action:  [-0.008, -0.989] n_targets:  1 reward:  80\n",
      "21.3 action:  [-0.0079, -0.9834] n_targets:  1 reward:  80\n",
      "34.9 action:  [-0.0088, -0.9874] n_targets:  1 reward:  80\n",
      "40.4 action:  [-0.0089, -0.9906] n_targets:  1 reward:  80\n",
      "72.1 action:  [-0.0069, -0.9874] n_targets:  1 reward:  80\n",
      "77.5 action:  [-0.0092, -0.9923] n_targets:  1 reward:  80\n",
      "78.9 action:  [-0.0097, -0.9946] n_targets:  1 reward:  80\n",
      "86.1 action:  [-0.007, -0.9888] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  10 ff for 102.49999999999845 s: -------------------> 0.1\n",
      "Total reward for the episode:  800.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  402\n",
      "Eval num_timesteps=160000, episode_reward=666.67 +/- 135.97\n",
      "Episode length: 1025.00 +/- 0.00\n",
      "3.0 action:  [-0.0066, -0.9999] n_targets:  2 reward:  160\n",
      "4.6 action:  [-0.0067, -0.9895] n_targets:  2 reward:  160\n",
      "6.3 action:  [-0.0082, -0.9921] n_targets:  1 reward:  80\n",
      "7.5 action:  [0.0035, -0.9999] n_targets:  1 reward:  80\n",
      "12.4 action:  [0.0027, -0.9997] n_targets:  1 reward:  80\n",
      "15.7 action:  [0.0065, -0.9942] n_targets:  1 reward:  80\n",
      "20.7 action:  [0.0003, -0.9994] n_targets:  1 reward:  80\n",
      "27.0 action:  [-0.0023, -0.9947] n_targets:  1 reward:  80\n",
      "28.5 action:  [-0.0008, -0.9982] n_targets:  1 reward:  80\n",
      "32.6 action:  [-0.001, -0.9915] n_targets:  1 reward:  80\n",
      "35.5 action:  [0.0007, -0.9988] n_targets:  3 reward:  240\n",
      "36.1 action:  [-0.0059, -0.983] n_targets:  1 reward:  80\n",
      "37.3 action:  [-0.0091, -0.9859] n_targets:  1 reward:  80\n",
      "41.2 action:  [0.0005, -0.9927] n_targets:  1 reward:  80\n",
      "43.1 action:  [-0.0001, -0.9907] n_targets:  1 reward:  80\n",
      "49.3 action:  [0.0057, -0.9884] n_targets:  1 reward:  80\n",
      "57.1 action:  [-0.0042, -0.9988] n_targets:  1 reward:  80\n",
      "58.7 action:  [-0.0094, -0.9872] n_targets:  2 reward:  160\n",
      "60.7 action:  [-0.005, -0.9987] n_targets:  1 reward:  80\n",
      "67.6 action:  [-0.0086, -0.9979] n_targets:  1 reward:  80\n",
      "70.0 action:  [0.0029, -0.9969] n_targets:  1 reward:  80\n",
      "77.3 action:  [-0.0083, -0.998] n_targets:  1 reward:  80\n",
      "78.9 action:  [0.0053, -0.9951] n_targets:  1 reward:  80\n",
      "84.0 action:  [0.0023, -0.9977] n_targets:  1 reward:  80\n",
      "87.3 action:  [-0.0, -0.9977] n_targets:  1 reward:  80\n",
      "90.6 action:  [0.0073, -0.9994] n_targets:  1 reward:  80\n",
      "95.8 action:  [0.0026, -0.9995] n_targets:  1 reward:  80\n",
      "96.5 action:  [0.0056, -0.9907] n_targets:  1 reward:  80\n",
      "98.2 action:  [0.0078, -0.9947] n_targets:  1 reward:  80\n",
      "99.1 action:  [0.0052, -0.9905] n_targets:  1 reward:  80\n",
      "100.7 action:  [0.0025, -0.9881] n_targets:  2 reward:  160\n",
      "Firely capture rate for the episode:  37 ff for 102.49999999999845 s: -------------------> 0.36\n",
      "Total reward for the episode:  2960.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  403\n",
      "2.4 action:  [-0.0006, -0.9966] n_targets:  1 reward:  80\n",
      "4.1 action:  [0.0096, -0.9983] n_targets:  2 reward:  160\n",
      "5.9 action:  [-0.0004, -0.9867] n_targets:  1 reward:  80\n",
      "8.5 action:  [-0.0016, -0.9828] n_targets:  1 reward:  80\n",
      "9.4 action:  [-0.0099, -0.9983] n_targets:  1 reward:  80\n",
      "24.9 action:  [0.0015, -0.9944] n_targets:  1 reward:  80\n",
      "26.5 action:  [-0.0001, -0.9809] n_targets:  1 reward:  80\n",
      "30.6 action:  [-0.0064, -0.9956] n_targets:  1 reward:  80\n",
      "34.2 action:  [0.0085, -0.9996] n_targets:  1 reward:  80\n",
      "36.1 action:  [0.005, -0.9939] n_targets:  1 reward:  80\n",
      "45.2 action:  [-0.0095, -0.9999] n_targets:  1 reward:  80\n",
      "47.7 action:  [-0.0034, -0.9906] n_targets:  1 reward:  80\n",
      "50.1 action:  [0.0045, -0.9991] n_targets:  1 reward:  80\n",
      "51.5 action:  [0.0085, -0.9993] n_targets:  2 reward:  160\n",
      "55.1 action:  [-0.0005, -0.9884] n_targets:  1 reward:  80\n",
      "56.4 action:  [0.0081, -0.999] n_targets:  1 reward:  80\n",
      "70.0 action:  [0.0068, -0.9815] n_targets:  1 reward:  80\n",
      "76.5 action:  [-0.0057, -0.9988] n_targets:  1 reward:  80\n",
      "77.8 action:  [-0.0026, -0.9993] n_targets:  1 reward:  80\n",
      "78.9 action:  [0.0006, -0.9996] n_targets:  1 reward:  80\n",
      "82.4 action:  [-0.0092, -0.9922] n_targets:  2 reward:  160\n",
      "86.5 action:  [-0.0043, -0.9841] n_targets:  1 reward:  80\n",
      "88.7 action:  [-0.0046, -0.9946] n_targets:  1 reward:  80\n",
      "96.3 action:  [-0.0033, -0.9857] n_targets:  1 reward:  80\n",
      "97.1 action:  [-0.009, -0.9935] n_targets:  2 reward:  160\n",
      "98.1 action:  [0.0036, -0.9852] n_targets:  1 reward:  80\n",
      "100.2 action:  [0.0024, -0.9997] n_targets:  1 reward:  80\n",
      "101.8 action:  [-0.0027, -0.9981] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  32 ff for 102.49999999999845 s: -------------------> 0.31\n",
      "Total reward for the episode:  2560.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  404\n",
      "3.4 action:  [-0.0092, -0.9864] n_targets:  1 reward:  80\n",
      "5.8 action:  [0.0085, -0.9869] n_targets:  1 reward:  80\n",
      "8.8 action:  [0.0074, -0.9937] n_targets:  1 reward:  80\n",
      "16.0 action:  [-0.0057, -0.9897] n_targets:  1 reward:  80\n",
      "17.5 action:  [0.0074, -0.9889] n_targets:  1 reward:  80\n",
      "19.1 action:  [0.0012, -0.9919] n_targets:  1 reward:  80\n",
      "21.8 action:  [-0.0024, -0.9813] n_targets:  1 reward:  80\n",
      "23.4 action:  [-0.0038, -0.9962] n_targets:  1 reward:  80\n",
      "25.4 action:  [0.0074, -0.9962] n_targets:  2 reward:  160\n",
      "26.4 action:  [-0.0052, -0.9977] n_targets:  2 reward:  160\n",
      "28.4 action:  [-0.0009, -0.9979] n_targets:  1 reward:  80\n",
      "31.1 action:  [0.0085, -0.9943] n_targets:  1 reward:  80\n",
      "36.0 action:  [-0.005, -0.9968] n_targets:  1 reward:  80\n",
      "50.8 action:  [0.0083, -0.9993] n_targets:  1 reward:  80\n",
      "59.4 action:  [0.0018, -0.9931] n_targets:  1 reward:  80\n",
      "70.6 action:  [0.0007, -0.9982] n_targets:  1 reward:  80\n",
      "73.1 action:  [-0.0008, -0.9959] n_targets:  1 reward:  80\n",
      "74.6 action:  [0.0068, -0.9957] n_targets:  1 reward:  80\n",
      "76.7 action:  [-0.0079, -0.9914] n_targets:  1 reward:  80\n",
      "78.0 action:  [-0.0047, -0.9994] n_targets:  2 reward:  160\n",
      "82.1 action:  [0.0095, -0.9808] n_targets:  1 reward:  80\n",
      "85.4 action:  [-0.0064, -0.9909] n_targets:  1 reward:  80\n",
      "86.3 action:  [-0.0032, -0.9871] n_targets:  1 reward:  80\n",
      "88.0 action:  [0.0071, -0.9966] n_targets:  1 reward:  80\n",
      "90.3 action:  [-0.0076, -0.9975] n_targets:  1 reward:  80\n",
      "91.9 action:  [0.0059, -0.9983] n_targets:  1 reward:  80\n",
      "93.5 action:  [-0.0064, -0.9923] n_targets:  1 reward:  80\n",
      "97.1 action:  [-0.0006, -0.9994] n_targets:  1 reward:  80\n",
      "102.1 action:  [-0.0083, -0.9899] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  32 ff for 102.49999999999845 s: -------------------> 0.31\n",
      "Total reward for the episode:  2560.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  405\n",
      "3.5 action:  [-0.0081, -0.9996] n_targets:  1 reward:  80\n",
      "5.8 action:  [-0.004, -0.9982] n_targets:  1 reward:  80\n",
      "11.0 action:  [0.0026, -0.9959] n_targets:  1 reward:  80\n",
      "14.8 action:  [-0.0063, -0.9991] n_targets:  1 reward:  80\n",
      "16.3 action:  [0.0045, -0.9923] n_targets:  1 reward:  80\n",
      "18.1 action:  [0.0036, -0.9894] n_targets:  1 reward:  80\n",
      "22.3 action:  [0.0042, -0.9982] n_targets:  2 reward:  160\n",
      "25.3 action:  [0.0094, -0.9987] n_targets:  1 reward:  80\n",
      "26.4 action:  [0.007, -0.9906] n_targets:  1 reward:  80\n",
      "32.0 action:  [-0.0022, -0.9965] n_targets:  1 reward:  80\n",
      "33.9 action:  [0.0098, -0.9955] n_targets:  2 reward:  160\n",
      "44.9 action:  [0.0012, -0.9988] n_targets:  1 reward:  80\n",
      "47.6 action:  [-0.0033, -0.9872] n_targets:  2 reward:  160\n",
      "68.0 action:  [-0.0056, -0.9946] n_targets:  1 reward:  80\n",
      "72.4 action:  [-0.0038, -0.9923] n_targets:  1 reward:  80\n",
      "73.9 action:  [-0.0059, -0.9995] n_targets:  2 reward:  160\n",
      "75.6 action:  [0.0018, -0.9902] n_targets:  1 reward:  80\n",
      "78.3 action:  [-0.0042, -0.997] n_targets:  1 reward:  80\n",
      "80.1 action:  [0.0017, -0.9835] n_targets:  1 reward:  80\n",
      "82.1 action:  [-0.0004, -0.9819] n_targets:  1 reward:  80\n",
      "86.2 action:  [-0.0071, -0.999] n_targets:  1 reward:  80\n",
      "94.4 action:  [0.008, -0.9967] n_targets:  1 reward:  80\n",
      "99.6 action:  [0.0, -0.9857] n_targets:  1 reward:  80\n",
      "101.4 action:  [0.0086, -0.995] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  28 ff for 102.49999999999845 s: -------------------> 0.27\n",
      "Total reward for the episode:  2240.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  406\n",
      "4.1 action:  [0.0027, -0.998] n_targets:  1 reward:  80\n",
      "5.1 action:  [-0.008, -0.9836] n_targets:  1 reward:  80\n",
      "7.2 action:  [0.0023, -0.9935] n_targets:  1 reward:  80\n",
      "12.3 action:  [0.0045, -0.9996] n_targets:  1 reward:  80\n",
      "13.5 action:  [-0.0042, -0.9979] n_targets:  1 reward:  80\n",
      "17.9 action:  [-0.0055, -0.996] n_targets:  1 reward:  80\n",
      "20.4 action:  [-0.009, -0.9982] n_targets:  1 reward:  80\n",
      "24.0 action:  [0.0002, -0.9968] n_targets:  1 reward:  80\n",
      "36.2 action:  [-0.0093, -0.9993] n_targets:  1 reward:  80\n",
      "37.7 action:  [0.0045, -0.9952] n_targets:  1 reward:  80\n",
      "40.1 action:  [0.0027, -0.994] n_targets:  1 reward:  80\n",
      "42.9 action:  [-0.0034, -0.9956] n_targets:  1 reward:  80\n",
      "45.1 action:  [0.0007, -0.9973] n_targets:  1 reward:  80\n",
      "47.0 action:  [0.003, -0.9914] n_targets:  1 reward:  80\n",
      "48.5 action:  [0.0069, -0.9831] n_targets:  1 reward:  80\n",
      "55.2 action:  [0.0024, -0.9981] n_targets:  1 reward:  80\n",
      "56.7 action:  [-0.0003, -0.9907] n_targets:  1 reward:  80\n",
      "67.3 action:  [-0.0066, -0.9958] n_targets:  1 reward:  80\n",
      "69.3 action:  [-0.0097, -0.9966] n_targets:  1 reward:  80\n",
      "71.6 action:  [-0.0097, -0.9941] n_targets:  1 reward:  80\n",
      "81.5 action:  [-0.0088, -0.9866] n_targets:  1 reward:  80\n",
      "88.0 action:  [0.0056, -0.9961] n_targets:  1 reward:  80\n",
      "90.8 action:  [-0.0087, -0.9909] n_targets:  1 reward:  80\n",
      "91.5 action:  [0.0034, -0.9963] n_targets:  1 reward:  80\n",
      "96.6 action:  [-0.0092, -0.9868] n_targets:  1 reward:  80\n",
      "97.9 action:  [-0.0001, -0.9998] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  26 ff for 102.49999999999845 s: -------------------> 0.25\n",
      "Total reward for the episode:  2080.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  407\n",
      "4.0 action:  [0.0073, -0.9885] n_targets:  1 reward:  80\n",
      "9.6 action:  [-0.0021, -0.9981] n_targets:  1 reward:  80\n",
      "13.9 action:  [-0.006, -0.9999] n_targets:  1 reward:  80\n",
      "19.2 action:  [0.0063, -0.9955] n_targets:  1 reward:  80\n",
      "22.1 action:  [0.0098, -0.998] n_targets:  1 reward:  80\n",
      "26.5 action:  [0.0083, -0.9888] n_targets:  1 reward:  80\n",
      "29.3 action:  [0.0093, -0.9971] n_targets:  2 reward:  160\n",
      "30.6 action:  [0.0085, -0.9997] n_targets:  1 reward:  80\n",
      "31.9 action:  [-0.0079, -0.9966] n_targets:  1 reward:  80\n",
      "33.6 action:  [0.0011, -0.9984] n_targets:  1 reward:  80\n",
      "34.9 action:  [-0.0005, -0.9817] n_targets:  1 reward:  80\n",
      "36.5 action:  [0.0023, -0.9919] n_targets:  1 reward:  80\n",
      "38.9 action:  [0.0062, -0.9964] n_targets:  1 reward:  80\n",
      "41.5 action:  [0.0057, -0.999] n_targets:  1 reward:  80\n",
      "51.9 action:  [-0.005, -0.987] n_targets:  1 reward:  80\n",
      "54.1 action:  [-0.0071, -0.9999] n_targets:  1 reward:  80\n",
      "55.7 action:  [0.0097, -0.9879] n_targets:  1 reward:  80\n",
      "59.2 action:  [0.0038, -0.9975] n_targets:  1 reward:  80\n",
      "63.5 action:  [0.0084, -0.9986] n_targets:  2 reward:  160\n",
      "64.6 action:  [-0.0037, -0.9908] n_targets:  1 reward:  80\n",
      "66.8 action:  [0.0005, -0.9973] n_targets:  1 reward:  80\n",
      "69.7 action:  [0.0072, -0.9988] n_targets:  1 reward:  80\n",
      "71.3 action:  [0.0086, -0.9871] n_targets:  2 reward:  160\n",
      "74.6 action:  [0.0089, -0.9919] n_targets:  1 reward:  80\n",
      "80.6 action:  [-0.0093, -0.9986] n_targets:  1 reward:  80\n",
      "83.7 action:  [0.0008, -0.9931] n_targets:  1 reward:  80\n",
      "84.9 action:  [0.0031, -0.9912] n_targets:  1 reward:  80\n",
      "86.9 action:  [-0.0015, -0.9925] n_targets:  1 reward:  80\n",
      "90.1 action:  [0.0001, -0.9995] n_targets:  1 reward:  80\n",
      "91.3 action:  [-0.0045, -0.9985] n_targets:  1 reward:  80\n",
      "92.8 action:  [0.0014, -0.9972] n_targets:  1 reward:  80\n",
      "97.6 action:  [0.004, -0.9933] n_targets:  2 reward:  160\n",
      "Firely capture rate for the episode:  36 ff for 102.49999999999845 s: -------------------> 0.35\n",
      "Total reward for the episode:  2880.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  408\n",
      "1.4 action:  [-0.0095, -0.9985] n_targets:  2 reward:  160\n",
      "5.6 action:  [-0.0098, -0.9925] n_targets:  1 reward:  80\n",
      "8.2 action:  [0.0086, -0.9883] n_targets:  1 reward:  80\n",
      "11.2 action:  [0.0007, -0.9981] n_targets:  1 reward:  80\n",
      "17.3 action:  [0.0058, -0.998] n_targets:  1 reward:  80\n",
      "21.2 action:  [-0.0045, -0.9934] n_targets:  1 reward:  80\n",
      "26.5 action:  [-0.007, -0.9978] n_targets:  1 reward:  80\n",
      "40.5 action:  [-0.0055, -0.9956] n_targets:  1 reward:  80\n",
      "42.3 action:  [0.0082, -0.999] n_targets:  1 reward:  80\n",
      "45.9 action:  [-0.0027, -0.9983] n_targets:  3 reward:  240\n",
      "46.9 action:  [-0.0021, -0.9984] n_targets:  1 reward:  80\n",
      "48.5 action:  [-0.0001, -0.995] n_targets:  1 reward:  80\n",
      "54.1 action:  [-0.0053, -0.9964] n_targets:  1 reward:  80\n",
      "59.7 action:  [-0.0041, -0.9969] n_targets:  1 reward:  80\n",
      "61.8 action:  [0.0061, -0.9962] n_targets:  1 reward:  80\n",
      "64.3 action:  [-0.0026, -0.9907] n_targets:  1 reward:  80\n",
      "69.8 action:  [0.007, -0.9859] n_targets:  1 reward:  80\n",
      "71.7 action:  [0.0003, -0.9923] n_targets:  1 reward:  80\n",
      "82.0 action:  [-0.0078, -0.9889] n_targets:  1 reward:  80\n",
      "86.1 action:  [-0.0033, -0.9964] n_targets:  1 reward:  80\n",
      "88.9 action:  [0.0055, -0.9941] n_targets:  2 reward:  160\n",
      "92.1 action:  [-0.0036, -0.9896] n_targets:  1 reward:  80\n",
      "93.8 action:  [0.0054, -0.9813] n_targets:  2 reward:  160\n",
      "100.1 action:  [-0.0047, -0.9952] n_targets:  2 reward:  160\n",
      "Firely capture rate for the episode:  30 ff for 102.49999999999845 s: -------------------> 0.29\n",
      "Total reward for the episode:  2400.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  409\n",
      "3.4 action:  [-0.0082, -0.9916] n_targets:  1 reward:  80\n",
      "8.7 action:  [-0.0094, -0.9931] n_targets:  1 reward:  80\n",
      "9.7 action:  [0.0064, -0.9826] n_targets:  1 reward:  80\n",
      "22.5 action:  [-0.0095, -0.9987] n_targets:  1 reward:  80\n",
      "30.2 action:  [-0.0001, -0.9935] n_targets:  1 reward:  80\n",
      "32.0 action:  [0.0034, -0.9884] n_targets:  1 reward:  80\n",
      "34.8 action:  [-0.0073, -0.998] n_targets:  1 reward:  80\n",
      "37.7 action:  [-0.0018, -0.9951] n_targets:  1 reward:  80\n",
      "42.1 action:  [-0.0022, -0.9982] n_targets:  2 reward:  160\n",
      "43.7 action:  [0.0071, -0.9991] n_targets:  1 reward:  80\n",
      "45.1 action:  [0.0089, -0.9959] n_targets:  1 reward:  80\n",
      "47.9 action:  [0.0033, -0.9989] n_targets:  1 reward:  80\n",
      "54.7 action:  [0.0036, -0.9934] n_targets:  1 reward:  80\n",
      "59.7 action:  [0.0086, -0.9875] n_targets:  1 reward:  80\n",
      "67.0 action:  [0.007, -0.9843] n_targets:  1 reward:  80\n",
      "69.6 action:  [0.0027, -0.9954] n_targets:  1 reward:  80\n",
      "71.5 action:  [-0.0027, -0.9991] n_targets:  1 reward:  80\n",
      "72.4 action:  [0.0057, -0.9931] n_targets:  2 reward:  160\n",
      "74.4 action:  [-0.0077, -0.9983] n_targets:  1 reward:  80\n",
      "76.4 action:  [-0.0049, -0.9995] n_targets:  1 reward:  80\n",
      "82.6 action:  [0.0008, -0.9962] n_targets:  1 reward:  80\n",
      "84.0 action:  [-0.0096, -0.9979] n_targets:  1 reward:  80\n",
      "94.7 action:  [-0.0003, -0.9975] n_targets:  1 reward:  80\n",
      "96.7 action:  [-0.0052, -0.9893] n_targets:  1 reward:  80\n",
      "100.6 action:  [-0.0099, -0.9816] n_targets:  1 reward:  80\n",
      "101.3 action:  [0.0036, -0.9804] n_targets:  2 reward:  160\n",
      "Firely capture rate for the episode:  29 ff for 102.49999999999845 s: -------------------> 0.28\n",
      "Total reward for the episode:  2320.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  410\n",
      "2.4 action:  [-0.0017, -0.9935] n_targets:  1 reward:  80\n",
      "3.8 action:  [0.0025, -0.9939] n_targets:  1 reward:  80\n",
      "5.0 action:  [-0.0075, -0.9897] n_targets:  1 reward:  80\n",
      "8.6 action:  [-0.0098, -0.9998] n_targets:  1 reward:  80\n",
      "11.5 action:  [0.0098, -0.9914] n_targets:  1 reward:  80\n",
      "19.8 action:  [0.0002, -0.9913] n_targets:  2 reward:  160\n",
      "29.2 action:  [-0.0071, -0.9968] n_targets:  1 reward:  80\n",
      "33.9 action:  [0.0055, -0.9866] n_targets:  1 reward:  80\n",
      "36.9 action:  [0.0037, -0.9948] n_targets:  1 reward:  80\n",
      "41.8 action:  [0.0058, -0.9993] n_targets:  1 reward:  80\n",
      "45.1 action:  [0.0003, -0.9959] n_targets:  1 reward:  80\n",
      "47.1 action:  [0.0079, -0.9997] n_targets:  1 reward:  80\n",
      "48.5 action:  [-0.005, -0.998] n_targets:  1 reward:  80\n",
      "50.0 action:  [0.0012, -0.9916] n_targets:  1 reward:  80\n",
      "52.3 action:  [0.0008, -0.9927] n_targets:  1 reward:  80\n",
      "54.1 action:  [0.0049, -0.9873] n_targets:  1 reward:  80\n",
      "56.1 action:  [0.0065, -0.9898] n_targets:  1 reward:  80\n",
      "59.3 action:  [-0.0076, -0.9929] n_targets:  1 reward:  80\n",
      "60.9 action:  [-0.0001, -0.9913] n_targets:  2 reward:  160\n",
      "64.6 action:  [0.0096, -0.9957] n_targets:  1 reward:  80\n",
      "65.6 action:  [-0.0023, -0.9993] n_targets:  1 reward:  80\n",
      "68.2 action:  [0.0034, -0.999] n_targets:  1 reward:  80\n",
      "70.3 action:  [-0.0064, -0.9889] n_targets:  1 reward:  80\n",
      "72.1 action:  [-0.0021, -0.9942] n_targets:  1 reward:  80\n",
      "77.9 action:  [-0.0044, -0.9996] n_targets:  2 reward:  160\n",
      "79.2 action:  [0.0083, -0.987] n_targets:  2 reward:  160\n",
      "81.2 action:  [-0.0045, -0.9954] n_targets:  1 reward:  80\n",
      "83.4 action:  [0.0024, -0.9964] n_targets:  1 reward:  80\n",
      "90.9 action:  [0.0072, -0.9997] n_targets:  1 reward:  80\n",
      "92.0 action:  [-0.0096, -0.9847] n_targets:  1 reward:  80\n",
      "99.3 action:  [-0.0057, -0.9985] n_targets:  1 reward:  80\n",
      "100.3 action:  [0.0005, -0.991] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  36 ff for 102.49999999999845 s: -------------------> 0.35\n",
      "Total reward for the episode:  2880.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  411\n",
      "3.5 action:  [-0.0072, -0.9969] n_targets:  1 reward:  80\n",
      "11.5 action:  [0.0087, -0.9997] n_targets:  1 reward:  80\n",
      "14.0 action:  [-0.0019, -0.9967] n_targets:  1 reward:  80\n",
      "15.3 action:  [0.0095, -0.9927] n_targets:  2 reward:  160\n",
      "22.1 action:  [-0.0048, -0.9998] n_targets:  1 reward:  80\n",
      "23.0 action:  [0.0038, -0.9946] n_targets:  1 reward:  80\n",
      "29.1 action:  [0.0068, -0.9984] n_targets:  1 reward:  80\n",
      "32.0 action:  [0.0043, -0.9908] n_targets:  1 reward:  80\n",
      "32.9 action:  [-0.0001, -0.9918] n_targets:  1 reward:  80\n",
      "36.1 action:  [0.0014, -0.9911] n_targets:  1 reward:  80\n",
      "37.4 action:  [0.0076, -0.9984] n_targets:  1 reward:  80\n",
      "39.9 action:  [-0.0035, -0.9988] n_targets:  1 reward:  80\n",
      "40.8 action:  [0.0095, -0.9957] n_targets:  1 reward:  80\n",
      "45.5 action:  [0.0086, -0.9997] n_targets:  1 reward:  80\n",
      "48.3 action:  [0.0093, -0.9972] n_targets:  1 reward:  80\n",
      "49.4 action:  [0.0096, -0.999] n_targets:  1 reward:  80\n",
      "51.3 action:  [0.0052, -0.9922] n_targets:  1 reward:  80\n",
      "57.8 action:  [0.0023, -0.9885] n_targets:  1 reward:  80\n",
      "59.9 action:  [-0.001, -0.999] n_targets:  2 reward:  160\n",
      "60.3 action:  [0.0078, -0.9983] n_targets:  2 reward:  160\n",
      "61.3 action:  [0.0072, -0.9984] n_targets:  2 reward:  160\n",
      "62.5 action:  [0.0047, -0.9906] n_targets:  1 reward:  80\n",
      "64.1 action:  [0.0022, -0.9968] n_targets:  2 reward:  160\n",
      "64.5 action:  [-0.003, -0.9947] n_targets:  1 reward:  80\n",
      "66.2 action:  [0.0098, -0.9969] n_targets:  1 reward:  80\n",
      "74.5 action:  [0.0045, -0.9971] n_targets:  1 reward:  80\n",
      "75.7 action:  [0.0013, -0.998] n_targets:  1 reward:  80\n",
      "TIME before resetting: 77.49999999999987\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  412\n",
      "2.2 action:  [0.0054, -0.9915] n_targets:  1 reward:  80\n",
      "4.6 action:  [0.0093, -0.9993] n_targets:  1 reward:  80\n",
      "11.4 action:  [0.0041, -0.997] n_targets:  1 reward:  80\n",
      "12.4 action:  [0.0097, -0.9969] n_targets:  1 reward:  80\n",
      "18.8 action:  [0.004, -0.9991] n_targets:  1 reward:  80\n",
      "20.7 action:  [0.0032, -0.9995] n_targets:  1 reward:  80\n",
      "22.2 action:  [0.008, -0.9981] n_targets:  1 reward:  80\n",
      "25.6 action:  [0.0039, -0.9914] n_targets:  1 reward:  80\n",
      "27.5 action:  [0.0006, -0.9949] n_targets:  1 reward:  80\n",
      "32.8 action:  [0.001, -0.9894] n_targets:  1 reward:  80\n",
      "33.7 action:  [-0.0042, -0.9935] n_targets:  1 reward:  80\n",
      "36.7 action:  [0.0018, -0.984] n_targets:  1 reward:  80\n",
      "37.9 action:  [-0.0023, -0.9866] n_targets:  2 reward:  160\n",
      "41.3 action:  [0.0042, -0.9954] n_targets:  1 reward:  80\n",
      "45.0 action:  [0.0009, -0.995] n_targets:  1 reward:  80\n",
      "46.0 action:  [0.0082, -0.993] n_targets:  1 reward:  80\n",
      "49.5 action:  [0.001, -0.9947] n_targets:  1 reward:  80\n",
      "54.4 action:  [0.0096, -0.9866] n_targets:  1 reward:  80\n",
      "55.4 action:  [0.0017, -0.9936] n_targets:  1 reward:  80\n",
      "59.4 action:  [0.0029, -0.9929] n_targets:  1 reward:  80\n",
      "63.9 action:  [0.0007, -0.9973] n_targets:  1 reward:  80\n",
      "66.2 action:  [0.0009, -0.9891] n_targets:  1 reward:  80\n",
      "68.2 action:  [0.0029, -0.997] n_targets:  1 reward:  80\n",
      "70.4 action:  [0.0015, -0.9975] n_targets:  1 reward:  80\n",
      "71.9 action:  [0.0013, -0.9856] n_targets:  1 reward:  80\n",
      "76.1 action:  [0.0013, -0.9914] n_targets:  1 reward:  80\n",
      "78.5 action:  [0.0028, -0.9985] n_targets:  1 reward:  80\n",
      "80.4 action:  [0.0007, -0.9935] n_targets:  2 reward:  160\n",
      "81.9 action:  [0.0009, -0.9811] n_targets:  1 reward:  80\n",
      "84.2 action:  [0.0017, -0.9979] n_targets:  1 reward:  80\n",
      "92.4 action:  [0.0047, -0.9877] n_targets:  1 reward:  80\n",
      "94.4 action:  [-0.006, -0.9912] n_targets:  1 reward:  80\n",
      "95.6 action:  [0.0005, -0.981] n_targets:  2 reward:  160\n",
      "97.8 action:  [0.0004, -0.9861] n_targets:  1 reward:  80\n",
      "101.4 action:  [0.0015, -0.9987] n_targets:  1 reward:  80\n",
      "102.3 action:  [0.0041, -0.9834] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  39 ff for 102.49999999999845 s: -------------------> 0.38\n",
      "Total reward for the episode:  3120.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  413\n",
      "3.7 action:  [0.0037, -0.9903] n_targets:  1 reward:  80\n",
      "7.3 action:  [0.0009, -0.9922] n_targets:  1 reward:  80\n",
      "8.3 action:  [0.0057, -0.99] n_targets:  1 reward:  80\n",
      "10.0 action:  [-0.0036, -0.9992] n_targets:  1 reward:  80\n",
      "11.9 action:  [0.0017, -0.9952] n_targets:  1 reward:  80\n",
      "14.3 action:  [0.0073, -0.9981] n_targets:  1 reward:  80\n",
      "17.3 action:  [0.0026, -0.9948] n_targets:  1 reward:  80\n",
      "18.7 action:  [0.0082, -0.981] n_targets:  1 reward:  80\n",
      "19.3 action:  [0.0019, -0.9964] n_targets:  1 reward:  80\n",
      "20.7 action:  [0.0068, -0.985] n_targets:  1 reward:  80\n",
      "22.6 action:  [0.0001, -0.9942] n_targets:  1 reward:  80\n",
      "25.5 action:  [0.005, -0.9997] n_targets:  2 reward:  160\n",
      "27.5 action:  [0.0093, -0.996] n_targets:  1 reward:  80\n",
      "29.2 action:  [0.0051, -0.9942] n_targets:  1 reward:  80\n",
      "29.7 action:  [0.0025, -0.9987] n_targets:  1 reward:  80\n",
      "37.8 action:  [0.0015, -0.9925] n_targets:  1 reward:  80\n",
      "42.5 action:  [-0.008, -0.9822] n_targets:  1 reward:  80\n",
      "43.9 action:  [0.0072, -0.9837] n_targets:  1 reward:  80\n",
      "44.9 action:  [-0.0094, -0.9981] n_targets:  1 reward:  80\n",
      "45.8 action:  [0.0038, -0.9989] n_targets:  1 reward:  80\n",
      "50.9 action:  [0.0089, -0.9977] n_targets:  1 reward:  80\n",
      "53.1 action:  [0.0031, -0.9964] n_targets:  1 reward:  80\n",
      "60.2 action:  [0.0059, -0.9996] n_targets:  1 reward:  80\n",
      "62.3 action:  [0.0078, -0.9901] n_targets:  1 reward:  80\n",
      "62.6 action:  [-0.0033, -0.9963] n_targets:  1 reward:  80\n",
      "65.0 action:  [0.0049, -0.9922] n_targets:  1 reward:  80\n",
      "67.4 action:  [0.0048, -0.9953] n_targets:  1 reward:  80\n",
      "69.2 action:  [0.0049, -0.985] n_targets:  1 reward:  80\n",
      "70.2 action:  [0.0001, -0.9977] n_targets:  2 reward:  160\n",
      "73.3 action:  [0.0094, -0.9901] n_targets:  1 reward:  80\n",
      "76.8 action:  [-0.001, -0.9964] n_targets:  1 reward:  80\n",
      "83.6 action:  [0.0021, -0.9965] n_targets:  1 reward:  80\n",
      "86.0 action:  [0.0029, -0.9978] n_targets:  2 reward:  160\n",
      "87.6 action:  [0.0059, -0.9961] n_targets:  1 reward:  80\n",
      "88.0 action:  [0.003, -0.9989] n_targets:  1 reward:  80\n",
      "96.0 action:  [0.0024, -0.9967] n_targets:  1 reward:  80\n",
      "100.3 action:  [0.0092, -0.9864] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  40 ff for 102.49999999999845 s: -------------------> 0.39\n",
      "Total reward for the episode:  3200.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  414\n",
      "2.5 action:  [0.0082, -0.9959] n_targets:  1 reward:  80\n",
      "4.0 action:  [-0.0056, -0.9871] n_targets:  1 reward:  80\n",
      "6.3 action:  [0.005, -0.9919] n_targets:  1 reward:  80\n",
      "9.8 action:  [-0.0015, -0.998] n_targets:  1 reward:  80\n",
      "12.6 action:  [0.0006, -0.9938] n_targets:  1 reward:  80\n",
      "14.6 action:  [-0.0075, -0.9843] n_targets:  1 reward:  80\n",
      "19.3 action:  [0.0022, -0.9975] n_targets:  2 reward:  160\n",
      "22.3 action:  [0.0005, -0.9879] n_targets:  1 reward:  80\n",
      "27.1 action:  [-0.0054, -0.9899] n_targets:  1 reward:  80\n",
      "34.2 action:  [0.0087, -0.997] n_targets:  2 reward:  160\n",
      "36.9 action:  [0.0075, -0.9987] n_targets:  1 reward:  80\n",
      "39.5 action:  [0.0034, -0.995] n_targets:  3 reward:  240\n",
      "43.4 action:  [0.0018, -0.9982] n_targets:  1 reward:  80\n",
      "44.9 action:  [0.0086, -0.9992] n_targets:  2 reward:  160\n",
      "49.2 action:  [0.0027, -0.9988] n_targets:  2 reward:  160\n",
      "50.8 action:  [0.0033, -0.998] n_targets:  1 reward:  80\n",
      "56.8 action:  [0.0043, -0.9903] n_targets:  1 reward:  80\n",
      "58.3 action:  [0.0062, -0.9962] n_targets:  1 reward:  80\n",
      "63.4 action:  [-0.0067, -0.996] n_targets:  2 reward:  160\n",
      "64.7 action:  [0.003, -0.9979] n_targets:  1 reward:  80\n",
      "67.1 action:  [0.0019, -0.9956] n_targets:  1 reward:  80\n",
      "69.7 action:  [0.0064, -0.9925] n_targets:  3 reward:  240\n",
      "78.1 action:  [0.0033, -0.9987] n_targets:  1 reward:  80\n",
      "82.2 action:  [0.0022, -0.9982] n_targets:  1 reward:  80\n",
      "84.4 action:  [0.0087, -0.9886] n_targets:  1 reward:  80\n",
      "87.2 action:  [0.0019, -0.9936] n_targets:  2 reward:  160\n",
      "90.6 action:  [0.0071, -0.9884] n_targets:  1 reward:  80\n",
      "95.6 action:  [0.0077, -0.9942] n_targets:  1 reward:  80\n",
      "99.1 action:  [0.0027, -0.9985] n_targets:  1 reward:  80\n",
      "101.1 action:  [0.0079, -0.9911] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  40 ff for 102.49999999999845 s: -------------------> 0.39\n",
      "Total reward for the episode:  3200.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  415\n",
      "Eval num_timesteps=170000, episode_reward=3173.33 +/- 37.71\n",
      "Episode length: 1025.00 +/- 0.00\n",
      "New best mean reward!\n",
      "4.9 action:  [0.0075, -0.9969] n_targets:  1 reward:  80\n",
      "7.2 action:  [-0.0046, -0.9952] n_targets:  1 reward:  80\n",
      "9.3 action:  [-0.0071, -0.9951] n_targets:  1 reward:  80\n",
      "18.3 action:  [-0.0074, -0.9962] n_targets:  1 reward:  80\n",
      "19.9 action:  [0.0083, -0.9832] n_targets:  1 reward:  80\n",
      "21.0 action:  [-0.0088, -0.983] n_targets:  1 reward:  80\n",
      "21.7 action:  [-0.0041, -0.9939] n_targets:  1 reward:  80\n",
      "23.5 action:  [0.0075, -0.9944] n_targets:  1 reward:  80\n",
      "25.4 action:  [0.0019, -0.9996] n_targets:  1 reward:  80\n",
      "29.5 action:  [-0.0055, -0.9818] n_targets:  1 reward:  80\n",
      "31.7 action:  [0.0077, -0.987] n_targets:  1 reward:  80\n",
      "36.3 action:  [-0.009, -0.9941] n_targets:  1 reward:  80\n",
      "39.4 action:  [-0.0084, -0.9869] n_targets:  2 reward:  160\n",
      "42.5 action:  [-0.0031, -0.9916] n_targets:  1 reward:  80\n",
      "43.3 action:  [-0.0057, -0.9809] n_targets:  1 reward:  80\n",
      "45.2 action:  [0.0027, -0.9996] n_targets:  1 reward:  80\n",
      "50.6 action:  [0.0046, -0.9948] n_targets:  1 reward:  80\n",
      "53.2 action:  [0.0052, -0.9884] n_targets:  1 reward:  80\n",
      "55.2 action:  [-0.0081, -0.9941] n_targets:  1 reward:  80\n",
      "62.9 action:  [0.0004, -0.9894] n_targets:  1 reward:  80\n",
      "63.9 action:  [-0.0067, -0.998] n_targets:  1 reward:  80\n",
      "66.2 action:  [-0.002, -0.9981] n_targets:  1 reward:  80\n",
      "69.7 action:  [-0.006, -0.9947] n_targets:  1 reward:  80\n",
      "72.6 action:  [0.0002, -0.9997] n_targets:  1 reward:  80\n",
      "79.8 action:  [0.0029, -0.9998] n_targets:  1 reward:  80\n",
      "81.3 action:  [0.0066, -0.9965] n_targets:  1 reward:  80\n",
      "85.6 action:  [0.0026, -0.9972] n_targets:  1 reward:  80\n",
      "88.3 action:  [-0.0096, -0.995] n_targets:  1 reward:  80\n",
      "90.6 action:  [-0.0075, -0.9983] n_targets:  1 reward:  80\n",
      "94.4 action:  [-0.0031, -0.986] n_targets:  1 reward:  80\n",
      "95.6 action:  [-0.0062, -0.9802] n_targets:  1 reward:  80\n",
      "100.2 action:  [0.0095, -0.9932] n_targets:  1 reward:  80\n",
      "101.6 action:  [0.008, -0.9968] n_targets:  2 reward:  160\n",
      "Firely capture rate for the episode:  35 ff for 102.49999999999845 s: -------------------> 0.34\n",
      "Total reward for the episode:  2800.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  416\n",
      "2.9 action:  [-0.0034, -0.9926] n_targets:  1 reward:  80\n",
      "4.2 action:  [-0.0045, -0.9999] n_targets:  1 reward:  80\n",
      "8.6 action:  [0.0021, -0.9903] n_targets:  1 reward:  80\n",
      "11.4 action:  [-0.0033, -0.9995] n_targets:  1 reward:  80\n",
      "14.7 action:  [0.0017, -0.9982] n_targets:  1 reward:  80\n",
      "17.9 action:  [-0.003, -0.9989] n_targets:  1 reward:  80\n",
      "19.6 action:  [0.0042, -0.9875] n_targets:  2 reward:  160\n",
      "22.5 action:  [0.0086, -0.9903] n_targets:  2 reward:  160\n",
      "24.1 action:  [0.0087, -0.9976] n_targets:  1 reward:  80\n",
      "26.3 action:  [-0.0039, -0.9957] n_targets:  2 reward:  160\n",
      "28.0 action:  [0.0013, -0.9986] n_targets:  1 reward:  80\n",
      "29.8 action:  [-0.0084, -0.9993] n_targets:  2 reward:  160\n",
      "32.1 action:  [0.0038, -0.9983] n_targets:  1 reward:  80\n",
      "36.4 action:  [-0.008, -0.9977] n_targets:  1 reward:  80\n",
      "37.7 action:  [0.0074, -0.9958] n_targets:  1 reward:  80\n",
      "44.4 action:  [-0.0005, -0.9951] n_targets:  1 reward:  80\n",
      "45.3 action:  [0.0074, -0.9984] n_targets:  1 reward:  80\n",
      "50.9 action:  [-0.0053, -0.9949] n_targets:  2 reward:  160\n",
      "55.6 action:  [0.0086, -0.9928] n_targets:  1 reward:  80\n",
      "63.6 action:  [-0.0024, -0.9975] n_targets:  1 reward:  80\n",
      "68.2 action:  [-0.0039, -0.9967] n_targets:  2 reward:  160\n",
      "69.9 action:  [-0.0088, -0.9939] n_targets:  1 reward:  80\n",
      "73.2 action:  [0.0084, -0.9977] n_targets:  1 reward:  80\n",
      "76.9 action:  [-0.0061, -0.9975] n_targets:  1 reward:  80\n",
      "79.8 action:  [-0.0076, -0.9939] n_targets:  1 reward:  80\n",
      "82.3 action:  [-0.0002, -0.9955] n_targets:  1 reward:  80\n",
      "85.8 action:  [-0.0058, -0.9995] n_targets:  1 reward:  80\n",
      "87.7 action:  [0.0007, -0.9906] n_targets:  1 reward:  80\n",
      "90.6 action:  [-0.0038, -0.9899] n_targets:  2 reward:  160\n",
      "92.9 action:  [0.0034, -0.9902] n_targets:  1 reward:  80\n",
      "96.2 action:  [0.0016, -0.996] n_targets:  1 reward:  80\n",
      "98.3 action:  [0.0073, -0.9998] n_targets:  1 reward:  80\n",
      "99.5 action:  [-0.0006, -0.9882] n_targets:  1 reward:  80\n",
      "100.8 action:  [0.0075, -0.9945] n_targets:  2 reward:  160\n",
      "Firely capture rate for the episode:  42 ff for 102.49999999999845 s: -------------------> 0.41\n",
      "Total reward for the episode:  3360.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  417\n",
      "3.7 action:  [-0.0091, -0.9997] n_targets:  1 reward:  80\n",
      "5.1 action:  [-0.0064, -0.9962] n_targets:  1 reward:  80\n",
      "7.4 action:  [0.0034, -0.9889] n_targets:  1 reward:  80\n",
      "9.6 action:  [0.0031, -0.9975] n_targets:  1 reward:  80\n",
      "14.8 action:  [-0.009, -0.9857] n_targets:  1 reward:  80\n",
      "16.8 action:  [0.0067, -0.9825] n_targets:  1 reward:  80\n",
      "24.9 action:  [-0.0033, -0.9974] n_targets:  1 reward:  80\n",
      "25.8 action:  [-0.0082, -0.9878] n_targets:  1 reward:  80\n",
      "26.9 action:  [0.0064, -0.9883] n_targets:  1 reward:  80\n",
      "29.4 action:  [-0.0046, -0.9971] n_targets:  1 reward:  80\n",
      "32.5 action:  [0.0095, -0.998] n_targets:  1 reward:  80\n",
      "34.5 action:  [0.0077, -0.989] n_targets:  1 reward:  80\n",
      "37.1 action:  [-0.0099, -0.9889] n_targets:  1 reward:  80\n",
      "38.0 action:  [-0.0016, -0.9973] n_targets:  1 reward:  80\n",
      "42.9 action:  [-0.0024, -0.992] n_targets:  1 reward:  80\n",
      "52.0 action:  [-0.0043, -0.9879] n_targets:  1 reward:  80\n",
      "52.7 action:  [-0.0073, -0.9983] n_targets:  2 reward:  160\n",
      "54.1 action:  [0.003, -0.9961] n_targets:  1 reward:  80\n",
      "58.8 action:  [-0.0051, -0.9956] n_targets:  1 reward:  80\n",
      "60.4 action:  [0.0049, -0.9974] n_targets:  2 reward:  160\n",
      "62.6 action:  [0.0069, -0.9997] n_targets:  2 reward:  160\n",
      "65.3 action:  [0.0039, -0.9936] n_targets:  1 reward:  80\n",
      "70.3 action:  [0.0028, -0.9983] n_targets:  1 reward:  80\n",
      "81.4 action:  [-0.0047, -0.9993] n_targets:  1 reward:  80\n",
      "82.9 action:  [-0.0027, -0.9939] n_targets:  1 reward:  80\n",
      "87.5 action:  [0.0011, -0.9845] n_targets:  1 reward:  80\n",
      "87.8 action:  [-0.0052, -0.9954] n_targets:  1 reward:  80\n",
      "96.1 action:  [0.008, -0.9966] n_targets:  2 reward:  160\n",
      "100.0 action:  [0.0005, -0.9995] n_targets:  1 reward:  80\n",
      "100.9 action:  [-0.0023, -0.9994] n_targets:  1 reward:  80\n",
      "102.2 action:  [0.0023, -0.9908] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  35 ff for 102.49999999999845 s: -------------------> 0.34\n",
      "Total reward for the episode:  2800.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  418\n",
      "3.8 action:  [-0.002, -0.9999] n_targets:  1 reward:  80\n",
      "4.7 action:  [0.0024, -0.991] n_targets:  1 reward:  80\n",
      "8.1 action:  [0.0064, -0.9979] n_targets:  1 reward:  80\n",
      "16.4 action:  [-0.0001, -0.9979] n_targets:  2 reward:  160\n",
      "22.1 action:  [-0.0036, -0.9849] n_targets:  1 reward:  80\n",
      "24.6 action:  [0.0065, -0.9966] n_targets:  1 reward:  80\n",
      "27.0 action:  [0.0005, -0.9935] n_targets:  1 reward:  80\n",
      "29.6 action:  [-0.0057, -0.9966] n_targets:  1 reward:  80\n",
      "34.9 action:  [0.0049, -0.9935] n_targets:  1 reward:  80\n",
      "45.0 action:  [0.0033, -0.9929] n_targets:  1 reward:  80\n",
      "45.7 action:  [0.0052, -0.9962] n_targets:  1 reward:  80\n",
      "49.2 action:  [0.0039, -0.9957] n_targets:  1 reward:  80\n",
      "50.9 action:  [0.0007, -0.999] n_targets:  1 reward:  80\n",
      "52.2 action:  [0.0045, -0.9897] n_targets:  1 reward:  80\n",
      "54.3 action:  [-0.0093, -0.9981] n_targets:  1 reward:  80\n",
      "62.0 action:  [0.0006, -0.989] n_targets:  1 reward:  80\n",
      "64.2 action:  [0.0032, -0.9963] n_targets:  1 reward:  80\n",
      "67.6 action:  [-0.0011, -0.9847] n_targets:  1 reward:  80\n",
      "74.1 action:  [0.0063, -0.9952] n_targets:  1 reward:  80\n",
      "80.3 action:  [-0.0017, -0.9802] n_targets:  1 reward:  80\n",
      "84.1 action:  [-0.0046, -0.9982] n_targets:  1 reward:  80\n",
      "87.0 action:  [-0.0002, -0.997] n_targets:  1 reward:  80\n",
      "93.0 action:  [-0.0062, -0.9962] n_targets:  2 reward:  160\n",
      "94.1 action:  [-0.002, -0.9994] n_targets:  1 reward:  80\n",
      "95.2 action:  [0.0081, -0.9986] n_targets:  1 reward:  80\n",
      "97.7 action:  [-0.0016, -0.9997] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  28 ff for 102.49999999999845 s: -------------------> 0.27\n",
      "Total reward for the episode:  2240.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  419\n",
      "5.2 action:  [0.0097, -0.9964] n_targets:  1 reward:  80\n",
      "8.1 action:  [-0.0019, -0.9836] n_targets:  1 reward:  80\n",
      "11.5 action:  [-0.0033, -0.991] n_targets:  1 reward:  80\n",
      "13.3 action:  [-0.0, -0.9987] n_targets:  1 reward:  80\n",
      "16.0 action:  [0.0025, -0.986] n_targets:  1 reward:  80\n",
      "16.6 action:  [0.0056, -0.9955] n_targets:  1 reward:  80\n",
      "18.1 action:  [-0.0073, -0.9922] n_targets:  1 reward:  80\n",
      "22.7 action:  [-0.0097, -0.9973] n_targets:  1 reward:  80\n",
      "24.4 action:  [-0.0008, -0.9819] n_targets:  1 reward:  80\n",
      "29.1 action:  [-0.0038, -0.9922] n_targets:  1 reward:  80\n",
      "32.9 action:  [0.0016, -0.994] n_targets:  1 reward:  80\n",
      "36.4 action:  [-0.0029, -0.999] n_targets:  1 reward:  80\n",
      "37.2 action:  [-0.0073, -0.9988] n_targets:  1 reward:  80\n",
      "39.4 action:  [-0.0007, -0.9971] n_targets:  1 reward:  80\n",
      "41.3 action:  [0.0075, -0.9819] n_targets:  1 reward:  80\n",
      "43.1 action:  [0.01, -0.9939] n_targets:  1 reward:  80\n",
      "45.3 action:  [-0.0082, -0.9986] n_targets:  2 reward:  160\n",
      "48.0 action:  [-0.0055, -0.9997] n_targets:  2 reward:  160\n",
      "49.0 action:  [-0.0039, -0.9951] n_targets:  1 reward:  80\n",
      "51.2 action:  [-0.0009, -0.9932] n_targets:  2 reward:  160\n",
      "54.4 action:  [0.0067, -0.9978] n_targets:  1 reward:  80\n",
      "58.4 action:  [0.0036, -0.9993] n_targets:  1 reward:  80\n",
      "62.8 action:  [-0.0095, -0.9901] n_targets:  1 reward:  80\n",
      "64.6 action:  [-0.0083, -0.9912] n_targets:  1 reward:  80\n",
      "69.5 action:  [-0.0012, -0.9983] n_targets:  1 reward:  80\n",
      "71.8 action:  [-0.0094, -0.9995] n_targets:  1 reward:  80\n",
      "76.9 action:  [-0.001, -0.9947] n_targets:  1 reward:  80\n",
      "77.6 action:  [0.0034, -0.9995] n_targets:  1 reward:  80\n",
      "81.8 action:  [-0.005, -0.9966] n_targets:  1 reward:  80\n",
      "85.2 action:  [-0.0069, -0.9981] n_targets:  1 reward:  80\n",
      "87.4 action:  [0.0011, -0.9996] n_targets:  1 reward:  80\n",
      "89.6 action:  [0.0064, -0.9917] n_targets:  1 reward:  80\n",
      "91.1 action:  [0.009, -0.9932] n_targets:  1 reward:  80\n",
      "94.2 action:  [-0.0055, -0.9964] n_targets:  1 reward:  80\n",
      "98.5 action:  [0.0077, -0.9995] n_targets:  2 reward:  160\n",
      "101.5 action:  [-0.0093, -0.9973] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  40 ff for 102.49999999999845 s: -------------------> 0.39\n",
      "Total reward for the episode:  3200.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  420\n",
      "1.2 action:  [0.0009, -0.9947] n_targets:  1 reward:  80\n",
      "4.3 action:  [-0.0072, -0.9842] n_targets:  1 reward:  80\n",
      "6.6 action:  [-0.0052, -0.9988] n_targets:  1 reward:  80\n",
      "9.8 action:  [-0.0047, -0.995] n_targets:  1 reward:  80\n",
      "22.9 action:  [0.004, -0.9815] n_targets:  1 reward:  80\n",
      "23.9 action:  [0.0014, -0.9928] n_targets:  2 reward:  160\n",
      "26.1 action:  [-0.0067, -0.9872] n_targets:  1 reward:  80\n",
      "39.6 action:  [-0.0018, -0.9993] n_targets:  1 reward:  80\n",
      "41.7 action:  [0.0087, -0.9969] n_targets:  3 reward:  240\n",
      "42.2 action:  [0.0055, -0.9987] n_targets:  1 reward:  80\n",
      "47.6 action:  [-0.0029, -0.9823] n_targets:  1 reward:  80\n",
      "50.5 action:  [0.0092, -0.9894] n_targets:  1 reward:  80\n",
      "51.2 action:  [0.0019, -0.9971] n_targets:  1 reward:  80\n",
      "51.7 action:  [0.0081, -0.9803] n_targets:  1 reward:  80\n",
      "53.1 action:  [0.0016, -0.9804] n_targets:  1 reward:  80\n",
      "56.9 action:  [0.0081, -0.9959] n_targets:  1 reward:  80\n",
      "61.1 action:  [-0.0029, -0.9971] n_targets:  1 reward:  80\n",
      "63.5 action:  [0.0098, -0.9874] n_targets:  1 reward:  80\n",
      "68.3 action:  [0.006, -0.981] n_targets:  1 reward:  80\n",
      "69.4 action:  [0.0032, -0.9888] n_targets:  1 reward:  80\n",
      "73.8 action:  [-0.0006, -0.989] n_targets:  1 reward:  80\n",
      "77.3 action:  [0.001, -0.9944] n_targets:  1 reward:  80\n",
      "79.5 action:  [-0.0092, -0.9887] n_targets:  1 reward:  80\n",
      "84.2 action:  [-0.0025, -0.9927] n_targets:  1 reward:  80\n",
      "87.1 action:  [0.0017, -0.9926] n_targets:  1 reward:  80\n",
      "88.8 action:  [-0.0087, -0.9823] n_targets:  1 reward:  80\n",
      "92.8 action:  [0.0039, -0.9889] n_targets:  1 reward:  80\n",
      "97.3 action:  [0.006, -0.999] n_targets:  1 reward:  80\n",
      "98.1 action:  [0.0079, -0.9939] n_targets:  1 reward:  80\n",
      "99.5 action:  [0.0049, -0.998] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  33 ff for 102.49999999999845 s: -------------------> 0.32\n",
      "Total reward for the episode:  2640.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  421\n",
      "2.6 action:  [-0.0059, -0.9998] n_targets:  1 reward:  80\n",
      "5.8 action:  [-0.0014, -0.985] n_targets:  1 reward:  80\n",
      "8.3 action:  [-0.0076, -0.9962] n_targets:  2 reward:  160\n",
      "12.2 action:  [-0.0071, -0.9951] n_targets:  1 reward:  80\n",
      "15.8 action:  [-0.0082, -0.9999] n_targets:  1 reward:  80\n",
      "18.4 action:  [0.0003, -0.9997] n_targets:  1 reward:  80\n",
      "19.8 action:  [0.0098, -0.9891] n_targets:  1 reward:  80\n",
      "24.8 action:  [-0.0055, -0.9992] n_targets:  1 reward:  80\n",
      "27.8 action:  [-0.0066, -0.9836] n_targets:  2 reward:  160\n",
      "30.1 action:  [0.0083, -0.9973] n_targets:  1 reward:  80\n",
      "32.9 action:  [-0.0049, -0.9821] n_targets:  1 reward:  80\n",
      "37.7 action:  [0.0065, -0.9988] n_targets:  1 reward:  80\n",
      "41.9 action:  [-0.0062, -0.9995] n_targets:  1 reward:  80\n",
      "44.2 action:  [0.0081, -0.9941] n_targets:  1 reward:  80\n",
      "47.0 action:  [-0.0043, -0.9967] n_targets:  1 reward:  80\n",
      "57.7 action:  [-0.0073, -0.9977] n_targets:  1 reward:  80\n",
      "61.4 action:  [0.0058, -0.9929] n_targets:  1 reward:  80\n",
      "64.4 action:  [-0.0006, -0.9993] n_targets:  1 reward:  80\n",
      "66.7 action:  [0.006, -0.9962] n_targets:  1 reward:  80\n",
      "68.6 action:  [-0.0006, -0.9929] n_targets:  1 reward:  80\n",
      "69.9 action:  [-0.0073, -0.9959] n_targets:  1 reward:  80\n",
      "76.9 action:  [0.0011, -0.9996] n_targets:  1 reward:  80\n",
      "78.8 action:  [0.0078, -0.999] n_targets:  1 reward:  80\n",
      "80.1 action:  [0.0066, -0.9996] n_targets:  1 reward:  80\n",
      "84.0 action:  [0.0075, -0.9948] n_targets:  1 reward:  80\n",
      "85.4 action:  [-0.0045, -0.9893] n_targets:  1 reward:  80\n",
      "87.4 action:  [-0.009, -0.9887] n_targets:  1 reward:  80\n",
      "94.7 action:  [0.0025, -0.998] n_targets:  1 reward:  80\n",
      "95.6 action:  [-0.0066, -0.9987] n_targets:  1 reward:  80\n",
      "98.9 action:  [0.0056, -0.999] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  32 ff for 102.49999999999845 s: -------------------> 0.31\n",
      "Total reward for the episode:  2560.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  422\n",
      "3.5 action:  [0.0099, -0.9852] n_targets:  1 reward:  80\n",
      "6.1 action:  [0.0055, -0.9976] n_targets:  1 reward:  80\n",
      "9.4 action:  [-0.0013, -0.9949] n_targets:  1 reward:  80\n",
      "10.3 action:  [-0.0008, -0.9945] n_targets:  1 reward:  80\n",
      "12.3 action:  [0.0027, -0.9941] n_targets:  1 reward:  80\n",
      "16.4 action:  [-0.0032, -0.994] n_targets:  1 reward:  80\n",
      "20.4 action:  [-0.0094, -0.9883] n_targets:  1 reward:  80\n",
      "23.4 action:  [-0.0054, -0.9917] n_targets:  1 reward:  80\n",
      "29.3 action:  [-0.0005, -0.9974] n_targets:  1 reward:  80\n",
      "31.8 action:  [0.0024, -0.9936] n_targets:  2 reward:  160\n",
      "32.8 action:  [-0.0049, -0.9994] n_targets:  1 reward:  80\n",
      "35.6 action:  [-0.0056, -0.9983] n_targets:  1 reward:  80\n",
      "36.8 action:  [0.0037, -0.9959] n_targets:  1 reward:  80\n",
      "39.5 action:  [-0.0069, -0.9929] n_targets:  1 reward:  80\n",
      "44.4 action:  [-0.0004, -0.9938] n_targets:  1 reward:  80\n",
      "46.4 action:  [-0.0086, -0.9986] n_targets:  1 reward:  80\n",
      "51.8 action:  [-0.0, -0.9956] n_targets:  1 reward:  80\n",
      "54.3 action:  [0.0035, -0.9916] n_targets:  1 reward:  80\n",
      "57.4 action:  [0.0001, -0.9945] n_targets:  1 reward:  80\n",
      "58.6 action:  [0.0003, -0.9978] n_targets:  1 reward:  80\n",
      "61.7 action:  [0.0006, -0.9947] n_targets:  1 reward:  80\n",
      "65.8 action:  [-0.0031, -0.9957] n_targets:  1 reward:  80\n",
      "67.1 action:  [0.0029, -0.9908] n_targets:  1 reward:  80\n",
      "68.5 action:  [-0.0089, -0.9975] n_targets:  1 reward:  80\n",
      "74.2 action:  [-0.0096, -0.9897] n_targets:  1 reward:  80\n",
      "76.3 action:  [0.0086, -0.9969] n_targets:  1 reward:  80\n",
      "77.5 action:  [0.0028, -0.9954] n_targets:  1 reward:  80\n",
      "80.6 action:  [-0.0047, -0.9988] n_targets:  1 reward:  80\n",
      "83.4 action:  [-0.0045, -0.9925] n_targets:  1 reward:  80\n",
      "87.4 action:  [-0.0022, -0.9979] n_targets:  1 reward:  80\n",
      "90.3 action:  [-0.0055, -0.9985] n_targets:  1 reward:  80\n",
      "92.7 action:  [-0.0006, -0.9985] n_targets:  2 reward:  160\n",
      "93.5 action:  [0.0025, -0.9977] n_targets:  1 reward:  80\n",
      "95.5 action:  [-0.0025, -0.9919] n_targets:  1 reward:  80\n",
      "99.6 action:  [0.0002, -0.9966] n_targets:  1 reward:  80\n",
      "101.8 action:  [-0.0088, -0.9941] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  38 ff for 102.49999999999845 s: -------------------> 0.37\n",
      "Total reward for the episode:  3040.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  423\n",
      "2.5 action:  [0.0001, -0.996] n_targets:  1 reward:  80\n",
      "4.4 action:  [-0.0045, -0.9947] n_targets:  1 reward:  80\n",
      "5.9 action:  [0.0054, -0.9812] n_targets:  1 reward:  80\n",
      "6.9 action:  [-0.0067, -0.9983] n_targets:  2 reward:  160\n",
      "10.6 action:  [-0.0094, -0.9893] n_targets:  1 reward:  80\n",
      "14.9 action:  [0.0008, -0.9998] n_targets:  2 reward:  160\n",
      "16.4 action:  [-0.0095, -0.9995] n_targets:  1 reward:  80\n",
      "19.3 action:  [0.0034, -0.9985] n_targets:  1 reward:  80\n",
      "20.6 action:  [0.0088, -0.9985] n_targets:  1 reward:  80\n",
      "39.5 action:  [-0.0094, -0.9807] n_targets:  1 reward:  80\n",
      "41.2 action:  [-0.0044, -0.9993] n_targets:  1 reward:  80\n",
      "44.7 action:  [-0.0031, -0.9944] n_targets:  1 reward:  80\n",
      "47.6 action:  [0.0025, -0.9842] n_targets:  1 reward:  80\n",
      "51.9 action:  [-0.0091, -0.9949] n_targets:  2 reward:  160\n",
      "53.1 action:  [0.0, -0.9983] n_targets:  2 reward:  160\n",
      "58.0 action:  [0.0081, -0.9879] n_targets:  1 reward:  80\n",
      "60.0 action:  [-0.0092, -0.9981] n_targets:  1 reward:  80\n",
      "61.4 action:  [-0.0028, -0.9961] n_targets:  1 reward:  80\n",
      "63.6 action:  [-0.0002, -0.9988] n_targets:  1 reward:  80\n",
      "65.6 action:  [0.0089, -0.9986] n_targets:  1 reward:  80\n",
      "70.0 action:  [0.0022, -0.9802] n_targets:  1 reward:  80\n",
      "72.8 action:  [0.0039, -0.9993] n_targets:  1 reward:  80\n",
      "74.8 action:  [0.006, -0.9921] n_targets:  1 reward:  80\n",
      "77.7 action:  [0.0058, -0.9975] n_targets:  1 reward:  80\n",
      "83.7 action:  [-0.0014, -0.9921] n_targets:  1 reward:  80\n",
      "87.8 action:  [0.0062, -0.9832] n_targets:  1 reward:  80\n",
      "90.6 action:  [-0.0012, -0.9932] n_targets:  1 reward:  80\n",
      "96.3 action:  [-0.0, -0.9846] n_targets:  2 reward:  160\n",
      "101.8 action:  [0.0045, -0.9958] n_targets:  1 reward:  80\n",
      "Firely capture rate for the episode:  34 ff for 102.49999999999845 s: -------------------> 0.33\n",
      "Total reward for the episode:  2720.0\n",
      "Cost breakdown:  {'dv_cost': 0.0, 'dw_cost': 0.0, 'w_cost': 0.0}\n",
      "TIME before resetting: 102.49999999999845\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  424\n",
      "3.9 action:  [0.0035, -0.9972] n_targets:  2 reward:  160\n",
      "8.1 action:  [0.0006, -0.9976] n_targets:  1 reward:  80\n",
      "13.6 action:  [-0.0058, -0.9816] n_targets:  1 reward:  80\n",
      "15.9 action:  [0.0017, -0.9891] n_targets:  2 reward:  160\n",
      "21.4 action:  [0.0055, -0.9936] n_targets:  1 reward:  80\n",
      "22.8 action:  [-0.0012, -0.998] n_targets:  2 reward:  160\n",
      "23.6 action:  [0.0083, -0.9942] n_targets:  1 reward:  80\n",
      "24.1 action:  [-0.0048, -0.9868] n_targets:  1 reward:  80\n",
      "28.7 action:  [0.0037, -0.9918] n_targets:  1 reward:  80\n",
      "31.9 action:  [0.0031, -0.9884] n_targets:  1 reward:  80\n",
      "33.3 action:  [0.0048, -0.9993] n_targets:  2 reward:  160\n",
      "34.7 action:  [0.002, -0.998] n_targets:  1 reward:  80\n",
      "35.8 action:  [-0.0076, -0.9974] n_targets:  1 reward:  80\n",
      "38.8 action:  [-0.0088, -0.9911] n_targets:  1 reward:  80\n",
      "40.0 action:  [-0.0091, -0.9857] n_targets:  1 reward:  80\n",
      "42.1 action:  [0.0017, -0.9937] n_targets:  2 reward:  160\n",
      "49.9 action:  [-0.0078, -0.9916] n_targets:  1 reward:  80\n",
      "55.1 action:  [-0.0049, -0.9819] n_targets:  1 reward:  80\n",
      "57.5 action:  [-0.0061, -0.9878] n_targets:  1 reward:  80\n",
      "60.2 action:  [0.0043, -0.9975] n_targets:  1 reward:  80\n",
      "65.1 action:  [-0.0098, -0.9984] n_targets:  1 reward:  80\n",
      "72.8 action:  [-0.0071, -0.9897] n_targets:  1 reward:  80\n",
      "75.9 action:  [0.0079, -0.999] n_targets:  1 reward:  80\n",
      "77.1 action:  [0.0005, -0.9993] n_targets:  2 reward:  160\n",
      "TIME before resetting: 77.49999999999987\n",
      "current linear_terminal_vel:  0.01\n",
      "current angular_terminal_vel:  0.01\n",
      "current dt:  0.1\n",
      "current dv_cost_factor:  0\n",
      "current dw_cost_factor:  0\n",
      "current w_cost_factor:  0\n",
      "current distance2center_cost:  0\n",
      "current flash_on_interval:  0.3\n",
      "current num_obs_ff:  5\n",
      "current max_in_memory_time:  3\n",
      "\n",
      " episode:  425\n",
      "2.9 action:  [0.0068, -0.994] n_targets:  1 reward:  80\n",
      "8.6 action:  [0.0001, -0.9951] n_targets:  1 reward:  80\n",
      "14.3 action:  [0.0092, -0.9917] n_targets:  1 reward:  80\n",
      "18.3 action:  [0.0092, -0.9838] n_targets:  1 reward:  80\n",
      "22.4 action:  [-0.0087, -0.9923] n_targets:  1 reward:  80\n"
     ]
    }
   ],
   "source": [
    "model_folder_name = \"RL_models/SB3_stored_models/all_agents/oct_13\"\n",
    "RLforFF = sb3_for_multiff_class.SB3forMultifirefly(\n",
    "    model_folder_name=model_folder_name)\n",
    "RLforFF.streamline_everything(currentTrial_for_animation = 10, num_trials_for_animation = 5)\n",
    "RLforFF.collect_data(exists_ok=True, save_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3778cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CurrentTrial and num_trials are set to be None because of the following error: index 0 is out of bounds for axis 0 with size 0\n",
      "Number of frames is: 298\n",
      "CurrentTrial and num_trials are set to be None because of the following error: index 0 is out of bounds for axis 0 with size 0\n",
      "Number of frames for the animation is: 149\n",
      "Saving animation as: RL_models/SB3_stored_models/all_collected_data/processed_data/oct_12_6/individual_data_sessions/data_0/dv10_dw10_w3_memT3__10s_to_40s_rate_0.25.mp4\n",
      "2025-10-13 12:53:38,709 - INFO - Animation.save using <class 'matplotlib.animation.FFMpegWriter'>\n",
      "2025-10-13 12:53:38,711 - INFO - MovieWriter._run: running command: /home/cicid/miniconda3/envs/multiff_clean/bin/ffmpeg -f rawvideo -vcodec rawvideo -s 640x480 -pix_fmt rgba -framerate 5 -loglevel error -i pipe: -vcodec h264 -pix_fmt yuv420p -y RL_models/SB3_stored_models/all_collected_data/processed_data/oct_12_6/individual_data_sessions/data_0/dv10_dw10_w3_memT3__10s_to_40s_rate_0.25.mp4\n",
      "Animation is saved at: RL_models/SB3_stored_models/all_collected_data/processed_data/oct_12_6/individual_data_sessions/data_0/dv10_dw10_w3_memT3__10s_to_40s_rate_0.25.mp4\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RLforFF.set_animation_parameters(duration=[10, 40],k=1)\n",
    "RLforFF.call_animation_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6daf2389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video controls  >\n",
       " <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\">\n",
       " Your browser does not support the video tag.\n",
       " </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "Video(RLforFF.video_path_name, embed=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b254917d",
   "metadata": {
    "id": "b254917d"
   },
   "source": [
    "# Streamline loading agent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bd105957",
   "metadata": {
    "id": "bd105957"
   },
   "source": [
    "## Make files for logging"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e447c5f9",
   "metadata": {
    "id": "e447c5f9"
   },
   "source": [
    "(Only have to do once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nuE-aO1digZv",
   "metadata": {
    "id": "nuE-aO1digZv"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def ensure_csv(folder: str, filename: str, columns: list[str]) -> None:\n",
    "    filepath = os.path.join(folder, filename)\n",
    "    if not os.path.exists(filepath):\n",
    "        pd.DataFrame(columns=columns).to_csv(filepath, index=False)\n",
    "\n",
    "common_cols = ['dv_cost_factor', 'dw_cost_factor', 'w_cost_factor',\n",
    "               'v_noise_std', 'w_noise_std', 'ffr_noise_scale', 'num_obs_ff', 'max_in_memory_time']\n",
    "\n",
    "ensure_csv(overall_folder, 'family_of_agents_log.csv',\n",
    "           common_cols + ['finished_training', 'year', 'month', 'date',\n",
    "                          'training_time', 'successful_training'])\n",
    "\n",
    "ensure_csv(overall_folder, 'parameters_record.csv',\n",
    "           common_cols + ['working'])\n",
    "\n",
    "ensure_csv(overall_folder, 'pattern_frequencies_record.csv',\n",
    "           common_cols + ['two_in_a_row', 'three_in_a_row', 'four_in_a_row', 'one_in_a_row',\n",
    "                          'multiple_in_a_row', 'multiple_in_a_row_all', 'visible_before_last_one',\n",
    "                          'disappear_latest', 'ignore_sudden_flash', 'try_a_few_times',\n",
    "                          'give_up_after_trying', 'cluster_around_target',\n",
    "                          'waste_cluster_around_target', 'ff_capture_rate', 'stop_success_rate'])\n",
    "\n",
    "feature_cols = common_cols + ['t', 't_last_vis', 'd_last_vis', 'abs_angle_last_vis',\n",
    "                              'hitting_arena_edge', 'num_stops', 'num_stops_since_last_vis',\n",
    "                              'num_stops_near_target', 'num_alive_ff_around_target', 'n_ff_in_a_row']\n",
    "\n",
    "ensure_csv(overall_folder, 'feature_means_record.csv', feature_cols)\n",
    "ensure_csv(overall_folder, 'feature_medians_record.csv', feature_cols)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f22ab42",
   "metadata": {
    "id": "0f22ab42"
   },
   "source": [
    "Make daily backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d245d8ed",
   "metadata": {
    "id": "d245d8ed"
   },
   "outputs": [],
   "source": [
    "# back_up_path = overall_folder + 'family_of_agents_log_' + str(time_package.localtime().tm_mon) + '_' + str(time_package.localtime().tm_mday) + '.csv'\n",
    "# if not exists(back_up_path):\n",
    "#     family_of_agents_log = pd.read_csv(overall_folder + 'family_of_agents_log.csv').drop([\"Unnamed: 0\"], axis=1)\n",
    "#     family_of_agents_log.to_csv(back_up_path)\n",
    "#     print('A back up of family_of_agents_log is stored in', back_up_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1558f19f",
   "metadata": {
    "id": "1558f19f"
   },
   "source": [
    "## Get monkey data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86c2e1f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31188,
     "status": "ok",
     "timestamp": 1684945834636,
     "user": {
      "displayName": "Cici Du",
      "userId": "17701548280142155870"
     },
     "user_tz": -480
    },
    "id": "c86c2e1f",
    "outputId": "250ac96e-7813-4618-a219-2dab7c7e9556"
   },
   "outputs": [],
   "source": [
    "raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Bruno/data_0220\"\n",
    "data_item = further_processing_class.FurtherProcessing(raw_data_folder_path=raw_data_folder_path)\n",
    "data_item.retrieve_or_make_monkey_data()\n",
    "data_item.make_or_retrieve_ff_dataframe(exists_ok=True)\n",
    "data_item.find_patterns()\n",
    "data_item.make_or_retrieve_all_trial_patterns(exists_ok=True)\n",
    "data_item.make_or_retrieve_pattern_frequencies(exists_ok=True)\n",
    "data_item.make_or_retrieve_all_trial_features(exists_ok=True)\n",
    "data_item.make_or_retrieve_feature_statistics(exists_ok=True)\n",
    "data_item.make_info_of_monkey()\n",
    "\n",
    "all_trial_patterns_m = data_item.all_trial_patterns\n",
    "pattern_frequencies_m = data_item.pattern_frequencies\n",
    "all_trial_features_m = data_item.all_trial_features\n",
    "feature_statistics_m = data_item.feature_statistics\n",
    "info_of_monkey = data_item.info_of_monkey"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27fc48d",
   "metadata": {},
   "source": [
    "## Run the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2f4ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_folder = 'RL_models/SB3_stored_models/all_agents/gen_0/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49318ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "RLforFF = sb3_for_multiff_class.SB3forMultifirefly(overall_folder=overall_folder, add_date_to_model_folder_name=False)\n",
    "#RLforFF.import_monkey_data(info_of_monkey, all_trial_features_m, pattern_frequencies_m, feature_statistics_m)\n",
    "RLforFF.streamline_everything(currentTrial_for_animation = 10, num_trials_for_animation = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2d8944",
   "metadata": {},
   "outputs": [],
   "source": [
    "RLforFF.lstm = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb7a0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# timesteps = 10000000        \n",
    "# stop_train_callback = SB3_functions.StopTrainingOnNoModelImprovement(max_no_improvement_evals=20, min_evals=20, verbose=1, model_folder_name=RLforFF.model_folder_name,                                                   overall_folder=RLforFF.overall_folder, agent_id=RLforFF.agent_id)\n",
    "# callback = EvalCallback(RLforFF.env, eval_freq=12000, callback_after_eval=stop_train_callback, verbose=1, best_model_save_path=RLforFF.model_folder_name, n_eval_episodes=3)\n",
    "# RLforFF.sac_model.learn(total_timesteps=int(timesteps), callback=callback)\n",
    "# RLforFF.successful_training = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0edc7f",
   "metadata": {},
   "source": [
    "## Collect data (experimentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e658475",
   "metadata": {},
   "outputs": [],
   "source": [
    "RLforFF.collect_data(exists_ok=True, save_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25cf7cf",
   "metadata": {},
   "source": [
    "## load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8747d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_kwargs = {'overall_folder': overall_folder, \n",
    "            #   'v_noise_std': 0, \n",
    "            #   \"w_noise_std\": 0,\n",
    "            #   'ffr_noise_scale': 0, \n",
    "            #   'num_obs_ff': 2, \n",
    "            #   'max_in_memory_time': 2.5, \n",
    "            #   'add_date_to_model_folder_name': False,\n",
    "            #   \n",
    "            #   'dv_cost_factor': 10,\n",
    "            #   'dw_cost_factor': 10,\n",
    "              }\n",
    "\n",
    "# alternatively...\n",
    "RLforFF = sb3_for_multiff_class.SB3forMultifirefly(**env_kwargs)\n",
    "RLforFF.make_env()\n",
    "RLforFF.make_agent()\n",
    "RLforFF.load_agent(load_replay_buffer=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c1046e",
   "metadata": {},
   "source": [
    "## call_animation_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "918921f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RLforFF' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mRLforFF\u001b[49m.set_animation_parameters(currentTrial=\u001b[32m10\u001b[39m, num_trials=\u001b[32m5\u001b[39m, k=\u001b[32m1\u001b[39m)\n\u001b[32m      2\u001b[39m RLforFF.call_animation_function()\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# RLforFF.combine_6_plots_for_neural_network()\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# RLforFF.calculate_pattern_frequencies_and_feature_statistics()\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# RLforFF.plot_side_by_side()\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# plot_statistics.plot_pattern_frequencies(RLforFF.combd_pattern_frequencies, compare_monkey_and_agent=True, data_folder_name=RLforFF.model_folder_name)\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# #plot_statistics.plot_feature_statistics_for_monkey_and_agent(RLforFF.combd_feature_statistics, data_folder_name = RLforFF.model_folder_name)\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# plot_statistics.plot_feature_histograms_for_monkey_and_agent(RLforFF.all_trial_features_valid, RLforFF.all_trial_features_valid, data_folder_name = RLforFF.model_folder_name)\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'RLforFF' is not defined"
     ]
    }
   ],
   "source": [
    "RLforFF.set_animation_parameters(currentTrial=10, num_trials=5, k=1)\n",
    "RLforFF.call_animation_function()\n",
    "# RLforFF.combine_6_plots_for_neural_network()\n",
    "# RLforFF.calculate_pattern_frequencies_and_feature_statistics()\n",
    "# RLforFF.plot_side_by_side()\n",
    "# plot_statistics.plot_pattern_frequencies(RLforFF.combd_pattern_frequencies, compare_monkey_and_agent=True, data_folder_name=RLforFF.model_folder_name)\n",
    "# #plot_statistics.plot_feature_statistics_for_monkey_and_agent(RLforFF.combd_feature_statistics, data_folder_name = RLforFF.model_folder_name)\n",
    "# plot_statistics.plot_feature_histograms_for_monkey_and_agent(RLforFF.all_trial_features_valid, RLforFF.all_trial_features_valid, data_folder_name = RLforFF.model_folder_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da26c42f",
   "metadata": {
    "id": "da26c42f"
   },
   "source": [
    "# Loop (for hyperparameter tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf95e9a6",
   "metadata": {},
   "source": [
    "## test params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93592d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for dw_cost_factor in range(30, 100, 10):\n",
    "    params = {\n",
    "        'dv_cost_factor': 10,\n",
    "        'dw_cost_factor': dw_cost_factor,\n",
    "        'w_cost_factor': 10,\n",
    "    }\n",
    "\n",
    "    num_obs_ff = 3\n",
    "    max_in_memory_time = 3\n",
    "    overall_folder = f'RL_models/SB3_stored_models/all_agents/env1_test_params/ff{num_obs_ff}/'\n",
    "\n",
    "    env_kwargs = {'num_obs_ff': num_obs_ff,\n",
    "                'max_in_memory_time': max_in_memory_time,\n",
    "                'print_ff_capture_incidents': False\n",
    "                #'reward_per_ff': 120,\n",
    "    }\n",
    "\n",
    "    # check if num_obs_ff is consistent with the name in overall_folder. If not, raise an error\n",
    "    if not f'ff{env_kwargs[\"num_obs_ff\"]}' in overall_folder:\n",
    "        raise ValueError('num_obs_ff is not consistent with the name in overall_folder')\n",
    "\n",
    "    gc.collect()\n",
    "    print(\"Current parameters: \", params)\n",
    "\n",
    "    # params = {'time_cost': 0.0, 'dv_cost_factor': 0.0, 'dw_cost_factor': 0.0, 'w_cost_factor': 0.0, 'v_noise_std': 0.0, 'w_noise_std': 0.0}\n",
    "    RLforFF = sb3_for_multiff_class.SB3forMultifirefly(**params, overall_folder=overall_folder,\n",
    "                                                        **env_kwargs)\n",
    "\n",
    "    #RLforFF.import_monkey_data(info_of_monkey, all_trial_features_m, pattern_frequencies_m, feature_statistics_m)\n",
    "    RLforFF.streamline_everything(currentTrial_for_animation=None, num_trials_for_animation=None, duration=[10, 40],\n",
    "                                    best_model_after_curriculum_exists_ok=True, model_exists_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8260b092",
   "metadata": {},
   "source": [
    "## env1 loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300552bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {\n",
    "#           'dv_cost_factor': 10,\n",
    "#           'dw_cost_factor': 10,\n",
    "#           'w_cost_factor': 10,\n",
    "# }\n",
    "\n",
    "params = {\n",
    "          'dv_cost_factor': 0,\n",
    "          'dw_cost_factor': 0,\n",
    "          'w_cost_factor': 0,\n",
    "}\n",
    "\n",
    "for max_in_memory_time in [3, 2, 1]:\n",
    "    for num_obs_ff in [2, 1, 3, 4, 5]:\n",
    "        overall_folder = f'RL_models/SB3_stored_models/all_agents/env1_relu/ff{num_obs_ff}/'\n",
    "\n",
    "        env_kwargs = {'num_obs_ff': num_obs_ff,\n",
    "                    'max_in_memory_time': max_in_memory_time,\n",
    "                    'print_ff_capture_incidents': False\n",
    "                    #'reward_per_ff': 120,\n",
    "        }\n",
    "\n",
    "        # check if num_obs_ff is consistent with the name in overall_folder. If not, raise an error\n",
    "        if not f'ff{env_kwargs[\"num_obs_ff\"]}' in overall_folder:\n",
    "            raise ValueError('num_obs_ff is not consistent with the name in overall_folder')\n",
    "\n",
    "        gc.collect()\n",
    "        print(\"Current parameters: \", params)\n",
    "\n",
    "        # params = {'time_cost': 0.0, 'dv_cost_factor': 0.0, 'dw_cost_factor': 0.0, 'w_cost_factor': 0.0, 'v_noise_std': 0.0, 'w_noise_std': 0.0}\n",
    "        RLforFF = sb3_for_multiff_class.SB3forMultifirefly(**params, overall_folder=overall_folder,\n",
    "                                                            **env_kwargs)\n",
    "\n",
    "        #RLforFF.import_monkey_data(info_of_monkey, all_trial_features_m, pattern_frequencies_m, feature_statistics_m)\n",
    "        RLforFF.streamline_everything(currentTrial_for_animation=None, num_trials_for_animation=None, duration=[10, 40],\n",
    "                                        best_model_after_curriculum_exists_ok=True, model_exists_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b893a2",
   "metadata": {},
   "source": [
    "## combos of params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26965f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly sample from a range\n",
    "param_combinations = []\n",
    "num_param_combinations = 20\n",
    "for i in range(num_param_combinations):\n",
    "    combo = dict()\n",
    "    #combo[\"time_cost\"] = round(random.uniform(20, 50), 2)\n",
    "    combo[\"dv_cost_factor\"] = round(random.uniform(50, 150), 2)\n",
    "    combo[\"dw_cost_factor\"] = round(random.uniform(50, 150), 2)\n",
    "    combo[\"w_cost_factor\"] = round(random.uniform(50, 150), 2)\n",
    "    param_combinations.append(combo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224212ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_folder = 'RL_models/SB3_stored_models/all_agents/env1_relu/ff3/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c7c048",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_kwargs = {'num_obs_ff': 3,\n",
    "              'print_ff_capture_incidents': False\n",
    "              #'reward_per_ff': 120,\n",
    "}\n",
    "\n",
    "for count, params in enumerate(param_combinations):\n",
    "    gc.collect()\n",
    "    print(\"Running\", str(count), \"out of\", len(param_combinations), \"combinations of parameters\")\n",
    "    print(\"Current parameters: \", params)\n",
    "\n",
    "    # params = {'time_cost': 0.0, 'dv_cost_factor': 0.0, 'dw_cost_factor': 0.0, 'w_cost_factor': 0.0, 'v_noise_std': 0.0, 'w_noise_std': 0.0}\n",
    "\n",
    "    RLforFF = sb3_for_multiff_class.SB3forMultifirefly(**params, overall_folder=overall_folder,\n",
    "                                                     **env_kwargs)\n",
    "\n",
    "    #RLforFF.import_monkey_data(info_of_monkey, all_trial_features_m, pattern_frequencies_m, feature_statistics_m)\n",
    "    RLforFF.streamline_everything(currentTrial_for_animation=None, num_trials_for_animation=None, duration=[10, 40],\n",
    "                                  best_model_after_curriculum_exists_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5061dae",
   "metadata": {},
   "source": [
    "### make animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81448918",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RLforFF.load_best_model_after_curriculum()\n",
    "RLforFF.streamline_making_animation(duration=[10, 40])\n",
    "#RLforFF.streamline_loading_and_making_animation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d25498",
   "metadata": {},
   "source": [
    "## env2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077effa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly sample from a range\n",
    "param_combinations = []\n",
    "num_param_combinations = 10\n",
    "for i in range(num_param_combinations):\n",
    "    combo = dict()\n",
    "    #combo[\"time_cost\"] = round(random.uniform(20, 50), 2)\n",
    "    combo[\"dv_cost_factor\"] = round(random.uniform(50, 150), 2)\n",
    "    combo[\"dw_cost_factor\"] = round(random.uniform(50, 150), 2)\n",
    "    combo[\"w_cost_factor\"] = round(random.uniform(50, 150), 2)\n",
    "    param_combinations.append(combo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5d689e",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_folder = 'RL_models/SB3_stored_models/all_agents/gen_28_env2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935605e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_kwargs = {'num_obs_ff': 10,\n",
    "              'make_ff_always_flash_on': False,\n",
    "              'max_in_memory_time': 3,\n",
    "              'linear_terminal_vel': 0.01,\n",
    "              'angular_terminal_vel': 0.01,\n",
    "              'reward_per_ff': 100,\n",
    "              'dt': 0.1,\n",
    "              'add_cost_when_catching_ff_only': False,\n",
    "              #'reward_per_ff': 120,\n",
    "}\n",
    "\n",
    "for count, params in enumerate(param_combinations):\n",
    "    gc.collect()\n",
    "    print(\"Running\", str(count), \"out of\", len(param_combinations), \"combinations of parameters\")\n",
    "    print(\"Current parameters: \", params)\n",
    "\n",
    "    # params = {'time_cost': 0.0, 'dv_cost_factor': 0.0, 'dw_cost_factor': 0.0, 'w_cost_factor': 0.0, 'v_noise_std': 0.0, 'w_noise_std': 0.0}\n",
    "\n",
    "    RLforFF = sb3_for_multiff_class.SB3forMultifirefly(**params, overall_folder=overall_folder,\n",
    "                                                     **env_kwargs,\n",
    "                                                     use_env2=True)\n",
    "\n",
    "    #RLforFF.import_monkey_data(info_of_monkey, all_trial_features_m, pattern_frequencies_m, feature_statistics_m)\n",
    "    RLforFF.streamline_everything(currentTrial_for_animation=None, num_trials_for_animation=None, duration=[10, 40],\n",
    "                                  best_model_after_curriculum_exists_ok=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffb305a",
   "metadata": {},
   "source": [
    "## remake animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94af079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For every folder in slow:\n",
    "# # Extract parameters from the folder name\n",
    "# # Re-make the env\n",
    "# # Load agent\n",
    "# # And then make animation\n",
    "\n",
    "# overall_folder = 'RL_models/SB3_stored_models/all_agents/gen_30_env1_4ff'\n",
    "# folders = os.listdir(overall_folder)\n",
    "# for folder in folders:\n",
    "#     if 'time' not in folder:\n",
    "#         continue\n",
    "#     print(\"Currently working on\", folder)\n",
    "#     folder_path = overall_folder + '/' + folder\n",
    "#     params = rl_for_multiff_utils.extract_cost_params_from_folder_name(folder)\n",
    "#     RLforFF = sb3_for_multiff_class.SB3forMultifirefly(**params, overall_folder=overall_folder,\n",
    "#                                                      make_ff_always_flash_on=True)\n",
    "#     #RLforFF.import_monkey_data(info_of_monkey, all_trial_features_m, pattern_frequencies_m, feature_statistics_m)\n",
    "#     RLforFF.model_folder_name = folder_path\n",
    "#     RLforFF.make_env()\n",
    "#     RLforFF.make_agent()\n",
    "#     RLforFF.load_agent(load_replay_buffer=False)\n",
    "#     RLforFF.collect_data()\n",
    "#     RLforFF.set_animation_parameters(currentTrial=5, num_trials=5, k=1)\n",
    "#     RLforFF.call_animation_function()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5409a0bd",
   "metadata": {
    "id": "5409a0bd"
   },
   "source": [
    "# Env"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9703eb0c",
   "metadata": {
    "id": "9703eb0c"
   },
   "source": [
    "## regular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a34ff5",
   "metadata": {
    "id": "30a34ff5"
   },
   "outputs": [],
   "source": [
    "env_kwargs = {}\n",
    "env = env_for_sb3.EnvForSB3(**env_kwargs)\n",
    "model_folder_name = \"RL_models/SB3_stored_models/all_agents/regular\"\n",
    "os.makedirs(model_folder_name, exist_ok=True)\n",
    "env = Monitor(env, model_folder_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb32d7b0",
   "metadata": {},
   "source": [
    "## make_ff_always_flash_on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a6ab06",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_kwargs = {'make_ff_always_flash_on': True,\n",
    "              'ffr_noise_scale': 0,}\n",
    "env = env_for_sb3.EnvForSB3(**env_kwargs)\n",
    "model_folder_name = \"RL_models/SB3_stored_models/all_agents/regular\"\n",
    "os.makedirs(model_folder_name, exist_ok=True)\n",
    "env = Monitor(env, model_folder_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "83d0be92",
   "metadata": {
    "id": "83d0be92"
   },
   "source": [
    "## one-ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c7fb7e",
   "metadata": {
    "id": "08c7fb7e"
   },
   "outputs": [],
   "source": [
    "env_kwargs = {'num_obs_ff': 1}\n",
    "env = env_for_sb3.EnvForSB3(**env_kwargs)\n",
    "env = Monitor(env, model_folder_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6d8ef0c3",
   "metadata": {
    "id": "6d8ef0c3"
   },
   "source": [
    "## with noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70417219",
   "metadata": {
    "id": "70417219"
   },
   "outputs": [],
   "source": [
    "env_kwargs = {\"v_noise_std\": 0.1, \n",
    "              \"w_noise_std\": 0.1,\n",
    "              \"ffr_noise_scale\": 4, \n",
    "              \"num_obs_ff\": 2,\n",
    "              \"max_in_memory_time\": 2.5}\n",
    "# model_folder_name =overall_folder + \"/SB3_stored_models/all_agents/v\" +\n",
    "#                        str(v_noise_std) + \"_w_\" + str(w_noise_std) + \"_o_\" + str(ffr_noise_scale) + \\\n",
    "#                        \"_ff_\" + str(num_obs_ff) + \"_m_\" + str(max_in_memory_time)\n",
    "env = env.MultiFF(**env_kwargs)\n",
    "env = Monitor(env, model_folder_name)\n",
    "model_folder_name = \"RL_models/SB3_stored_models/all_agents/A0.2_O4_ff2_M3\"\n",
    "os.makedirs(model_folder_name, exist_ok=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "50d88dc9",
   "metadata": {
    "id": "50d88dc9"
   },
   "source": [
    "# Agent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ea153406",
   "metadata": {
    "id": "ea153406"
   },
   "source": [
    "## make agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c411166",
   "metadata": {
    "id": "4c411166"
   },
   "outputs": [],
   "source": [
    "# For direct training\n",
    "sac_model = SAC(\"MlpPolicy\", \n",
    "            env,\n",
    "            gamma=0.995,\n",
    "            learning_rate=0.0015,\n",
    "            batch_size=1024,\n",
    "            target_update_interval=50,\n",
    "            buffer_size=1000000,\n",
    "            learning_starts=10000,\n",
    "            train_freq=10,\n",
    "            ent_coef='auto',\n",
    "            policy_kwargs=dict(activation_fn=nn.Tanh, net_arch=[128, 128])\n",
    "            )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "37268cc4",
   "metadata": {
    "id": "37268cc4"
   },
   "source": [
    "## load agent (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd640727",
   "metadata": {
    "id": "fd640727"
   },
   "outputs": [],
   "source": [
    "path = os.path.join(model_folder_name, 'best_model.zip')\n",
    "path2 = os.path.join(model_folder_name, 'buffer.pkl')\n",
    "sac_model = sac_model.load(path,env=env) \n",
    "sac_model.load_replay_buffer(path2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b372808d",
   "metadata": {
    "id": "b372808d"
   },
   "source": [
    "## Train agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeba8196",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder_name = \"RL_models/SB3_stored_models/all_agents/temp_9_29\"\n",
    "os.makedirs(model_folder_name, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a908ec",
   "metadata": {
    "id": "21a908ec"
   },
   "outputs": [],
   "source": [
    "callback = SB3_functions.SaveOnBestTrainingRewardCallback(check_freq=20000, model_folder_name=model_folder_name)\n",
    "#timesteps = 50000000\n",
    "timesteps = 500000\n",
    "sac_model.learn(total_timesteps=int(timesteps), callback=callback)\n",
    "plot_results([model_folder_name], timesteps, results_plotter.X_TIMESTEPS, \"env.MultiFF\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1919e84e",
   "metadata": {
    "id": "1919e84e"
   },
   "outputs": [],
   "source": [
    "plot_results([model_folder_name], timesteps, results_plotter.X_TIMESTEPS, \"env.MultiFF\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3d27b8",
   "metadata": {
    "id": "9c3d27b8"
   },
   "outputs": [],
   "source": [
    "sac_model.save(os.path.join(model_folder_name, 'best_model'))\n",
    "sac_model.save_replay_buffer(os.path.join(model_folder_name, 'buffer')) # I added this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492f1169",
   "metadata": {
    "id": "492f1169"
   },
   "outputs": [],
   "source": [
    "stop_train_callback = SB3_functions.StopTrainingOnNoModelImprovement(max_no_improvement_evals=10, min_evals=20, verbose=1)\n",
    "callback = EvalCallback(env, eval_freq=5000, callback_after_eval=stop_train_callback, verbose=1)\n",
    "# timesteps = 50000000\n",
    "timesteps = 5000000\n",
    "sac_model.learn(total_timesteps=int(timesteps), callback=callback)\n",
    "\n",
    "sac_model.save(os.path.join(model_folder_name, 'best_model'))\n",
    "sac_model.save_replay_buffer(os.path.join(model_folder_name, 'buffer')) # I added this"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d075400f",
   "metadata": {
    "id": "d075400f"
   },
   "source": [
    "# Animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382fcebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "RL_anim = sb3_for_multiff_class.SB3forMultifirefly(overall_folder='RL_models/SB3_stored_models/all_agents/gen_12/', **env_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bad6a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "RL_anim.sac_model = sac_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8974494c",
   "metadata": {},
   "outputs": [],
   "source": [
    "RL_anim.streamline_making_animation()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b056b5cc",
   "metadata": {
    "id": "b056b5cc"
   },
   "source": [
    "# Interpret neural network (only works on 1-ff env rn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cb94d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## use combine_6_plots_for_neural_network from rl_for_multiff_class"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "efeeb570",
   "metadata": {
    "id": "efeeb570"
   },
   "source": [
    "# Plot statistics (pasted from visualization.ipynb)\n",
    "compare monkey and agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a06c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the function above from rl_for_multiff_class:   \n",
    "    # def save_plots_in_data_folders(self):\n",
    "    #     plot_statistics.plot_pattern_frequencies(self.agent_monkey_pattern_frequencies, compare_monkey_and_agent=True, data_folder_name=self.processed_data_folder_path)\n",
    "    #     plot_statistics.plot_feature_statistics(self.agent_monkey_feature_statistics, compare_monkey_and_agent=True, data_folder_name = self.processed_data_folder_path)\n",
    "\n",
    "    #     plot_statistics.plot_feature_histograms_for_monkey_and_agent(self.all_trial_features_valid_m, self.all_trial_features_valid, data_folder_name = self.model_folder_name)\n",
    "    #     print(\"Made new plots\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "TQMCDI-FqP6s",
   "metadata": {
    "id": "TQMCDI-FqP6s"
   },
   "source": [
    "# Plot side_by_side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57d26ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the function plot_side_by_side from rl_for_multiff_class:   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "xIYoWAIyDieu",
   "metadata": {
    "id": "xIYoWAIyDieu"
   },
   "source": [
    "# Polar"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aoaz1_ng6QMt",
   "metadata": {
    "id": "aoaz1_ng6QMt"
   },
   "source": [
    "## Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZrSbMmCJ5uN1",
   "metadata": {
    "id": "ZrSbMmCJ5uN1"
   },
   "outputs": [],
   "source": [
    "num_trials = 1\n",
    "for currentTrial in range(40,43):\n",
    "    print(currentTrial)\n",
    "    #duration = [ff_caught_T_new[currentTrial-num_trials], ff_caught_T_new[currentTrial]]\n",
    "    duration = [ff_caught_T_new[currentTrial]-1.25, ff_caught_T_new[currentTrial]]\n",
    "\n",
    "\n",
    "    plot_polar.PlotPolar(duration,\n",
    "              monkey_information,\n",
    "              ff_dataframe, \n",
    "              ff_life_sorted,\n",
    "              ff_real_position_sorted,\n",
    "              ff_caught_T_new,\n",
    "              ff_flash_sorted,\n",
    "              rmax = 100,\n",
    "              currentTrial = currentTrial,\n",
    "              num_trials = num_trials,\n",
    "              show_visible_ff = True,\n",
    "              show_visible_target = True,\n",
    "              show_ff_in_memory = True,\n",
    "              show_target_in_memory = True,\n",
    "              show_alive_ff = True\n",
    "                )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ZLNksPtjduik",
   "metadata": {
    "id": "ZLNksPtjduik"
   },
   "source": [
    "## Animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79OZSKw1duBP",
   "metadata": {
    "id": "79OZSKw1duBP"
   },
   "outputs": [],
   "source": [
    "currentTrial = 10\n",
    "num_trials = 2\n",
    "filename = f\"Trials {currentTrial-num_trials+1}-{currentTrial}\"\n",
    "print(filename)\n",
    "k = 4\n",
    "rmax = 100\n",
    "colors_Reds = plt.get_cmap(\"Reds\")(np.linspace(0,1,101))\n",
    "colors_YlGn = plt.get_cmap(\"YlGn\")(np.linspace(0,1,101))\n",
    "cum_pos_index = np.where((monkey_information['time'] > ff_caught_T_new[currentTrial-num_trials]) & \n",
    "                       (monkey_information['time'] <= ff_caught_T_new[currentTrial]))\n",
    "\n",
    "if len(cum_pos_index) > 0:\n",
    "    fig = plt.figure(figsize=(7,7))\n",
    "    ax = fig.add_axes([0.1, 0.1, 0.8, 0.8], polar=True)\n",
    "    ax = plot_behaviors_utils.set_polar_background_for_animation(ax, rmax)\n",
    "\n",
    "    ff_in_time_frame, ff_visible, ff_in_memory = animation_func.subset_ff_dataframe(ff_dataframe, currentTrial, num_trials)\n",
    "    anim_indices = cum_pos_index[0][0:-1:k]\n",
    "    num_frames = anim_indices.size\n",
    "    animate_func = partial(animation_func.animate_polar, ax=ax, anim_indices=anim_indices, rmax=400, ff_in_time_frame=ff_in_time_frame, ff_visible=ff_visible, ff_in_memory=ff_in_memory)\n",
    "    anim = animation.FuncAnimation(fig, animate_func, frames=num_frames, interval=int(250*k), repeat=True) \n",
    "\n",
    "    #gif_dir = '/content/gdrive/My Drive/fireflies_anim/???'\n",
    "    #anim.save(f\"{gif_dir}/{filename}.gif\", writer='pillow', fps=60)\n",
    "else:\n",
    "    print(\"Please try another number for currentTrial, or increase num_trials.\")\n",
    "\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "r6j1e2uxq5OG",
   "metadata": {
    "id": "r6j1e2uxq5OG"
   },
   "source": [
    "# Test agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k9-tdbAHq5_U",
   "metadata": {
    "id": "k9-tdbAHq5_U"
   },
   "outputs": [],
   "source": [
    "obs, _ = env.reset()\n",
    "cum_rewards = 0\n",
    "for step in range(1000):\n",
    "    action, _ = sac_model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, _, info = env.step(action)\n",
    "    cum_rewards += reward\n",
    "    if done:\n",
    "        obs, _ = env.reset()\n",
    "    # print(step, ffxy_visible[-1])\n",
    "print(cum_rewards)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1Inbx37xwycW",
   "metadata": {
    "id": "1Inbx37xwycW"
   },
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "hfHYXONdz7qJ",
   "metadata": {
    "id": "hfHYXONdz7qJ"
   },
   "source": [
    "## parameters to sample from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xhBysB7hz_F8",
   "metadata": {
    "id": "xhBysB7hz_F8"
   },
   "outputs": [],
   "source": [
    "def sample_sac_params(trial):\n",
    "    \"\"\"\n",
    "    Sampler for SAC hyperparams.\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    trial: (optuna.trial)\n",
    "\n",
    "    Return: (dict)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    gamma = 1.0 - trial.suggest_float(\"gamma\", 0.0001, 0.1, log=True)\n",
    "    learning_rate = trial.suggest_float(\"lr\", 1e-5, 1, log=True)\n",
    "    tau = trial.suggest_float(\"tau\", 1e-6, 1, log=True)\n",
    "    #batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128, 256, 512, 1024])\n",
    "    target_update_interval = trial.suggest_categorical('target_update_interval', [5, 10, 20, 40, 60, 100, 200])\n",
    "    #buffer_size = trial.suggest_categorical('buffer_size', [int(1e5), int(1e6)]) # This actually doesn't matter much here because of limited timesteps\n",
    "    learning_starts = trial.suggest_categorical('learning_starts', [5000, 10000, 15000])\n",
    "    train_freq = trial.suggest_categorical('train_freq', [1, 10, 100, 300])\n",
    "    ## gradient_steps takes too much time\n",
    "    # gradient_steps = trial.suggest_categorical('gradient_steps', [1, 100, 300])\n",
    "    gradient_steps = train_freq\n",
    "    ent_coef = trial.suggest_float(\"ent_coef\", 0.00000001, 0.1, log=True)\n",
    "    net_arch = trial.suggest_categorical('net_arch', [\"small\", \"medium\", \"big\"])\n",
    "    activation_fn = trial.suggest_categorical(\"activation_fn\", [\"tanh\"])\n",
    "\n",
    "    net_arch = {\n",
    "        'small': [100, 100],\n",
    "        'medium': [128, 128],\n",
    "        'big': [200, 200],\n",
    "    }[net_arch]\n",
    "\n",
    "    activation_fn = {\"tanh\": nn.Tanh, \"relu\": nn.ReLU}[activation_fn]\n",
    "\n",
    "    target_entropy = 'auto'\n",
    "    if ent_coef == 'auto':\n",
    "        target_entropy = trial.suggest_categorical('target_entropy', ['auto', -1, -10, -20, -50, -100])\n",
    "\n",
    "\n",
    "    ## Display true values\n",
    "    # trial.set_user_attr(\"gamma_\", gamma)\n",
    "    # trial.set_user_attr(\"n_steps\", n_steps)\n",
    "\n",
    "\n",
    "    return {\n",
    "        'gamma': gamma,\n",
    "        'learning_rate': learning_rate,\n",
    "        'tau': tau,\n",
    "        #'batch_size': batch_size,\n",
    "        'target_update_interval': target_update_interval,\n",
    "        #'buffer_size': buffer_size,\n",
    "        'learning_starts': learning_starts,\n",
    "        'train_freq': train_freq,\n",
    "        'gradient_steps': gradient_steps,\n",
    "        'ent_coef': ent_coef,\n",
    "        'target_entropy': target_entropy,\n",
    "        'policy_kwargs': {\n",
    "            \"net_arch\": net_arch,\n",
    "            \"activation_fn\": activation_fn\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "iV2OVSz10Bve",
   "metadata": {
    "id": "iV2OVSz10Bve"
   },
   "source": [
    "## objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-3YZk9SVxCWh",
   "metadata": {
    "id": "-3YZk9SVxCWh"
   },
   "outputs": [],
   "source": [
    "def objective(trial: optuna.Trial) -> float:\n",
    "    kwargs = DEFAULT_HYPERPARAMS.copy()\n",
    "    # Sample hyperparameters\n",
    "    kwargs.update(sample_sac_params(trial))\n",
    "    # Create the RL model\n",
    "    model = SAC(**kwargs)\n",
    "    # Create env used for evaluation\n",
    "    eval_env = env\n",
    "    # Create the callback that will periodically evaluate\n",
    "    # and report the performance\n",
    "    eval_callback = SB3_functions.TrialEvalCallback(\n",
    "      eval_env, trial, n_eval_episodes=N_EVAL_EPISODES, eval_freq=eval_freq, deterministic=True\n",
    "    )\n",
    "\n",
    "    nan_encountered = False\n",
    "    try:\n",
    "      model.learn(N_TIMESTEPS, callback=eval_callback)\n",
    "    except AssertionError as e:\n",
    "      # Sometimes, random hyperparams can generate NaN\n",
    "      print(e)\n",
    "      nan_encountered = True\n",
    "    finally:\n",
    "      # Free memory\n",
    "      model.env.close()\n",
    "      eval_env.close()\n",
    "\n",
    "    # Tell the optimizer that the trial failed\n",
    "    if nan_encountered:\n",
    "      return float(\"nan\")\n",
    "\n",
    "    if eval_callback.is_pruned:\n",
    "      raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return eval_callback.last_mean_reward"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "UVyTUNFzHtLP",
   "metadata": {
    "id": "UVyTUNFzHtLP"
   },
   "source": [
    "## run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Dhl9rpZPH0w6",
   "metadata": {
    "id": "Dhl9rpZPH0w6"
   },
   "outputs": [],
   "source": [
    "env = env_for_sb3.EnvForSB3()\n",
    "env.reset()\n",
    "\n",
    "\n",
    "\n",
    "DEFAULT_HYPERPARAMS = {\n",
    "    \"policy\": \"MlpPolicy\",\n",
    "    \"env\": env,\n",
    "}\n",
    "\n",
    "N_TRIALS = 100\n",
    "N_STARTUP_TRIALS = 5\n",
    "N_EVALUATIONS = 2\n",
    "\n",
    "\n",
    "N_TIMESTEPS = 100000\n",
    "eval_freq = int(N_TIMESTEPS / N_EVALUATIONS)\n",
    "N_EVAL_EPISODES = 1\n",
    "\n",
    "\n",
    "\n",
    "# Set pytorch num threads to 1 for faster training\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "sampler = TPESampler(n_startup_trials=N_STARTUP_TRIALS)\n",
    "# Do not prune before 1/3 of the max budget is used\n",
    "pruner = MedianPruner(n_startup_trials=N_STARTUP_TRIALS, n_warmup_steps=N_EVALUATIONS//3)\n",
    "\n",
    "study = optuna.create_study(sampler=sampler, pruner=pruner, direction=\"maximize\")\n",
    "try:\n",
    "    study.optimize(objective, n_trials=N_TRIALS)\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "print(\"  User attrs:\")\n",
    "for key, value in trial.user_attrs.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "Pj9Wo6ylF0MI",
   "metadata": {
    "id": "Pj9Wo6ylF0MI"
   },
   "source": [
    "# Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HQmdse_eWe-F",
   "metadata": {
    "id": "HQmdse_eWe-F"
   },
   "outputs": [],
   "source": [
    "!pip install -Uqq ipdb\n",
    "import ipdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hkdgF8xfWgS-",
   "metadata": {
    "id": "hkdgF8xfWgS-"
   },
   "outputs": [],
   "source": [
    "%pdb on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3743ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "category_item.clean_out_cross_boundary_trials()\n",
    "category_item.clean_out_trials_where_target_cluster_was_not_seen_for_a_long_time_before_capture()\n",
    "category_item.make_polar_plot_of_target_last_seen_positions()\n",
    "category_item.make_histograms_of_target_last_seen_attributes()\n",
    "category_item.make_histogram_of_distances_from_previous_targets()\n",
    "category_item.make_polar_plot_of_positions_from_previous_targets()\n",
    "category_item.plot_trajectories(trials=category_item.sort_1_trials[17:18])\n",
    "category_item.plot_distributions_of_visible_ff_and_in_memory_ff()\n",
    "#category_item.make_and_visualize_free_selection_predictions_using_trained_model(trained_model = gnb)\n",
    "# category_item.inspect_special_cases(weird_trials=[98, 180, 212, 649])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "1GN5Y_MBqDlaM8t8KZqZZKeERvOau11W_",
     "timestamp": 1681009447473
    }
   ]
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "multiff_clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
