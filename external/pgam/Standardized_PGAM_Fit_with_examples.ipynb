{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Standardizing PGAM fits\n",
    "--------------------------------------\n",
    "\n",
    "In this notebook we propose a pipeline for fitting PGAMs (Balzani et al., 2020, NeurIPS) with a standard input and output format. The procedure has the following advantages:\n",
    "\n",
    "1. It requires minimal coding (limited to formatting the input data in a standard format)\n",
    "2. The output format is easily exportable to other tools and programing languages (e.g., MATLAB)\n",
    "3. The pipeline is compatible with the PGAM <a href=\"https://hub.docker.com/r/edoardobalzani87/pgam\"> Docker</a> and <a href=\"https://osf.io/pcsav/\"> Singularity</a> images.\n",
    "4. The pipleine can be easily parallelized for HPC usage (singularity containers).\n",
    "\n",
    "\n",
    "## Table of contents\n",
    "* [Standard input format](#standard-input-format)\n",
    "    * [Neural & behavioral inputs](#inputs)\n",
    "    * [Configuration files](#configuration-files)\n",
    "* [Example with synthetic data](#example-with-synthetic-data)\n",
    "    * [Create and save an example configuration file](#create-and-save-conf)\n",
    "    * [Fittng the model](#fitting-model)\n",
    "        * [List the fits by experiment, condition, neuron, and model configuration](#fit-list)\n",
    "        * [Load inputs, fit and postprocess](#load-fit-save)\n",
    "* [Example with real data](#example-with-real-data)\n",
    "    * [Create and save an example configuration file](#create-and-save-conf-for-real-data)\n",
    "    * [Fittng the model](#fitting-model-for-real-data)\n",
    "        * [List the fits by experiment, condition, neuron, and model configuration](#fit-list-real-data)\n",
    "        * [Load inputs, fit and postprocess](#load-fit-save-real-data)\n",
    "* [Fit via docker](#docker)\n",
    "    * [Mounting volumes](#mount-volumes)\n",
    "    * [Setting paths](#container-path)\n",
    "    * [Fit by running the container](#fit-container)\n",
    "* [HPC & singularity ](#HPC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Standard input format <a name=\"standard-input-format\"></a>\n",
    "\n",
    "In order to fully specify the PGAM model we need:\n",
    "\n",
    "1. A matrix containing the population <a href=\"#counts\" >spike counts</a> (to be used as response variable and/or covariates), and a list with the <a href=\"#neu_names\">neuron identifiers</a>\n",
    "\n",
    "2. A matrix containing the <a href=\"#variables\" >task variables</a> (additional covariates), and a list with the respective <a href=\"#variable_names\" >identifiers</a>\n",
    "\n",
    "3. A vector containing <a href=\"#trial_id\" >trial IDs</a>\n",
    "\n",
    "4. A list of <a href=\"#trial_id\" >covariates</a> to be included and the ID of the unit that we want to fit.\n",
    "\n",
    "5. A <a href=\"#configuration-files\" >configuration file</a> listing the model parameters in \"YAML\" file format\n",
    "    \n",
    "We propose the following standard input format to facilitate model specification, fitting, and saving. \n",
    "\n",
    "### **Neural & behavioral inputs [1-4].** <a name=\"inputs\"></a>\n",
    "\n",
    "Inputs [1-4] will be contained in a single \".npz\" files with keys: \n",
    "\n",
    "- **counts**: <a name=\"counts\"></a>numpy.array of dimension (T x N), where T is the total number of time points (bins), and N is the number of neurons. The code has been extensively tested with bins of size 6ms, but works for any size.\n",
    "- **variables**: <a name=\"variables\"></a>numpy.array of dimension (T x M), T as above (same bin size), M the number of task variables. For continous variables, each entry is the value taken by the associated variable. For event variables, it is \"1-hot-encoding\": zeros everywhere except for when the event happens, taking a value of '1'. \n",
    "- **trial_id**: <a name=\"trial_id\"></a>numpy.array of dimension T, trial ids associated to each time point. All time points for a given trial take the same value. \n",
    "- **variable_names**: <a name=\"variable_names\"></a>numpy.array of strings of dimension M, name of each covariate in 'variables'.\n",
    "- **neu_names**: <a name=\"neu_names\"></a>numpy.array of strings of dimension N, label uniquely identifying the neurons in 'counts'\n",
    "- **neu_info**: dict, *optional\n",
    "    neu_info[neu_names[k]]: <a name=\"neu_info\"></a>dict, info about the k-th neuron, $k=0,\\dots,N-1$. keys are the information label.\n",
    "\n",
    "\n",
    "### **Configuration files [5]**<a name=\"configuration-files\"></a>\n",
    "\n",
    "Input [5], the configuration parameters for the B-spline, will be stored with the \"YAML\" file format. Python dict objects can be readily saved in YAML format as follows,\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "    import yaml\n",
    "    with open('data.yml', 'w') as outfile:\n",
    "        yaml.dump(python_dictionary, outfile, default_flow_style=False)\n",
    "```\n",
    "\n",
    "where *python_dictionary* is any python dict object whose values are strings, numeric, list containig strings or numeric, or dict.\n",
    "\n",
    "To read a \"YAML\"\n",
    "\n",
    "```python\n",
    "    with open(\"data.yaml\", \"r\") as stream:\n",
    "        try:\n",
    "            python_dicttionary = yaml.safe_load(stream)\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(exc)\n",
    "```\n",
    "\n",
    "The dictionary with the \"YAML\" should be structured as follows:\n",
    "```yaml\n",
    "    var_1: # This is an example of a 2D continous variable. \n",
    "      is_temporal_kernel: false # false for continous variable, true for events (example below)\n",
    "      kernel_direction: .nan # '.nan' when \"is_temporal_kernel: false\". When \"true\", 0 = acausal (bidirectional), '1' = causal (i.e., firing change after the event happens), '-1' = anticipatory (i.e., firing change before event happens).\n",
    "      kernel_length: .nan\n",
    "      order: 4 # Order of the B-spline. A step-function is order = 1. \n",
    "      knots_num: .nan # \".nan\" will use the knots passed (below). An integer will place knots equi-spaced.\n",
    "      knots: # list of knot lists for tuning function. Provide 1 set of knots for each dimesion of the basis. The first and last entries are repeated order-times (given recursive nature of B-spline definition).\n",
    "      - # first set of knots\n",
    "        - -5.0\n",
    "        - -5.0\n",
    "        - -5.0\n",
    "        - -5.0\n",
    "        - -3.888888888888889\n",
    "        - -2.7777777777777777\n",
    "        - -1.6666666666666665\n",
    "        - -0.5555555555555554\n",
    "        - 0.5555555555555554\n",
    "        - 1.666666666666667\n",
    "        - 2.7777777777777786\n",
    "        - 3.8888888888888893\n",
    "        - 5.0\n",
    "        - 5.0\n",
    "        - 5.0\n",
    "        - 5.0\n",
    "      - # second set of knots\n",
    "        - -5.0\n",
    "        - -5.0\n",
    "        - -5.0\n",
    "        - -5.0\n",
    "        - -3.888888888888889\n",
    "        - -2.7777777777777777\n",
    "        - -1.6666666666666665\n",
    "        - -0.5555555555555554\n",
    "        - 0.5555555555555554\n",
    "        - 1.666666666666667\n",
    "        - 2.7777777777777786\n",
    "        - 3.8888888888888893\n",
    "        - 5.0\n",
    "        - 5.0\n",
    "        - 5.0\n",
    "        - 5.0\n",
    "      penalty_type: der # either 'der' or 'diff'. 'der' is derivate-based penality, 'diff' is difference based penality (see Tutorial 1:PGAM tutorial.ipynb) \n",
    "      der: 2 # order of the derivative used for penalization ('2' means second derivative, penalized wiggliness). Default is '2'.\n",
    "      is_cyclic: # note: should be a vector, one element for each dimension.\n",
    "      - false\n",
    "      - false\n",
    "      lam: 10 # initial lambda for regularization (i.e., how strong is the smoothing penalization. The larger, the more smoothing you get. This is just for initial value, hyper-parameter that is learned in fitting).\n",
    "      samp_period: 0.006 #size of bin for spike counts and behavioral variables (in seconds).\n",
    "      join_var: ['varname_1', 'varname_2'] # list of the variables that should be combined in a multidim response function. \"varname_i\" should match an entry in \"variable_names\". \n",
    "    var_2: # This is an example of an event variable (temporal kernel). Name of \"var_2\" has to be in \"variable_names\". \n",
    "      is_temporal_kernel: true\n",
    "      kernel_direction: 0 # acausal filter (meaning impact of event on firing could happen earlier or after event)\n",
    "      kernel_length: 201 # number of time-points of the total kernel (in this example 201 samples x 6ms). Suggested to be odd number of samples.\n",
    "      order: 4\n",
    "      knots_num: 8 # will create equi-spaces knots.\n",
    "      knots: .nan # '.nan' when 'knots_num' is an integer.\n",
    "      penalty_type: der\n",
    "      der: 2\n",
    "      is_cyclic:\n",
    "      - false\n",
    "      lam: 10\n",
    "      samp_period: 0.006\n",
    "    neuron_X: # Where \"neuron X\" is an input in \"neu_names\"\n",
    "      is_temporal_kernel: true\n",
    "      kernel_direction: 1 # must be '1' for auto-correlation.\n",
    "      kernel_length: 201\n",
    "      order: 4\n",
    "      knots_num: 8\n",
    "      knots: .nan\n",
    "      penalty_type: der      \n",
    "      der: 2  \n",
    "      is_cyclic:\n",
    "      - false\n",
    "      lam: 10\n",
    "      samp_period: 0.006\n",
    "      join_var: .nan # not needed for temporal variables (assumed 1 dimensional). \n",
    "    \n",
    "```\n",
    "\n",
    "where ***var_1*** is a prototypical continous (spatial) variable, ***var_2*** is a prototypical event (temporal) variable. ***spike_history*** will be used for the spike-count auto-correlation term (e.g. the neural spike history as its own predictor, as in an auto-regressive model) **and for the neural couplings (e.g. the counts of simultaneously recoded neurons as predictors)**. See the \"PGAM tutorial.ipynb\" for a definition of continous (spatial) and event (temporal) variables, as well as for a description of the B-spline parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# Example with synthetic data <a name=\"example-with-synthetic-data\"></a>\n",
    "\n",
    "Before diving into the pipeline implementation, we will create an example synthetic dataset below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('src/PGAM/')\n",
    "from GAM_library import *\n",
    "import gam_data_handlers as gdh\n",
    "import matplotlib.pylab as plt\n",
    "import pandas as pd\n",
    "from post_processing import postprocess_results\n",
    "import scipy.stats as sts\n",
    "\n",
    "\n",
    "# flag if you want to save the in-silico data\n",
    "save_output = True\n",
    "\n",
    "## inputs parameters\n",
    "num_events = 50\n",
    "time_points = int(0.5 * 10 ** 4)  # 30 sec at 0.006 second resolution\n",
    "rate = 150. * 0.006  # Hz rate of the final kernel\n",
    "variance = 5.  # variance of input (modeled to be Gaussian)\n",
    "int_knots_num = 20  # num of internal knots for the spline basis\n",
    "order = 4  # spline order\n",
    "\n",
    "## trial_ids: numpy.array of dim T, trial ids associated to each time point, assume 200 trials\n",
    "trial_ids = np.repeat(np.arange(2), time_points // 2)\n",
    "\n",
    "## create event (temporal) input\n",
    "idx = np.random.choice(np.arange(time_points), num_events, replace=False)\n",
    "events = np.zeros(time_points)\n",
    "events[idx] = 1\n",
    "\n",
    "## create continous (spatial) variables (spatial_var will drive the neuron, nuisance_var will not drive the neuron)\n",
    "cov = np.array([[1, 0.], [0., 1]])\n",
    "rv = sts.multivariate_normal(mean=[0, 0], cov=variance * cov)\n",
    "samp = rv.rvs(time_points)\n",
    "spatial_var_x = samp[:, 0]\n",
    "spatial_var_y = np.sqrt(5)*np.random.normal(size=time_points)\n",
    "nuisance_var = samp[:, 1]\n",
    "\n",
    "# truncate inputs to avoid jumps in the resp function\n",
    "sele_idx = (np.abs(spatial_var_x) < 5) & (np.abs(spatial_var_y) < 5)\n",
    "spatial_var_x = spatial_var_x[sele_idx]\n",
    "spatial_var_y = spatial_var_y[sele_idx]\n",
    "\n",
    "nuisance_var = nuisance_var[sele_idx]\n",
    "while (spatial_var_x.shape[0] < time_points) or \\\n",
    "        (spatial_var_y.shape[0] < time_points):\n",
    "    tmpX = rv.rvs(time_points)\n",
    "    tmpY = np.random.normal(size=time_points)\n",
    "    sele_idx = (np.abs(tmpX[:, 0]) < 5) & (np.abs(tmpY) < 5)\n",
    "    tmpX = tmpX[sele_idx, :]\n",
    "    tmpY = tmpY[sele_idx]\n",
    "    spatial_var_x = np.hstack((spatial_var_x, tmpX[:, 0]))\n",
    "    nuisance_var = np.hstack((nuisance_var, tmpX[:, 1]))\n",
    "    spatial_var_y = np.hstack((spatial_var_y, tmpY))\n",
    "\n",
    "    spatial_var_x = spatial_var_x[:time_points]\n",
    "    spatial_var_y = spatial_var_y[:time_points]\n",
    "    nuisance_var = nuisance_var[:time_points]\n",
    "\n",
    "# create a response function (for ground-truth)\n",
    "knots = np.hstack(([-5] * 3, np.linspace(-5, 5, 8), [5] * 3))\n",
    "resp_func = lambda X1,X2: sts.multivariate_normal(mean=[0,0], cov=cov).pdf(np.array([X1,X2]).T)*10.\n",
    "\n",
    "filter_used_conv = sts.gamma.pdf(np.linspace(0, 20, 100), a=2) - sts.gamma.pdf(np.linspace(0, 20, 100), a=5)\n",
    "filter_used_conv = np.hstack((np.zeros(101), filter_used_conv)) * 2\n",
    "\n",
    "# # mean of the spike counts depending on spatial_var and events\n",
    "log_mu0 = resp_func(spatial_var_x, spatial_var_y)\n",
    "for tr in np.unique(trial_ids):\n",
    "    log_mu0[trial_ids == tr] = log_mu0[trial_ids == tr] + np.convolve(events[trial_ids == tr],\n",
    "                                                                      filter_used_conv, mode='same')\n",
    "\n",
    "# adjust mean rate\n",
    "scale = np.log(np.mean(np.exp(log_mu0)) / rate)\n",
    "log_mu0 = log_mu0 - scale\n",
    "\n",
    "# generate spikes\n",
    "spk_counts = np.random.poisson(np.exp(log_mu0))\n",
    "\n",
    "# plot the firing rate and the spike counts generated\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.subplot(121)\n",
    "plt.plot(np.arange(200) * 0.006, np.exp(log_mu0)[:200] / 0.006)\n",
    "plt.title('firing rate [Hz]')\n",
    "plt.xlabel('time[sec]')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(np.arange(200) * 0.006, spk_counts[:200])\n",
    "plt.title('6ms binned spike counts')\n",
    "plt.xlabel('time[sec]')\n",
    "\n",
    "### save the inputs in the standard formats\n",
    "\n",
    "## counts: numpy.array of dim (T x N), where T is the total number of time points, N is the number of neurons\n",
    "spk_counts = spk_counts.reshape(-1, 1)  # population of a single neuron\n",
    "\n",
    "## variables: numpy.array of dim (T x M), T as above, M the number of task variables\n",
    "variables = np.zeros((spk_counts.shape[0], 4))\n",
    "variables[:, 0] = spatial_var_x\n",
    "variables[:, 1] = spatial_var_y\n",
    "variables[:, 2] = nuisance_var\n",
    "variables[:, 3] = events\n",
    "\n",
    "## variable_names: numpy.array of strings of dimension M, name of each covariate in 'variables'\n",
    "variable_names = ['spatial_x','spatial_y', 'spatial_nuis', 'events']\n",
    "\n",
    "## neu_names: numpy.array of strings of dimension N, label uniquely identifying the neurons in 'counts'\n",
    "neu_names = ['neuron_A']\n",
    "\n",
    "## neu_info: dict, *optional neu_info[neu_names[k]]: dict, info about the k-th neuron, ð‘˜=0,â€¦,ð‘-1.\n",
    "## keys are the information label.\n",
    "neu_info = {'neuron_A': {'unit_type': 'SUA', 'area': 'dlPFC'}}\n",
    "if save_output:\n",
    "    np.savez('example_data_2D.npz', counts=spk_counts, variables=variables,\n",
    "             variable_names=variable_names,\n",
    "            neu_names=neu_names, neu_info=neu_info, trial_ids=trial_ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Create and save an example configuration file <a name=\"create-and-save-conf\"></a>\n",
    "\n",
    "\n",
    "Below the code for an example configuration file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import numpy as np\n",
    "\n",
    "# Create a YAML cofiguration file and save\n",
    "order = 4\n",
    "knots_1 = np.hstack(([-5] * (order - 1), np.linspace(-5, 5, 10), [5] * (order - 1)))\n",
    "knots_2 = np.hstack(([-5] * (order - 1), np.linspace(-5, 5, 8), [5] * (order - 1)))\n",
    "\n",
    "# convert to float (instead np.float64, produce a yaml human readable yaml list, optional)\n",
    "knots_1 = [float(k) for k in knots_1]\n",
    "knots_2 = [float(k) for k in knots_2]\n",
    "\n",
    "## change order of this example to match above\n",
    "cov_dict = {\n",
    "    'spatial_1': {\n",
    "        'lam': 10,\n",
    "        'penalty_type': 'der',\n",
    "        'der': 2,\n",
    "        'knots': [knots_1,knots_1],\n",
    "        'order': order,\n",
    "        'is_temporal_kernel': False,\n",
    "        'is_cyclic': [False,False],\n",
    "        'knots_num': np.nan,\n",
    "        'kernel_length': np.nan,\n",
    "        'kernel_direction': np.nan,\n",
    "        'samp_period': 0.006,\n",
    "        'join_var': ['spatial_x', 'spatial_y']  # for >1 dim response function\n",
    "    },\n",
    "    'spatial_2': {\n",
    "        'lam': 10,\n",
    "        'penalty_type': 'der',\n",
    "        'der': 2,\n",
    "        'knots': [knots_2],  # need to change this above, in the description of the variables\n",
    "        'order': order,\n",
    "        'is_temporal_kernel': False,\n",
    "        'is_cyclic': [False],\n",
    "        'knots_num': np.nan,\n",
    "        'kernel_length': np.nan,\n",
    "        'kernel_direction': np.nan,\n",
    "        'samp_period': 0.006,\n",
    "        'join_var': ['spatial_nuis']\n",
    "    },\n",
    "    'events':\n",
    "        {\n",
    "            'lam': 10,\n",
    "            'penalty_type': 'der',\n",
    "            'der': 2,\n",
    "            'knots': np.nan,\n",
    "            'order': order,\n",
    "            'is_temporal_kernel': True,\n",
    "            'is_cyclic': [False],\n",
    "            'knots_num': 10,\n",
    "            'kernel_length': 301,\n",
    "            'kernel_direction': 1,\n",
    "            'samp_period': 0.006,\n",
    "            'join_var': ['events']\n",
    "        },\n",
    "    'neuron_A':\n",
    "        {\n",
    "            'lam': 10,\n",
    "            'penalty_type': 'der',\n",
    "            'der': 2,\n",
    "            'knots': np.nan,\n",
    "            'order': order,\n",
    "            'is_temporal_kernel': True,\n",
    "            'is_cyclic': [False],\n",
    "            'knots_num': 8,\n",
    "            'kernel_length': 201,\n",
    "            'kernel_direction': 1,\n",
    "            'samp_period': 0.006,\n",
    "            'join_var': ['neuron_A']\n",
    "        }\n",
    "}\n",
    "\n",
    "# save the yaml config\n",
    "with open('config_example_data.yml', 'w') as outfile:\n",
    "    yaml.dump(cov_dict, outfile, default_flow_style=False)\n",
    "  \n",
    "\n",
    "# # load the yaml config\n",
    "# with open(\"demo/config_pgam.yml\", \"r\") as stream:\n",
    "#     try:\n",
    "#         cov_dict = yaml.safe_load(stream)\n",
    "#     except yaml.YAMLError as exc:\n",
    "#         print(exc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Fitting the model <a name=\"fitting-model\"></a>\n",
    "\n",
    "### List the fits by experiment, condition, neuron, and model configuration <a name=\"fit-list\"></a>\n",
    "\n",
    "Once the configuration file has been set-up, we will need a list of fits to be performed. An individual fit is identified by the experiment type, the session, the neuron ID, and the model configuation.\n",
    "\n",
    "When testing different models (including different subset of task variables, changing knots location and density, etc.) one needs to only modify the configuration files, and update the list of fits appropriately. \n",
    "\n",
    "We will again organize the list of fits as a YAML file with the following categories:\n",
    " \n",
    "- **experiment_ID**: string, experiment ID\n",
    "\n",
    "- **session_ID**: string, session ID\n",
    "\n",
    "- **neuron_num**: int, the neuron number (from 0 to N-1, whee N is the number of units in the session)\n",
    "\n",
    "- **path_to_input**: string, the path to the input data\n",
    "\n",
    "- **path_to_config**: string, the path to the configuration file\n",
    "\n",
    "- **path_to_output**: string, the path to the output folder where results will be saved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and save the YAML with the fit list (1 fit only)\n",
    "fit_dict = {\n",
    "    'experiment_ID': ['exp_2D_spatial'],\n",
    "    'session_ID': ['session_A'],\n",
    "    'neuron_num': [0],\n",
    "    'path_to_input': ['example_data_2D.npz'],\n",
    "    'path_to_config': ['config_example_data.yml'],\n",
    "    'path_to_output': ['.']\n",
    "}\n",
    "# save the yaml fit list\n",
    "with open('fit_list_example_data.yml', 'w') as outfile:\n",
    "    yaml.dump(fit_dict, outfile, default_flow_style=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Load inputs, fit, and postprocess <a name=\"load-fit-save\"></a>\n",
    "\n",
    " \n",
    "The code below loads the input data, the configurations, fits the model, post-process, and save the results. The following lines of code are also saved as separate script in 'PGAM/utils/fit_from_config.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libs\n",
    "import numpy as np\n",
    "import sys, os\n",
    "\n",
    "sys.path.append('src/PGAM/')\n",
    "\n",
    "import GAM_library as gl\n",
    "import gam_data_handlers as gdh\n",
    "from post_processing import postprocess_results\n",
    "import yaml\n",
    "import statsmodels.api as sm\n",
    "from scipy.io import savemat\n",
    "from time import perf_counter\n",
    "\n",
    "np.random.seed(4)\n",
    "\n",
    "#################################################\n",
    "# User defined input\n",
    "#################################################\n",
    "\n",
    "# frac of the trials used for fit eval\n",
    "frac_eval = 0.2\n",
    "\n",
    "# PATH to fit list\n",
    "path_fit_list = 'fit_list_example_data.yml'\n",
    "\n",
    "# save as mat\n",
    "save_as_mat = True\n",
    "#################################################\n",
    "\n",
    "\n",
    "# load the job id (either as an input form the command line, or as a default value if not passed)\n",
    "argv = sys.argv\n",
    "if len(argv) == 2:  # assumes the script is run the command \"python fit_from_config.py fit_num\"\n",
    "    fit_num = int(sys.argv[1]) - 1  # HPC job-array indices starts from 1.\n",
    "else:\n",
    "    fit_num = 0  # set a default value\n",
    "\n",
    "# load fit info\n",
    "with open(path_fit_list, 'r') as stream:\n",
    "    fit_dict = yaml.safe_load(stream)\n",
    "\n",
    "# unpack the info and load the data\n",
    "experiment_ID = fit_dict['experiment_ID'][fit_num]\n",
    "session_ID = fit_dict['session_ID'][fit_num]\n",
    "neuron_num = fit_dict['neuron_num'][fit_num]\n",
    "path_to_input = fit_dict['path_to_input'][fit_num]\n",
    "path_to_config = fit_dict['path_to_config'][fit_num]\n",
    "path_out = fit_dict['path_to_output'][fit_num]\n",
    "\n",
    "print('FIT INFO:\\nEXP ID: %s\\nSESSION ID: %s\\nNEURON NUM: %d\\nINPUT DATA PATH: %s\\nCONFIG PATH: %s\\n\\n' % (\n",
    "    experiment_ID, session_ID, neuron_num + 1, path_to_input, path_to_config))\n",
    "\n",
    "# load & unpack data and config\n",
    "data = np.load(path_to_input, allow_pickle=True)\n",
    "counts = data['counts']\n",
    "variables = data['variables']\n",
    "variable_names = data['variable_names']\n",
    "neu_names = data['neu_names']\n",
    "trial_ids = data['trial_ids']\n",
    "if 'neu_info' in data.keys():\n",
    "    neu_info = data['neu_info'].all()\n",
    "else:\n",
    "    neu_info = {}\n",
    "\n",
    "with open(path_to_config, 'r') as stream:\n",
    "    config_dict = yaml.safe_load(stream)\n",
    "\n",
    "# create a train and eval set (approximately with the right frac of trials)\n",
    "train_trials = trial_ids % (np.round(1 / frac_eval)) != 0\n",
    "eval_trials = ~train_trials\n",
    "\n",
    "# create and populate the smooth handler object\n",
    "sm_handler = gdh.smooths_handler()\n",
    "for var in config_dict.keys():\n",
    "    # if var == 'spatial_1':\n",
    "    #     continue\n",
    "    print('adding %s...' % var)\n",
    "    # check if var is a neuron or a variable\n",
    "    var_list = []\n",
    "    for join_var in config_dict[var]['join_var']:\n",
    "        if join_var in variable_names:\n",
    "            x_var = np.squeeze(variables[:, np.array(variable_names) == join_var])\n",
    "        elif join_var in neu_names:\n",
    "            x_var = np.squeeze(counts[:, np.array(neu_names) == join_var])\n",
    "        else:\n",
    "            raise ValueError('Variable \"%s\" not found in the input data!' % var)\n",
    "        var_list.append(x_var)\n",
    "\n",
    "    knots = config_dict[var]['knots']\n",
    "\n",
    "    if np.isscalar(knots):\n",
    "        knots = None\n",
    "    else:\n",
    "        knots = np.array(knots)\n",
    "\n",
    "    lam = config_dict[var]['lam']\n",
    "    penalty_type = config_dict[var]['penalty_type']\n",
    "    der = config_dict[var]['der']\n",
    "    order = config_dict[var]['order']\n",
    "    is_temporal_kernel = config_dict[var]['is_temporal_kernel']\n",
    "    is_cyclic = config_dict[var]['is_cyclic']\n",
    "    knots_num = config_dict[var]['knots_num']\n",
    "    kernel_length = config_dict[var]['kernel_length']\n",
    "    kernel_direction = config_dict[var]['kernel_direction']\n",
    "    samp_period = config_dict[var]['samp_period']\n",
    "\n",
    "    # rename the variable as spike hist if the input is the spike counts of the neuron we are fitting\n",
    "    if var == neu_names[neuron_num]:\n",
    "        label = 'spike_hist'\n",
    "    else:\n",
    "        label = var\n",
    "\n",
    "    sm_handler.add_smooth(label, var_list, knots=knots, ord=order, is_temporal_kernel=is_temporal_kernel,\n",
    "                          trial_idx=trial_ids, is_cyclic=is_cyclic, penalty_type=penalty_type, der=der, lam=lam,\n",
    "                          knots_num=knots_num, kernel_length=kernel_length, kernel_direction=kernel_direction,\n",
    "                          time_bin=samp_period)\n",
    "\n",
    "link = sm.genmod.families.links.Log()\n",
    "poissFam = sm.genmod.families.family.Poisson(link=link)\n",
    "\n",
    "spk_counts = np.squeeze(counts[:, neuron_num])\n",
    "\n",
    "# create the pgam model\n",
    "pgam = gl.general_additive_model(sm_handler,\n",
    "                                 sm_handler.smooths_var,  # list of coovarate we want to include in the model\n",
    "                                 spk_counts,  # vector of spike counts\n",
    "                                 poissFam  # poisson family with exponential link from statsmodels.api\n",
    "                                 )\n",
    "\n",
    "print('\\nfitting neuron %s...\\n' % neu_names[neuron_num])\n",
    "t0 = perf_counter()\n",
    "full, reduced = pgam.fit_full_and_reduced(sm_handler.smooths_var,\n",
    "                                          th_pval=0.001,  # pval for significance of covariate icluseioon\n",
    "                                          max_iter=10 ** 2,  # max number of iteration\n",
    "                                          use_dgcv=True,  # learn the smoothing penalties by dgcv\n",
    "                                          trial_num_vec=trial_ids,\n",
    "                                          filter_trials=train_trials,\n",
    "                                          compute_MI=False)\n",
    "print('tot fit time: %.2f sec'%(perf_counter()-t0))  \n",
    "\n",
    "print('post-process fit results...')\n",
    "res = postprocess_results(neu_names[neuron_num], spk_counts, full, reduced, train_trials,\n",
    "                        sm_handler, poissFam, trial_ids,\n",
    "                          var_zscore_par=None, info_save=neu_info, bins=15)\n",
    "\n",
    "\n",
    "# saving the file: save_name will be expID_sessionID_neuID_configName\n",
    "\n",
    "config_basename = os.path.basename(path_to_config).split('.')[0]\n",
    "save_name = '%s_%s_%s_%s'%(experiment_ID, session_ID, neu_names[neuron_num], config_basename)\n",
    "\n",
    "if save_as_mat:\n",
    "    savemat(os.path.join(path_out, save_name+'.mat'), mdict={'results':res})\n",
    "else:\n",
    "    np.savez(os.path.join(path_out, save_name+'.npz'), results=res)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "# Plot the 2D tuning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.subplot(projection='3d')\n",
    "ax.plot_surface(res[2]['x_kernel'][0],res[2]['x_kernel'][1],np.exp(res[2]['y_kernel']),cmap='autumn')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "# Fit via docker <a name=\"docker\"></a>\n",
    "\n",
    "\n",
    "Start docker. Make sure that you have the PGAM docker image by listing all available images with the terminal command,\n",
    "```sh\n",
    "    docker images\n",
    "```\n",
    "It should list a repository named edoardobalzani87/pgam, \n",
    "\n",
    "```\n",
    "REPOSITORY              TAG       IMAGE ID       CREATED          SIZE\n",
    "edoardobalzani87/pgam   1.0       052c7daf6ff7   19 minutes ago   4.64GB\n",
    "\n",
    "```\n",
    "\n",
    "### Running the container and mounting volumes <a name=\"mount-volumes\"></a>\n",
    "\n",
    "If you are running the PGAM image in a docker container for fitting the model you must specify which of the host folders (e.g. the folders of the operating system running the container) will be mounted as volumes by the container. This is a necessary step if the container needs to read from or write into the host file system. The syntax of the command is the following,\n",
    "\n",
    "```sh\n",
    "    docker run -v <path to local folder 1>:<path to image folder 1> \\\n",
    "               -v <path to local folder 2>:<path to image folder 2> \\\n",
    "               ...\n",
    "               -ti <docker image to be run> <command> -c \"<additional commands>\"\n",
    "```\n",
    "\n",
    "More specifically, mounting a folder means that: 1) when the container is started, the content of *\\<local folder i>* in the file system of the host will be copied in *\\<image folder i>* of the container temporary file system, 2) whenever the container writes in *\\<image folder i>*  (saving/deleting files or creating/deleting directories), the same will be performed on the host  *\\<local folder i>*.  \n",
    "\n",
    "The docker image has a few folders that one may use to mount local folders to:\n",
    "\n",
    "- **/input**: mount this container folder to the host folder containing the input data and the fit list YAML file \n",
    "- **/output**: mount this container folder to the host folder that will contain the output to be saved\n",
    "- **/config**: mount this container folder to the host folder containing the config YAML file\n",
    "- **/scripts**: mount this container folder to the host folder containing the fitting script (fit_from_config.py)\n",
    "\n",
    "### Setting paths <a name=\"container-path\"></a>\n",
    "\n",
    "Make sure to edit the fit list YAML file replacing host folders with container folders, for example:\n",
    "\n",
    "```python\n",
    "fit_dict = {\n",
    "    'experiment_ID': ['exp_1'],\n",
    "    'session_ID': ['session_A'],\n",
    "    'neuron_num': [0],\n",
    "    'path_to_input': ['/input/example_data.npz'],         # '/input/', '/config/', and '/output/' are the path \n",
    "    'path_to_config': ['/config/config_example_data.yml'] # to the folder in the temp file system \n",
    "    'path_to_output': ['/output/']                        # of the container\n",
    "}                                                        \n",
    "# if saving the yaml fit list outsider the docker container\n",
    "with open('<path to local config folder>/fit_list_example_data.yml', 'w') as outfile:\n",
    "    yaml.dump(fit_dict, outfile, default_flow_style=False)\n",
    "# if saving the yaml fit list inside the docker cointainer\n",
    "#with open('/config/fit_list_example_data.yml', 'w') as outfile:\n",
    "#    yaml.dump(fit_dict, outfile, default_flow_style=False)\n",
    "```\n",
    "Similarly edit the fit script that will load the fit list YAML, pointing the \"config/\" folder of the container. In \"fit_from_config.py\" we would need to set appropriately the variable \"path_fit_list\",\n",
    "\n",
    "```python\n",
    "...\n",
    "#################################################\n",
    "# User defined input\n",
    "#################################################\n",
    "\n",
    "# frac of the trials used for fit eval\n",
    "frac_eval = 0.2 \n",
    "\n",
    "# PATH to fit list\n",
    "path_fit_list = '/input/fit_list_example_data.yml'\n",
    "...\n",
    "```\n",
    "\n",
    "### Fit by running the container <a name=\"fit-container\"></a>\n",
    "\n",
    "Enter the following command in the terminal to create and run the container,\n",
    "\n",
    "```sh\n",
    "docker run -v <path to host input>:/input \\\n",
    "           -v <path to host output>:/output \\\n",
    "           -v <path to host config>:/config \\\n",
    "           -v <path to host scripts>:/scripts \\\n",
    "           -ti edoardobalzani87/pgam:1.0 bin/bash -c \"python /scripts/fit_from_config.py 0\"\n",
    "# note the '0' at the end of the call denotes the first neuron is being fit. Can be looped to fit other neurons in the list\n",
    "```\n",
    "\n",
    "Delete unused containers by listing all the container with the command\n",
    "\n",
    "```sh\n",
    "docker ps -a\n",
    "```\n",
    "\n",
    "which will produce something similar to,\n",
    "```sh\n",
    "CONTAINER ID   IMAGE                       COMMAND                  CREATED              STATUS                     \n",
    "3f3644b47861   edoardobalzani87/pgam:1.0   \"bin/bash -c 'pythonâ€¦\"   About a minute ago   Exited (0)\n",
    "```\n",
    "\n",
    "Remove the container with,\n",
    "```sh\n",
    "docker rm <CONTAINER ID> \n",
    "```\n",
    "\n",
    "Fit results will be stored in \\<path to host output>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "# HPC & singularity <a name=\"HPC\"></a>\n",
    "\n",
    "This protocol is currently tested on the NYU green HPC. Suppose that you have a script named \"run_all_fits.py\" that runs the j-th fit in the <a href=\"fit-list\">fit list YAML</a> with the following command,\n",
    "\n",
    "```sh\n",
    "python run_all_fits.py j\n",
    "```\n",
    "\n",
    "If you want to run in parallel N=100 fits on HPC, each fit with a max duration of 2 hours and memory requirement of 16GB you can follow the procedure below:\n",
    "\n",
    "\n",
    "1. Download and copy the singularity container <a href=\"https://osf.io/pcsav/\">pgam_1.0.sif</a> in one of your HPC folders.\n",
    "\n",
    "2. Copy all the configuration files, the inputs and the YAML with the fit list to your HPC folders.\n",
    "\n",
    "4. Make sure that the paths in the fit list YAML and in \"run_all_fits.py\" are appropriately set (paths should ponit to the file locations on HPC instead of the local folders).\n",
    "\n",
    "4. cd to the \"run_all_fits.py\" folder on the HPC \n",
    "\n",
    "5. Create a batch script (a text file) \"run-pgam.sh\" with following content,  \n",
    "   ```sh\n",
    "        #!/bin/bash\n",
    "\n",
    "        #SBATCH --job-name=fit_GAM\n",
    "        #SBATCH --nodes=1\n",
    "        #SBATCH --cpus-per-task=1\n",
    "        #SBATCH --mem=16GB\n",
    "        #SBATCH --time=0-02:00:00\n",
    "        #SBATCH --array=1-100\n",
    "\n",
    "        if [[ ! -z \"$SLURM_ARRAY_TASK_ID\" ]]; then\n",
    "            IID=${SLURM_ARRAY_TASK_ID}\n",
    "        fi\n",
    "\n",
    "        singularity exec $nv --bind /etc/passwd   path_to_PGAM_sif_image/pgam_1_0.sif /bin/bash -c \"python run_all_fits.py $IID\"\n",
    "   ```\n",
    "\n",
    "5. cd to the folder of the batch script and run jobs with the following command:\n",
    "```sh\n",
    "sbatch run-pgam.sh\n",
    "```\n",
    "\n",
    "\n",
    "The batch script configurations (#SBATCH --option=...) depend on specific dataset that one needs to fit and should be adjusted at need. For a complete list of options check https://slurm.schedmd.com/sbatch.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
