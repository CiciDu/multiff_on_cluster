{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "\n",
    "# PGAM Tutorial\n",
    "\n",
    "## Introduction\n",
    "This tutorial is aimed to introduce the user to some key concepts of Generalized Additive Model (GAMs), how these concepts are implemented in this specific PGAM library, and describe in detail how to estimate tuning functions with the PGAM library on an example syntetic dataset.\n",
    "\n",
    "\n",
    "## Why GAMs?\n",
    "<!--\n",
    "Estimating tuning functions entails finding maps that characterize how a set of task variables affects the firing rate of a recorded neuron. Some of the challenges that comes with this estimation problem are: (i) the experimenter has no direct access to the firing rate, but can only measure spikes; (ii) there may be no a priori hypothesis on the shape of the tuning functions (excluding some well characterized special case, e.g. the Gabor filter-like responses in V1); (iii) correlations between task variables may be a confunding factor (e.g. if eye movemet and hand movement are correlated during a reaching movement, it becomes hard to discriminate if the hand, the eye or a combination of both is driving a neuron).  \n",
    "\n",
    "During naturalistic experiments, where there is a lack of identical trial repeats and the behavior is less constrained, those challenges become even more prominent: (i) the firing rate cannot be easily estimated by trial averaging over \"identical\" experimental conditions; (ii) cortical neurons manifest a strong mixed selectivity to a multitude of behavioral covariates and stimuli features that may not be apparent in trial-based experiments due to the simpler, usually multi-alternative stimulus and response space. Mixed selective reponses are not yet fully characterized for most brain areas; (iii) finally, no/weaker control over the animal behavior and sensory experience may introduce additional correlations (e.g. a correlation between eye position and visual stimuli will arise if no eye-fixation is imposed).\n",
    "-->\n",
    "\n",
    "Generalized Linear Models (GLMs) have been succesful in characterizing mixed selective responses by capturing well the statistics of spike trains (which can be modelled as Poisson distributed observations, no averaging needed) and by jointly estimating the contribution of a (potentially) large number of task variables. \n",
    "\n",
    "However, GLMs comes with their own limitations. In particular, one needs to carefully choose how to represent tuning functions (in the case of GLMs this translates into chosing an appropriate basis of functions, e.g. Gaussian-shaped, Fourrier, cosine raised... and the number of basis element to be used). More importantly, defining the minimal subset of variable which drive the neural activity becomes cumbersome for a naive implementations. Variables are usually selected through model comparison, an approach that becomes quickly unfeasible when the number of task variable increases (combinatorial explosion of candidate models). Additionally, classical stepwise methods comes with well known theoretical and practical flaws (e.g., Frank Harrell (2001)).\n",
    "\n",
    "Our solution takles those limitation by taking advantage of GAM theory. GAMs are non-linear extensions of GLMs that retain the advantages mentioned above (model counts directly and jointly infers responses), but additoinally learns from the data the type of non-linearities that are suited to represent each individual response function. As we will see in the tutorial, this will translate into learning the proper prior distribution over a set of possible response functions. The appproach comes with the additional benefit of deriving confidence intervals over the model parameters that can be used to select the minimal subset of variables driving neural acticity. Since selection is based on statistical testing, we completely circumvented costly model comparison. \n",
    "\n",
    "Overall, our approach is both user-friendly, requiring less choices for the user, and computatoinally advantageous when variable selection is required.\n",
    "\n",
    "\n",
    "\n",
    "## What is covered in the tutorial?\n",
    "\n",
    "The tutorial will cover the main components of the model, in particular:\n",
    "\n",
    "1. <a href=\"#repr-nl\">**Representing non-linearities**: </a>\n",
    "\n",
    "    1.1 <a href=\"#b-spline\">**B-spline definition and properties**</a> To familiarize ourselves with the concept of B-splines, we will plot b-splines for different type of response functions: 1D and 2D responses, cyclic or not, and temporal kernels.\n",
    "    \n",
    "    1.2 <a href=\"#sm-prior\">**Smoothing prior**</a> We will introduce the concept of smoothing prior and clarify the role of the smoothing prior in GAM fitting. We will provide intuitive insight into the role of the prior by drawing and plottnig functions sampled from different levels of smoothing. \n",
    "\n",
    "\n",
    "2. <a href=\"#pgam-lib\">**Introduction to the PGAM library**</a> We will illustrate how the concepts introduced in the tutorial are implemented in the PGAM library and applied to the problem of tuning function estimation.\n",
    "\n",
    "    2.1 <a href=\"#sm-handl\">**Define the B-spline via the *smooths_handler* class**</a>: We will use a particular class (smooths_handler) to appropriately format the model covariates, as well as design the B-spline and the corresponding smoothing pealization.\n",
    "\n",
    "    2.2. <a href=\"#model-fit\">**Model fit**</a>: Fit the model by jointly learning the smoothing levels and the B-spline parametters.\n",
    "    \n",
    "    2.3. <a href=\"#post-proc\">**Post processing**</a>: Post-process model outputs, plot and explore the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "# import sys\n",
    "# ## if working outside the docker container, uncomment the line below and add the path to [YOUR PATH TO PGAM FOLDER]/src/\n",
    "# ## sys.path.append('[YOUR PATH TO PGAM FOLDER]/src/')\n",
    "# sys.path.append('src/')\n",
    "\n",
    "pgam_path = '/Users/dusiyi/Documents/Multifirefly-Project/multiff_analysis/external/pgam/src/'\n",
    "import sys\n",
    "if not pgam_path in sys.path: \n",
    "    sys.path.append(pgam_path)\n",
    "    \n",
    "import numpy as np\n",
    "import sys\n",
    "from PGAM.GAM_library import *\n",
    "import PGAM.gam_data_handlers as gdh\n",
    "import matplotlib.pylab as plt\n",
    "import pandas as pd\n",
    "from post_processing import postprocess_results\n",
    "from scipy.io import savemat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "<!-- We want a section here, just saying that you are going to show them b-splines for fun, because it's important -->\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 1. Representing non-linearities <a name=\"repr-nl\">\n",
    "\n",
    "An approach to build a tuning function is to describe this tuning function by a weighted sum of simple basis elements. For example, visual responses to gratings are classicaly described by a basis of Gaussian tuning functions. In our case we will use B-splines because of their flexibility in describing arbitrarily shaped response functions with few components. Creating an appropriate set of basis is important and can go wrong. In this section we will build B-splines for illustrative purposes.\n",
    "\n",
    "## 1.1 B-spline definition and properties <a name=\"b-spline\">\n",
    "B-splines are a basis set of piecewise polynomials. This basis is defined recursively, see (https://en.wikipedia.org/wiki/B-spline) over a set of knots where the polynomial pieces meet. Given this recursive nature, in order to fully cover the the domain of interest (e.g., x-axis of the eventual tuning function), one needs to add additional knots outside or at the edge of the domain.  The most common choice is to simply repeat the first and last knot $n-1$ times, where n is the degree of the polynomial used. The total number of knots and the degree of the polynomial determine the number of basis elements. \n",
    "\n",
    "\n",
    "<!-- split this block of text (and code) so they correspond 1 to 1 -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cubic B-spline covering [-1,1]\n",
    "\n",
    "# Input (100 equispaced samples in which the B-spline will be evaluated)\n",
    "x = np.linspace(-1, 1, 100)\n",
    "order = 4 # the order of the spline is the number of coefficient of the polynomials (1-degree of the poly)\n",
    "\n",
    "# specify some internal knots\n",
    "int_knots = np.array([-1., -.9, -0.7, -.4, 0, .2, .5, .8, 1.0])\n",
    "\n",
    "# Repeat external knots. These are repeated order-1 times. Thus, in this example, with 9 internal knots and order 4\n",
    "# you will end up with 15 knots (3+9+3).\n",
    "knots = np.hstack(([int_knots[0]]*(order-1), int_knots, [int_knots[-1]]*(order-1)))\n",
    "print('Knots number: %d'%len(knots))\n",
    "\n",
    "# Evaluate the spline basis at x. X is of shape (samples x betas/basis functions)\n",
    "# in this example, we have 100 samples and the number of betas is defined by number of knots - order \n",
    "# Thus, x should be 100 x 11\n",
    "X = gdh.splineDesign(knots, x, order, der=0)\n",
    "\n",
    "print('B-spline dimension: %d'%X.shape[1])\n",
    "\n",
    "plt.figure()\n",
    "plt.title('B-splines of order %d'%(order))\n",
    "for spline_i in range(X.shape[1]):\n",
    "    plt.plot(x,X[:,spline_i])\n",
    "plt.vlines(int_knots, 0,1,ls ='--', color='k',lw=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Representing circular variables: cyclic B-spline\n",
    "\n",
    "Cyclical B-spline can be defined to describe responses to cyclical input variables (angles); since there are no borders for cyclic variables, the specification of the internal knots fully characterizes the cyclical B-spline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the cyclical b-spline over the same domain \n",
    "# (for cyclic spline basis, knots repetition is not required)\n",
    "cX = gdh.cSplineDes(int_knots, x, order, der=0)\n",
    "\n",
    "# plot the cyclical basis\n",
    "plt.figure()\n",
    "plt.title('Cyclical B-splines of order %d'%(order))\n",
    "for spline_i in range(cX.shape[1]):\n",
    "    plt.plot(x,cX[:,spline_i])\n",
    "plt.vlines(int_knots, 0,1,ls ='--', color='k',lw=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### B-spline coverage\n",
    "\n",
    "If the domain is correcty specified, the sum of the B-splines over internal points should equal to one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert that the B-spline convers the input range\n",
    "assert(all(np.abs(X.sum(axis=1) - 1) < 10**-12))\n",
    "assert(all(np.abs(cX.sum(axis=1) - 1) < 10**-12))\n",
    "print('Sum over the basis functions of a regular B-spline: ', X.sum(axis=1)[:10])\n",
    "print('Sum over the basis functions of a cyclical B-spline: ', X.sum(axis=1)[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### B-spline of different order\n",
    "Below some example of B-splines of different order over the same knots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Linear B-spline\n",
    "order = 2\n",
    "knots = np.hstack(([int_knots[0]]*(order-1), int_knots, [int_knots[-1]]*(order-1)))\n",
    "\n",
    "# get the spline basis evaluated at x for the domain\n",
    "X = gdh.splineDesign(knots, x, order, der=0)\n",
    "\n",
    "# get the cyclical b-spline over the same domain \n",
    "# (for cyclic spline basis, knots repetition is not required)\n",
    "cX = gdh.cSplineDes(int_knots, x, order, der=0)\n",
    "\n",
    "# plot the basis\n",
    "plt.figure(figsize=(8,3))\n",
    "plt.suptitle('Linear B-Spline')\n",
    "\n",
    "ax1 = plt.subplot(121)\n",
    "ax2 = plt.subplot(122)\n",
    "\n",
    "x_rep = np.repeat(x, X.shape[1]).reshape(x.shape[0],-1)\n",
    "p1 = ax1.plot(x_rep,X)\n",
    "ax1.set_title('regular B-spline')\n",
    "\n",
    "x_rep = np.repeat(x, cX.shape[1]).reshape(x.shape[0],-1)\n",
    "ax2.set_title('cyclic B-spline')\n",
    "p2 = ax2.plot(x_rep,cX)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "\n",
    "## 5-degree B-spline\n",
    "order = 6\n",
    "knots = np.hstack(([int_knots[0]]*(order-1), int_knots, [int_knots[-1]]*(order-1)))\n",
    "\n",
    "# get the spline basis evaluated at x for the domain\n",
    "X = gdh.splineDesign(knots, x, order, der=0, outer_ok=True)\n",
    "\n",
    "# get the cyclical b-spline over the same domain \n",
    "# (for cyclic spline basis, knots repetition is not required)\n",
    "cX = gdh.cSplineDes(int_knots, x, order, der=0)\n",
    "\n",
    "# plot the basis\n",
    "plt.figure(figsize=(8,3))\n",
    "plt.suptitle('5 degree poly. B-Spline')\n",
    "\n",
    "ax1 = plt.subplot(121)\n",
    "ax2 = plt.subplot(122)\n",
    "\n",
    "x_rep = np.repeat(x, X.shape[1]).reshape(x.shape[0],-1)\n",
    "p1 = ax1.plot(x_rep,X)\n",
    "ax1.set_title('regular B-spline')\n",
    "\n",
    "x_rep = np.repeat(x, cX.shape[1]).reshape(x.shape[0],-1)\n",
    "ax2.set_title('cyclic B-spline')\n",
    "p2 = ax2.plot(x_rep,cX)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Higer dimensional B-spline\n",
    "\n",
    "It is possible to define a basis for multidimensinal response functions as a tensor product of uni-dimensional basis. For example, a 2D response function could be expanded as $f(x,y) \\approx \\sum_{ij} \\alpha_i \\beta_j a_i(x)b_j(y)$, where $\\{a_1, \\dots, a_n\\}$  and $\\{b_1, \\dots, b_m\\}$ are B-splines over the domain of $x$ and $y$ respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import axes3d\n",
    "%matplotlib notebook\n",
    "# 2D basis set for product of cubic interpolaties\n",
    "order = 4\n",
    "int_knots_x = np.linspace(-1,1,8)\n",
    "int_knots_y = np.linspace(0,1,6)\n",
    "knots_x = np.hstack(([int_knots_x[0]]*(order-1), int_knots_x, [int_knots_x[-1]]*(order-1)))\n",
    "knots_y = np.hstack(([int_knots_y[0]]*(order-1), int_knots_y, [int_knots_y[-1]]*(order-1)))\n",
    "\n",
    "\n",
    "\n",
    "# input variables\n",
    "x = np.linspace(-1,1,100)\n",
    "y = np.linspace(0,1,100)\n",
    "x_mesh, y_mesh = np.meshgrid(x,y)\n",
    "X = gdh.splineDesign(knots_x, x_mesh.flatten(), order, der=0)\n",
    "Y = gdh.splineDesign(knots_y, y_mesh.flatten(), order, der=0)\n",
    "\n",
    "# product basis\n",
    "XY = multiRowWiseKron(X,Y, sparseX=False)\n",
    "\n",
    "# plot a bais element\n",
    "i = 20\n",
    "bi = XY[:,i]\n",
    "bi = bi.reshape(x_mesh.shape)\n",
    "fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n",
    "surf = ax.plot_surface(x_mesh, y_mesh, bi, cmap=plt.get_cmap('coolwarm'),\n",
    "                       linewidth=0, antialiased=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "# 1.2 Smoothing prior<a name=\"sm-prior\">\n",
    "\n",
    "One strategy for learning an arbitrary non-linear response for a task variable is to start from a very flexible representation, such as that provided by the B-spline, and then constrain the space of possible solutions to a subset for which \"smoothness\" level is controlled by a single scalar parameter. \n",
    "\n",
    "The problem of learning an arbitrary non-linear response is then mapped onto the problem of appropriately setting the smoothing level.\n",
    "\n",
    "## Single covariate example\n",
    "\n",
    "For illustration purposes we can focus on the case of inferring the response to a single variable of interest $x_t\\in\\mathbb{R}$ as a predictor for the spike counts $y_t\\in\\mathbb{N}$. The GAM assumes a specific form for the likelihood of observing a specific spike count vector given our task variable: $\\mathcal{L}(\\theta) = \\mathbb{P}(\\mathbf{y} | \\mathbf{x}, \\mathbf{f}_\\beta)$, where $\\mathbf{f}_\\beta$ is the non-linear response we aim to learn. \n",
    "\n",
    "As previously mentioned, the response is described in terms of the B-spline $\\{b_1(x), ..., b_m(x)\\}$, e.g., $\\mathbf{f}_\\beta(x) = \\sum \\beta_j b_j(x)$.\n",
    "\n",
    "Without constraints on the values that $\\beta$ can assume, the a-priori smoothness level of the response function depends solely on the number basis elements we assumed (and on the choice of basis itself in GLMs).\n",
    "\n",
    "<!-- In order to fit the PGAM model we need to create a *smooth_handler* object with all the model covariates. As input to the *smooths_handler* class we can specify the type of regularization that we want to enforce over the response function.\n",
    "\n",
    "In the context of the PGAM, this will consist in a penalization of the \"energy\" or \"wiggliness\" of the response function. For any response function of $\\text{order} \\ge 4$, the most common choice is penalizing the integral of the squared second derivative of the response,\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathcal{L}_j (\\lambda) = \\lambda \\int_{D} f''(x)^2 dx,\n",
    "\\end{equation}\n",
    "\n",
    "where $D$ is the domain of the variable $x$. \n",
    "\n",
    "Other options that do not require the response to be twice differentiable (as for the case of a basis of order 2 splines) are available. These type of penalties are based on appropriate differences of the coefficient of the splines that very roughly approximates the second derivative squared.\n",
    "\n",
    "The $j$-th covariate will have an associated $\\lambda_j$ that controls how much wiggliness is allowed. The larger the $\\lambda_j$ the smoother the response function will be. These parameters will be learned from the data by maximizing a cross-validation objective function (the Generalized Cross Valiadtion score).\n",
    "\n",
    "**Additional penalization terms**. Both second derivative based penalization will not penalize straight lines (second derivative =0). This means that even for extremely large $\\lambda_j$ the response function would not be forced to $f=0$, but instead it will live in the space of straight lines. To avoid that, we explicitly penalized straight lines with an additional term that operates orthogonally to the wiggliness penalization.\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here T = 200 samples of the variable x_t are created\n",
    "x = np.linspace(-1, 1, 200)\n",
    "\n",
    "# we evaluated the B-spline in some knots covering the range of x_t following the steps above\n",
    "order = 4 \n",
    "int_knots = np.linspace(-1,1,10)\n",
    "knots = np.hstack(([int_knots[0]]*(order-1), int_knots, [int_knots[-1]]*(order-1)))\n",
    "X = gdh.splineDesign(knots, x, order, der=0)\n",
    "\n",
    "# if we assign some random coefficients to the basis we can define candidate response functions\n",
    "plt.figure()\n",
    "plt.suptitle('Sampling random response functions')\n",
    "ax = plt.subplot(121)\n",
    "ax.set_title('%d basis element'%X.shape[1])\n",
    "\n",
    "for k in range(5):\n",
    "    beta = np.random.normal(size=X.shape[1])\n",
    "    plt.plot(x, np.dot(X, beta))\n",
    "plt.xlabel('$x_t$', fontsize=12)\n",
    "plt.ylabel('$f(x_t)$', fontsize=12)\n",
    "\n",
    "order = 4 \n",
    "int_knots = np.linspace(-1,1,40)\n",
    "knots = np.hstack(([int_knots[0]]*(order-1), int_knots, [int_knots[-1]]*(order-1)))\n",
    "X = gdh.splineDesign(knots, x, order, der=0)\n",
    "\n",
    "ax = plt.subplot(122)\n",
    "ax.set_title('%d basis element'%X.shape[1])\n",
    "for k in range(5):\n",
    "    beta = np.random.normal(size=X.shape[1])\n",
    "    plt.plot(x, np.dot(X, beta))\n",
    "plt.xlabel('$x_t$', fontsize=12)\n",
    "plt.ylabel('$f(x_t)$', fontsize=12)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "Given the high-flexibility of B-splines, even for a relatively low dimensional basis we may end up over-fitting the our data and estimating a very wiggly rersponse when $\\beta$ are learned by maximum likelihood.\n",
    "\n",
    "We can control the level of smoothess of the response by introducing a regulariation term to our likelihood which penalizes high energy (very wiggley) responses. The terms is simply the area under the curve of the second derivative of the rersponse function, obtaining the following:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{L}_p = \\mathcal{L} - \\lambda \\int \\mathbf{f}_\\beta^{''} (x) dx = \\mathcal{L} - \\lambda \\beta^\\top \\mathbf{\\text{  S}} \\beta \\label{eq:pen-LL} \\tag{1}\n",
    "\\end{equation}\n",
    "\n",
    "Here $\\lambda$ is a positive scalar controlling the level of smootheness required; the higher $\\lambda$ the smoother the function. In the last equation we noted that the integral can be written in terms of the B-spline coefficients.\n",
    "\n",
    "Below we will compute the second derivative penalty matrix $\\mathbf{\\text{  S}}$, and we will demonstrate how the integral of the second derivative of a response function can be computed only in terms of $\\beta$ and $\\mathbf{\\lambda \\text{  S}}$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.integrate import simps\n",
    "\n",
    "# compute the penalization matrix S and the cholesky decomp. B such that S = B.T.dot(B)\n",
    "samples_for_integral_approx = 10**4\n",
    "# the function takes as input the knots, the range of the variable, the number of \n",
    "# samples used for approx. the integral, the order of the spline, the order of derivative, \n",
    "# 2 for second derivative and a flag for cyclic vs non-cyclic B-spline\n",
    "S,B = smPenalty_1D_derBased(knots, x.min(), x.max(), samples_for_integral_approx, \n",
    "                            ord=order, der=2, cyclic=False)\n",
    "S = S.toarray()\n",
    "print('Check Cholesky decomposition: ', np.max(np.abs(S - B.T.dot(B))))\n",
    "\n",
    "# set some random parameters for the response function\n",
    "beta = np.random.normal(size=S.shape[0])\n",
    "# We calculate the numerical integral of the square of the response function second derivative and\n",
    "# show that the integral can be decribed as in equation (1)\n",
    "func = lambda x : np.dot(gdh.splineDesign(knots, x, order, der=2),beta)**2\n",
    "\n",
    "# compute numerically the integral of func and using (1)\n",
    "num_integr = simps(func(np.linspace(-1,1,10**4)), x=np.linspace(-1,1,10**4))\n",
    "coeff_based_integr = np.dot(np.dot(beta, S),beta)\n",
    "print('Numerical integral:                 %f\\nPenalization matrix based integral: %f'%(num_integr,coeff_based_integr))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "\n",
    "Equation 1 takes the form of an (improper) Gaussian  distribution with zero mean and precision  $\\mathbf{\\lambda \\text{  S}}$ over the $\\beta$. This allows us to interpret the penalization in terms of prior over candidate functions, large $\\lambda$ are equivalent to sharper prior over smooth responses.\n",
    "\n",
    "To visualize the effect of $\\lambda$ over the prior, we can draw samples form it for increasingly large values of the regularizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "## define different level of regularization\n",
    "lams = [0.0001,0.001,0.01,0.1]\n",
    "pseudo_inv = np.linalg.pinv(S)\n",
    "eig,U = np.linalg.eigh(pseudo_inv)\n",
    "\n",
    "sqrt_pseudo_inv = U.dot(np.diag(np.abs(eig)**0.5)).dot(U.T)\n",
    "\n",
    "# sample from different priors (gaussian)\n",
    "plt.figure(figsize=(10,3))\n",
    "\n",
    "ax = plt.subplot(1,5,1)\n",
    "plt.title('Ridge prior')\n",
    "for k in range(5):\n",
    "    beta = np.random.normal(size=B.shape[0]) / (10*np.sqrt(lams[0]))\n",
    "    plt.plot(x, np.dot(X, beta))\n",
    "plt.xlabel('$x_t$',fontsize=15)\n",
    "plt.ylabel('$f(x_t)$',fontsize=15)\n",
    "    \n",
    "\n",
    "for i, lam in enumerate(lams):\n",
    "    plt.subplot(1,5,i+2)\n",
    "    plt.title('$\\lambda$: %s'%lam)\n",
    "    for k in range(5):\n",
    "        beta = sqrt_pseudo_inv.dot(np.random.normal(size=B.shape[0])) / np.sqrt(lam) \n",
    "        plt.plot(x, np.dot(X, beta))\n",
    "    if i == 0:\n",
    "        ylim = plt.ylim()\n",
    "    else:\n",
    "        plt.ylim(ylim)  \n",
    "    plt.xlabel('$x_t$',fontsize=15)\n",
    "\n",
    "ax.set_ylim(ylim)\n",
    "plt.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "What distinguishes GAMs from GLMs with a smoothing prior is the fact that GAMs learn an individual smoothing enforcing parameters $\\lambda$ for each task variable, while in general GLMs  selects a single global regularization parameter by means of cross-validation. \n",
    "\n",
    "The advantage is four fold. First, GAM is more flexible in capturing differrent types of non-linearities. Second, learning a large $\\lambda$ for a task variable will automatically shrinks the response function to zero (more precisely, to the null-space of the penalty matrix). Third, a single GAM fit will find the appropriate regularization, circumventing the need for cross-validating for setting the regularizer. Forth, and perhaps most imporantly from a practical standopoint, learning the appropriate degree of smoothess enables statistical testing for task-variable inclusion. Thus, there is no costly and unstable forward and backward selection procedures.\n",
    "\n",
    "The optimization objective implemented for our particular instantiation of the GAM is known as double Generalized Cross Validation score (dGCV), and is a form of leave-one-out cross-validation. \n",
    "\n",
    "The dGCV is a function of the smoothing parameters $\\lambda$ and can be optimized numerically with efficient algorithms. Once the $\\lambda$ are found, there is a unique set of $\\beta$ that maximizes likelihood of the form of (\\ref{eq:pen-LL}). For more information on the learning algorithm please see [(Balzani et al, 2020)](#1). \n",
    "\n",
    "### Derivative based penalty vs difference based penalty <a name='der-diff-pen'>\n",
    "\n",
    "For low order B-spline (which have null second derivative) the derivative based penalization matrix $\\mathbf{\\text{S}}$ must be replaced with one based on the difference between consecutive coefficient difference. The difference based penalization computes a rough estimate of the second derivative. Below we demonstrate a comparison between the derivative and the difference based penalty matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sbs\n",
    "S_diff,B_diff = gdh.non_eqSpaced_diff_pen(knots, order, outer_ok=False, cyclic=False)\n",
    "S_diff = S_diff.toarray()\n",
    "plt.figure(figsize=(8,3))\n",
    "ax1 = plt.subplot(121)\n",
    "ax2 = plt.subplot(122)\n",
    "ax1.set_title('Derivative based')\n",
    "ax2.set_title('Difference based')\n",
    "sbs.heatmap(S/np.linalg.norm(S),ax=ax1)\n",
    "sbs.heatmap(S_diff/np.linalg.norm(S_diff),ax=ax2)\n",
    "ax1.set_xlabel('$ \\\\beta $',fontsize=20)\n",
    "ax1.set_ylabel('$ \\\\beta $',fontsize=20)\n",
    "ax2.set_xlabel('$ \\\\beta $',fontsize=20)\n",
    "ax2.set_ylabel('$ \\\\beta $',fontsize=20)\n",
    "ax1.set_xticks([])\n",
    "ax1.set_yticks([])\n",
    "ax2.set_xticks([])\n",
    "ax2.set_yticks([])\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "# 2 Introduction to the PGAM library<a name=\"pgam-lib\">\n",
    "\n",
    "The PGAM library simplifies the process of constructing and fitting GAM models for tuning function estimation. The key classes of the library are:\n",
    "    \n",
    "* **smooths_handler**: constructs B-spline and the penalty matrix for each variable and allows to concatenate multiple B-spline constructing a global model matrix.\n",
    "    \n",
    "* **general_additive_model**: a class that contains methods for fitting GAMs by means of dGCV optimization\n",
    "\n",
    "## 2.1 Define the B-spline via the *smooths_handler* class <a name=\"sm-handl\">\n",
    "\n",
    "\n",
    "The *smooths_handler* class will construct the appropriate B-spline for all the covariates of interest. \n",
    "\n",
    "Each task variable needs to be inputed to the *smooths_handler* class one at the time via the method\n",
    "    \n",
    "        smooths_handler.add_smooth\n",
    "\n",
    "The the method requires the following inputs:\n",
    "* **name**: string, the label of the task variable\n",
    "\n",
    "* **x_cov**: list containing the input variable (the list will contain 1 vector per dimension of the variable)\n",
    "    \n",
    "* **is_temporal_kernel**: boolean, True if the variable is \"temporal\", False if \"spatial\" (see <a href=\"spatial-temporal\">below </a> for the definitioin of temporal and spatial variables)\n",
    "\n",
    "* **kernel_direction**: int or None, None when \"is_temporal_kernel == False\". When \"True\", 0 = acausal (bidirectional), '1' = causal (i.e., firing change after the event happens), '-1' = anticipatory (i.e., firing change before event happens). See the <a href=\"tempcov\"> temporal covariate</a> session for examples.\n",
    "\n",
    "* **kernel_length**: int or None. None when \"is_temporal_kernel == False\". When \"True the number of time points used for the kernel. Suggested to use an odd number of samples.\n",
    "\n",
    "* **ord**: integer, the order of the B-spline\n",
    "\n",
    "* **knots**: list or None. None when \"is_temporal_kernel == True\". If list, each element of the list is a vector of knots locations for a specific dimension of the variable. \n",
    "\n",
    "* **knots_num**: integer or None. If integer, the number of equispaced knots over the x_cov range (for spatial variables) or the temporal kernel range (for temporal variable); knots_num must be smaller then the number of time points that for the filter.\n",
    "\n",
    "* **penalty_type**: 'der' for derivative based penalty matrix, or 'diff' for a difference based penalty matrix. see <a href='der-diff-pen'> above </a>.\n",
    "\n",
    "* **der**: int or None. None if 'diff' penalty is used. The order of the derivative used for the penalizatoin. Default is 2 for a smoother penalty.\n",
    "\n",
    "* **is_cyclic**: list of bool, \"is_cyclic$[i]$ = True\" if the i-th dimension of the task variable is cyclic\n",
    "\n",
    "* **lam**: float, initial smoothing controlling parameter $\\lambda$.\n",
    "\n",
    "* **samp_period**: float, the sampling period in seconds.\n",
    "\n",
    "* **trial_idx**: vector of length the number of samples containing the trial ids of each sample\n",
    "\n",
    "### 2.1.1 Spatial vs. temporal covariates <a name=\"spatial-temporal\">\n",
    "We label the covariates as \"spatial\" or \"temporal\" in order to specify two different type of response functions. In a neuroscience application, a \"spatial\" variable would be a traditional tuning function, where the X-axis defines a range of stimuli, for example position of an animal in an arena, or orientation of gratings. While a \"temporal\" variable would describe response to events, such as stimulus onset.  \n",
    "\n",
    "1. Response to **spatial variable** are instantaneous non-linear effects (the task variable $x_t$ immediately affects the rate in a non-linear way),\n",
    "    \\begin{align}\n",
    "    f(x_t) = \\sum_j\\beta_j b_j(x_t).\n",
    "    \\end{align}\n",
    "\n",
    "2. The response to a **temporal variable** is assumed to be the convolution of a kernel function (described in terms of B-spline) and the variable:\n",
    "\n",
    "    \\begin{align}\n",
    "    f(x_t) &= \\int_{-\\infty}^{\\infty} x(\\tau) h(t-\\tau) d \\tau \\\\\n",
    "    h(t) &= \\sum_j \\beta_j b_j(t)\n",
    "    \\end{align}\n",
    "\n",
    "    where $b_j$ are spline basis elements. This means that past and/or future values of the variable $x_t$ will affect the current firing rate with a linear contribution weighted by $h$.\n",
    "\n",
    "\n",
    "\n",
    "<!--Fitting a PGAM will entail learning the appropriate $\\mathbf{\\beta}$ coefficients characterizing the response function.\n",
    "\n",
    "Below we will create an example of three syntetic covariates (an event indicator, a 1D continous variable and a 2D continuous variable) for an experiment with 2 trials of 500 time points per trial.-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### 2.1.2 Temporal covariates <a name=\"tempcov\">\n",
    "\n",
    "Below we define and plot an acausal and the two directional filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a series of event marker\n",
    "tot_tp = 10**3\n",
    "\n",
    "# set some trial ids\n",
    "trial_ids = np.zeros(tot_tp)\n",
    "trial_ids[400:] = 1\n",
    "\n",
    "# sample some event marker at random\n",
    "event = np.zeros(tot_tp)\n",
    "event[[100, 200, 600, 900]] = 1\n",
    "\n",
    "# define the b-spline params\n",
    "kernel_h_legnth = 121 # duration of the kernel h(t) in time points \n",
    "num_int_knots = 12 # number of internal knots used to represent h\n",
    "order = 4\n",
    "dict_kernel = {0:'Acausal',1:'Direction %d'%1, -1:'Direction %d'%(-1)}\n",
    "\n",
    "\n",
    "for kernel_direction in [0,1,-1]:\n",
    "    # define the \"smooths_handler\" container\n",
    "    sm_handler = gdh.smooths_handler()\n",
    "    \n",
    "    # add the covariate & evaluate the convolution\n",
    "    sm_handler.add_smooth('this_event', \n",
    "                          [event], \n",
    "                          is_temporal_kernel=True, \n",
    "                          ord=order, \n",
    "                          knots_num=num_int_knots,\n",
    "                          trial_idx=trial_ids,\n",
    "                          kernel_length=kernel_h_legnth,\n",
    "                          kernel_direction=kernel_direction)\n",
    "\n",
    "    # sm_handler['varname'] process and stores the B-spline for the variable\n",
    "    # below we retrive the B-spline convolved with the \"event\" variable\n",
    "    convolved_ev = sm_handler['this_event'].X.toarray()\n",
    "    \n",
    "    # retrive the B-spline used for the convolution\n",
    "    basis = sm_handler['this_event'].basis_kernel.toarray()\n",
    "\n",
    "    # plot the basis & the convolved events\n",
    "    plt.figure(figsize=(8,3))\n",
    "    plt.suptitle('%s Filter'%dict_kernel[kernel_direction])\n",
    "    \n",
    "    # basis for the kenel h\n",
    "    plt.subplot(121)\n",
    "    plt.title('kernel basis')\n",
    "    tps = np.repeat(np.arange(kernel_h_legnth)-kernel_h_legnth//2, basis.shape[1]).reshape(basis.shape)\n",
    "    plt.plot(tps, basis)\n",
    "    plt.xlabel('time points')\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plt.title('convolved events')\n",
    "\n",
    "    # select a time point interval containing an event\n",
    "    idx0, idx1 = np.where(event == 1)[0][2] - 100, np.where(event == 1)[0][2] + 400\n",
    "\n",
    "    # extract the events convolved with each of the B-spline elements\n",
    "    conv = convolved_ev[idx0:idx1,:]\n",
    "\n",
    "    tps = np.arange(0,idx1-idx0) - 100\n",
    "    tps = np.repeat(tps,conv.shape[1]).reshape(conv.shape)\n",
    "    plt.plot(tps, conv)\n",
    "    plt.vlines(tps[0,0] + np.where(event[idx0:idx1])[0],0,1.5,'k',ls='--',label='event')\n",
    "    plt.xlabel('time points')\n",
    "    plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### 2.1.3 Spatial variable 1D and 2D\n",
    "\n",
    "Example \"spatial\" variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate three covariate\n",
    "x = np.random.normal(size=tot_tp)\n",
    "y = np.random.normal(size=tot_tp)\n",
    "z = np.random.normal(size=tot_tp)\n",
    "\n",
    "# add the 1d spatial variable\n",
    "int_knots = np.linspace(-2,2,10)\n",
    "order = 4\n",
    "knots = np.hstack(([int_knots[0]]*(order-1), int_knots, [int_knots[-1]]*(order-1)))\n",
    "\n",
    "# remove out of range values\n",
    "x[np.abs(x)>2] = np.nan\n",
    "y[np.abs(y)>2] = np.nan\n",
    "z[np.abs(z)>2] = np.nan\n",
    "\n",
    "# add the variable\n",
    "if 'spatial_1D' in sm_handler.smooths_var:\n",
    "    sm_handler.smooths_var.remove('spatial_1D')\n",
    "    sm_handler.smooths_dict.pop('spatial_1D')\n",
    "    \n",
    "sm_handler.add_smooth('spatial_1D', [x], \n",
    "                      knots=[knots], \n",
    "                      ord=order, \n",
    "                      is_temporal_kernel=False,\n",
    "                      trial_idx=trial_ids, \n",
    "                      is_cyclic=[False])\n",
    "\n",
    "\n",
    "# retrive the b-spline evaluated at x.\n",
    "X_1D = sm_handler['spatial_1D'].X.toarray()\n",
    "\n",
    "\n",
    "# sort for plotting\n",
    "plt.figure()\n",
    "plt.title('1D spatial response')\n",
    "idx_srt = np.argsort(x)\n",
    "X_srt = X_1D[idx_srt]\n",
    "p = plt.plot(X_srt)\n",
    "\n",
    "# add a 2D response with one cyclic variable and one acyclic\n",
    "if 'spatial_2D' in sm_handler.smooths_var:\n",
    "    sm_handler.smooths_var.remove('spatial_2D')\n",
    "    sm_handler.smooths_dict.pop('spatial_2D')\n",
    "\n",
    "sm_handler.add_smooth('spatial_2D', [y,z], knots=[knots, knots], ord=order, is_temporal_kernel=False,\n",
    "                     trial_idx=trial_ids, is_cyclic=[False,True])\n",
    "X_2D = sm_handler['spatial_2D'].X.toarray()\n",
    "\n",
    "\n",
    "# the size of basis set grows as n^m where n is the basis in the 1D case, and m is the number of dimensions\n",
    "print('Size of X_1D',X_1D.shape)\n",
    "print('Size of X_2D',X_2D.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## 2.2 Model fit <a name=\"model-fit\">\n",
    "Putting all the pieces together, here we will create a *smooths_handler* object containing the appropriate covariates, fit the model, evaluate the fit quality, and save the post-processed outputs in a standard numpy structured array or as a MATLAB structure.\n",
    "    \n",
    "The class **general_additive_model** is used for defining the GAM. It requires the following inputs:\n",
    "    \n",
    "* **sm_handler**: the smooths_handler object\n",
    "* **var_list**: list of variable names\n",
    "* **y**: the array with the spike counts (all trials must be stacked in a 1D array)\n",
    "* **link**: statsmoldels.genmod.families.links.link class which describe the link function (the library allows to fit any exponential family observation noise)\n",
    "\n",
    "\n",
    "You can fit the GAM with the method **general_additive_model.fit_full_and_reduced**, which fits a model with all the variables in **var_list**, then selects a minimal subset of variables that drive the neural activity by statistical testing, and re-fits the model with the significant variables only.\n",
    "    \n",
    "The inputs for **fit_full_and_reduced** requires are the following:\n",
    "\n",
    "* **var_list**: list with the subset of variables to be used for model fitting \n",
    "* **th_pval**: float between 0 and 1,the significance level for task variable inclusion\n",
    "* **max_iter**: int, max number of iteration of the optization routine\n",
    "* **use_dgcv**: True for learning the smoothing constants via dgcv\n",
    "* **trial_idx**: vector of length the number of samples containing the trial ids of each sample\n",
    "* **filter_trials**: vector of boolean, of the same length of *trial_idx*, indicates which time points should be used for training the model\n",
    "\n",
    "    \n",
    "In the following subsection we will provide an example where spike counts are generated according to the PGAM assumptions, and estimate the response function within the GAM framework.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "### 2.2.1 Generate synthetic data\n",
    "Below we generate a syntetic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## inputs parameters\n",
    "num_events = 6000\n",
    "time_points = 3 * 10 ** 5  # 30 mins at 0.006 ms resolution\n",
    "rate = 5. * 0.006  # Hz rate of the final kernel\n",
    "variance = 5.  # spatial input and nuisance variance\n",
    "int_knots_num = 20  # num of internal knots for the spline basis\n",
    "order = 4  # spline order\n",
    "\n",
    "## assume 200 trials\n",
    "trial_ids = np.repeat(np.arange(200),time_points//200)\n",
    "\n",
    "## create temporal input\n",
    "idx = np.random.choice(np.arange(time_points), num_events, replace=False)\n",
    "events = np.zeros(time_points)\n",
    "events[idx] = 1\n",
    "\n",
    "rv = sts.multivariate_normal(mean=[0, 0], cov= variance * np.eye(2))\n",
    "samp = rv.rvs(time_points)\n",
    "spatial_var = samp[:, 0]\n",
    "nuisance_var = samp[:, 1]\n",
    "\n",
    "# truncate X to avoid jumps in the resp function\n",
    "sele_idx = np.abs(spatial_var) < 5\n",
    "spatial_var = spatial_var[sele_idx]\n",
    "nuisance_var = nuisance_var[sele_idx]\n",
    "while spatial_var.shape[0] < time_points:\n",
    "    tmpX = rv.rvs(10 ** 4)\n",
    "    sele_idx = np.abs(tmpX[:, 0]) < 5\n",
    "    tmpX = tmpX[sele_idx, :]\n",
    "\n",
    "    spatial_var = np.hstack((spatial_var, tmpX[:, 0]))\n",
    "    nuisance_var = np.hstack((nuisance_var, tmpX[:, 1]))\n",
    "spatial_var = spatial_var[:time_points]\n",
    "nuisance_var = nuisance_var[:time_points]\n",
    "\n",
    "# create a resp function\n",
    "knots = np.hstack(([-5]*3, np.linspace(-5,5,8),[5]*3))\n",
    "beta = np.arange(10)\n",
    "beta = beta / np.linalg.norm(beta)\n",
    "beta = np.hstack((beta[5:], beta[:5][::-1]))\n",
    "resp_func = lambda x : np.dot(gdh.splineDesign(knots, x, order, der=0),beta)\n",
    "\n",
    "filter_used_conv = sts.gamma.pdf(np.linspace(0,20,100),a=2) - sts.gamma.pdf(np.linspace(0,20,100),a=5)\n",
    "filter_used_conv = np.hstack((np.zeros(101),filter_used_conv))*2\n",
    "# mean of the spike counts depending on spatial_var and events\n",
    "log_mu0 = resp_func(spatial_var)\n",
    "for tr in np.unique(trial_ids):\n",
    "    log_mu0[trial_ids == tr] = log_mu0[trial_ids == tr] + np.convolve(events[trial_ids == tr], filter_used_conv, mode='same')\n",
    "\n",
    "# adjust mean rate\n",
    "const = np.log(np.mean(np.exp(log_mu0)) / rate)\n",
    "log_mu0 = log_mu0 - const\n",
    "\n",
    "# generate spikes\n",
    "spk_counts = np.random.poisson(np.exp(log_mu0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the firing rate and the spike counts generated\n",
    "plt.figure(figsize=(8,3))\n",
    "plt.subplot(121)\n",
    "plt.plot(np.arange(1000) * 0.006, np.exp(log_mu0)[:1000]/0.006)\n",
    "plt.title('firing rate [Hz]', fontsize=16)\n",
    "plt.xlabel('time[sec]', fontsize=12)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(np.arange(1000) * 0.006, spk_counts[:1000])\n",
    "plt.title('6ms binned spike counts', fontsize=16)\n",
    "plt.xlabel('time[sec]', fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "### 2.2.2 Create the *smooths_handler* object and fit the model\n",
    "Below we create the smooths_handler object and run a fit. We include a \"nuisance\" spatial variable, that is not driving the neuron, the fit will learn to discard the variable;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Creating the class\n",
    "sm_handler = smooths_handler()\n",
    "# Creating the knots (notice the over-representation of edge knots)\n",
    "knots = np.hstack(([-5]*3, np.linspace(-5,5,15),[5]*3))\n",
    "# Using smooths_handler class to add variables \n",
    "sm_handler.add_smooth('spatial', [spatial_var], knots=[knots], ord=4, is_temporal_kernel=False,\n",
    "                     trial_idx=trial_ids, is_cyclic=[False],penalty_type='der', der=2)\n",
    "\n",
    "sm_handler.add_smooth('nuisance', [nuisance_var], knots=[knots], ord=4, is_temporal_kernel=False,\n",
    "                     trial_idx=trial_ids, is_cyclic=[False],penalty_type='der', der=2)\n",
    "\n",
    "sm_handler.add_smooth('temporal', [events], knots=None, ord=4, is_temporal_kernel=True,\n",
    "                     trial_idx=trial_ids, is_cyclic=[False],penalty_type='der', der=2,\n",
    "                     knots_num=10, kernel_length=500, kernel_direction=1)\n",
    "\n",
    "\n",
    "# split trial in train and eval\n",
    "train_trials = trial_ids % 10 != 0\n",
    "eval_trials = ~train_trials\n",
    "\n",
    "\n",
    "link = sm.genmod.families.links.log()\n",
    "poissFam = sm.genmod.families.family.Poisson(link=link)\n",
    "\n",
    "# create the pgam model\n",
    "pgam = general_additive_model(sm_handler,\n",
    "                              sm_handler.smooths_var, # list of covariate we want to include in the model\n",
    "                              spk_counts, # vector of spike counts\n",
    "                              poissFam # poisson family with exponential link from statsmodels.api\n",
    "                             )\n",
    "\n",
    "# with with all covariate, remove according to stat testing, and then refit\n",
    "full, reduced = pgam.fit_full_and_reduced(sm_handler.smooths_var, \n",
    "                                          th_pval=0.001,# pval for significance of covariate icluseioon\n",
    "                                          max_iter=10 ** 2, # max number of iteration\n",
    "                                          use_dgcv=True, # learn the smoothing penalties by dgcv\n",
    "                                          trial_num_vec=trial_ids,\n",
    "                                          filter_trials=train_trials)\n",
    "\n",
    "print('Minimal subset of variables driving the activity:')\n",
    "print(reduced.var_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "## 2.3 Post processing<a name=\"post-proc\">\n",
    "After a fit, it is possible to post-process the model fit output to obtain an easy to parse result in the form of a numpy.structarray. \n",
    "\n",
    "Each row will represent results for a specific input variable, additional information about the neuron (e.g. channel ID, electrode ID, or anything else) can be provided in the form of a dictionary, each dictionary value will be stored in the structured array with type \"object\".\n",
    "\n",
    "The output structure can be saved either as a \".npy\" via *numpy.save(\\<filename\\>)* or as a .mat (for MATLAB) via *scipy.io.savemat(\\<filename*\\>)*.\n",
    "\n",
    "Below is an example of the post-processing applied to the fit just obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# string with the neuron identifier\n",
    "neuron_id = 'neuron_000_session_1_monkey_001'\n",
    "# dictionary containing some information about the neuron, keys must be strings and values can be anything\n",
    "# since are stored with type object.\n",
    "info_save = {'x':100,\n",
    "             'y':801.2,\n",
    "             'z':301,\n",
    "             'brain_region': 'V1',\n",
    "             'subject':'monkey_001'\n",
    "            }\n",
    "\n",
    "# assume that we used 90% of the trials for training, 10% for evaluation\n",
    "res = postprocess_results(neuron_id, spk_counts, full, reduced, train_trials,\n",
    "                        sm_handler, poissFam, trial_ids, var_zscore_par=None,info_save=info_save,bins=100)\n",
    "\n",
    "# each row of res contains the info about a variable\n",
    "# some info are shared for all the variables (p-rsquared for example is a goodness of fit measure for the model\n",
    "# it is shared, not a property of the variable), while other, like the parameters of the b-splines, \n",
    "# are variable specific\n",
    "print('\\n\\n')\n",
    "print('Result structarray types\\n========================\\n')\n",
    "for name in res.dtype.names:\n",
    "    print('%s: \\t %s'%(name, type(res[name][0])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plot tuning functions\n",
    "plt.figure(figsize=(8,4))\n",
    "\n",
    "for k in range(3):\n",
    "    plt.subplot(2,3,k+1)\n",
    "    plt.title('log-space %s'%res['variable'][k])\n",
    "    x_kernel = res['x_kernel'][k]\n",
    "    \n",
    "    # changed from the original tutorial\n",
    "    x_kernel = x_kernel.reshape(-1)  # reshape for plotting\n",
    "    \n",
    "    \n",
    "    y_kernel = res['y_kernel'][k]\n",
    "    ypCI_kernel = res['y_kernel_pCI'][k]\n",
    "    ymCI_kernel = res['y_kernel_mCI'][k]\n",
    "    \n",
    "    plt.plot(x_kernel, y_kernel, color='r')\n",
    "    plt.fill_between(x_kernel, ymCI_kernel, ypCI_kernel, color='r', alpha=0.3)\n",
    "    \n",
    "    \n",
    "    \n",
    "    x_firing = res['x_rate_Hz'][k]\n",
    "    y_firing_model = res['model_rate_Hz'][k]\n",
    "    y_firing_raw = res['raw_rate_Hz'][k]\n",
    "    \n",
    "    plt.subplot(2,3,k+4)\n",
    "    plt.title('rate-space %s'%res['variable'][k])\n",
    "    \n",
    "    plt.plot(x_firing, y_firing_raw, color='k',label='raw')\n",
    "    plt.plot(x_firing, y_firing_model, color='r',label='model')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the output for further analysis\n",
    "#np.save('/notebooks/result_pgam.npy', res)\n",
    "#savemat('/notebooks/result_pgam.mat', mdict = {'result_pgam':res})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "# References <a name=\"refs\"></a>\n",
    "<a id=\"1\">[1]</a> \n",
    "<a href=\"https://proceedings.neurips.cc/paper/2020/hash/94d2a3c6dd19337f2511cdf8b4bf907e-Abstract.html\">\n",
    "Balzani, Edoardo , et al., \n",
    "\"Efficient estimation of neural tuning during naturalistic behavior.\"\n",
    "Advances in Neural Information Processing Systems 33 (2020): 12604-12614.<a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "bibtex_bibfiles": "references.bib",
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "pgam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "references.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
